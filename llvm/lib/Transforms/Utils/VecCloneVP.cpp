//=------ VecCloneVP.cpp - Vector function to loop transform -*- C++ -*------=//
//
// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
//===----------------------------------------------------------------------===//
///
/// \file
/// This pass inserts the body of a vector function inside a vector length
/// trip count scalar loop for functions that are declared SIMD. The pass
/// currently follows the gcc vector ABI requirements for name mangling
/// encodings, but will be extended in the future to also support the Intel
/// vector ABI. References to both ABIs can be found here:
///
/// https://sourceware.org/glibc/wiki/libmvec?action=AttachFile&do=view&target=VectorABI.txt
/// https://software.intel.com/sites/default/files/managed/b4/c8/Intel-Vector-Function-ABI.pdf
///
/// Conceptually, this pass performs the following transformation:
///
/// Before Translation:
///
/// main.cpp
///
/// #pragma omp declare simd uniform(a) linear(k)
/// extern float dowork(float *a, float b, int k);
///
/// float a[4096];
/// float b[4096];
/// int main() {
///   int k;
///   for (k = 0; k < 4096; k++) {
///     b[k] = k;
///   }
/// #pragma clang loop vectorize(enable)
///   for (k = 0; k < 4096; k++) {
///     a[k] = k * 0.5;
///     a[k] = dowork(a, b[k], k);
///   }
/// }
///
/// dowork.cpp
///
/// #pragma omp declare simd uniform(a) linear(k) #0
/// float dowork(float *a, float b, int k) {
///   return sinf(a[k]) + b;
/// }
///
/// attributes #0 = { nounwind uwtable "vector-variants"="_ZGVbM4uvl_",
/// "ZGVbN4uvl_", ... }
///
/// After Translation:
///
/// dowork.cpp
///
/// // Non-masked variant
///
/// <VL x float> "_ZGVbN4uvl_dowork(float *a, <VL x float> b, int k) {
///   alloc <VL x float> vec_ret;
///   alloc <VL x float> vec_b;
///   // casts from vector to scalar pointer allows loop to be in a scalar form
///   // that can be vectorized easily.
///   ret_cast = bitcast <VL x float>* vec_ret to float*;
///   vec_b_cast = bitcast <VL x float>* vec_b to float*;
///   store <VL x float> b, <VL x float>* vec_b;
///   for (int i = 0; i < VL; i++, k++) {
///     ret_cast[i] = sinf(a[k]) + vec_b_cast[i];
///   }
///   return vec_ret;
/// }
///
/// // Masked variant
///
/// <VL x float> "_ZGVbM4uvl_dowork(float *a, <VL x float> b, int k, <VL x int>
/// mask) {
///   alloc <VL x float> vec_ret;
///   alloc <VL x float> vec_b;
///   ret_cast = bitcast <VL x float>* vec_ret to float*;
///   vec_b_cast = bitcast <VL x float>* vec_b to float*;
///   store <VL x float> b, <VL x float>* vec_b;
///   for (int i = 0; i < VL; i++, k++) {
///     if (mask[i] != 0)
///       ret_cast[i] = sinf(a[k]) + vec_b_cast[i];
///   }
///   return vec_ret;
/// }
///
// ===--------------------------------------------------------------------=== //

// This pass is flexible enough to recognize whether or not parameters have been
// registerized so that the users of the parameter can be properly updated. For
// instance, we need to know where the users of linear parameters are so that
// the stride can be added to them.
//
// In the following example, %i and %x are used directly by %add directly, so
// in this case the pass can just look for users of %i and %x.
//
// define i32 @foo(i32 %i, i32 %x) #0 {
// entry:
//   %add = add nsw i32 %x, %i
//   ret i32 %add
// }
//
// When parameters have not been registerized, parameters are used indirectly
// through a store/load of the parameter to/from memory that has been allocated
// for them in the function. Thus, in this case, the pass looks for users of
// %0 and %1.
//
// define i32 @foo(i32 %i, i32 %x) #0 {
// entry:
// %i.addr = alloca i32, align 4
// %x.addr = alloca i32, align 4
// store i32 %i, i32* %i.addr, align 4
// store i32 %x, i32* %x.addr, align 4
// %0 = load i32, i32* %x.addr, align 4
// %1 = load i32, i32* %i.addr, align 4
// %add = add nsw i32 %0, %1
//  ret i32 %add
// }
//
// The pass must run at all optimization levels because it is possible that
// a loop calling the vector function is vectorized, but the vector function
// itself is not vectorized. For example, above main.cpp may be compiled at
// -O2, but dowork.cpp may be compiled at -O0. Therefore, it is required that
// the attribute list for the vector function specify all variants that must
// be generated by this pass so as to avoid any linking problems. This pass
// also serves to canonicalize the input IR to the loop vectorizer.

#include "llvm/Transforms/Utils/VecCloneVP.h"
#include "llvm/ADT/SmallVector.h"
#include "llvm/Analysis/Passes.h"
#include "llvm/Analysis/TargetTransformInfo.h"
#include "llvm/Analysis/VectorUtils.h"
#include "llvm/IR/BasicBlock.h"
#include "llvm/IR/Constants.h"
#include "llvm/IR/Function.h"
#include "llvm/IR/IRBuilder.h"
#include "llvm/IR/Instructions.h"
#include "llvm/IR/IntrinsicInst.h"
#include "llvm/IR/VectorBuilder.h"
#include "llvm/InitializePasses.h"
#include "llvm/PassRegistry.h"
#include "llvm/Support/Debug.h"
#include "llvm/Support/raw_ostream.h"
#include "llvm/Transforms/Utils/Cloning.h"
#include <map>
#include <set>

#define SV_NAME "vec-clone-vp"
#define DEBUG_TYPE "vec-clone-vp"

using namespace llvm;

VecCloneVP::VecCloneVP() : ModulePass(ID) {}

void VecCloneVP::getAnalysisUsage(AnalysisUsage &AU) const {
  AU.addRequired<AssumptionCacheTracker>();
  AU.addRequired<TargetTransformInfoWrapperPass>();
}

void VecCloneVPPass::getFunctionsToVectorize(llvm::Module &M,
                                             FunctionVariants &FuncVars) {

  // FuncVars will contain a one-to-many mapping between the original scalar
  // function and the vector variant encoding strings (represented as
  // attributes). The encodings correspond to functions that will be created by
  // the caller of this function as vector versions of the original function.
  // For example, if foo() is a function marked as a simd function, it will have
  // several vector variant encodings like: "_ZGVbM4_foo", "_ZGVbN4_foo",
  // "_ZGVcM8_foo", "_ZGVcN8_foo", "_ZGVdM8_foo", "_ZGVdN8_foo", "_ZGVeM16_foo",
  // "_ZGVeN16_foo". The caller of this function will then clone foo() and name
  // the clones using the above name manglings. The variant encodings correspond
  // to differences in masked/non-masked execution, vector length, and target
  // vector register size, etc. For more details, please refer to the vector
  // function abi references listed at the top of this file.

  for (auto &F : M.functions()) {
    auto AttrSet = F.getAttributes().getFnAttrs();
    // parse SIMD signatures
    for (const auto &Attr : AttrSet) {
      if (!Attr.isStringAttribute())
        continue;
      StringRef AttrText = Attr.getKindAsString();
      std::optional<VFInfo> VFInfo = VFABI::tryDemangleForVFABI(
          AttrText, M, /* RequireDeclaration */ false);
      if (!VFInfo) {
        LLVM_DEBUG(llvm::dbgs()
                   << "[VecCloneVP] Discarding |" << AttrText << "|\n");
        continue;
      }
      LLVM_DEBUG(llvm::dbgs()
                 << "[VecCloneVP] Considering |" << AttrText << "|\n");
      FuncVars[&F].push_back(*VFInfo);
    }
  }
}

template Constant *
VecCloneVPPass::getConstantValue<int>(Type *Ty, LLVMContext &Context, int Val);
template Constant *VecCloneVPPass::getConstantValue<float>(Type *Ty,
                                                           LLVMContext &Context,
                                                           float Val);
template Constant *
VecCloneVPPass::getConstantValue<double>(Type *Ty, LLVMContext &Context,
                                         double Val);
template <typename T>
Constant *VecCloneVPPass::getConstantValue(Type *Ty, LLVMContext &Context,
                                           T Val) {
  Constant *ConstVal = nullptr;
  if (Ty->isIntegerTy())
    ConstVal = ConstantInt::get(Ty, Val);
  else if (Ty->isFloatTy())
    ConstVal = ConstantFP::get(Ty, Val);

  assert(ConstVal && "Could not generate constant for type");
  return ConstVal;
}

static bool isMasked(const VFShape &Shape) {
  return std::find_if(Shape.Parameters.begin(), Shape.Parameters.end(),
                      [](const VFParameter &P) {
                        return P.ParamKind == VFParamKind::GlobalPredicate;
                      }) != Shape.Parameters.end();
}

static bool hasVL(const VFShape &Shape) {
  return std::find_if(Shape.Parameters.begin(), Shape.Parameters.end(),
                      [](const VFParameter &P) {
                        return P.ParamKind == VFParamKind::GlobalVL;
                      }) != Shape.Parameters.end();
}

Function *VecCloneVPPass::cloneFunction(Module &M, Function &F,
                                        const VFInfo &V) {
  std::string VariantName = V.VectorName;
  if (M.getFunction(VariantName))
    return nullptr;

  FunctionType *OrigFunctionType = F.getFunctionType();
  Type *ReturnType = F.getReturnType();

  // Expand return type to vector.
  if (!ReturnType->isVoidTy())
    ReturnType = VectorType::get(ReturnType, V.Shape.VF);

  const auto &ParmKinds = V.Shape.Parameters;
  SmallVector<Type *, 4> ParmTypes;
  const auto *VKIt = ParmKinds.begin();
  for (auto *ParamTy : OrigFunctionType->params()) {
    if (VKIt->ParamKind == VFParamKind::Vector)
      ParmTypes.push_back(
          VectorType::get(ParamTy->getScalarType(), V.Shape.VF));
    else
      ParmTypes.push_back(ParamTy);
    ++VKIt;
  }

  if (isMasked(V.Shape)) {
    Type *MaskVecTy =
        VectorType::get(Type::getInt1Ty(M.getContext()), V.Shape.VF);
    ParmTypes.push_back(MaskVecTy);
  }

  if (hasVL(V.Shape))
    ParmTypes.push_back(Type::getInt32Ty(M.getContext()));

  FunctionType *CloneFuncType = FunctionType::get(ReturnType, ParmTypes, false);
  Function *Clone = Function::Create(
      CloneFuncType, GlobalValue::ExternalLinkage, VariantName, F.getParent());

  ValueToValueMapTy Vmap;
  Function::arg_iterator NewArgIt = Clone->arg_begin();
  for (auto &Arg : F.args()) {
    NewArgIt->setName(Arg.getName());
    Vmap[&Arg] = &*NewArgIt;
    ++NewArgIt;
  }

  if (isMasked(V.Shape)) {
    Argument &MaskArg = *NewArgIt;
    MaskArg.setName("mask");
    ++NewArgIt;
  }

  if (hasVL(V.Shape)) {
    Argument &VLArg = *NewArgIt;
    VLArg.setName("vl");
  }

  SmallVector<ReturnInst *, 8> Returns;
  CloneFunctionInto(Clone, &F, Vmap, CloneFunctionChangeType::LocalChangesOnly,
                    Returns);

  // Remove incompatible argument attributes (applied to the scalar argument,
  // does not apply to its vector counterpart). This must be done after cloning
  // the function because CloneFunctionInto() transfers parameter attributes
  // from the original parameters in the Vmap.
  uint64_t Idx = 0;
  for (auto &Arg : Clone->args()) {
    Type *ArgType = Arg.getType();
    auto AM = AttributeFuncs::typeIncompatible(ArgType);
    Clone->removeParamAttrs(Idx, AM);
    ++Idx;
  }

  auto AM = AttributeFuncs::typeIncompatible(ReturnType);
  Clone->removeRetAttrs(AM);

  // Add zeroext attribute to VL operand.
  if (hasVL(V.Shape))
    Clone->addParamAttr(Clone->arg_size() - 1, Attribute::ZExt);

  LLVM_DEBUG(
      dbgs() << "[VecCloneVP] After Cloning and Function Signature widening\n");
  LLVM_DEBUG(Clone->dump());

  return Clone;
}

bool VecCloneVPPass::isSimpleFunction(Function *Clone, BasicBlock &EntryBlock) {
  // For really simple functions, there is no need to go through the process
  // of inserting a loop.

  // Example:
  //
  // void foo(void) {
  //   return;
  // }
  //
  // No need to insert a loop for this case since it's basically a no-op. Just
  // clone the function and return. It's possible that we could have some code
  // inside of a vector function that modifies global memory. Let that case go
  // through.
  ReturnInst *RetInst = dyn_cast<ReturnInst>(EntryBlock.getTerminator());
  if (RetInst && Clone->getReturnType()->isVoidTy())
    return true;

  return false;
}

void VecCloneVPPass::removeIncompatibleAttributes(Function *Clone) {
  for (auto &Arg : Clone->args()) {
    // For functions that only have a return instruction and are not void,
    // the return type is widened to vector. For this case, the returned
    // attribute becomes incompatible and must be removed.
    if (Clone->hasParamAttribute(Arg.getArgNo(), Attribute::Returned))
      Clone->removeParamAttr(Arg.getArgNo(), Attribute::Returned);
  }
}

void VecCloneVPPass::insertSplitForMaskedVariant(Function *Clone,
                                                 const DataLayout &DL,
                                                 BasicBlock *LoopBlock,
                                                 BasicBlock *LoopExitBlock,
                                                 AllocaInst *MaskAlloca,
                                                 PHINode *Phi) {
  BasicBlock *LoopThenBlock =
      LoopBlock->splitBasicBlock(LoopBlock->getFirstNonPHI(), "simd.loop.then");

  IRBuilder<> Builder(LoopBlock->getTerminator());
  auto *MaskElemType =
      cast<VectorType>(MaskAlloca->getAllocatedType())->getElementType();
  Value *MaskPtr = MaskAlloca;
  if (!MaskAlloca->getType()->isOpaquePointerTy()) {
    PointerType *MaskElemTypePtr =
        PointerType::get(MaskElemType, MaskAlloca->getType()->getAddressSpace());
    MaskPtr = Builder.CreateBitOrPointerCast(MaskAlloca, MaskElemTypePtr,
                                             MaskAlloca->getName() + ".cast");
  }
  auto *MaskGEP =
      Builder.CreateGEP(MaskElemType, MaskPtr, Phi, MaskPtr->getName() + ".gep");
  auto *MaskLoad = Builder.CreateAlignedLoad(
      MaskElemType, MaskGEP, DL.getPrefTypeAlign(MaskElemType), "mask.parm");
  Constant *Zero = ConstantInt::get(MaskElemType, 0);
  auto *MaskValue = Builder.CreateICmpNE(MaskLoad, Zero, "mask.value");
  Builder.CreateCondBr(MaskValue, LoopThenBlock, LoopExitBlock);
  LoopBlock->getTerminator()->eraseFromParent();

  LLVM_DEBUG(
      dbgs() << "[VecCloneVP] After Split Insertion For Masked Variant\n");
  LLVM_DEBUG(Clone->dump());
}

void VecCloneVPPass::addLoopMetadata(BasicBlock *Latch, ElementCount VF) {
  // This function sets the loop metadata for the new loop inserted around
  // the simd function body. This metadata includes disabling unrolling just
  // in case for some reason that unrolling occurs in between this pass and
  // the vectorizer. Also, the loop vectorization metadata is set to try
  // and force vectorization at the specified VF of the simd function.
  //
  // Set disable unroll metadata on the conditional branch of the loop latch
  // for the simd loop. The following is an example of what the loop latch
  // and Metadata will look like. The !llvm.loop marks the beginning of the
  // loop Metadata and is always placed on the terminator of the loop latch.
  // (i.e., simd.loop.exit in this case). According to LLVM documentation, to
  // properly set the loop Metadata, the 1st operand of !16 must be a self-
  // reference to avoid some type of Metadata merging conflicts that have
  // apparently arisen in the past. This is part of LLVM history that I do not
  // know. Also, according to LLVM documentation, any Metadata nodes referring
  // to themselves are marked as distinct. As such, all Metadata corresponding
  // to a loop belongs to that loop alone and no sharing of Metadata can be
  // done across different loops.
  //
  // simd.loop.exit:        ; preds = %simd.loop, %if.else, %if.then
  //  %indvar = add nuw i32 %index, 1
  //  %vl.cond = icmp ult i32 %indvar, 2
  //  br i1 %vl.cond, label %simd.loop, label %simd.end.region, !llvm.loop !16
  //
  // !16 = distinct !{!16, !17}
  // !17 = !{!"llvm.loop.unroll.disable"}

  SmallVector<Metadata *, 4> MDs;

  // Reserve first location for self reference to the LoopID metadata node.
  MDs.push_back(nullptr);

  // Add unroll(disable) metadata to disable future unrolling.
  LLVMContext &Context = Latch->getContext();
  MDs.push_back(
      MDNode::get(Context, MDString::get(Context, "llvm.loop.unroll.disable")));
  MDs.push_back(MDNode::get(
      Context, MDString::get(Context, "llvm.loop.vectorize.enable")));
  Metadata *Vals[] = {MDString::get(Context, "llvm.loop.vectorize.width"),
                      ConstantAsMetadata::get(ConstantInt::get(
                          Type::getInt32Ty(Context), VF.getKnownMinValue()))};
  MDs.push_back(MDNode::get(Context, Vals));
  if (VF.isScalable()) {
    MDs.push_back(MDNode::get(
        Context, {MDString::get(Context, "llvm.loop.vectorize.scalable.enable"),
                  ConstantAsMetadata::get(
                      ConstantInt::get(Type::getInt32Ty(Context), 2))}));
  }
  MDs.push_back(MDNode::get(
      Context, {MDString::get(Context, "llvm.loop.vectorize.predicate.enable"),
                ConstantAsMetadata::get(
                    ConstantInt::get(Type::getInt1Ty(Context), 1))}));
  // Add metadata for the loop-vectorize pass in order to change the latch
  // branching condition to always exit the loop, since only a single iteration
  // will ever happen; this way later passes will remove the loop.
  MDs.push_back(MDNode::get(
      Context, MDString::get(Context, "llvm.loop.single.iteration")));
  // Do not allow the scalar epilogue.
  MDs.push_back(MDNode::get(
      Context, MDString::get(Context, "llvm.loop.epilogue.forbid")));

  MDNode *NewLoopID = MDNode::get(Context, MDs);
  // Set operand 0 to refer to the loop id itself.
  NewLoopID->replaceOperandWith(0, NewLoopID);
  Latch->getTerminator()->setMetadata("llvm.loop", NewLoopID);
}

void VecCloneVPPass::widenAllocaInstructions(
    Function *Clone, DenseMap<AllocaInst *, Instruction *> &AllocaMap,
    BasicBlock &EntryBlock, const VFInfo &Variant, const DataLayout &DL,
    Value *Mask, Value *VL) {
  DenseMap<AllocaInst *, Instruction *>::iterator AllocaMapIt;
  SmallVector<StoreInst *, 4> StoresToRemove;

  for (auto &Arg : Clone->args()) {
    SmallVector<User *, 4> ArgUsers;
    for (auto *U : Arg.users()) {
      // Only update parameter users in the loop.
      if (Instruction *Inst = dyn_cast<Instruction>(U))
        if (Inst->getParent() != &EntryBlock)
          ArgUsers.push_back(U);
    }

    Type *ArgTy = Arg.getType();
    StringRef ArgName = Arg.getName();
    for (auto *U : ArgUsers) {
      // For non-optimized parameters, i.e., for parameters that are loads and
      // stores through memory (allocas), we need to know which alloca belongs
      // to which parameter. This can be done by finding the store of the
      // parameter to an alloca. Set up a map that maintains this relationship
      // so that we can update the users of the original allocas with the new
      // widened ones. When widening the allocas, vector parameters will be
      // stored to a vector alloca, and linear/uniform parameters will be
      // stored to an array, using the loop index as the "lane". Nothing else
      // needs to be done for optimized parameters. Later, this map will be
      // used to update all alloca users.
      if (StoreInst *StoreUser = dyn_cast<StoreInst>(U)) {
        if (AllocaInst *Alloca =
                dyn_cast<AllocaInst>(StoreUser->getPointerOperand())) {
          AllocaMapIt = AllocaMap.find(Alloca);
          if (AllocaMapIt == AllocaMap.end()) {
            IRBuilder<> Builder(EntryBlock.getTerminator());

            if (VectorType *VecArgType = dyn_cast<VectorType>(ArgTy)) {
              AllocaInst *VecAlloca = Builder.CreateAlloca(
                  VecArgType, DL.getAllocaAddrSpace(), nullptr,
                  "vec." + ArgName);

              if (VL) {
                auto VPBuilder = VectorBuilder(cast<IRBuilderBase>(Builder));
                if (Mask)
                  VPBuilder.setMask(Mask);
                else {
                  auto *AllOnes = ConstantInt::getAllOnesValue(
                      VectorType::get(Builder.getInt1Ty(), VecArgType));
                  VPBuilder.setMask(AllOnes);
                }
                VPBuilder.setEVL(VL);
                VPBuilder.createVectorInstruction(Instruction::Store, nullptr,
                                                  {&Arg, VecAlloca});
              } else
                Builder.CreateAlignedStore(
                    &Arg, VecAlloca, DL.getPrefTypeAlign(VecArgType), false);

              AllocaMap[Alloca] = VecAlloca;
              StoresToRemove.push_back(StoreUser);
            } else {
              llvm_unreachable(
                  "ArgTy is not a VectorType: this shouldn't happen");
              // FIXME - This won't fly at all.
              // ArrayType *ArrType =
              //     ArrayType::get(ArgTy, Variant.Shape.VF.getKnownMinValue());
              // AllocaInst *ArrAlloca =
              //     new AllocaInst(ArrType, DL.getAllocaAddrSpace(),
              //                    "arr." + ArgName, EntryBlock.getTerminator());
              // AllocaMap[Alloca] = ArrAlloca;
            }
          }
        }
      }
    }
  }

  // Remove the store of the parameter to the original alloca. A new one
  // was just created for the new alloca.
  for (auto *Store : StoresToRemove)
    Store->eraseFromParent();
}

void VecCloneVPPass::updateAllocaUsers(
    Function *Clone, PHINode *Phi,
    DenseMap<AllocaInst *, Instruction *> &AllocaMap) {

  SmallVector<Use *, 10> AllocaUses;
  for (auto Pair : AllocaMap) {
    AllocaInst *OldAlloca = Pair.first;
    for (auto &U : OldAlloca->uses()) {
      if (isa<Instruction>(U.getUser()))
        AllocaUses.push_back(&U);
    }
  }

  // Update all alloca users by doing an a -> &a[i] transformation. This
  // involves inserting a gep just before each use of the alloca. The only
  // exception is for vector stores to an alloca. These are moved to the
  // entry block of the function just after the widened alloca.
  for (auto *Use : AllocaUses) {
    User *User = Use->getUser();
    // If the User is a PHI node, we can't insert new instructions
    // before it; walk back to the predecessor BB from which the
    // operand comes, so that the value can be loaded there already.
    Instruction *InsertPoint = nullptr;
    if (auto *PHIInst = dyn_cast<PHINode>(User)) {
      BasicBlock *OriginBB = PHIInst->getIncomingBlock(*Use);
      InsertPoint = OriginBB->getTerminator();
    } else {
      InsertPoint = cast<Instruction>(User);
    }
    IRBuilder<> Builder(InsertPoint);
    for (unsigned K = 0; K < User->getNumOperands(); K++) {
      if (AllocaInst *OldAlloca = dyn_cast<AllocaInst>(User->getOperand(K))) {
        if (AllocaInst *NewAlloca =
                dyn_cast<AllocaInst>(AllocaMap[OldAlloca])) {
          // If this is an alloca for a linear/uniform parameter, then insert
          // a gep for the load/store and use the loop index to reference the
          // proper value for each "lane".
          SmallVector<Value *, 2> GepIndices;
          Constant *Idx0 =
              ConstantInt::get(Type::getInt32Ty(Clone->getContext()), 0);
          GepIndices.push_back(Idx0);
          GepIndices.push_back(Phi);
          auto *AllocaGep = Builder.CreateGEP(NewAlloca->getAllocatedType(),
                                              NewAlloca, GepIndices,
                                              NewAlloca->getName() + ".gep");
          User->setOperand(K, AllocaGep);
        } else if (BitCastInst *NewAllocaCast =
                       dyn_cast<BitCastInst>(AllocaMap[OldAlloca])) {
          SmallVector<Value *, 2> GepIndices;
          GepIndices.push_back(Phi);
          auto *AllocaCastGep =
              Builder.CreateGEP(OldAlloca->getAllocatedType(), NewAllocaCast,
                                GepIndices, NewAllocaCast->getName() + ".gep");
          User->setOperand(K, AllocaCastGep);
        } else {
          llvm_unreachable(
              "Expected array alloca for linear/uniform parameters or a "
              "cast of vector alloca for vector parameters");
        }
      }
    }
  }
}

void VecCloneVPPass::updateParameterUsers(Function *Clone,
                                          const VFInfo &Variant,
                                          BasicBlock &EntryBlock, PHINode *Phi,
                                          const DataLayout &DL, Value *Mask,
                                          Value *VL) {
  // Update non-alloca parameter users based on type of parameter. Any users of
  // the parameters that are also users of an alloca will not be updated again
  // here since this has already been done.
  const auto &ParmKinds = Variant.Shape.Parameters;
  DenseMap<Argument *, Instruction *> VecParmCasts;
  DenseMap<Argument *, Instruction *>::iterator VecParmCastsIt;

  for (auto &Arg : Clone->args()) {
    SmallVector<Use *, 4> ArgUses;
    for (auto &U : Arg.uses()) {
      // Only update parameter users in the loop.
      if (Instruction *Inst = dyn_cast<Instruction>(U.getUser()))
        if (Inst->getParent() != &EntryBlock)
          ArgUses.push_back(&U);
    }

    Type *ArgTy = Arg.getType();
    unsigned ArgNo = Arg.getArgNo();
    StringRef ArgName = Arg.getName();
    VectorType *VecArgType = dyn_cast<VectorType>(ArgTy);
    for (unsigned J = 0; J < ArgUses.size(); J++) {
      Use *Use = ArgUses[J];
      User *User = Use->getUser();
      Instruction *InsertPoint = nullptr;
      if (auto *PHIInst = dyn_cast<PHINode>(User)) {
        BasicBlock *OriginBB = PHIInst->getIncomingBlock(*Use);
        InsertPoint = OriginBB->getTerminator();
      } else {
        InsertPoint = cast<Instruction>(User);
      }
      IRBuilder<> Builder(InsertPoint);
      if (ParmKinds[ArgNo].ParamKind == VFParamKind::Vector) {
        VecParmCastsIt = VecParmCasts.find(&Arg);
        if (VecParmCastsIt == VecParmCasts.end()) {
          Builder.SetInsertPoint(EntryBlock.getTerminator());
          AllocaInst *VecAlloca = Builder.CreateAlloca(
              VecArgType, DL.getAllocaAddrSpace(), nullptr, "vec." + ArgName);

          if (VL) {
            auto VPBuilder = VectorBuilder(cast<IRBuilderBase>(Builder));
            if (Mask)
              VPBuilder.setMask(Mask);
            else {
              auto *AllOnes = ConstantInt::getAllOnesValue(
                  VectorType::get(Builder.getInt1Ty(), VecArgType));
              VPBuilder.setMask(AllOnes);
            }
            VPBuilder.setEVL(VL);
            VPBuilder.createVectorInstruction(Instruction::Store, nullptr,
                                              {&Arg, VecAlloca});
          } else
            Builder.CreateAlignedStore(&Arg, VecAlloca,
                                       DL.getPrefTypeAlign(VecArgType), false);

          VecParmCasts[&Arg] = VecAlloca;
          if (!VecAlloca->getType()->isOpaquePointerTy()) {
            PointerType *ElemTypePtr =
                PointerType::get(VecArgType->getElementType(),
                                 VecAlloca->getType()->getAddressSpace());
            Value *VecAllocaCast = Builder.CreateBitOrPointerCast(
                VecAlloca, ElemTypePtr, VecAlloca->getName() + ".cast");
            VecParmCasts[&Arg] = cast<Instruction>(VecAllocaCast);
          }
          Builder.SetInsertPoint(InsertPoint);
        }

        auto *VecAllocaCastGep =
            Builder.CreateGEP(VecArgType->getElementType(), VecParmCasts[&Arg],
                              Phi, VecParmCasts[&Arg]->getName() + ".gep");

        Value *ArgElemLoad =
            Builder.CreateAlignedLoad(VecArgType->getElementType(), VecAllocaCastGep,
                                      DL.getPrefTypeAlign(VecArgType->getElementType()),
                                      false, "vec." + ArgName + ".elem");

        User->setOperand(Use->getOperandNo(), ArgElemLoad);
      } else if (ParmKinds[ArgNo].ParamKind == VFParamKind::OMP_Linear) {
        int Stride = ParmKinds[ArgNo].LinearStepOrPos;
        Constant *StrideConst =
            ConstantInt::get(Type::getInt32Ty(Clone->getContext()), Stride);
        auto *Mul = Builder.CreateNSWMul(StrideConst, Phi, "stride.mul");

        Value *UserOp = nullptr;
        if (auto *ParmPtrType = dyn_cast<PointerType>(ArgTy)) {
          Type *PointeeType = nullptr;
          if (ParmPtrType->isOpaque())
            // NOTE: with opaque pointers, the LinearStepOrPos field yields the
            // value of the stride as number of bytes, which means we have to
            // use i8 as pointee type in the GEP.
            PointeeType = Type::getInt8Ty(Clone->getContext());
          else
            PointeeType = ParmPtrType->getNonOpaquePointerElementType();

          UserOp = Builder.CreateGEP(PointeeType, &Arg, Mul, ArgName + ".gep");
        } else {
          if (Mul->getType() != ArgTy)
            Mul = Builder.CreateSExtOrBitCast(Mul, ArgTy,
                                              Mul->getName() + ".cast");

          UserOp = Builder.CreateAdd(&Arg, Mul, "stride.add");
        }

        User->setOperand(Use->getOperandNo(), UserOp);
      }
    }
  }
}

bool VecCloneVPPass::runImpl(
    Module &M, Function &F, const VFInfo &Variant,
    std::function<AssumptionCache &(Function &F)> GetAC) {

  LLVM_DEBUG(dbgs() << "[VecCloneVP] Before SIMD Function Cloning\n");
  LLVM_DEBUG(F.dump());
  LLVM_DEBUG(dbgs() << "[VecCloneVP] Generating variant '" << Variant.VectorName
                    << "'\n");

  // Clone the original function.
  Function *Clone = cloneFunction(M, F, Variant);
  if (!Clone) {
    LLVM_DEBUG(dbgs() << "[VecCloneVP] Could not clone\n");
    return false;
  }

  // Remove any incompatible attributes that happen as part of widening
  // function vector parameters.
  removeIncompatibleAttributes(Clone);

  // Everything else beyond this point deals with function definitions,
  // so if we are dealing with a function declaration, we're done.
  if (F.isDeclaration())
    return true; // LLVM IR has been modified

  IRBuilder<> Builder(Clone->getContext());

  // Retrieve Mask and VL values.
  size_t NumArgs = Clone->arg_size() - 1;
  Value *VL = nullptr;
  if (hasVL(Variant.Shape))
    VL = Clone->getArg(NumArgs--);

  Value *Mask = nullptr;
  if (isMasked(Variant.Shape))
    Mask = Clone->getArg(NumArgs--);

  const DataLayout &DL = Clone->getParent()->getDataLayout();
  DenseMap<AllocaInst *, Instruction *> AllocaMap;

  BasicBlock &EntryBlock = Clone->getEntryBlock();
  if (isSimpleFunction(Clone, EntryBlock)) {
    LLVM_DEBUG(dbgs() << "[VecCloneVP] Function is too simple\n");
    return false;
  }

  // Split the entry block to create the one for the loop body.
  BasicBlock *LoopBlock =
      EntryBlock.splitBasicBlock(EntryBlock.begin(), "simd.loop");

  AllocaInst *MaskAlloca = nullptr;
  // For masked variants, zero extend the mask to the smallest possible integer
  // type before storing it.
  if (Mask) {
    auto *MaskType = cast<VectorType>(Mask->getType());
    VectorType *DestType = nullptr;
    switch (Variant.Shape.VF.getKnownMinValue()) {
    default:
      llvm_unreachable("Invalid VF value");
    case 1:
      DestType = VectorType::get(Builder.getInt64Ty(), MaskType);
      break;
    case 2:
      DestType = VectorType::get(Builder.getInt32Ty(), MaskType);
      break;
    case 4:
      DestType = VectorType::get(Builder.getInt16Ty(), MaskType);
      break;
    case 8:
    case 16:
    case 32:
    case 64:
      DestType = VectorType::get(Builder.getInt8Ty(), MaskType);
      break;
    }
    Builder.SetInsertPoint(EntryBlock.getFirstNonPHI());
    auto *ZExt = Builder.CreateZExt(Mask, DestType, "zext." + Mask->getName());
    MaskAlloca = Builder.CreateAlloca(DestType, DL.getAllocaAddrSpace(),
                                      nullptr, "vec." + Mask->getName());
    if (VL) {
      auto VPBuilder = VectorBuilder(cast<IRBuilderBase>(Builder));
      auto *AllOnes = ConstantInt::getAllOnesValue(
          VectorType::get(Builder.getInt1Ty(), DestType));
      VPBuilder.setMask(AllOnes);
      VPBuilder.setEVL(VL);
      VPBuilder.createVectorInstruction(Instruction::Store, nullptr,
                                        {ZExt, MaskAlloca});
    } else
      Builder.CreateAlignedStore(ZExt, MaskAlloca, DL.getPrefTypeAlign(DestType),
                                 false);
  }

  // On the split, the alloca instructions are moved into LoopBlock. Move
  // them back to the entry block.
  SmallVector<AllocaInst *, 4> Allocas;
  SmallVector<StoreInst *, 4> VecStores;
  BasicBlock::iterator BBIt = LoopBlock->begin();
  BasicBlock::iterator BBEnd = LoopBlock->end();
  for (; BBIt != BBEnd; ++BBIt)
    if (AllocaInst *Alloca = dyn_cast<AllocaInst>(&*BBIt))
      Allocas.push_back(Alloca);
  for (auto *Alloca : Allocas)
    Alloca->moveBefore(EntryBlock.getTerminator());

  widenAllocaInstructions(Clone, AllocaMap, EntryBlock, Variant, DL, Mask, VL);

  // Generate the phi for the loop index
  Builder.SetInsertPoint(LoopBlock->getFirstNonPHI());
  PHINode *Phi =
      Builder.CreatePHI(Type::getInt32Ty(Clone->getContext()), 2, "index");

  updateAllocaUsers(Clone, Phi, AllocaMap);
  updateParameterUsers(Clone, Variant, EntryBlock, Phi, DL, Mask, VL);

  // Remove old allocas
  for (auto Pair : AllocaMap) {
    AllocaInst *OldAlloca = Pair.first;
    OldAlloca->eraseFromParent();
  }

  // Find all the basic blocks containing a return instruction: we need to
  // know where to replace the return instructions with a store to the return
  // vector.
  SmallVector<ReturnInst *> OldReturns;
  for (BasicBlock &BB : *Clone)
    if (auto *RetInst = dyn_cast<ReturnInst>(BB.getTerminator()))
      OldReturns.push_back(RetInst);

  // Create the new return block that will contain the load of the return
  // vector and the new return instruction.
  BasicBlock *NewReturnBlock =
      BasicBlock::Create(Clone->getContext(), "return", Clone);
  Builder.SetInsertPoint(NewReturnBlock);
  Builder.CreateRetVoid();
  // Create the loop exit basic block.
  BasicBlock *LoopExitBlock = BasicBlock::Create(
      Clone->getContext(), "simd.loop.exit", Clone, NewReturnBlock);
  Builder.SetInsertPoint(LoopExitBlock);
  Builder.CreateBr(NewReturnBlock);

  // Create a vector alloca for the return. The return type of the clone
  // has already been widened, so the type can be used directly.
  AllocaInst *VecRetAlloca = nullptr;
  Type *VecRetTy = Clone->getReturnType();
  Builder.SetInsertPoint(EntryBlock.getTerminator());
  if (!VecRetTy->isVoidTy())
    VecRetAlloca = Builder.CreateAlloca(VecRetTy, DL.getAllocaAddrSpace(),
                                        nullptr, "vec.ret");

  // Store to the return vector the previously returned value, then
  // (unconditionally) jump to the LoopExitBlock.
  for (ReturnInst *RetInst : OldReturns) {
    assert(RetInst == RetInst->getParent()->getTerminator());
    Builder.SetInsertPoint(RetInst);
    Value *StoreVal = RetInst->getOperand(0);
    Type *StoreValTy = StoreVal->getType();
    if (!StoreValTy->isVoidTy()) {
      Value *MemPtr = VecRetAlloca;
      if (!VecRetAlloca->getType()->isOpaquePointerTy()) {
        // Cast it to a pointer to the type of the old return instruction.
        PointerType *StoreValPtrTy =
            PointerType::get(StoreValTy, DL.getAllocaAddrSpace());
        MemPtr = Builder.CreateBitOrPointerCast(VecRetAlloca, StoreValPtrTy,
                                                VecRetAlloca->getName() + ".cast");
      }
      auto *RetAllocaGep = Builder.CreateGEP(StoreValTy, MemPtr, Phi,
                                             MemPtr->getName() + ".gep");
      Builder.CreateAlignedStore(StoreVal, RetAllocaGep,
                                 DL.getPrefTypeAlign(StoreValTy),
                                 false);
    }

    Builder.CreateBr(LoopExitBlock);
  }

  if (MaskAlloca)
    insertSplitForMaskedVariant(Clone, DL, LoopBlock, LoopExitBlock,
                                MaskAlloca, Phi);

  // Check VL value, then either jump to the loop entry point or to the new
  // return block.
  Builder.SetInsertPoint(EntryBlock.getTerminator());
  Constant *Zero = ConstantInt::get(Type::getInt32Ty(Clone->getContext()), 0);
  Value *VLCheck = nullptr;
  if (VL)
    VLCheck = Builder.CreateICmpUGE(VL, Zero, "vl.check");
  else {
    auto *KMV = ConstantInt::get(Type::getInt32Ty(Clone->getContext()),
                                 Variant.Shape.VF.getKnownMinValue());
    VLCheck = Builder.CreateICmpUGE(Builder.CreateVScale(KMV, "vscale"), Zero,
                                    "vl.check");
  }
  Builder.CreateCondBr(VLCheck, LoopBlock, NewReturnBlock);
  EntryBlock.getTerminator()->eraseFromParent();

  // Generate the loop index increment and the loop exit condition, then jump
  // either back to the loop body or to the new return block.
  Builder.SetInsertPoint(LoopExitBlock->getTerminator());
  Constant *Increment =
      ConstantInt::get(Type::getInt32Ty(Clone->getContext()), 1);
  auto *Induction = Builder.CreateNSWAdd(Phi, Increment, "indvar");
  Value *ExitCond = nullptr;
  if (VL)
    ExitCond = Builder.CreateICmpEQ(Induction, VL, "exit.cond");
  else {
    auto *KMV = ConstantInt::get(Type::getInt32Ty(Clone->getContext()),
                                 Variant.Shape.VF.getKnownMinValue());
    ExitCond = Builder.CreateICmpEQ(
        Induction, Builder.CreateVScale(KMV, "vscale"), "exit.cond");
  }
  Builder.CreateCondBr(ExitCond, NewReturnBlock, LoopBlock);
  LoopExitBlock->getTerminator()->eraseFromParent();

  // Add incoming edges to the PHI node
  Phi->addIncoming(Zero, &EntryBlock);
  Phi->addIncoming(Induction, LoopExitBlock);

  // Generate the load from the return vector and the new return instruction
  // and put them in the new return basic block.
  Value *VecReturn = nullptr;
  Instruction *RetInst = NewReturnBlock->getTerminator();
  Builder.SetInsertPoint(RetInst);
  if (!VecRetTy->isVoidTy()) {
    if (VL) {
      auto VPBuilder = VectorBuilder(cast<IRBuilderBase>(Builder));
      if (Mask)
        VPBuilder.setMask(Mask);
      else {
        auto *AllOnes = ConstantInt::getAllOnesValue(
            VectorType::get(Builder.getInt1Ty(), cast<VectorType>(VecRetTy)));
        VPBuilder.setMask(AllOnes);
      }
      VPBuilder.setEVL(VL);
      VecReturn = VPBuilder.createVectorInstruction(Instruction::Load, VecRetTy,
                                                    {VecRetAlloca}, "vec.ret");
    } else
      VecReturn = Builder.CreateAlignedLoad(VecRetTy, VecRetAlloca,
                                            DL.getPrefTypeAlign(VecRetTy),
                                            "vec.ret");
  }
  Builder.CreateRet(VecReturn);
  RetInst->eraseFromParent();

  // Remove all old return instructions.
  for (ReturnInst *RetInst : OldReturns)
    RetInst->eraseFromParent();

  LLVM_DEBUG(dbgs() << "[VecCloneVP] After Loop Insertion\n");
  LLVM_DEBUG(Clone->dump());

  // Prevent unrolling from kicking in before loop vectorization and force
  // vectorization of the loop to the VF of the simd function.
  addLoopMetadata(LoopExitBlock, Variant.Shape.VF);

  // Add llvm.assume(vl <= vscale x k)
  if (VL) {
    Builder.SetInsertPoint(EntryBlock.getFirstNonPHI());
    auto *KMV = ConstantInt::get(Type::getInt32Ty(Clone->getContext()),
                                 Variant.Shape.VF.getKnownMinValue());
    Value *VScale = Builder.CreateVScale(KMV, "vscale");
    auto *AssumeCond = Builder.CreateICmpULE(VL, VScale, "assume.cond");
    auto *Assume = Builder.CreateAssumption(AssumeCond);
    assert(isa<AssumeInst>(Assume) &&
           "Created a llvm.assume that is not an assume instruction?");

    // Register new assumption
    auto &AC = GetAC(*Clone);
    AC.registerAssumption(cast<AssumeInst>(Assume));
  }

  LLVM_DEBUG(dbgs() << "[VecCloneVP] After SIMD Function Cloning\n");
  LLVM_DEBUG(Clone->dump());

  return true; // LLVM IR has been modified
}

bool VecCloneVP::runOnModule(Module &M) {
  auto GetAC = [this](Function &F) -> AssumptionCache & {
    return this->getAnalysis<AssumptionCacheTracker>().getAssumptionCache(F);
  };

  bool Changed = false;
  FunctionVariants FunctionsToVectorize;
  Impl.getFunctionsToVectorize(M, FunctionsToVectorize);
  for (auto Pair : FunctionsToVectorize) {
    Function &F = *(Pair.first);
    std::vector<VFInfo> Variants = Pair.second;
    // TargetTransformInfo *TTI =
    //   &getAnalysis<TargetTransformInfoWrapperPass>().getTTI(F);
    for (auto V : Variants) {
      Changed |= Impl.runImpl(M, F, V, GetAC);
    }
  }

  return Changed;
}

PreservedAnalyses VecCloneVPPass::run(Module &M, ModuleAnalysisManager &AM) {
  auto &FAM = AM.getResult<FunctionAnalysisManagerModuleProxy>(M).getManager();
  auto GetAC = [&FAM](Function &F) -> AssumptionCache & {
    return FAM.getResult<AssumptionAnalysis>(F);
  };

  bool Changed = false;
  FunctionVariants FunctionsToVectorize;
  getFunctionsToVectorize(M, FunctionsToVectorize);
  for (auto Pair : FunctionsToVectorize) {
    Function &F = *(Pair.first);
    std::vector<VFInfo> Variants = Pair.second;
    // TargetTransformInfo *TTI = &FAM.getResult<TargetIRAnalysis>(F);
    for (auto V : Variants) {
      Changed |= runImpl(M, F, V, GetAC);
    }
  }

  if (Changed)
    return PreservedAnalyses::none();
  return PreservedAnalyses::all();
}

void VecCloneVP::print(raw_ostream &OS, const Module *M) const {
  // TODO
}

ModulePass *llvm::createVecCloneVPPass() { return new llvm::VecCloneVP(); }

char VecCloneVP::ID = 0;

static const char LVNAME[] = "VecCloneVP";
INITIALIZE_PASS_BEGIN(VecCloneVP, SV_NAME, LVNAME, false /* modifies CFG */,
                      false /* transform pass */)
INITIALIZE_PASS_DEPENDENCY(AssumptionCacheTracker)
INITIALIZE_PASS_DEPENDENCY(TargetTransformInfoWrapperPass)
INITIALIZE_PASS_END(VecCloneVP, SV_NAME, LVNAME, false /* modififies CFG */,
                    false /* transform pass */)
