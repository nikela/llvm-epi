#!/usr/bin/env python2

import string

print """; NOTE: Tests autogenerated by utils/EPI/generate-intrinsics-tests.py
; RUN: llc -mtriple=riscv64 -mattr=+m,+f,+d,+a,+c,+experimental-v \\
; RUN:    -verify-machineinstrs --riscv-no-aliases < %s | FileCheck %s

@scratch = global i8 0, align 16
"""

MAX_VLMUL = 8
MAX_SEW = 64

class SingleType(object):
    def __init__(self, value_type, llvm_type, sew, is_mask_type, corr_integer_type = None):
        self.value_type = value_type
        self.llvm_type = llvm_type
        self.sew = sew
        self.is_mask_type = is_mask_type
        # Corresponding integer type (ie. integer type with same SEW)
        self.corr_integer_type = corr_integer_type

        if self.corr_integer_type is None:
            self.corr_integer_type = self

    def get_base_scale(self):
        if self.is_mask_type:
            return 1
        else:
            return MAX_SEW / self.sew

class IntrinsicType(object):
    def __init__(self, result, operands):
        self.result = result
        self.operands = operands

mask_types = [
    # Mask types do not scale according to a SEW, and thus it is set to -1
    SingleType("i1", "i1", -1, True),
]

def generate_unary_mask_types():
    assert(len(mask_types) == 1)
    yield IntrinsicType(mask_types[0], [mask_types[0]])

def generate_binary_mask_types():
    assert(len(mask_types) == 1)
    yield IntrinsicType(mask_types[0], [mask_types[0]] * 2)

integer_type_i8 = SingleType("i8", "i8", 8, False)
integer_type_i16 = SingleType("i16", "i16", 16, False)
integer_type_i32 = SingleType("i32", "i32", 32, False)
integer_type_i64 = SingleType("i64", "i64", 64, False)

integer_types = [
        integer_type_i8,
        integer_type_i16,
        integer_type_i32,
        integer_type_i64,
]

def generate_unary_mask_to_integer_types():
    for x in integer_types:
        assert(len(mask_types) == 1)
        yield IntrinsicType(x, [mask_types[0]])

def generate_binary_integer_types():
    for x in integer_types:
        yield IntrinsicType(x, [x] * 2)

def generate_binary_integer_types_mask_out():
    for x in integer_types:
        assert(len(mask_types) == 1)
        yield IntrinsicType(mask_types[0], [x] * 2)

def generate_binary_integer_types_widened():
    for i in range(0, len(integer_types) - 1):
        yield IntrinsicType(integer_types[i + 1], \
                [integer_types[i]]*2)

def generate_binary_integer_types_widened_rhs():
    for i in range(0, len(integer_types) - 1):
        yield IntrinsicType(integer_types[i + 1], \
                [integer_types[i + 1], integer_types[i]])

def generate_binary_integer_types_narrowed_lhs():
    for i in range(0, len(integer_types) - 1):
        yield IntrinsicType(integer_types[i], \
                [integer_types[i + 1], integer_types[i]])

def generate_ternary_integer_types():
    for x in integer_types:
        yield IntrinsicType(x, [x] * 3)

def generate_ternary_integer_types_widened():
    for i in range(0, len(integer_types) - 1):
        yield IntrinsicType(integer_types[i + 1], \
                [integer_types[i]] * 3)

float_type_f32 = SingleType("f32", "float", 32, False, integer_type_i32)
float_type_f64 = SingleType("f64", "double", 64, False, integer_type_i64)

float_types = [
        float_type_f32,
        float_type_f64,
]

def generate_unary_integer_types():
    for x in integer_types:
        yield IntrinsicType(x, [x])

def generate_unary_float_types():
    for x in float_types:
        yield IntrinsicType(x, [x])

def generate_unary_integer_float_types():
    for x in integer_types + float_types:
        yield IntrinsicType(x, [x])

def generate_unary_vfclass_types():
    for i in [integer_type_i32, integer_type_i64]:
        for x in float_types:
            yield IntrinsicType(i, [x])

def generate_binary_float_types():
    for x in float_types:
        yield IntrinsicType(x, [x, x])

def generate_binary_float_types_mask_out():
    for x in float_types:
        assert(len(mask_types) == 1)
        yield IntrinsicType(mask_types[0], [x, x])

def generate_binary_float_types_widened():
    for i in range(0, len(float_types) - 1):
        yield IntrinsicType(float_types[i + 1], \
                [float_types[i]]*2)

def generate_binary_float_types_widened_rhs():
    for i in range(0, len(float_types) - 1):
        yield IntrinsicType(float_types[i + 1], \
                [float_types[i + 1], float_types[i]])

def generate_binary_float_types_widened():
    for i in range(0, len(float_types) - 1):
        yield IntrinsicType(float_types[i + 1], \
                [float_types[i]]*2)

def generate_ternary_float_types():
    for x in float_types:
        yield IntrinsicType(x, [x]*3)

def generate_ternary_float_types_widened():
    for i in range(0, len(float_types) - 1):
        yield IntrinsicType(float_types[i + 1], \
                [float_types[i]]*3)

# Any
def generate_binary_any_types():
    for x in integer_types + float_types + mask_types:
        yield IntrinsicType(x, [x] * 2)

def generate_binary_any_and_mask_types():
    for x in integer_types + float_types:
        assert(len(mask_types) == 1)
        yield IntrinsicType(x, [x, mask_types[0]])

def generate_binary_any_and_integer_types():
    for x in integer_types + float_types:
        assert(len(mask_types) == 1)
        yield IntrinsicType(x, [x, x.corr_integer_type])

def generate_same_size_float_to_integer_types():
    yield IntrinsicType(integer_type_i32, [float_type_f32])
    yield IntrinsicType(integer_type_i64, [float_type_f64])

def generate_same_size_integer_to_float_types():
    yield IntrinsicType(float_type_f32, [integer_type_i32])
    yield IntrinsicType(float_type_f64, [integer_type_i64])

def generate_integer_to_widened_integer_types():
    for i in range(0, len(integer_types) - 1):
        yield IntrinsicType(integer_types[i + 1], \
                [integer_types[i]])

def generate_float_to_widened_float_types():
    yield IntrinsicType(float_type_f64, [float_type_f32])

def generate_integer_to_widened_float_types():
    yield IntrinsicType(float_type_f32, [integer_type_i16])
    yield IntrinsicType(float_type_f64, [integer_type_i32])

def generate_float_to_widened_integer_types():
    yield IntrinsicType(integer_type_i64, [float_type_f32])

def generate_integer_to_narrowed_integer_types():
    for i in range(0, len(integer_types) - 1):
        yield IntrinsicType(integer_types[i], \
                [integer_types[i + 1]])

def generate_float_to_narrowed_float_types():
    yield IntrinsicType(float_type_f32, [float_type_f64])

def generate_integer_to_narrowed_float_types():
    yield IntrinsicType(float_type_f32, [integer_type_i64])

def generate_float_to_narrowed_integer_types():
    yield IntrinsicType(integer_type_i32, [float_type_f64])
    yield IntrinsicType(integer_type_i16, [float_type_f32])

def generate_nullary_integer_types():
    for x in integer_types:
        yield IntrinsicType(x, [])

def generate_mask_to_int_types():
    yield IntrinsicType(integer_type_i64, [mask_types[0]])

################################################################################
################################################################################
################################################################################

class Intrinsic(object):
    def __init__(self, intr_name, type_generator, **extra_info):
        self.intr_name = intr_name
        self.type_generator = type_generator
        self.instruction = extra_info.get("instruction", self.intr_name)
        self.variants = extra_info["variants"]
        self.mask = extra_info.get("mask", True)
        self.vlmul_values = extra_info.get("vlmul_values", [1, 2, 4, 8])
        self.prepend_extra_ops = extra_info.get("prepend_extra_ops", None)

    def render(self):
        raise Exception("abstract method")

class NullaryIntrinsic(Intrinsic):
    pattern_v = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}(
  i64);

define void @intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} {{v[0-9]+}}
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}(
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    pattern_v_mask = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}(
  <vscale x ${result_type_scale} x ${llvm_result_type}>,
  <vscale x ${result_type_scale} x i1>,
  i64);

define void @intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} {{v[0-9]+}}, v0.t
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}(
    <vscale x ${result_type_scale} x ${llvm_result_type}> undef,
    <vscale x ${result_type_scale} x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    def __init__(self, intr_name, type_generator, **extra_info):
        super(NullaryIntrinsic, self).__init__(intr_name, type_generator, **extra_info)

    def get_template(self, variant):
        result = ""
        if variant in ["v"]:
            result = NullaryIntrinsic.pattern_v
            if self.mask:
                result += NullaryIntrinsic.pattern_v_mask
        else:
            raise Exception("Unhandled variant '{}'".format(variant))
        return string.Template(result)

    def render(self):
        for v in self.variants:
            template = self.get_template(v)

            op_subs = {}
            op_subs["intrinsic"] = self.intr_name
            op_subs["suffix"] = v
            for intrinsic_type in self.type_generator():
                result = intrinsic_type.result

                subs = op_subs.copy()
                subs["instruction"] = self.instruction
                subs["value_result_type"] = result.value_type
                subs["llvm_result_type"] = result.llvm_type

                sew = MAX_SEW
                if not result.is_mask_type:
                    sew = min(sew, result.sew)
                subs["sew"] = "e" + str(sew)

                for vlmul in self.vlmul_values:
                    # vlmul here is 'base' vlmul, vlmul for SEW operand
                    # (as opposed to 2*SEW operand)
                    subs["vlmul"] = "m" + str(vlmul)

                    result_scale = result.get_base_scale()
                    subs["result_type_scale"] = result_scale*vlmul

                    if result_scale*vlmul >= 64:
                        continue

                    print template.substitute(subs)

class UnaryIntrinsicMask(Intrinsic):
    pattern_m = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}(
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>,
  i64);

define void @intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} [[VR:v[0-9]+]], [[VR]]
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}(
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    pattern_m_mask = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask(
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>,
  <vscale x ${lhs_type_scale} x i1>,
  i64);

define void @intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} [[VR:v[0-9]+]], [[VR]], v0.t
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask(
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    <vscale x ${lhs_type_scale} x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""

    def __init__(self, intr_name, type_generator, **extra_info):
        super(UnaryIntrinsicMask, self).__init__(intr_name, type_generator, **extra_info)

    def get_template(self, variant):
        result = ""
        if variant in ["m"]:
            result = UnaryIntrinsicMask.pattern_m
            if self.mask:
                result += UnaryIntrinsicMask.pattern_m_mask
        else:
            raise Exception("Unhandled variant '{}'".format(variant))
        return string.Template(result)

    def render(self):
        for v in self.variants:
            template = self.get_template(v)

            op_subs = {}
            op_subs["intrinsic"] = self.intr_name
            op_subs["suffix"] = v
            for intrinsic_type in self.type_generator():
                result = intrinsic_type.result
                lhs = intrinsic_type.operands[0]

                subs = op_subs.copy()
                subs["instruction"] = self.instruction
                subs["value_result_type"] = result.value_type
                subs["llvm_result_type"] = result.llvm_type

                subs["llvm_lhs_type"] = lhs.llvm_type
                subs["value_lhs_type"] = lhs.value_type

                sew = MAX_SEW
                if not result.is_mask_type:
                    sew = min(sew, result.sew)
                if not lhs.is_mask_type:
                    sew = min(sew, lhs.sew)
                subs["sew"] = "e" + str(sew)

                for vlmul in self.vlmul_values:
                    # vlmul here is 'base' vlmul, vlmul for SEW operand
                    # (as opposed to 2*SEW operand)
                    subs["vlmul"] = "m" + str(vlmul)

                    # Ensure all operands have the same scale (ie. number of elements)
                    result_scale = result.get_base_scale()
                    lhs_scale = lhs.get_base_scale()
                    max_scale = max(result_scale, lhs_scale)

                    # Check legal VLMUL for non-mask types
                    if (not result.is_mask_type) and (result_scale != max_scale):
                        result_vlmul = vlmul*(max_scale/result_scale)
                        assert(result_vlmul <= MAX_VLMUL)

                    if (not lhs.is_mask_type) and (lhs_scale != max_scale):
                        lhs_vlmul = vlmul*(max_scale/lhs_scale)
                        assert(lhs_vlmul <= MAX_VLMUL)

                    # FIXME: nxv64T types not defined (ie. nxv64i8)
                    if max_scale*vlmul >= 64:
                        continue

                    subs["result_type_scale"] = max_scale*vlmul
                    subs["lhs_type_scale"] = max_scale*vlmul

                    print template.substitute(subs)

class UnaryIntrinsic(Intrinsic):
    pattern_v = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}(
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>,
  i64);

define void @intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}(
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    pattern_v_mask = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}(
  <vscale x ${result_type_scale} x ${llvm_result_type}>,
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>,
  <vscale x ${lhs_type_scale} x i1>,
  i64);

define void @intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}(
    <vscale x ${result_type_scale} x ${llvm_result_type}> undef,
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    <vscale x ${lhs_type_scale} x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    pattern_vload = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}(
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>*,
  i64);

define void @intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu${nontemporal}
; CHECK:       ${instruction}${eew}.${suffix} {{v[0-9]+}}, (${scalar_register})
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}(
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    pattern_vload_mask = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}(
  <vscale x ${result_type_scale} x ${llvm_result_type}>,
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>*,
  <vscale x ${lhs_type_scale} x i1>,
  i64);

define void @intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu${nontemporal}
; CHECK:       ${instruction}${eew}.${suffix} {{v[0-9]+}}, (${scalar_register}), v0.t
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}(
    <vscale x ${result_type_scale} x ${llvm_result_type}> undef,
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>* undef,
    <vscale x ${lhs_type_scale} x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    pattern_vload_strided = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}(
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>*,
  i64,
  i64);

define void @intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu${nontemporal}
; CHECK:       ${instruction}${eew}.${suffix} {{v[0-9]+}}, (${scalar_register}), ${scalar_register}
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}(
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    pattern_vload_strided_mask = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}(
  <vscale x ${result_type_scale} x ${llvm_result_type}>,
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>*,
  i64,
  <vscale x ${lhs_type_scale} x i1>,
  i64);

define void @intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu${nontemporal}
; CHECK:       ${instruction}${eew}.${suffix} {{v[0-9]+}}, (${scalar_register}), ${scalar_register}, v0.t
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}(
    <vscale x ${result_type_scale} x ${llvm_result_type}> undef,
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>* undef,
    i64 undef,
    <vscale x ${lhs_type_scale} x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    pattern_vload_indexed = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}(
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>*,
  <vscale x ${lhs_type_scale} x ${llvm_lhs_integer_type}>,
  i64);

define void @intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu${nontemporal}
; CHECK:       ${instruction}i${eew}.${suffix} {{v[0-9]+}}, (${scalar_register}), {{v[0-9]+}}
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}(
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>* undef,
    <vscale x ${lhs_type_scale} x ${llvm_lhs_integer_type}> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    pattern_vload_indexed_mask = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}(
  <vscale x ${result_type_scale} x ${llvm_result_type}>,
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>*,
  <vscale x ${lhs_type_scale} x ${llvm_lhs_integer_type}>,
  <vscale x ${lhs_type_scale} x i1>,
  i64);

define void @intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu${nontemporal}
; CHECK:       ${instruction}i${eew}.${suffix} {{v[0-9]+}}, (${scalar_register}), {{v[0-9]+}}, v0.t
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}(
    <vscale x ${result_type_scale} x ${llvm_result_type}> undef,
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>* undef,
    <vscale x ${lhs_type_scale} x ${llvm_lhs_integer_type}> undef,
    <vscale x ${lhs_type_scale} x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    pattern_vstore = """
declare void @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}(
  <vscale x ${result_type_scale} x ${llvm_result_type}>,
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>*,
  i64);

define void @intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu${nontemporal}
; CHECK:       ${instruction}${eew}.${suffix} {{v[0-9]+}}, (${scalar_register})
  call void @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}(
    <vscale x ${result_type_scale} x ${llvm_result_type}> undef,
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>* undef,
    i64 undef)

  ret void
}
"""
    pattern_vstore_mask = """
declare void @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}(
  <vscale x ${result_type_scale} x ${llvm_result_type}>,
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>*,
  <vscale x ${lhs_type_scale} x i1>,
  i64);

define void @intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu${nontemporal}
; CHECK:       ${instruction}${eew}.${suffix} {{v[0-9]+}}, (${scalar_register}), v0.t
  call void @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}(
    <vscale x ${result_type_scale} x ${llvm_result_type}> undef,
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>* undef,
    <vscale x ${lhs_type_scale} x i1> undef,
    i64 undef)

  ret void
}
"""
    pattern_vstore_strided = """
declare void @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}(
  <vscale x ${result_type_scale} x ${llvm_result_type}>,
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>*,
  i64,
  i64);

define void @intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu${nontemporal}
; CHECK:       ${instruction}${eew}.${suffix} {{v[0-9]+}}, (${scalar_register}), ${scalar_register}
  call void @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}(
    <vscale x ${result_type_scale} x ${llvm_result_type}> undef,
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>* undef,
    i64 undef,
    i64 undef)

  ret void
}
"""
    pattern_vstore_strided_mask = """
declare void @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}(
  <vscale x ${result_type_scale} x ${llvm_result_type}>,
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>*,
  i64,
  <vscale x ${lhs_type_scale} x i1>,
  i64);

define void @intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu${nontemporal}
; CHECK:       ${instruction}${eew}.${suffix} {{v[0-9]+}}, (${scalar_register}), ${scalar_register}, v0.t
  call void @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}(
    <vscale x ${result_type_scale} x ${llvm_result_type}> undef,
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>* undef,
    i64 undef,
    <vscale x ${lhs_type_scale} x i1> undef,
    i64 undef)

  ret void
}
"""
    pattern_vstore_indexed = """
declare void @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}(
  <vscale x ${result_type_scale} x ${llvm_result_type}>,
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>*,
  <vscale x ${lhs_type_scale} x ${llvm_lhs_integer_type}>,
  i64);

define void @intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu${nontemporal}
; CHECK:       ${instruction}i${eew}.${suffix} {{v[0-9]+}}, (${scalar_register}), {{v[0-9]+}}
  call void @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}(
    <vscale x ${result_type_scale} x ${llvm_result_type}> undef,
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>* undef,
    <vscale x ${lhs_type_scale} x ${llvm_lhs_integer_type}> undef,
    i64 undef)

  ret void
}
"""
    pattern_vstore_indexed_mask = """
declare void @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}(
  <vscale x ${result_type_scale} x ${llvm_result_type}>,
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>*,
  <vscale x ${lhs_type_scale} x ${llvm_lhs_integer_type}>,
  <vscale x ${lhs_type_scale} x i1>,
  i64);

define void @intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu${nontemporal}
; CHECK:       ${instruction}i${eew}.${suffix} {{v[0-9]+}}, (${scalar_register}), {{v[0-9]+}}, v0.t
  call void @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}(
    <vscale x ${result_type_scale} x ${llvm_result_type}> undef,
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>* undef,
    <vscale x ${lhs_type_scale} x ${llvm_lhs_integer_type}> undef,
    <vscale x ${lhs_type_scale} x i1> undef,
    i64 undef)

  ret void
}
"""
    def __init__(self, intr_name, type_generator, **extra_info):
        self.nontemporal = extra_info.get("nontemporal", False)
        super(UnaryIntrinsic, self).__init__(intr_name, type_generator, **extra_info)

    def get_template(self, variant):
        result = ""
        if variant in ["v", "m"]:
            result = UnaryIntrinsic.pattern_v
            if self.mask:
                result += UnaryIntrinsic.pattern_v_mask
        elif variant == "vload":
            result = UnaryIntrinsic.pattern_vload
            if self.mask:
                result += UnaryIntrinsic.pattern_vload_mask
        elif variant == "vload.strided":
            result = UnaryIntrinsic.pattern_vload_strided
            if self.mask:
                result += UnaryIntrinsic.pattern_vload_strided_mask
        elif variant == "vload.indexed":
            result = UnaryIntrinsic.pattern_vload_indexed
            if self.mask:
                result += UnaryIntrinsic.pattern_vload_indexed_mask
        elif variant == "vstore":
            result = UnaryIntrinsic.pattern_vstore
            if self.mask:
                result += UnaryIntrinsic.pattern_vstore_mask
        elif variant == "vstore.strided":
            result = UnaryIntrinsic.pattern_vstore_strided
            if self.mask:
                result += UnaryIntrinsic.pattern_vstore_strided_mask
        elif variant == "vstore.indexed":
            result = UnaryIntrinsic.pattern_vstore_indexed
            if self.mask:
                result += UnaryIntrinsic.pattern_vstore_indexed_mask
        else:
            raise Exception("Unhandled variant '{}'".format(variant))
        return string.Template(result)

    def render(self):
        for v in self.variants:
            template = self.get_template(v)

            op_subs = {}
            op_subs["intrinsic"] = self.intr_name
            op_subs["suffix"] = v
            if v in ["vload", "vload.strided", "vload.indexed", "vstore", "vstore.strided", "vstore.indexed"]:
                op_subs["suffix"] = "v"
                op_subs["scalar_register"] = "a0"
                op_subs["nontemporal"] = ", nt" if self.nontemporal else ""

            for intrinsic_type in self.type_generator():
                result = intrinsic_type.result
                lhs = intrinsic_type.operands[0]

                subs = op_subs.copy()
                subs["instruction"] = self.instruction
                subs["value_result_type"] = result.value_type
                subs["llvm_result_type"] = result.llvm_type

                if v in ["vload", "vload.strided", "vload.indexed", "vstore", "vstore.strided", "vstore.indexed"]:
                    subs["eew"] = result.corr_integer_type.value_type[1:]

                subs["llvm_lhs_type"] = lhs.llvm_type
                if v in ["vload.indexed", "vstore.indexed"]:
                    subs["llvm_lhs_integer_type"] = lhs.corr_integer_type.llvm_type
                subs["value_lhs_type"] = lhs.value_type

                sew = MAX_SEW
                if not result.is_mask_type:
                    sew = min(sew, result.sew)
                if not lhs.is_mask_type:
                    sew = min(sew, lhs.sew)
                subs["sew"] = "e" + str(sew)

                for vlmul in self.vlmul_values:
                    # vlmul here is 'base' vlmul, vlmul for SEW operand
                    # (as opposed to 2*SEW operand)
                    subs["vlmul"] = "m" + str(vlmul)

                    # Ensure all operands have the same scale (ie. number of elements)
                    result_scale = result.get_base_scale()
                    lhs_scale = lhs.get_base_scale()
                    max_scale = max(result_scale, lhs_scale)

                    # Check legal VLMUL for non-mask types
                    if (not result.is_mask_type) and (result_scale != max_scale):
                        result_vlmul = vlmul*(max_scale/result_scale)
                        assert(result_vlmul <= MAX_VLMUL)

                    if (not lhs.is_mask_type) and (lhs_scale != max_scale):
                        lhs_vlmul = vlmul*(max_scale/lhs_scale)
                        assert(lhs_vlmul <= MAX_VLMUL)

                    # FIXME: nxv64T types not defined (ie. nxv64i8)
                    if max_scale*vlmul >= 64:
                        continue

                    subs["result_type_scale"] = max_scale*vlmul
                    subs["lhs_type_scale"] = max_scale*vlmul

                    print template.substitute(subs)

class UnaryIntrinsicScalarResult(Intrinsic):
    pattern_v = """
declare ${llvm_result_type} @llvm.epi.${intrinsic}.${value_result_type}.nxv${lhs_type_scale}${value_lhs_type}(
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>,
  i64);

define void @intrinsic_${intrinsic}_${suffix}_${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_${suffix}_${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} ${scalar_register}, {{v[0-9]+}}
  %a = call ${llvm_result_type} @llvm.epi.${intrinsic}.${value_result_type}.nxv${lhs_type_scale}${value_lhs_type}(
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    i64 undef)

  %p = bitcast i8* @scratch to ${llvm_result_type}*
  store ${llvm_result_type} %a, ${llvm_result_type}* %p

  ret void
}
"""
    pattern_v_mask = """
declare ${llvm_result_type} @llvm.epi.${intrinsic}.mask.${value_result_type}.nxv${lhs_type_scale}${value_lhs_type}(
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>,
  <vscale x ${lhs_type_scale} x i1>,
  i64);

define void @intrinsic_${intrinsic}_mask_${suffix}_${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_mask_${suffix}_${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} ${scalar_register}, {{v[0-9]+}}, v0.t
  %a = call ${llvm_result_type} @llvm.epi.${intrinsic}.mask.${value_result_type}.nxv${lhs_type_scale}${value_lhs_type}(
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    <vscale x ${lhs_type_scale} x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to ${llvm_result_type}*
  store ${llvm_result_type} %a, ${llvm_result_type}* %p

  ret void
}
"""
    def __init__(self, intr_name, type_generator, **extra_info):
        self.scalar_register = extra_info["scalar_register"]
        del extra_info["scalar_register"]
        self.bits_per_mask = extra_info.get("bits_per_mask", False)
        super(UnaryIntrinsicScalarResult, self).__init__(intr_name, type_generator, **extra_info)

    def get_template(self, variant):
        result = ""
        if variant in ["m", "s"]:
            result = UnaryIntrinsicScalarResult.pattern_v
            if self.mask:
                result += UnaryIntrinsicScalarResult.pattern_v_mask
        else:
            raise Exception("Unhandled variant '{}'".format(variant))
        return string.Template(result)

    def render(self):
        for v in self.variants:
            template = self.get_template(v)

            op_subs = {}
            op_subs["intrinsic"] = self.intr_name
            op_subs["suffix"] = v
            op_subs["scalar_register"] = self.scalar_register
            for intrinsic_type in self.type_generator():
                result = intrinsic_type.result
                lhs = intrinsic_type.operands[0]

                subs = op_subs.copy()
                subs["instruction"] = self.instruction
                subs["value_result_type"] = result.value_type
                subs["llvm_result_type"] = result.llvm_type

                subs["llvm_lhs_type"] = lhs.llvm_type
                subs["value_lhs_type"] = lhs.value_type

                sew = MAX_SEW
                if not result.is_mask_type:
                    sew = min(sew, result.sew)
                if not lhs.is_mask_type:
                    sew = min(sew, lhs.sew)
                subs["sew"] = "e" + str(sew)

                for vlmul in self.vlmul_values:
                    # vlmul here is 'base' vlmul, vlmul for SEW operand
                    # (as opposed to 2*SEW operand)
                    if self.bits_per_mask:
                        # Overwrite SEW here
                        subs["sew"] = "e" + str(MAX_SEW / vlmul)
                        subs["vlmul"] = "m1"
                    else:
                        subs["vlmul"] = "m" + str(vlmul)

                    # Ensure all operands have the same scale (ie. number of elements)
                    result_scale = result.get_base_scale()
                    lhs_scale = lhs.get_base_scale()
                    max_scale = max(result_scale, lhs_scale)

                    # Check legal VLMUL for non-mask types
                    if (not result.is_mask_type) and (result_scale != max_scale):
                        result_vlmul = vlmul*(max_scale/result_scale)
                        assert(result_vlmul <= MAX_VLMUL)

                    if (not lhs.is_mask_type) and (lhs_scale != max_scale):
                        lhs_vlmul = vlmul*(max_scale/lhs_scale)
                        assert(lhs_vlmul <= MAX_VLMUL)

                    # FIXME: nxv64T types not defined (ie. nxv64i8)
                    if max_scale*vlmul >= 64:
                        continue

                    subs["result_type_scale"] = max_scale*vlmul
                    subs["lhs_type_scale"] = max_scale*vlmul

                    print template.substitute(subs)

class UnaryIntrinsicScalarResultNoVL(Intrinsic):
    pattern_v = """
declare ${llvm_result_type} @llvm.epi.${intrinsic}.${value_result_type}.nxv${lhs_type_scale}${value_lhs_type}(
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>);

define void @intrinsic_${intrinsic}_${suffix}_${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_${suffix}_${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli zero, zero, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} ${scalar_register}, {{v[0-9]+}}
  %a = call ${llvm_result_type} @llvm.epi.${intrinsic}.${value_result_type}.nxv${lhs_type_scale}${value_lhs_type}(
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef)

  %p = bitcast i8* @scratch to ${llvm_result_type}*
  store ${llvm_result_type} %a, ${llvm_result_type}* %p

  ret void
}
"""
    pattern_v_mask = """
declare ${llvm_result_type} @llvm.epi.${intrinsic}.mask.${value_result_type}.nxv${lhs_type_scale}${value_lhs_type}(
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>,
  <vscale x ${lhs_type_scale} x i1>,
  i64);

define void @intrinsic_${intrinsic}_mask_${suffix}_${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_mask_${suffix}_${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli zero, zero, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} ${scalar_register}, {{v[0-9]+}}, v0.t
  %a = call ${llvm_result_type} @llvm.epi.${intrinsic}.mask.${value_result_type}.nxv${lhs_type_scale}${value_lhs_type}(
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    <vscale x ${lhs_type_scale} x i1> undef)

  %p = bitcast i8* @scratch to ${llvm_result_type}*
  store ${llvm_result_type} %a, ${llvm_result_type}* %p

  ret void
}
"""
    def __init__(self, intr_name, type_generator, **extra_info):
        self.scalar_register = extra_info["scalar_register"]
        del extra_info["scalar_register"]
        self.bits_per_mask = extra_info.get("bits_per_mask", False)
        super(UnaryIntrinsicScalarResultNoVL, self).__init__(intr_name, type_generator, **extra_info)

    def get_template(self, variant):
        result = ""
        if variant in ["s"]:
            result = UnaryIntrinsicScalarResultNoVL.pattern_v
            if self.mask:
                result += UnaryIntrinsicScalarResultNoVL.pattern_v_mask
        else:
            raise Exception("Unhandled variant '{}'".format(variant))
        return string.Template(result)

    def render(self):
        for v in self.variants:
            template = self.get_template(v)

            op_subs = {}
            op_subs["intrinsic"] = self.intr_name
            op_subs["suffix"] = v
            op_subs["scalar_register"] = self.scalar_register
            for intrinsic_type in self.type_generator():
                result = intrinsic_type.result
                lhs = intrinsic_type.operands[0]

                subs = op_subs.copy()
                subs["instruction"] = self.instruction
                subs["value_result_type"] = result.value_type
                subs["llvm_result_type"] = result.llvm_type

                subs["llvm_lhs_type"] = lhs.llvm_type
                subs["value_lhs_type"] = lhs.value_type

                sew = MAX_SEW
                if not result.is_mask_type:
                    sew = min(sew, result.sew)
                if not lhs.is_mask_type:
                    sew = min(sew, lhs.sew)
                subs["sew"] = "e" + str(sew)

                for vlmul in self.vlmul_values:
                    # vlmul here is 'base' vlmul, vlmul for SEW operand
                    # (as opposed to 2*SEW operand)
                    if self.bits_per_mask:
                        # Overwrite SEW here
                        subs["sew"] = "e" + str(MAX_SEW / vlmul)
                        subs["vlmul"] = "m1"
                    else:
                        subs["vlmul"] = "m" + str(vlmul)

                    # Ensure all operands have the same scale (ie. number of elements)
                    result_scale = result.get_base_scale()
                    lhs_scale = lhs.get_base_scale()
                    max_scale = max(result_scale, lhs_scale)

                    # Check legal VLMUL for non-mask types
                    if (not result.is_mask_type) and (result_scale != max_scale):
                        result_vlmul = vlmul*(max_scale/result_scale)
                        assert(result_vlmul <= MAX_VLMUL)

                    if (not lhs.is_mask_type) and (lhs_scale != max_scale):
                        lhs_vlmul = vlmul*(max_scale/lhs_scale)
                        assert(lhs_vlmul <= MAX_VLMUL)

                    # FIXME: nxv64T types not defined (ie. nxv64i8)
                    if max_scale*vlmul >= 64:
                        continue

                    subs["result_type_scale"] = max_scale*vlmul
                    subs["lhs_type_scale"] = max_scale*vlmul

                    print template.substitute(subs)

class UnaryIntrinsicScalarInput(Intrinsic):
    pattern_v = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}.${value_lhs_type}(
  ${llvm_lhs_type},
  i64);

define void @intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} ${prepend_extra_ops}{{v[0-9]+}}, ${scalar_register}
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}.${value_lhs_type}(
    ${llvm_lhs_type} undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    pattern_v_mask = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}.${value_lhs_type}(
  <vscale x ${result_type_scale} x ${llvm_result_type}>,
  ${llvm_lhs_type},
  <vscale x ${result_type_scale} x i1>,
  i64);

define void @intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} ${prepend_extra_ops}{{v[0-9]+}}, ${scalar_register}, v0.t
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}.${value_lhs_type}(
    <vscale x ${result_type_scale} x ${llvm_result_type}> undef,
    ${llvm_lhs_type} undef,
    <vscale x ${result_type_scale} x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    def __init__(self, intr_name, type_generator, **extra_info):
        self.scalar_register = extra_info["scalar_register"]
        del extra_info["scalar_register"]
        super(UnaryIntrinsicScalarInput, self).__init__(intr_name, type_generator, **extra_info)

    def get_template(self, variant):
        result = ""
        if variant in ["x", "f"]:
            result = UnaryIntrinsicScalarInput.pattern_v
            if self.mask:
                result += UnaryIntrinsicScalarInput.pattern_v_mask
        else:
            raise Exception("Unhandled variant '{}'".format(variant))
        return string.Template(result)

    def render(self):
        for v in self.variants:
            template = self.get_template(v)

            op_subs = {}
            op_subs["intrinsic"] = self.intr_name
            op_subs["suffix"] = v
            op_subs["scalar_register"] = self.scalar_register
            if self.prepend_extra_ops is not None:
                self.prepend_extra_ops = self.prepend_extra_ops + ", "
                op_subs["prepend_extra_ops"] = self.prepend_extra_ops
            else:
                op_subs["prepend_extra_ops"] = ""
            for intrinsic_type in self.type_generator():
                result = intrinsic_type.result
                lhs = intrinsic_type.operands[0]

                subs = op_subs.copy()
                subs["instruction"] = self.instruction
                subs["value_result_type"] = result.value_type
                subs["llvm_result_type"] = result.llvm_type

                subs["llvm_lhs_type"] = lhs.llvm_type
                subs["value_lhs_type"] = lhs.value_type

                sew = MAX_SEW
                if not result.is_mask_type:
                    sew = min(sew, result.sew)
                if not lhs.is_mask_type:
                    sew = min(sew, lhs.sew)
                subs["sew"] = "e" + str(sew)

                for vlmul in self.vlmul_values:
                    # vlmul here is 'base' vlmul, vlmul for SEW operand
                    # (as opposed to 2*SEW operand)
                    subs["vlmul"] = "m" + str(vlmul)

                    # Ensure all operands have the same scale (ie. number of elements)
                    result_scale = result.get_base_scale()
                    lhs_scale = lhs.get_base_scale()
                    max_scale = max(result_scale, lhs_scale)

                    # Check legal VLMUL for non-mask types
                    if (not result.is_mask_type) and (result_scale != max_scale):
                        result_vlmul = vlmul*(max_scale/result_scale)
                        assert(result_vlmul <= MAX_VLMUL)

                    if (not lhs.is_mask_type) and (lhs_scale != max_scale):
                        lhs_vlmul = vlmul*(max_scale/lhs_scale)
                        assert(lhs_vlmul <= MAX_VLMUL)

                    # FIXME: nxv64T types not defined (ie. nxv64i8)
                    if max_scale*vlmul >= 64:
                        continue

                    subs["result_type_scale"] = max_scale*vlmul
                    subs["lhs_type_scale"] = max_scale*vlmul

                    print template.substitute(subs)

class UnaryIntrinsicScalarInputMerge(Intrinsic):
    pattern_v = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}.${value_lhs_type}(
  <vscale x ${result_type_scale} x ${llvm_result_type}>,
  ${llvm_lhs_type},
  i64);

define void @intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} ${prepend_extra_ops}{{v[0-9]+}}, ${scalar_register}
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}.${value_lhs_type}(
    <vscale x ${result_type_scale} x ${llvm_result_type}> undef,
    ${llvm_lhs_type} undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    def __init__(self, intr_name, type_generator, **extra_info):
        self.scalar_register = extra_info["scalar_register"]
        del extra_info["scalar_register"]
        super(UnaryIntrinsicScalarInputMerge, self).__init__(intr_name, type_generator, **extra_info)

    def get_template(self, variant):
        result = ""
        if variant in ["x", "f"]:
            result = UnaryIntrinsicScalarInputMerge.pattern_v
        else:
            raise Exception("Unhandled variant '{}'".format(variant))
        return string.Template(result)

    def render(self):
        for v in self.variants:
            template = self.get_template(v)

            op_subs = {}
            op_subs["intrinsic"] = self.intr_name
            op_subs["suffix"] = v
            op_subs["scalar_register"] = self.scalar_register
            if self.prepend_extra_ops is not None:
                self.prepend_extra_ops = self.prepend_extra_ops + ", "
                op_subs["prepend_extra_ops"] = self.prepend_extra_ops
            else:
                op_subs["prepend_extra_ops"] = ""
            for intrinsic_type in self.type_generator():
                result = intrinsic_type.result
                lhs = intrinsic_type.operands[0]

                subs = op_subs.copy()
                subs["instruction"] = self.instruction
                subs["value_result_type"] = result.value_type
                subs["llvm_result_type"] = result.llvm_type

                subs["llvm_lhs_type"] = lhs.llvm_type
                subs["value_lhs_type"] = lhs.value_type

                sew = min(MAX_SEW, lhs.sew)
                subs["sew"] = "e" + str(sew)

                for vlmul in self.vlmul_values:
                    # vlmul here is 'base' vlmul, vlmul for SEW operand
                    # (as opposed to 2*SEW operand)
                    subs["vlmul"] = "m" + str(vlmul)

                    # Ensure all operands have the same scale (ie. number of elements)
                    result_scale = result.get_base_scale()
                    lhs_scale = lhs.get_base_scale()
                    max_scale = max(result_scale, lhs_scale)

                    # Check legal VLMUL
                    if result_scale != max_scale:
                        result_vlmul = vlmul*(max_scale/result_scale)
                        assert(result_vlmul <= MAX_VLMUL)

                    if lhs_scale != max_scale:
                        lhs_vlmul = vlmul*(max_scale/lhs_scale)
                        assert(lhs_vlmul <= MAX_VLMUL)

                    # FIXME: nxv64T types not defined (ie. nxv64i8)
                    if max_scale*vlmul >= 64:
                        continue

                    subs["result_type_scale"] = max_scale*vlmul
                    subs["lhs_type_scale"] = max_scale*vlmul

                    print template.substitute(subs)

class Conversion(Intrinsic):
    pattern_v = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.${suffix}.nxv${result_type_scale}${value_result_type}.nxv${lhs_type_scale}${value_lhs_type}(
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>,
  i64);

define void @intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix}.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.${suffix}.nxv${result_type_scale}${value_result_type}.nxv${lhs_type_scale}${value_lhs_type}(
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    pattern_v_mask = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.${suffix}.mask.nxv${result_type_scale}${value_result_type}.nxv${lhs_type_scale}${value_lhs_type}(
  <vscale x ${lhs_type_scale} x ${llvm_result_type}>,
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>,
  <vscale x ${lhs_type_scale} x i1>,
  i64);

define void @intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix}.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.${suffix}.mask.nxv${result_type_scale}${value_result_type}.nxv${lhs_type_scale}${value_lhs_type}(
    <vscale x ${lhs_type_scale} x ${llvm_result_type}> undef,
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    <vscale x ${lhs_type_scale} x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    def __init__(self, intr_name, type_generator, **extra_info):
        super(Conversion, self).__init__(intr_name, type_generator, **extra_info)

    def get_template(self, variant):
        result = ""
        result += Conversion.pattern_v
        if self.mask:
            result += Conversion.pattern_v_mask
        return string.Template(result)

    def render(self):
        for v in self.variants:
            template = self.get_template(v)

            op_subs = {}
            op_subs["intrinsic"] = self.intr_name
            op_subs["suffix"] = v
            for intrinsic_type in self.type_generator():
                result = intrinsic_type.result
                lhs = intrinsic_type.operands[0]

                subs = op_subs.copy()
                subs["instruction"] = self.instruction
                subs["value_result_type"] = result.value_type
                subs["llvm_result_type"] = result.llvm_type

                subs["llvm_lhs_type"] = lhs.llvm_type
                subs["value_lhs_type"] = lhs.value_type

                sew = MAX_SEW
                if not result.is_mask_type:
                    sew = min(sew, result.sew)
                if not lhs.is_mask_type:
                    sew = min(sew, lhs.sew)
                subs["sew"] = "e" + str(sew)

                for vlmul in self.vlmul_values:
                    # vlmul here is 'base' vlmul, vlmul for SEW operand
                    # (as opposed to 2*SEW operand)
                    subs["vlmul"] = "m" + str(vlmul)

                    # Ensure all operands have the same scale (ie. number of elements)
                    result_scale = result.get_base_scale()
                    lhs_scale = lhs.get_base_scale()
                    max_scale = max(result_scale, lhs_scale)

                    # Check legal VLMUL for non-mask types
                    if (not result.is_mask_type) and (result_scale != max_scale):
                        result_vlmul = vlmul*(max_scale/result_scale)
                        assert(result_vlmul <= MAX_VLMUL)

                    if (not lhs.is_mask_type) and (lhs_scale != max_scale):
                        lhs_vlmul = vlmul*(max_scale/lhs_scale)
                        assert(lhs_vlmul <= MAX_VLMUL)

                    # FIXME: nxv64T types not defined (ie. nxv64i8)
                    if max_scale*vlmul >= 64:
                        continue

                    subs["result_type_scale"] = max_scale*vlmul
                    subs["lhs_type_scale"] = max_scale*vlmul

                    print template.substitute(subs)

class NarrowingConversion(Intrinsic):
    pattern_v = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.${suffix}.nxv${result_type_scale}${value_result_type}.nxv${lhs_type_scale}${value_lhs_type}(
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>,
  i64);

define void @intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix}.w {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.${suffix}.nxv${result_type_scale}${value_result_type}.nxv${lhs_type_scale}${value_lhs_type}(
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    pattern_v_mask = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.${suffix}.mask.nxv${result_type_scale}${value_result_type}.nxv${lhs_type_scale}${value_lhs_type}(
  <vscale x ${lhs_type_scale} x ${llvm_result_type}>,
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>,
  <vscale x ${lhs_type_scale} x i1>,
  i64);

define void @intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix}.w {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.${suffix}.mask.nxv${result_type_scale}${value_result_type}.nxv${lhs_type_scale}${value_lhs_type}(
    <vscale x ${lhs_type_scale} x ${llvm_result_type}> undef,
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    <vscale x ${lhs_type_scale} x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    def __init__(self, intr_name, type_generator, **extra_info):
        super(NarrowingConversion, self).__init__(intr_name, type_generator, **extra_info)

    def get_template(self, variant):
        result = ""
        result += NarrowingConversion.pattern_v
        if self.mask:
            result += NarrowingConversion.pattern_v_mask
        return string.Template(result)

    def render(self):
        for v in self.variants:
            template = self.get_template(v)

            op_subs = {}
            op_subs["intrinsic"] = self.intr_name
            op_subs["suffix"] = v
            for intrinsic_type in self.type_generator():
                result = intrinsic_type.result
                lhs = intrinsic_type.operands[0]

                subs = op_subs.copy()
                subs["instruction"] = self.instruction
                subs["value_result_type"] = result.value_type
                subs["llvm_result_type"] = result.llvm_type

                subs["llvm_lhs_type"] = lhs.llvm_type
                subs["value_lhs_type"] = lhs.value_type

                sew = MAX_SEW
                if not result.is_mask_type:
                    sew = min(sew, result.sew)
                if not lhs.is_mask_type:
                    sew = min(sew, lhs.sew)
                subs["sew"] = "e" + str(sew)

                for vlmul in self.vlmul_values:
                    # vlmul here is 'base' vlmul, vlmul for SEW operand
                    # (as opposed to 2*SEW operand)
                    subs["vlmul"] = "m" + str(vlmul)

                    # Ensure all operands have the same scale (ie. number of elements)
                    result_scale = result.get_base_scale()
                    lhs_scale = lhs.get_base_scale()
                    max_scale = max(result_scale, lhs_scale)

                    # Check legal VLMUL for non-mask types
                    if (not result.is_mask_type) and (result_scale != max_scale):
                        result_vlmul = vlmul*(max_scale/result_scale)
                        assert(result_vlmul <= MAX_VLMUL)

                    if (not lhs.is_mask_type) and (lhs_scale != max_scale):
                        lhs_vlmul = vlmul*(max_scale/lhs_scale)
                        assert(lhs_vlmul <= MAX_VLMUL)

                    # FIXME: nxv64T types not defined (ie. nxv64i8)
                    if max_scale*vlmul >= 64:
                        continue

                    subs["result_type_scale"] = max_scale*vlmul
                    subs["lhs_type_scale"] = max_scale*vlmul

                    print template.substitute(subs)

class BinaryIntrinsic(Intrinsic):
    pattern_vv = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}${vector_value_lhs_type}.nxv${rhs_type_scale}${value_rhs_type}(
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>,
  <vscale x ${rhs_type_scale} x ${llvm_rhs_type}>,
  i64);

define void @intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_nxv${rhs_type_scale}${value_rhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_nxv${rhs_type_scale}${value_rhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}${vector_value_lhs_type}.nxv${rhs_type_scale}${value_rhs_type}(
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    <vscale x ${rhs_type_scale} x ${llvm_rhs_type}> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    pattern_vv_mask = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}${vector_value_lhs_type}.nxv${rhs_type_scale}${value_rhs_type}(
  <vscale x ${result_type_scale} x ${llvm_result_type}>,
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>,
  <vscale x ${rhs_type_scale} x ${llvm_rhs_type}>,
  <vscale x ${lhs_type_scale} x i1>,
  i64);

define void @intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_nxv${rhs_type_scale}${value_rhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_nxv${rhs_type_scale}${value_rhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}${vector_value_lhs_type}.nxv${rhs_type_scale}${value_rhs_type}(
    <vscale x ${result_type_scale} x ${llvm_result_type}> undef,
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    <vscale x ${rhs_type_scale} x ${llvm_rhs_type}> undef,
    <vscale x ${lhs_type_scale} x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    pattern_vx = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}${vector_value_lhs_type}.${value_rhs_type}(
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>,
  ${llvm_rhs_type},
  i64);

define void @intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_${value_rhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_${value_rhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} {{v[0-9]+}}, {{v[0-9]+}}, ${scalar_register}
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}${vector_value_lhs_type}.${value_rhs_type}(
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    ${llvm_rhs_type} undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    pattern_vx_mask = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}${vector_value_lhs_type}.${value_rhs_type}(
  <vscale x ${result_type_scale} x ${llvm_result_type}>,
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>,
  ${llvm_rhs_type},
  <vscale x ${lhs_type_scale} x i1>,
  i64);

define void @intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_${value_rhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_${value_rhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} {{v[0-9]+}}, {{v[0-9]+}}, ${scalar_register}, v0.t
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}${vector_value_lhs_type}.${value_rhs_type}(
    <vscale x ${result_type_scale} x ${llvm_result_type}> undef,
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    ${llvm_rhs_type} undef,
    <vscale x ${lhs_type_scale} x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    pattern_vi = """
define void @intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_${value_rhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_${value_rhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}${vector_value_lhs_type}.${value_rhs_type}(
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    ${llvm_rhs_type} 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    pattern_vi_mask = """
define void @intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_${value_rhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_${value_rhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}${vector_value_lhs_type}.${value_rhs_type}(
    <vscale x ${result_type_scale} x ${llvm_result_type}> undef,
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    ${llvm_rhs_type} 9,
    <vscale x ${lhs_type_scale} x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""

    pattern_vm = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}${vector_value_lhs_type}.nxv${rhs_type_scale}${value_rhs_type}(
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>,
  <vscale x ${rhs_type_scale} x ${llvm_rhs_type}>,
  i64);

define void @intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_${value_rhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_${value_rhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}${vector_value_lhs_type}.${value_rhs_type}(
    <vscale x ${result_type_scale} x ${llvm_result_type}> undef,
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    ${llvm_rhs_type} 9,
    <vscale x ${lhs_type_scale} x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""

    def __init__(self, intr_name, type_generator, **extra_info):
        self.generates_mask = extra_info.get("generates_mask", False)
        self.is_widening = extra_info.get("widening", False)
        self.is_narrowing = extra_info.get("narrowing", False)
        super(BinaryIntrinsic, self).__init__(intr_name, type_generator, **extra_info)

    def get_template(self, variant):
        result = ""
        if variant in ["vv", "wv", "vm", "mm", "vs"]:
            result += BinaryIntrinsic.pattern_vv
            if self.mask:
                result += BinaryIntrinsic.pattern_vv_mask
        elif variant in ["vx", "vf", "wx", "wf"]:
            result += BinaryIntrinsic.pattern_vx
            if self.mask:
                result += BinaryIntrinsic.pattern_vx_mask
        elif variant in ["vi", "wi"]:
            result += BinaryIntrinsic.pattern_vi
            if self.mask:
                result += BinaryIntrinsic.pattern_vi_mask
        else:
            raise Exception("Unhandled variant '{}'".format(variant))
        return string.Template(result)

    def render(self):
        for v in self.variants:
            template = self.get_template(v)

            op_subs = {}
            op_subs["intrinsic"] = self.intr_name
            op_subs["suffix"] = v
            for intrinsic_type in self.type_generator():
                result = intrinsic_type.result
                lhs = intrinsic_type.operands[0]
                rhs = intrinsic_type.operands[1]

                subs = op_subs.copy()
                subs["instruction"] = self.instruction
                subs["value_result_type"] = result.value_type
                subs["llvm_result_type"] = result.llvm_type

                subs["llvm_lhs_type"] = lhs.llvm_type
                subs["llvm_rhs_type"] = rhs.llvm_type
                subs["value_lhs_type"] = lhs.value_type
                subs["value_rhs_type"] = rhs.value_type

                subs["vector_value_lhs_type"] = ""

                if v in ["vf", "wf"]:
                    subs["scalar_register"] = "ft0"
                elif v in ["vx", "wx", "vm", "mm"]:
                    subs["scalar_register"] = "a0"

                sew = MAX_SEW
                if not result.is_mask_type:
                    sew = min(sew, result.sew)
                if not lhs.is_mask_type:
                    sew = min(sew, lhs.sew)
                if not rhs.is_mask_type:
                    sew = min(sew, rhs.sew)
                subs["sew"] = "e" + str(sew)

                # Special check for mask operations
                mask_binary = result.is_mask_type and \
                        lhs.is_mask_type and \
                        rhs.is_mask_type

                for vlmul in self.vlmul_values:
                    if mask_binary:
                        subs["sew"] = "e{}".format(max(MAX_SEW / vlmul, 8))
                        subs["vlmul"] = "m1"
                    else:
                        # vlmul here is 'base' vlmul, vlmul for SEW operand
                        # (as opposed to 2*SEW operand)
                        subs["vlmul"] = "m" + str(vlmul)

                    # Ensure all operands have the same scale (ie. number of elements)
                    result_scale = result.get_base_scale()
                    lhs_scale = lhs.get_base_scale()
                    rhs_scale = rhs.get_base_scale()
                    max_scale = max(result_scale, lhs_scale, rhs_scale)

                    # Check legal VLMUL for non-mask types
                    if (not result.is_mask_type) and (result_scale != max_scale):
                        result_vlmul = vlmul*(max_scale/result_scale)
                        assert(result_vlmul <= MAX_VLMUL)

                    if (not lhs.is_mask_type) and (lhs_scale != max_scale):
                        lhs_vlmul = vlmul*(max_scale/lhs_scale)
                        assert(lhs_vlmul <= MAX_VLMUL)

                    if (not rhs.is_mask_type) and (rhs_scale != max_scale):
                        rhs_vlmul = vlmul*(max_scale/rhs_scale)
                        assert(rhs_vlmul <= MAX_VLMUL)

                    # FIXME: nxv64T types not defined (ie. nxv64i8)
                    if max_scale*vlmul >= 64:
                        continue

                    subs["result_type_scale"] = max_scale*vlmul
                    subs["lhs_type_scale"] = max_scale*vlmul
                    subs["rhs_type_scale"] = max_scale*vlmul

                    if self.is_widening or self.is_narrowing:
                        subs["vector_value_lhs_type"] = ".nxv{}{}".format(subs["lhs_type_scale"], subs["value_lhs_type"])

                    print template.substitute(subs)

class BinaryIntrinsicMaskIn(Intrinsic):
    pattern_vvm = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}.nxv${rhs_type_scale}${value_rhs_type}(
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>,
  <vscale x ${rhs_type_scale} x ${llvm_rhs_type}>,
  <vscale x ${lhs_type_scale} x i1>,
  i64);

define void @intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_nxv${rhs_type_scale}${value_rhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_nxv${rhs_type_scale}${value_rhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}.nxv${rhs_type_scale}${value_rhs_type}(
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    <vscale x ${rhs_type_scale} x ${llvm_rhs_type}> undef,
    <vscale x ${lhs_type_scale} x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    pattern_vxm = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}.${value_rhs_type}(
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>,
  ${llvm_rhs_type},
  <vscale x ${lhs_type_scale} x i1>,
  i64);

define void @intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_${value_rhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_${value_rhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} {{v[0-9]+}}, {{v[0-9]+}}, ${scalar_register}, v0
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}.${value_rhs_type}(
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    ${llvm_rhs_type} undef,
    <vscale x ${lhs_type_scale} x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    pattern_vim = """
define void @intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_${value_rhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_${value_rhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}.${value_rhs_type}(
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    ${llvm_rhs_type} 9,
    <vscale x ${lhs_type_scale} x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""

    def __init__(self, intr_name, type_generator, **extra_info):
        self.generates_mask = extra_info.get("generates_mask", False)
        super(BinaryIntrinsicMaskIn, self).__init__(intr_name, type_generator, **extra_info)

    def get_template(self, variant):
        result = ""
        if variant == "vvm":
            result += BinaryIntrinsicMaskIn.pattern_vvm
        elif variant in ["vxm", "vfm"]:
            result += BinaryIntrinsicMaskIn.pattern_vxm
        elif variant == "vim":
            result += BinaryIntrinsicMaskIn.pattern_vim
        else:
            raise Exception("Unhandled variant '{}'".format(variant))
        return string.Template(result)

    def render(self):
        for v in self.variants:
            template = self.get_template(v)

            op_subs = {}
            op_subs["intrinsic"] = self.intr_name
            op_subs["suffix"] = v
            for intrinsic_type in self.type_generator():
                result = intrinsic_type.result
                lhs = intrinsic_type.operands[0]
                rhs = intrinsic_type.operands[1]

                subs = op_subs.copy()
                subs["instruction"] = self.instruction
                subs["value_result_type"] = result.value_type
                subs["llvm_result_type"] = result.llvm_type

                subs["llvm_lhs_type"] = lhs.llvm_type
                subs["llvm_rhs_type"] = rhs.llvm_type
                subs["value_lhs_type"] = lhs.value_type
                subs["value_rhs_type"] = rhs.value_type

                if v == "vxm":
                    subs["scalar_register"] = "a0"
                elif v == "vfm":
                    subs["scalar_register"] = "ft0"

                sew = MAX_SEW
                if not result.is_mask_type:
                    sew = min(sew, result.sew)
                if not lhs.is_mask_type:
                    sew = min(sew, lhs.sew)
                if not rhs.is_mask_type:
                    sew = min(sew, rhs.sew)
                subs["sew"] = "e" + str(sew)

                for vlmul in self.vlmul_values:
                    # vlmul here is 'base' vlmul, vlmul for SEW operand
                    # (as opposed to 2*SEW operand)
                    subs["vlmul"] = "m" + str(vlmul)

                    # Ensure all operands have the same scale (ie. number of elements)
                    result_scale = result.get_base_scale()
                    lhs_scale = lhs.get_base_scale()
                    rhs_scale = rhs.get_base_scale()
                    max_scale = max(result_scale, lhs_scale, rhs_scale)

                    # Check legal VLMUL for non-mask types
                    if (not result.is_mask_type) and (result_scale != max_scale):
                        result_vlmul = vlmul*(max_scale/result_scale)
                        assert(result_vlmul <= MAX_VLMUL)

                    if (not lhs.is_mask_type) and (lhs_scale != max_scale):
                        lhs_vlmul = vlmul*(max_scale/lhs_scale)
                        assert(lhs_vlmul <= MAX_VLMUL)

                    if (not rhs.is_mask_type) and (rhs_scale != max_scale):
                        rhs_vlmul = vlmul*(max_scale/rhs_scale)
                        assert(rhs_vlmul <= MAX_VLMUL)

                    # FIXME: nxv64T types not defined (ie. nxv64i8)
                    if max_scale*vlmul >= 64:
                        continue

                    subs["result_type_scale"] = max_scale*vlmul
                    subs["lhs_type_scale"] = max_scale*vlmul
                    subs["rhs_type_scale"] = max_scale*vlmul

                    print template.substitute(subs)

class TernaryIntrinsic(Intrinsic):
    pattern_vv = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}.nxv${rhs_type_scale}${value_rhs_type}(
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>,
  <vscale x ${rhs_type_scale} x ${llvm_rhs_type}>,
  <vscale x ${result_type_scale} x ${llvm_result_type}>,
  i64);

define void @intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_nxv${rhs_type_scale}${value_rhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_nxv${rhs_type_scale}${value_rhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}.nxv${rhs_type_scale}${value_rhs_type}(
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    <vscale x ${rhs_type_scale} x ${llvm_rhs_type}> undef,
    <vscale x ${result_type_scale} x ${llvm_result_type}> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    pattern_vv_mask = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}.nxv${rhs_type_scale}${value_rhs_type}(
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>,
  <vscale x ${rhs_type_scale} x ${llvm_rhs_type}>,
  <vscale x ${result_type_scale} x ${llvm_result_type}>,
  <vscale x ${lhs_type_scale} x i1>,
  i64);

define void @intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_nxv${rhs_type_scale}${value_rhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_nxv${rhs_type_scale}${value_rhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}.nxv${rhs_type_scale}${value_rhs_type}(
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    <vscale x ${rhs_type_scale} x ${llvm_rhs_type}> undef,
    <vscale x ${result_type_scale} x ${llvm_result_type}> undef,
    <vscale x ${lhs_type_scale} x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    pattern_vx = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}.${value_rhs_type}(
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>,
  ${llvm_rhs_type},
  <vscale x ${result_type_scale} x ${llvm_result_type}>,
  i64);

define void @intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_${value_rhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_${value_rhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} [[VR:v[0-9]+]], ${scalar_register}, [[VR]]
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.nxv${result_type_scale}${value_result_type}.${value_rhs_type}(
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    ${llvm_rhs_type} undef,
    <vscale x ${result_type_scale} x ${llvm_result_type}> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    pattern_vx_mask = """
declare <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}.${value_rhs_type}(
  <vscale x ${lhs_type_scale} x ${llvm_lhs_type}>,
  ${llvm_rhs_type},
  <vscale x ${result_type_scale} x ${llvm_result_type}>,
  <vscale x ${lhs_type_scale} x i1>,
  i64);

define void @intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_${value_rhs_type}() nounwind {
entry:
; CHECK-LABEL: intrinsic_${intrinsic}_mask_${suffix}_nxv${result_type_scale}${value_result_type}_nxv${lhs_type_scale}${value_lhs_type}_${value_rhs_type}
; CHECK:       vsetvli {{.*}}, a0, ${sew}, ${vlmul}, ta, mu
; CHECK:       ${instruction}.${suffix} [[VR:v[0-9]+]], ${scalar_register}, [[VR]], v0.t
  %a = call <vscale x ${result_type_scale} x ${llvm_result_type}> @llvm.epi.${intrinsic}.mask.nxv${result_type_scale}${value_result_type}.${value_rhs_type}(
    <vscale x ${lhs_type_scale} x ${llvm_lhs_type}> undef,
    ${llvm_rhs_type} undef,
    <vscale x ${result_type_scale} x ${llvm_result_type}> undef,
    <vscale x ${lhs_type_scale} x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x ${result_type_scale} x ${llvm_result_type}>*
  store <vscale x ${result_type_scale} x ${llvm_result_type}> %a, <vscale x ${result_type_scale} x ${llvm_result_type}>* %p

  ret void
}
"""
    def __init__(self, intr_name, type_generator, **extra_info):
        super(TernaryIntrinsic, self).__init__(intr_name, type_generator, **extra_info)

    def get_template(self, variant):
        result = ""
        if variant in ["vv"]:
            result = TernaryIntrinsic.pattern_vv
            if self.mask:
                result += TernaryIntrinsic.pattern_vv_mask
        elif variant in ["vx", "vf"]:
            result = TernaryIntrinsic.pattern_vx
            if self.mask:
                result += TernaryIntrinsic.pattern_vx_mask
        else:
            raise Exception("Unhandled variant '{}' for intrinsic '{}'".format(variant, intr.intr_name))
        return string.Template(result)

    def render(self):
        for v in self.variants:
            template = self.get_template(v)

            op_subs = {}
            op_subs["intrinsic"] = self.intr_name
            op_subs["suffix"] = v
            for intrinsic_type in self.type_generator():
                result = intrinsic_type.result
                lhs = intrinsic_type.operands[0]
                rhs = intrinsic_type.operands[1]

                subs = op_subs.copy()
                subs["instruction"] = self.instruction
                subs["value_result_type"] = result.value_type
                subs["llvm_result_type"] = result.llvm_type

                subs["llvm_lhs_type"] = lhs.llvm_type
                subs["llvm_rhs_type"] = rhs.llvm_type
                subs["value_lhs_type"] = lhs.value_type
                subs["value_rhs_type"] = rhs.value_type

                if v in ["vf", "wf"]:
                    subs["scalar_register"] = "ft0"
                elif v in ["vx", "wx"]:
                    subs["scalar_register"] = "a0"

                sew = MAX_SEW
                if not result.is_mask_type:
                    sew = min(sew, result.sew)
                if not lhs.is_mask_type:
                    sew = min(sew, lhs.sew)
                if not rhs.is_mask_type:
                    sew = min(sew, rhs.sew)
                subs["sew"] = "e" + str(sew)

                for vlmul in self.vlmul_values:
                    # vlmul here is 'base' vlmul, vlmul for SEW operand
                    # (as opposed to 2*SEW operand)
                    subs["vlmul"] = "m" + str(vlmul)

                    # Ensure all operands have the same scale (ie. number of elements)
                    result_scale = result.get_base_scale()
                    lhs_scale = lhs.get_base_scale()
                    rhs_scale = rhs.get_base_scale()
                    max_scale = max(result_scale, lhs_scale, rhs_scale)

                    # Check legal VLMUL for non-mask types
                    if (not result.is_mask_type) and (result_scale != max_scale):
                        result_vlmul = vlmul*(max_scale/result_scale)
                        assert(result_vlmul <= MAX_VLMUL)

                    if (not lhs.is_mask_type) and (lhs_scale != max_scale):
                        lhs_vlmul = vlmul*(max_scale/lhs_scale)
                        assert(lhs_vlmul <= MAX_VLMUL)

                    if (not rhs.is_mask_type) and (rhs_scale != max_scale):
                        rhs_vlmul = vlmul*(max_scale/rhs_scale)
                        assert(rhs_vlmul <= MAX_VLMUL)

                    # FIXME: nxv64T types not defined (ie. nxv64i8)
                    if max_scale*vlmul >= 64:
                        continue

                    subs["result_type_scale"] = max_scale*vlmul
                    subs["lhs_type_scale"] = max_scale*vlmul
                    subs["rhs_type_scale"] = max_scale*vlmul

                    print template.substitute(subs)

################################################################################
################################################################################
################################################################################

vv_vx_vi = ["vv", "vx", "vi"]
vvm_vxm_vim = ["vvm", "vxm", "vim"]
vvm_vxm = ["vvm", "vxm"]
vv_vx = ["vv", "vx"]
wv_wx = ["wv", "wx"]
wv_wx_wi = ["wv", "wx", "wi"]
vx_vi = ["vx", "vi"]
vv_vf = ["vv", "vf"]
wv_wf = ["wv", "wf"]
vv = ["vv"]
vx = ["vx"]
vf = ["vf"]
vfm = ["vfm"]
vs = ["vs"]
mm = ["mm"]
vm = ["vm"]
m = ["m"]
v = ["v"]
x = ["x"]
f = ["f"]
s = ["s"]

################################################################################
################################################################################
################################################################################

intrinsics = [
        UnaryIntrinsicScalarResult("vpopc", type_generator = generate_mask_to_int_types, scalar_register = "a0", variants = m, bits_per_mask = True),
        UnaryIntrinsicScalarResult("vfirst", type_generator = generate_mask_to_int_types, scalar_register = "a0", variants = m, bits_per_mask = True),

        UnaryIntrinsic("vload", type_generator = generate_unary_integer_float_types, variants = ["vload"], instruction = "vle"),
        UnaryIntrinsic("vstore", type_generator = generate_unary_integer_float_types, variants = ["vstore"], instruction = "vse"),

        UnaryIntrinsic("vload.strided", type_generator = generate_unary_integer_float_types, variants = ["vload.strided"], instruction = "vlse"),
        UnaryIntrinsic("vstore.strided", type_generator = generate_unary_integer_float_types, variants = ["vstore.strided"], instruction = "vsse"),

        UnaryIntrinsic("vload.indexed", type_generator = generate_unary_integer_float_types, variants = ["vload.indexed"], instruction = "vluxe"),
        UnaryIntrinsic("vstore.indexed", type_generator = generate_unary_integer_float_types, variants = ["vstore.indexed"], instruction = "vsuxe"),

        UnaryIntrinsic("vload.nt", type_generator = generate_unary_integer_float_types, variants = ["vload"], instruction = "vle", nontemporal = True),
        UnaryIntrinsic("vstore.nt", type_generator = generate_unary_integer_float_types, variants = ["vstore"], instruction = "vse", nontemporal = True),

        UnaryIntrinsic("vload.nt.strided", type_generator = generate_unary_integer_float_types, variants = ["vload.strided"], instruction = "vlse", nontemporal = True),
        UnaryIntrinsic("vstore.nt.strided", type_generator = generate_unary_integer_float_types, variants = ["vstore.strided"], instruction = "vsse", nontemporal = True),

        UnaryIntrinsic("vload.nt.indexed", type_generator = generate_unary_integer_float_types, variants = ["vload.indexed"], instruction = "vluxe", nontemporal = True),
        UnaryIntrinsic("vstore.nt.indexed", type_generator = generate_unary_integer_float_types, variants = ["vstore.indexed"], instruction = "vsuxe", nontemporal = True),

        UnaryIntrinsic("vfsqrt", type_generator = generate_unary_float_types, variants = v),
        # This is a very special one
        #UnaryIntrinsic("vfclass", type_generator = generate_unary_vfclass_types, variants = v),

        UnaryIntrinsicScalarInput("vmv.v.x", type_generator = generate_unary_integer_types, variants = x, instruction = "vmv.v", scalar_register = "a0", mask = False),
        UnaryIntrinsicScalarInput("vfmv.v.f", type_generator = generate_unary_float_types, variants = f, instruction = "vfmv.v", scalar_register = "ft0", mask = False),

        UnaryIntrinsicScalarResultNoVL("vmv.x.s", type_generator = generate_unary_integer_types, variants = s, instruction = "vmv.x", scalar_register = "a0", mask = False),

        UnaryIntrinsicScalarInputMerge("vmv.s.x", type_generator = generate_unary_integer_types, variants = x, instruction = "vmv.s", scalar_register = "a0", mask = False),
        UnaryIntrinsicScalarInputMerge("vfmv.s.f", type_generator = generate_unary_float_types, variants = f, instruction = "vfmv.s", scalar_register = "ft0", mask = False),

        #UnaryIntrinsicMask("vmsbf", type_generator = generate_unary_mask_types, variants = m),
        #UnaryIntrinsicMask("vmsof", type_generator = generate_unary_mask_types, variants = m),
        #UnaryIntrinsicMask("vmsif", type_generator = generate_unary_mask_types, variants = m),

        UnaryIntrinsic("viota", type_generator = generate_unary_mask_to_integer_types, variants = m),

        UnaryIntrinsicScalarResultNoVL("vfmv.f.s", type_generator = generate_unary_float_types, variants = s, instruction = "vfmv.f", scalar_register = "ft0", mask = False),

        NullaryIntrinsic("vid", type_generator = generate_nullary_integer_types, variants = v),

        BinaryIntrinsic("vadd", type_generator = generate_binary_integer_types, variants = vv_vx_vi),
        BinaryIntrinsic("vsub", type_generator = generate_binary_integer_types, variants = vv_vx),

        BinaryIntrinsic("vrsub", type_generator = generate_binary_integer_types, variants = vx_vi),

        BinaryIntrinsic("vwaddu", type_generator = generate_binary_integer_types_widened, variants = vv_vx, vlmul_values = [1, 2, 4], widening = True),
        BinaryIntrinsic("vwadd", type_generator = generate_binary_integer_types_widened, variants = vv_vx, vlmul_values = [1, 2, 4], widening = True),

        BinaryIntrinsic("vwaddu.w", type_generator = generate_binary_integer_types_widened_rhs, variants = wv_wx, instruction = "vwaddu", vlmul_values = [1, 2, 4]),
        BinaryIntrinsic("vwadd.w", type_generator = generate_binary_integer_types_widened_rhs, variants = wv_wx, instruction = "vwadd", vlmul_values = [1, 2, 4]),

        BinaryIntrinsic("vwsubu", type_generator = generate_binary_integer_types_widened, variants = vv_vx, vlmul_values = [1, 2, 4], widening = True),
        BinaryIntrinsic("vwsub", type_generator = generate_binary_integer_types_widened, variants = vv_vx, vlmul_values = [1, 2, 4], widening = True),

        BinaryIntrinsic("vwsubu.w", type_generator = generate_binary_integer_types_widened_rhs, variants = wv_wx, instruction = "vwsubu", vlmul_values = [1, 2, 4]),
        BinaryIntrinsic("vwsub.w", type_generator = generate_binary_integer_types_widened_rhs, variants = wv_wx, instruction = "vwsub", vlmul_values = [1, 2, 4]),

        BinaryIntrinsic("vand", type_generator = generate_binary_integer_types, variants = vv_vx_vi),
        BinaryIntrinsic("vor", type_generator = generate_binary_integer_types, variants = vv_vx_vi),
        BinaryIntrinsic("vxor", type_generator = generate_binary_integer_types, variants = vv_vx_vi),

        BinaryIntrinsic("vsll", type_generator = generate_binary_integer_types, variants = vv_vx_vi),
        BinaryIntrinsic("vsrl", type_generator = generate_binary_integer_types, variants = vv_vx_vi),
        BinaryIntrinsic("vsra", type_generator = generate_binary_integer_types, variants = vv_vx_vi),

        BinaryIntrinsic("vnsrl", type_generator = generate_binary_integer_types_narrowed_lhs, variants = wv_wx_wi, vlmul_values = [1, 2, 4], narrowing = True),
        BinaryIntrinsic("vnsra", type_generator = generate_binary_integer_types_narrowed_lhs, variants = wv_wx_wi, vlmul_values = [1, 2, 4], narrowing = True),

        BinaryIntrinsic("vmseq", type_generator = generate_binary_integer_types_mask_out, variants = vv_vx_vi, generates_mask = True),
        BinaryIntrinsic("vmsne", type_generator = generate_binary_integer_types_mask_out, variants = vv_vx_vi, generates_mask = True),
        BinaryIntrinsic("vmsltu", type_generator = generate_binary_integer_types_mask_out, variants = vv_vx, generates_mask = True),
        BinaryIntrinsic("vmslt", type_generator = generate_binary_integer_types_mask_out, variants = vv_vx, generates_mask = True),
        BinaryIntrinsic("vmsleu", type_generator = generate_binary_integer_types_mask_out, variants = vv_vx_vi, generates_mask = True),
        BinaryIntrinsic("vmsle", type_generator = generate_binary_integer_types_mask_out, variants = vv_vx_vi, generates_mask = True),
        BinaryIntrinsic("vmsgtu", type_generator = generate_binary_integer_types_mask_out, variants = vv, instruction = "vmsltu", generates_mask = True),
        BinaryIntrinsic("vmsgtu", type_generator = generate_binary_integer_types_mask_out, variants = vx_vi, generates_mask = True),
        BinaryIntrinsic("vmsgt", type_generator = generate_binary_integer_types_mask_out, variants = vv, instruction = "vmslt", generates_mask = True),
        BinaryIntrinsic("vmsgt", type_generator = generate_binary_integer_types_mask_out, variants = vx_vi, generates_mask = True),

        BinaryIntrinsic("vminu", type_generator = generate_binary_integer_types, variants = vv_vx),
        BinaryIntrinsic("vmin", type_generator = generate_binary_integer_types, variants = vv_vx),
        BinaryIntrinsic("vmaxu", type_generator = generate_binary_integer_types, variants = vv_vx),
        BinaryIntrinsic("vmax", type_generator = generate_binary_integer_types, variants = vv_vx),

        BinaryIntrinsic("vmul", type_generator = generate_binary_integer_types, variants = vv_vx),
        BinaryIntrinsic("vmulh", type_generator = generate_binary_integer_types, variants = vv_vx),
        BinaryIntrinsic("vmulhu", type_generator = generate_binary_integer_types, variants = vv_vx),
        BinaryIntrinsic("vmulhsu", type_generator = generate_binary_integer_types, variants = vv_vx),

        BinaryIntrinsic("vwmul", type_generator = generate_binary_integer_types_widened, variants = vv_vx, vlmul_values = [1, 2, 4], widening = True),
        BinaryIntrinsic("vwmulu", type_generator = generate_binary_integer_types_widened, variants = vv_vx, vlmul_values = [1, 2, 4], widening = True),
        BinaryIntrinsic("vwmulsu", type_generator = generate_binary_integer_types_widened, variants = vv_vx, vlmul_values = [1, 2, 4], widening = True),

        BinaryIntrinsic("vdivu", type_generator = generate_binary_integer_types, variants = vv_vx),
        BinaryIntrinsic("vdiv", type_generator = generate_binary_integer_types, variants = vv_vx),
        BinaryIntrinsic("vremu", type_generator = generate_binary_integer_types, variants = vv_vx),
        BinaryIntrinsic("vrem", type_generator = generate_binary_integer_types, variants = vv_vx),

        BinaryIntrinsicMaskIn("vmerge", type_generator = generate_binary_integer_types, variants = vvm_vxm_vim),

        BinaryIntrinsic("vsaddu", type_generator = generate_binary_integer_types, variants = vv_vx_vi),
        BinaryIntrinsic("vsadd", type_generator = generate_binary_integer_types, variants = vv_vx_vi),
        BinaryIntrinsic("vssubu", type_generator = generate_binary_integer_types, variants = vv_vx),
        BinaryIntrinsic("vssub", type_generator = generate_binary_integer_types, variants = vv_vx),

        BinaryIntrinsic("vaadd", type_generator = generate_binary_integer_types, variants = vv_vx),
        BinaryIntrinsic("vasub", type_generator = generate_binary_integer_types, variants = vv_vx),

        BinaryIntrinsic("vsmul", type_generator = generate_binary_integer_types, variants = vv_vx),

        BinaryIntrinsic("vssrl", type_generator = generate_binary_integer_types, variants = vv_vx_vi),
        BinaryIntrinsic("vssra", type_generator = generate_binary_integer_types, variants = vv_vx_vi),

        #BinaryIntrinsic("vnclipu", type_generator = generate_binary_integer_types_narrowed, variants = vv_vx_vi),
        #BinaryIntrinsic("vnclip", type_generator = generate_binary_integer_types_narrowed, variants = vv_vx_vi),

        BinaryIntrinsic("vfadd", type_generator = generate_binary_float_types, variants = vv_vf),
        BinaryIntrinsic("vfsub", type_generator = generate_binary_float_types, variants = vv_vf),
        BinaryIntrinsic("vfwadd", type_generator = generate_binary_float_types_widened, variants = vv_vf, vlmul_values = [1, 2, 4], widening = True),
        BinaryIntrinsic("vfwsub", type_generator = generate_binary_float_types_widened, variants = vv_vf, vlmul_values = [1, 2, 4], widening = True),

        BinaryIntrinsic("vfwadd.w", type_generator = generate_binary_float_types_widened_rhs, variants = wv_wf, instruction = "vfwadd", vlmul_values = [1, 2, 4]),
        BinaryIntrinsic("vfwsub.w", type_generator = generate_binary_float_types_widened_rhs, variants = wv_wf, instruction = "vfwsub", vlmul_values = [1, 2, 4]),

        BinaryIntrinsic("vfmul", type_generator = generate_binary_float_types, variants = vv_vf),
        BinaryIntrinsic("vfdiv", type_generator = generate_binary_float_types, variants = vv_vf),
        BinaryIntrinsic("vfrdiv", type_generator = generate_binary_float_types, variants = vf),

        BinaryIntrinsic("vfwmul", type_generator = generate_binary_float_types_widened, variants = vv_vf, vlmul_values = [1, 2, 4], widening = True),

        BinaryIntrinsic("vfmin", type_generator = generate_binary_float_types, variants = vv_vf),
        BinaryIntrinsic("vfmax", type_generator = generate_binary_float_types, variants = vv_vf),

        BinaryIntrinsic("vfsgnj", type_generator = generate_binary_float_types, variants = vv_vf),
        BinaryIntrinsic("vfsgnjn", type_generator = generate_binary_float_types, variants = vv_vf),
        BinaryIntrinsic("vfsgnjx", type_generator = generate_binary_float_types, variants = vv_vf),

        BinaryIntrinsic("vmfeq", type_generator = generate_binary_float_types_mask_out, variants = vv_vf, generates_mask = True),
        BinaryIntrinsic("vmfne", type_generator = generate_binary_float_types_mask_out, variants = vv_vf, generates_mask = True),
        BinaryIntrinsic("vmflt", type_generator = generate_binary_float_types_mask_out, variants = vv_vf, generates_mask = True),
        BinaryIntrinsic("vmfle", type_generator = generate_binary_float_types_mask_out, variants = vv_vf, generates_mask = True),
        BinaryIntrinsic("vmfgt", type_generator = generate_binary_float_types_mask_out, variants = vv, instruction = "vmflt", generates_mask = True),
        BinaryIntrinsic("vmfgt", type_generator = generate_binary_float_types_mask_out, variants = vf, generates_mask = True),
        BinaryIntrinsic("vmfge", type_generator = generate_binary_float_types_mask_out, variants = vv, instruction = "vmfle", generates_mask = True),
        BinaryIntrinsic("vmfge", type_generator = generate_binary_float_types_mask_out, variants = vf, generates_mask = True),

        BinaryIntrinsicMaskIn("vfmerge", type_generator = generate_binary_float_types, variants = vfm),

        BinaryIntrinsic("vredsum", type_generator = generate_binary_integer_types, variants = vs),
        BinaryIntrinsic("vredand", type_generator = generate_binary_integer_types, variants = vs),
        BinaryIntrinsic("vredor", type_generator = generate_binary_integer_types, variants = vs),
        BinaryIntrinsic("vredxor", type_generator = generate_binary_integer_types, variants = vs),
        BinaryIntrinsic("vredminu", type_generator = generate_binary_integer_types, variants = vs),
        BinaryIntrinsic("vredmin", type_generator = generate_binary_integer_types, variants = vs),
        BinaryIntrinsic("vredmaxu", type_generator = generate_binary_integer_types, variants = vs),
        BinaryIntrinsic("vredmax", type_generator = generate_binary_integer_types, variants = vs),

        #BinaryIntrinsic("vwredsumu", type_generator = generate_binary_integer_types_widened, variants = vs),
        #BinaryIntrinsic("vwredsum", type_generator = generate_binary_integer_types_widened, variants = vs),

        BinaryIntrinsic("vfredsum", type_generator = generate_binary_float_types, variants = vs),
        BinaryIntrinsic("vfredosum", type_generator = generate_binary_float_types, variants = vs),
        BinaryIntrinsic("vfredmin", type_generator = generate_binary_float_types, variants = vs),
        BinaryIntrinsic("vfredmax", type_generator = generate_binary_float_types, variants = vs),

        #BinaryIntrinsic("vfwredsum", type_generator = generate_binary_float_types_widened, variants = vs),
        #BinaryIntrinsic("vfwredosum", type_generator = generate_binary_float_types_widened, variants = vs),

        BinaryIntrinsic("vmandnot", type_generator = generate_binary_mask_types, variants = mm, generates_mask = True, mask = False),
        BinaryIntrinsic("vmand", type_generator = generate_binary_mask_types, variants = mm, generates_mask = True, mask = False),
        BinaryIntrinsic("vmor", type_generator = generate_binary_mask_types, variants = mm, generates_mask = True, mask = False),
        BinaryIntrinsic("vmxor", type_generator = generate_binary_mask_types, variants = mm, generates_mask = True, mask = False),
        BinaryIntrinsic("vmornot", type_generator = generate_binary_mask_types, variants = mm, generates_mask = True, mask = False),
        BinaryIntrinsic("vmnand", type_generator = generate_binary_mask_types, variants = mm, generates_mask = True, mask = False),
        BinaryIntrinsic("vmnor", type_generator = generate_binary_mask_types, variants = mm, generates_mask = True, mask = False),
        BinaryIntrinsic("vmxnor", type_generator = generate_binary_mask_types, variants = mm, generates_mask = True, mask = False),

        BinaryIntrinsic("vcompress", type_generator = generate_binary_any_and_mask_types, variants = vm, mask = False),

        BinaryIntrinsic("vrgather", type_generator = generate_binary_any_and_integer_types, variants = vv_vx_vi),

        BinaryIntrinsic("vslideup", type_generator = generate_binary_any_and_integer_types, variants = vx_vi),
        BinaryIntrinsic("vslidedown", type_generator = generate_binary_any_and_integer_types, variants = vx_vi),
        BinaryIntrinsic("vslide1up", type_generator = generate_binary_any_and_integer_types, variants = vx),
        BinaryIntrinsic("vslide1down", type_generator = generate_binary_any_and_integer_types, variants = vx),

        BinaryIntrinsicMaskIn("vadc", type_generator = generate_binary_integer_types, variants = vvm_vxm_vim),
        BinaryIntrinsicMaskIn("vmadc.carry.in", type_generator = generate_binary_integer_types_mask_out, variants = vvm_vxm_vim, instruction = "vmadc", generates_mask = True),
        BinaryIntrinsic("vmadc", type_generator = generate_binary_integer_types_mask_out, variants = vv_vx_vi, mask = False, generates_mask = True),

        BinaryIntrinsicMaskIn("vsbc", type_generator = generate_binary_integer_types, variants = vvm_vxm),
        BinaryIntrinsicMaskIn("vmsbc.borrow.in", type_generator = generate_binary_integer_types_mask_out, variants = vvm_vxm, instruction = "vmsbc", generates_mask = True),
        BinaryIntrinsic("vmsbc", type_generator = generate_binary_integer_types_mask_out, variants = vv_vx, mask = False, generates_mask = True),

        TernaryIntrinsic("vmacc", type_generator = generate_ternary_integer_types, variants = vv_vx),
        TernaryIntrinsic("vnmsac", type_generator = generate_ternary_integer_types, variants = vv_vx),
        TernaryIntrinsic("vmadd", type_generator = generate_ternary_integer_types, variants = vv_vx),
        TernaryIntrinsic("vnmsub", type_generator = generate_ternary_integer_types, variants = vv_vx),

        #TernaryIntrinsic("vwmaccu", type_generator = generate_ternary_integer_types, variants = vv_vx),
        #TernaryIntrinsic("vwmacc", type_generator = generate_ternary_integer_types, variants = vv_vx),
        #TernaryIntrinsic("vwmsacu", type_generator = generate_ternary_integer_types, variants = vv_vx),
        #TernaryIntrinsic("vwmsac", type_generator = generate_ternary_integer_types, variants = vv_vx),

        TernaryIntrinsic("vfmadd", type_generator = generate_ternary_float_types, variants = vv_vf),
        TernaryIntrinsic("vfnmadd", type_generator = generate_ternary_float_types, variants = vv_vf),
        TernaryIntrinsic("vfmsub", type_generator = generate_ternary_float_types, variants = vv_vf),
        TernaryIntrinsic("vfnmsub", type_generator = generate_ternary_float_types, variants = vv_vf),
        TernaryIntrinsic("vfmacc", type_generator = generate_ternary_float_types, variants = vv_vf),
        TernaryIntrinsic("vfnmacc", type_generator = generate_ternary_float_types, variants = vv_vf),
        TernaryIntrinsic("vfmsac", type_generator = generate_ternary_float_types, variants = vv_vf),
        TernaryIntrinsic("vfnmsac", type_generator = generate_ternary_float_types, variants = vv_vf),

        #TernaryIntrinsic("vfwmacc", type_generator = generate_ternary_float_types_widened, variants = vv_vx),
        #TernaryIntrinsic("vfwnmacc", type_generator = generate_ternary_float_types_widened, variants = vv_vx),
        #TernaryIntrinsic("vfwmsac", type_generator = generate_ternary_float_types_widened, variants = vv_vx),
        #TernaryIntrinsic("vfwnmsac", type_generator = generate_ternary_float_types_widened, variants = vv_vx),

        #TernaryIntrinsic("vwsmaccu", type_generator = generate_ternary_integer_types_widened, variants = vv_vx),
        #TernaryIntrinsic("vwsmacc", type_generator = generate_ternary_integer_types_widened, variants = vv_vx),
        #TernaryIntrinsic("vwsmsacu", type_generator = generate_ternary_integer_types_widened, variants = vv_vx),
        #TernaryIntrinsic("vwsmsac", type_generator = generate_ternary_integer_types_widened, variants = vv_vx),

        Conversion("vfcvt", type_generator = generate_same_size_float_to_integer_types, variants = ["xu.f"]),
        Conversion("vfcvt", type_generator = generate_same_size_float_to_integer_types, variants = ["x.f"]),
        Conversion("vfcvt", type_generator = generate_same_size_integer_to_float_types, variants = ["f.xu"]),
        Conversion("vfcvt", type_generator = generate_same_size_integer_to_float_types, variants = ["f.x"]),

        Conversion("vfwcvt", type_generator = generate_float_to_widened_integer_types, variants = ["xu.f"], vlmul_values = [1, 2, 4]),
        Conversion("vfwcvt", type_generator = generate_float_to_widened_integer_types, variants = ["x.f"], vlmul_values = [1, 2, 4]),
        Conversion("vfwcvt", type_generator = generate_integer_to_widened_float_types, variants = ["f.xu"], vlmul_values = [1, 2, 4]),
        Conversion("vfwcvt", type_generator = generate_integer_to_widened_float_types, variants = ["f.x"], vlmul_values = [1, 2, 4]),
        Conversion("vfwcvt", type_generator = generate_float_to_widened_float_types, variants = ["f.f"], vlmul_values = [1, 2, 4]),

        # These differ from regular conversions in that they have a trailing '.w' (eg. vfncvt.x.f.w)
        NarrowingConversion("vfncvt", type_generator = generate_float_to_narrowed_integer_types, variants = ["xu.f"], vlmul_values = [1, 2, 4]),
        NarrowingConversion("vfncvt", type_generator = generate_float_to_narrowed_integer_types, variants = ["x.f"], vlmul_values = [1, 2, 4]),
        NarrowingConversion("vfncvt", type_generator = generate_integer_to_narrowed_float_types, variants = ["f.xu"], vlmul_values = [1, 2, 4]),
        NarrowingConversion("vfncvt", type_generator = generate_integer_to_narrowed_float_types, variants = ["f.x"], vlmul_values = [1, 2, 4]),
        NarrowingConversion("vfncvt", type_generator = generate_float_to_narrowed_float_types, variants = ["f.f"], vlmul_values = [1, 2, 4])
]

for intr in intrinsics:
    intr.render()
