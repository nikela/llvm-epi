; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt -S --mem2reg %s | FileCheck %s
define <vscale x 2 x i32> @vp_only_same_mask_and_vl(<vscale x 2 x i32> %x, <vscale x 2 x i32> %y, <vscale x 2 x i1> %mask, i32 %vl) {
; CHECK-LABEL: @vp_only_same_mask_and_vl(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[ADD:%.*]] = call <vscale x 2 x i32> @llvm.vp.add.nxv2i32(<vscale x 2 x i32> [[X:%.*]], <vscale x 2 x i32> [[Y:%.*]], <vscale x 2 x i1> [[MASK:%.*]], i32 [[VL:%.*]])
; CHECK-NEXT:    ret <vscale x 2 x i32> [[ADD]]
;
entry:
  %x.addr = alloca <vscale x 2 x i32>
  %y.addr = alloca <vscale x 2 x i32>
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %x, <vscale x 2 x i32>* %x.addr, <vscale x 2 x i1> %mask, i32 %vl)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %y, <vscale x 2 x i32>* %y.addr, <vscale x 2 x i1> %mask, i32 %vl)
  %0 = call <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>* %x.addr, <vscale x 2 x i1> %mask, i32 %vl)
  %1 = call <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>* %y.addr, <vscale x 2 x i1> %mask, i32 %vl)
  %add = call <vscale x 2 x i32> @llvm.vp.add.nxv2i32(<vscale x 2 x i32> %0, <vscale x 2 x i32> %1, <vscale x 2 x i1> %mask, i32 %vl)
  ret <vscale x 2 x i32> %add
}

define <vscale x 2 x i32> @vp_only_different_mask(<vscale x 2 x i32> %x, <vscale x 2 x i32> %y, <vscale x 2 x i1> %mask1, <vscale x 2 x i1> %mask2, i32 %vl) {
; CHECK-LABEL: @vp_only_different_mask(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[ADD:%.*]] = call <vscale x 2 x i32> @llvm.vp.add.nxv2i32(<vscale x 2 x i32> [[X:%.*]], <vscale x 2 x i32> [[Y:%.*]], <vscale x 2 x i1> [[MASK2:%.*]], i32 [[VL:%.*]])
; CHECK-NEXT:    ret <vscale x 2 x i32> [[ADD]]
;
entry:
  %x.addr = alloca <vscale x 2 x i32>
  %y.addr = alloca <vscale x 2 x i32>
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %x, <vscale x 2 x i32>* %x.addr, <vscale x 2 x i1> %mask1, i32 %vl)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %y, <vscale x 2 x i32>* %y.addr, <vscale x 2 x i1> %mask1, i32 %vl)
  %0 = call <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>* %x.addr, <vscale x 2 x i1> %mask1, i32 %vl)
  %1 = call <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>* %y.addr, <vscale x 2 x i1> %mask2, i32 %vl)
  %add = call <vscale x 2 x i32> @llvm.vp.add.nxv2i32(<vscale x 2 x i32> %0, <vscale x 2 x i32> %1, <vscale x 2 x i1> %mask2, i32 %vl)
  ret <vscale x 2 x i32> %add
}

define <vscale x 2 x i32> @vp_only_different_vl(<vscale x 2 x i32> %x, <vscale x 2 x i32> %y, <vscale x 2 x i1> %mask, i32 %vl1, i32 %vl2) {
; CHECK-LABEL: @vp_only_different_vl(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[Y_ADDR:%.*]] = alloca <vscale x 2 x i32>, align 8
; CHECK-NEXT:    call void @llvm.vp.store.nxv2i32.p0nxv2i32(<vscale x 2 x i32> [[Y:%.*]], <vscale x 2 x i32>* [[Y_ADDR]], <vscale x 2 x i1> [[MASK:%.*]], i32 [[VL1:%.*]])
; CHECK-NEXT:    [[TMP0:%.*]] = call <vscale x 2 x i32> @llvm.vp.load.nxv2i32.p0nxv2i32(<vscale x 2 x i32>* [[Y_ADDR]], <vscale x 2 x i1> [[MASK]], i32 [[VL2:%.*]])
; CHECK-NEXT:    [[ADD:%.*]] = call <vscale x 2 x i32> @llvm.vp.add.nxv2i32(<vscale x 2 x i32> [[X:%.*]], <vscale x 2 x i32> [[TMP0]], <vscale x 2 x i1> [[MASK]], i32 [[VL2]])
; CHECK-NEXT:    ret <vscale x 2 x i32> [[ADD]]
;
entry:
  %x.addr = alloca <vscale x 2 x i32>
  %y.addr = alloca <vscale x 2 x i32>
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %x, <vscale x 2 x i32>* %x.addr, <vscale x 2 x i1> %mask, i32 %vl1)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %y, <vscale x 2 x i32>* %y.addr, <vscale x 2 x i1> %mask, i32 %vl1)
  %0 = call <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>* %x.addr, <vscale x 2 x i1> %mask, i32 %vl1)
  %1 = call <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>* %y.addr, <vscale x 2 x i1> %mask, i32 %vl2)
  %add = call <vscale x 2 x i32> @llvm.vp.add.nxv2i32(<vscale x 2 x i32> %0, <vscale x 2 x i32> %1, <vscale x 2 x i1> %mask, i32 %vl2)
  ret <vscale x 2 x i32> %add
}

define <vscale x 2 x i32> @mixed_regular_and_vp_mem_ops(<vscale x 2 x i32> %x, <vscale x 2 x i32> %y, <vscale x 2 x i1> %mask, i32 %vl) {
; CHECK-LABEL: @mixed_regular_and_vp_mem_ops(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[Y_ADDR:%.*]] = alloca <vscale x 2 x i32>, align 8
; CHECK-NEXT:    call void @llvm.vp.store.nxv2i32.p0nxv2i32(<vscale x 2 x i32> [[Y:%.*]], <vscale x 2 x i32>* [[Y_ADDR]], <vscale x 2 x i1> [[MASK:%.*]], i32 [[VL:%.*]])
; CHECK-NEXT:    [[TMP0:%.*]] = load <vscale x 2 x i32>, <vscale x 2 x i32>* [[Y_ADDR]], align 8
; CHECK-NEXT:    [[ADD:%.*]] = call <vscale x 2 x i32> @llvm.vp.add.nxv2i32(<vscale x 2 x i32> [[X:%.*]], <vscale x 2 x i32> [[TMP0]], <vscale x 2 x i1> [[MASK]], i32 [[VL]])
; CHECK-NEXT:    ret <vscale x 2 x i32> [[ADD]]
;
entry:
  %x.addr = alloca <vscale x 2 x i32>
  %y.addr = alloca <vscale x 2 x i32>
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %x, <vscale x 2 x i32>* %x.addr, <vscale x 2 x i1> %mask, i32 %vl)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %y, <vscale x 2 x i32>* %y.addr, <vscale x 2 x i1> %mask, i32 %vl)
  %0 = call <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>* %x.addr, <vscale x 2 x i1> %mask, i32 %vl)
  %1 = load <vscale x 2 x i32>, <vscale x 2 x i32>* %y.addr
  %add = call <vscale x 2 x i32> @llvm.vp.add.nxv2i32(<vscale x 2 x i32> %0, <vscale x 2 x i32> %1, <vscale x 2 x i1> %mask, i32 %vl)
  ret <vscale x 2 x i32> %add
}

define <vscale x 2 x i32> @multiple_stores_single_bb_same_mask(<vscale x 2 x i32> %v1, <vscale x 2 x i32> %v2, <vscale x 2 x i32> %v3, <vscale x 2 x i1> %mask, i32 %vl) {
; CHECK-LABEL: @multiple_stores_single_bb_same_mask(
; CHECK-NEXT:    ret <vscale x 2 x i32> [[V3:%.*]]
;
  %p = alloca <vscale x 2 x i32>
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %v1, <vscale x 2 x i32>* %p, <vscale x 2 x i1> %mask, i32 %vl)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %v2, <vscale x 2 x i32>* %p, <vscale x 2 x i1> %mask, i32 %vl)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %v3, <vscale x 2 x i32>* %p, <vscale x 2 x i1> %mask, i32 %vl)
  %ret = call <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>* %p, <vscale x 2 x i1> %mask, i32 %vl)
  ret <vscale x 2 x i32> %ret
}

define <vscale x 2 x i32> @multiple_stores_single_bb_different_mask_1(<vscale x 2 x i32> %v1, <vscale x 2 x i32> %v2, <vscale x 2 x i1> %mask1, <vscale x 2 x i1> %mask2, i32 %vl) {
; CHECK-LABEL: @multiple_stores_single_bb_different_mask_1(
; CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 2 x i32> @llvm.vp.select.nxv2i32(<vscale x 2 x i1> [[MASK2:%.*]], <vscale x 2 x i32> [[V2:%.*]], <vscale x 2 x i32> [[V1:%.*]], i32 [[VL:%.*]])
; CHECK-NEXT:    ret <vscale x 2 x i32> [[TMP1]]
;
  %p = alloca <vscale x 2 x i32>
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %v1, <vscale x 2 x i32>* %p, <vscale x 2 x i1> %mask1, i32 %vl)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %v2, <vscale x 2 x i32>* %p, <vscale x 2 x i1> %mask2, i32 %vl)
  %ret = call <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>* %p, <vscale x 2 x i1> %mask1, i32 %vl)
  ret <vscale x 2 x i32> %ret
}

define <vscale x 2 x i32> @multiple_stores_single_bb_different_mask_2(<vscale x 2 x i32> %v1, <vscale x 2 x i32> %v2, <vscale x 2 x i32> %v3, <vscale x 2 x i1> %mask1, <vscale x 2 x i1> %mask2, i32 %vl) {
; CHECK-LABEL: @multiple_stores_single_bb_different_mask_2(
; CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 2 x i32> @llvm.vp.select.nxv2i32(<vscale x 2 x i1> [[MASK2:%.*]], <vscale x 2 x i32> [[V2:%.*]], <vscale x 2 x i32> [[V1:%.*]], i32 [[VL:%.*]])
; CHECK-NEXT:    [[TMP2:%.*]] = call <vscale x 2 x i32> @llvm.vp.select.nxv2i32(<vscale x 2 x i1> [[MASK2]], <vscale x 2 x i32> [[V3:%.*]], <vscale x 2 x i32> [[TMP1]], i32 [[VL]])
; CHECK-NEXT:    ret <vscale x 2 x i32> [[TMP2]]
;
  %p = alloca <vscale x 2 x i32>
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %v1, <vscale x 2 x i32>* %p, <vscale x 2 x i1> %mask1, i32 %vl)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %v2, <vscale x 2 x i32>* %p, <vscale x 2 x i1> %mask2, i32 %vl)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %v3, <vscale x 2 x i32>* %p, <vscale x 2 x i1> %mask2, i32 %vl)
  %ret = call <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>* %p, <vscale x 2 x i1> %mask1, i32 %vl)
  ret <vscale x 2 x i32> %ret
}

define <vscale x 2 x i32> @multiple_stores_single_bb_different_mask_3(<vscale x 2 x i32> %v1, <vscale x 2 x i32> %v2, <vscale x 2 x i32> %v3, <vscale x 2 x i1> %mask1, <vscale x 2 x i1> %mask2, i32 %vl) {
; CHECK-LABEL: @multiple_stores_single_bb_different_mask_3(
; CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 2 x i32> @llvm.vp.select.nxv2i32(<vscale x 2 x i1> [[MASK2:%.*]], <vscale x 2 x i32> [[V3:%.*]], <vscale x 2 x i32> [[V2:%.*]], i32 [[VL:%.*]])
; CHECK-NEXT:    ret <vscale x 2 x i32> [[TMP1]]
;
  %p = alloca <vscale x 2 x i32>
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %v1, <vscale x 2 x i32>* %p, <vscale x 2 x i1> %mask1, i32 %vl)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %v2, <vscale x 2 x i32>* %p, <vscale x 2 x i1> %mask1, i32 %vl)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %v3, <vscale x 2 x i32>* %p, <vscale x 2 x i1> %mask2, i32 %vl)
  %ret = call <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>* %p, <vscale x 2 x i1> %mask2, i32 %vl)
  ret <vscale x 2 x i32> %ret
}

define <vscale x 2 x i32> @multiple_stores_multiple_bb_same_mask(<vscale x 2 x i32> %v1, <vscale x 2 x i32> %v2, <vscale x 2 x i32> %v3, <vscale x 2 x i1> %mask, i32 %evl) {
; CHECK-LABEL: @multiple_stores_multiple_bb_same_mask(
; CHECK-NEXT:  bb1:
; CHECK-NEXT:    br label [[BB2:%.*]]
; CHECK:       bb2:
; CHECK-NEXT:    ret <vscale x 2 x i32> [[V3:%.*]]
;
bb1:
  %v.p = alloca <vscale x 2 x i32>
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %v1, <vscale x 2 x i32>* %v.p, <vscale x 2 x i1> %mask, i32 %evl)
  br label %bb2

bb2:
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %v2, <vscale x 2 x i32>* %v.p, <vscale x 2 x i1> %mask, i32 %evl)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %v3, <vscale x 2 x i32>* %v.p, <vscale x 2 x i1> %mask, i32 %evl)
  %ret = call <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>* %v.p, <vscale x 2 x i1> %mask, i32 %evl)
  ret <vscale x 2 x i32> %ret
}

define <vscale x 2 x i32> @multiple_stores_multiple_bb_different_mask_1(<vscale x 2 x i32> %v1, <vscale x 2 x i32> %v2, <vscale x 2 x i1> %mask1, <vscale x 2 x i1> %mask2, i32 %evl) {
; CHECK-LABEL: @multiple_stores_multiple_bb_different_mask_1(
; CHECK-NEXT:  bb1:
; CHECK-NEXT:    br label [[BB2:%.*]]
; CHECK:       bb2:
; CHECK-NEXT:    [[TMP0:%.*]] = call <vscale x 2 x i32> @llvm.vp.select.nxv2i32(<vscale x 2 x i1> [[MASK2:%.*]], <vscale x 2 x i32> [[V2:%.*]], <vscale x 2 x i32> [[V1:%.*]], i32 [[EVL:%.*]])
; CHECK-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
;
bb1:
  %v.p = alloca <vscale x 2 x i32>
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %v1, <vscale x 2 x i32>* %v.p, <vscale x 2 x i1> %mask1, i32 %evl)
  br label %bb2

bb2:
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %v2, <vscale x 2 x i32>* %v.p, <vscale x 2 x i1> %mask2, i32 %evl)
  %ret = call <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>* %v.p, <vscale x 2 x i1> %mask1, i32 %evl)
  ret <vscale x 2 x i32> %ret
}

define <vscale x 2 x i32> @multiple_stores_multiple_bb_different_mask_2(<vscale x 2 x i32> %v1, <vscale x 2 x i32> %v2, <vscale x 2 x i32> %v3, <vscale x 2 x i1> %mask1, <vscale x 2 x i1> %mask2, i32 %evl) {
; CHECK-LABEL: @multiple_stores_multiple_bb_different_mask_2(
; CHECK-NEXT:  bb1:
; CHECK-NEXT:    br label [[BB2:%.*]]
; CHECK:       bb2:
; CHECK-NEXT:    [[TMP0:%.*]] = call <vscale x 2 x i32> @llvm.vp.select.nxv2i32(<vscale x 2 x i1> [[MASK2:%.*]], <vscale x 2 x i32> [[V2:%.*]], <vscale x 2 x i32> [[V1:%.*]], i32 [[EVL:%.*]])
; CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 2 x i32> @llvm.vp.select.nxv2i32(<vscale x 2 x i1> [[MASK2]], <vscale x 2 x i32> [[V3:%.*]], <vscale x 2 x i32> [[TMP0]], i32 [[EVL]])
; CHECK-NEXT:    ret <vscale x 2 x i32> [[TMP1]]
;
bb1:
  %v.p = alloca <vscale x 2 x i32>
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %v1, <vscale x 2 x i32>* %v.p, <vscale x 2 x i1> %mask1, i32 %evl)
  br label %bb2

bb2:
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %v2, <vscale x 2 x i32>* %v.p, <vscale x 2 x i1> %mask2, i32 %evl)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %v3, <vscale x 2 x i32>* %v.p, <vscale x 2 x i1> %mask2, i32 %evl)
  %ret = call <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>* %v.p, <vscale x 2 x i1> %mask1, i32 %evl)
  ret <vscale x 2 x i32> %ret
}

define <vscale x 2 x i32> @multiple_stores_multiple_bb_different_mask_3(<vscale x 2 x i32> %v1, <vscale x 2 x i32> %v2, <vscale x 2 x i32> %v3, <vscale x 2 x i1> %mask1, <vscale x 2 x i1> %mask2, i32 %evl) {
; CHECK-LABEL: @multiple_stores_multiple_bb_different_mask_3(
; CHECK-NEXT:  bb1:
; CHECK-NEXT:    br label [[BB2:%.*]]
; CHECK:       bb2:
; CHECK-NEXT:    [[TMP0:%.*]] = call <vscale x 2 x i32> @llvm.vp.select.nxv2i32(<vscale x 2 x i1> [[MASK2:%.*]], <vscale x 2 x i32> [[V3:%.*]], <vscale x 2 x i32> [[V2:%.*]], i32 [[EVL:%.*]])
; CHECK-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
;
bb1:
  %v.p = alloca <vscale x 2 x i32>
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %v1, <vscale x 2 x i32>* %v.p, <vscale x 2 x i1> %mask1, i32 %evl)
  br label %bb2

bb2:
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %v2, <vscale x 2 x i32>* %v.p, <vscale x 2 x i1> %mask1, i32 %evl)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %v3, <vscale x 2 x i32>* %v.p, <vscale x 2 x i1> %mask2, i32 %evl)
  %ret = call <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>* %v.p, <vscale x 2 x i1> %mask2, i32 %evl)
  ret <vscale x 2 x i32> %ret
}

declare void @llvm.vp.store.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>*, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>*, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.add.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
