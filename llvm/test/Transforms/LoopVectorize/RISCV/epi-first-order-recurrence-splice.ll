; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt -mtriple riscv64 -mattr +m,+a,+f,+d,+zepi -S -dce -riscv-v-vector-bits-min=64 \
; RUN:    -scalable-vectorization=only -loop-vectorize -vector-register-width-factor=8 -instcombine -force-vector-interleave=1 < %s  -o - | FileCheck %s
; RUN: opt -mtriple riscv64 -mattr +m,+a,+f,+d,+zepi -S -dce -riscv-v-vector-bits-min=64 \
; RUN:    -scalable-vectorization=only -loop-vectorize -instcombine -force-vector-interleave=1 < %s  -o - | FileCheck %s --check-prefix=CHECK1
; RUN: opt -mtriple riscv64 -mattr +m,+a,+f,+d,+zepi -S -dce -riscv-v-vector-bits-min=64 \
; RUN:    -scalable-vectorization=only -prefer-predicate-over-epilogue=predicate-dont-vectorize \
; RUN:    -loop-vectorize -instcombine -force-vector-interleave=1 < %s  -o - | FileCheck %s --check-prefix=CHECKVP

target datalayout = "e-m:e-i64:64-i128:128-n32:64-S128"

; void recurrence_1(int *restrict a, int *restrict b, int n) {
;   for(int i = 0; i < n; i++)
;     b[i] =  a[i] + a[i - 1]
; }
;
define void @recurrence_1(i32* noalias nocapture readonly %a, i32* noalias nocapture %b, i32 %n) {
; CHECK-LABEL: @recurrence_1(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br label [[FOR_PREHEADER:%.*]]
; CHECK:       for.preheader:
; CHECK-NEXT:    [[PRE_LOAD:%.*]] = load i32, i32* [[A:%.*]], align 4
; CHECK-NEXT:    [[TMP0:%.*]] = add i32 [[N:%.*]], -1
; CHECK-NEXT:    [[TMP1:%.*]] = zext i32 [[TMP0]] to i64
; CHECK-NEXT:    [[TMP2:%.*]] = add nuw nsw i64 [[TMP1]], 1
; CHECK-NEXT:    [[TMP3:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-NEXT:    [[TMP4:%.*]] = shl i64 [[TMP3]], 4
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[TMP2]], [[TMP4]]
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[TMP5:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-NEXT:    [[TMP6:%.*]] = shl i64 [[TMP5]], 4
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[TMP2]], [[TMP6]]
; CHECK-NEXT:    [[N_VEC:%.*]] = sub nsw i64 [[TMP2]], [[N_MOD_VF]]
; CHECK-NEXT:    [[TMP7:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[TMP8:%.*]] = shl i32 [[TMP7]], 4
; CHECK-NEXT:    [[TMP9:%.*]] = add i32 [[TMP8]], -1
; CHECK-NEXT:    [[VECTOR_RECUR_INIT:%.*]] = insertelement <vscale x 16 x i32> poison, i32 [[PRE_LOAD]], i32 [[TMP9]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[VECTOR_RECUR:%.*]] = phi <vscale x 16 x i32> [ [[VECTOR_RECUR_INIT]], [[VECTOR_PH]] ], [ [[WIDE_LOAD:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP10:%.*]] = or i64 [[INDEX]], 1
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds i32, i32* [[A]], i64 [[TMP10]]
; CHECK-NEXT:    [[TMP12:%.*]] = bitcast i32* [[TMP11]] to <vscale x 16 x i32>*
; CHECK-NEXT:    [[WIDE_LOAD]] = load <vscale x 16 x i32>, <vscale x 16 x i32>* [[TMP12]], align 4
; CHECK-NEXT:    [[TMP13:%.*]] = call <vscale x 16 x i32> @llvm.experimental.vector.splice.nxv16i32(<vscale x 16 x i32> [[VECTOR_RECUR]], <vscale x 16 x i32> [[WIDE_LOAD]], i32 -1)
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds i32, i32* [[B:%.*]], i64 [[INDEX]]
; CHECK-NEXT:    [[TMP15:%.*]] = add <vscale x 16 x i32> [[WIDE_LOAD]], [[TMP13]]
; CHECK-NEXT:    [[TMP16:%.*]] = bitcast i32* [[TMP14]] to <vscale x 16 x i32>*
; CHECK-NEXT:    store <vscale x 16 x i32> [[TMP15]], <vscale x 16 x i32>* [[TMP16]], align 4
; CHECK-NEXT:    [[TMP17:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-NEXT:    [[TMP18:%.*]] = shl i64 [[TMP17]], 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], [[TMP18]]
; CHECK-NEXT:    [[TMP19:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP19]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N_MOD_VF]], 0
; CHECK-NEXT:    [[TMP20:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[TMP21:%.*]] = shl i32 [[TMP20]], 4
; CHECK-NEXT:    [[TMP22:%.*]] = add i32 [[TMP21]], -1
; CHECK-NEXT:    [[VECTOR_RECUR_EXTRACT:%.*]] = extractelement <vscale x 16 x i32> [[WIDE_LOAD]], i32 [[TMP22]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_EXIT:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[SCALAR_RECUR_INIT:%.*]] = phi i32 [ [[PRE_LOAD]], [[FOR_PREHEADER]] ], [ [[VECTOR_RECUR_EXTRACT]], [[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 0, [[FOR_PREHEADER]] ], [ [[N_VEC]], [[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    br label [[SCALAR_BODY:%.*]]
; CHECK:       scalar.body:
; CHECK-NEXT:    [[SCALAR_RECUR:%.*]] = phi i32 [ [[SCALAR_RECUR_INIT]], [[SCALAR_PH]] ], [ [[TMP23:%.*]], [[SCALAR_BODY]] ]
; CHECK-NEXT:    [[INDVARS_IV:%.*]] = phi i64 [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ], [ [[INDVARS_IV_NEXT:%.*]], [[SCALAR_BODY]] ]
; CHECK-NEXT:    [[INDVARS_IV_NEXT]] = add nuw nsw i64 [[INDVARS_IV]], 1
; CHECK-NEXT:    [[ARRAYIDX32:%.*]] = getelementptr inbounds i32, i32* [[A]], i64 [[INDVARS_IV_NEXT]]
; CHECK-NEXT:    [[TMP23]] = load i32, i32* [[ARRAYIDX32]], align 4
; CHECK-NEXT:    [[ARRAYIDX34:%.*]] = getelementptr inbounds i32, i32* [[B]], i64 [[INDVARS_IV]]
; CHECK-NEXT:    [[ADD35:%.*]] = add i32 [[TMP23]], [[SCALAR_RECUR]]
; CHECK-NEXT:    store i32 [[ADD35]], i32* [[ARRAYIDX34]], align 4
; CHECK-NEXT:    [[LFTR_WIDEIV:%.*]] = trunc i64 [[INDVARS_IV_NEXT]] to i32
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i32 [[LFTR_WIDEIV]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_EXIT]], label [[SCALAR_BODY]], !llvm.loop [[LOOP2:![0-9]+]]
; CHECK:       for.exit:
; CHECK-NEXT:    ret void
;
; CHECK1-LABEL: @recurrence_1(
; CHECK1-NEXT:  entry:
; CHECK1-NEXT:    br label [[FOR_PREHEADER:%.*]]
; CHECK1:       for.preheader:
; CHECK1-NEXT:    [[PRE_LOAD:%.*]] = load i32, i32* [[A:%.*]], align 4
; CHECK1-NEXT:    [[TMP0:%.*]] = add i32 [[N:%.*]], -1
; CHECK1-NEXT:    [[TMP1:%.*]] = zext i32 [[TMP0]] to i64
; CHECK1-NEXT:    [[TMP2:%.*]] = add nuw nsw i64 [[TMP1]], 1
; CHECK1-NEXT:    [[TMP3:%.*]] = call i64 @llvm.vscale.i64()
; CHECK1-NEXT:    [[TMP4:%.*]] = shl i64 [[TMP3]], 1
; CHECK1-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[TMP2]], [[TMP4]]
; CHECK1-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK1:       vector.ph:
; CHECK1-NEXT:    [[TMP5:%.*]] = call i64 @llvm.vscale.i64()
; CHECK1-NEXT:    [[TMP6:%.*]] = shl i64 [[TMP5]], 1
; CHECK1-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[TMP2]], [[TMP6]]
; CHECK1-NEXT:    [[N_VEC:%.*]] = sub nsw i64 [[TMP2]], [[N_MOD_VF]]
; CHECK1-NEXT:    [[TMP7:%.*]] = call i32 @llvm.vscale.i32()
; CHECK1-NEXT:    [[TMP8:%.*]] = shl i32 [[TMP7]], 1
; CHECK1-NEXT:    [[TMP9:%.*]] = add i32 [[TMP8]], -1
; CHECK1-NEXT:    [[VECTOR_RECUR_INIT:%.*]] = insertelement <vscale x 2 x i32> poison, i32 [[PRE_LOAD]], i32 [[TMP9]]
; CHECK1-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK1:       vector.body:
; CHECK1-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK1-NEXT:    [[VECTOR_RECUR:%.*]] = phi <vscale x 2 x i32> [ [[VECTOR_RECUR_INIT]], [[VECTOR_PH]] ], [ [[WIDE_LOAD:%.*]], [[VECTOR_BODY]] ]
; CHECK1-NEXT:    [[TMP10:%.*]] = or i64 [[INDEX]], 1
; CHECK1-NEXT:    [[TMP11:%.*]] = getelementptr inbounds i32, i32* [[A]], i64 [[TMP10]]
; CHECK1-NEXT:    [[TMP12:%.*]] = bitcast i32* [[TMP11]] to <vscale x 2 x i32>*
; CHECK1-NEXT:    [[WIDE_LOAD]] = load <vscale x 2 x i32>, <vscale x 2 x i32>* [[TMP12]], align 4
; CHECK1-NEXT:    [[TMP13:%.*]] = call <vscale x 2 x i32> @llvm.experimental.vector.splice.nxv2i32(<vscale x 2 x i32> [[VECTOR_RECUR]], <vscale x 2 x i32> [[WIDE_LOAD]], i32 -1)
; CHECK1-NEXT:    [[TMP14:%.*]] = getelementptr inbounds i32, i32* [[B:%.*]], i64 [[INDEX]]
; CHECK1-NEXT:    [[TMP15:%.*]] = add <vscale x 2 x i32> [[WIDE_LOAD]], [[TMP13]]
; CHECK1-NEXT:    [[TMP16:%.*]] = bitcast i32* [[TMP14]] to <vscale x 2 x i32>*
; CHECK1-NEXT:    store <vscale x 2 x i32> [[TMP15]], <vscale x 2 x i32>* [[TMP16]], align 4
; CHECK1-NEXT:    [[TMP17:%.*]] = call i64 @llvm.vscale.i64()
; CHECK1-NEXT:    [[TMP18:%.*]] = shl i64 [[TMP17]], 1
; CHECK1-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], [[TMP18]]
; CHECK1-NEXT:    [[TMP19:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK1-NEXT:    br i1 [[TMP19]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
; CHECK1:       middle.block:
; CHECK1-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N_MOD_VF]], 0
; CHECK1-NEXT:    [[TMP20:%.*]] = call i32 @llvm.vscale.i32()
; CHECK1-NEXT:    [[TMP21:%.*]] = shl i32 [[TMP20]], 1
; CHECK1-NEXT:    [[TMP22:%.*]] = add i32 [[TMP21]], -1
; CHECK1-NEXT:    [[VECTOR_RECUR_EXTRACT:%.*]] = extractelement <vscale x 2 x i32> [[WIDE_LOAD]], i32 [[TMP22]]
; CHECK1-NEXT:    br i1 [[CMP_N]], label [[FOR_EXIT:%.*]], label [[SCALAR_PH]]
; CHECK1:       scalar.ph:
; CHECK1-NEXT:    [[SCALAR_RECUR_INIT:%.*]] = phi i32 [ [[PRE_LOAD]], [[FOR_PREHEADER]] ], [ [[VECTOR_RECUR_EXTRACT]], [[MIDDLE_BLOCK]] ]
; CHECK1-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 0, [[FOR_PREHEADER]] ], [ [[N_VEC]], [[MIDDLE_BLOCK]] ]
; CHECK1-NEXT:    br label [[SCALAR_BODY:%.*]]
; CHECK1:       scalar.body:
; CHECK1-NEXT:    [[SCALAR_RECUR:%.*]] = phi i32 [ [[SCALAR_RECUR_INIT]], [[SCALAR_PH]] ], [ [[TMP23:%.*]], [[SCALAR_BODY]] ]
; CHECK1-NEXT:    [[INDVARS_IV:%.*]] = phi i64 [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ], [ [[INDVARS_IV_NEXT:%.*]], [[SCALAR_BODY]] ]
; CHECK1-NEXT:    [[INDVARS_IV_NEXT]] = add nuw nsw i64 [[INDVARS_IV]], 1
; CHECK1-NEXT:    [[ARRAYIDX32:%.*]] = getelementptr inbounds i32, i32* [[A]], i64 [[INDVARS_IV_NEXT]]
; CHECK1-NEXT:    [[TMP23]] = load i32, i32* [[ARRAYIDX32]], align 4
; CHECK1-NEXT:    [[ARRAYIDX34:%.*]] = getelementptr inbounds i32, i32* [[B]], i64 [[INDVARS_IV]]
; CHECK1-NEXT:    [[ADD35:%.*]] = add i32 [[TMP23]], [[SCALAR_RECUR]]
; CHECK1-NEXT:    store i32 [[ADD35]], i32* [[ARRAYIDX34]], align 4
; CHECK1-NEXT:    [[LFTR_WIDEIV:%.*]] = trunc i64 [[INDVARS_IV_NEXT]] to i32
; CHECK1-NEXT:    [[EXITCOND:%.*]] = icmp eq i32 [[LFTR_WIDEIV]], [[N]]
; CHECK1-NEXT:    br i1 [[EXITCOND]], label [[FOR_EXIT]], label [[SCALAR_BODY]], !llvm.loop [[LOOP2:![0-9]+]]
; CHECK1:       for.exit:
; CHECK1-NEXT:    ret void
;
; CHECKVP-LABEL: @recurrence_1(
; CHECKVP-NEXT:  entry:
; CHECKVP-NEXT:    br label [[FOR_PREHEADER:%.*]]
; CHECKVP:       for.preheader:
; CHECKVP-NEXT:    [[TMP0:%.*]] = add i32 [[N:%.*]], -1
; CHECKVP-NEXT:    [[TMP1:%.*]] = zext i32 [[TMP0]] to i64
; CHECKVP-NEXT:    [[TMP2:%.*]] = add nuw nsw i64 [[TMP1]], 1
; CHECKVP-NEXT:    br i1 false, label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECKVP:       vector.ph:
; CHECKVP-NEXT:    [[PRE_LOAD:%.*]] = load i32, i32* [[A:%.*]], align 4
; CHECKVP-NEXT:    [[TMP3:%.*]] = call i32 @llvm.vscale.i32()
; CHECKVP-NEXT:    [[TMP4:%.*]] = shl i32 [[TMP3]], 1
; CHECKVP-NEXT:    [[TMP5:%.*]] = add i32 [[TMP4]], -1
; CHECKVP-NEXT:    [[VECTOR_RECUR_INIT:%.*]] = insertelement <vscale x 2 x i32> poison, i32 [[PRE_LOAD]], i32 [[TMP5]]
; CHECKVP-NEXT:    [[TMP6:%.*]] = call i32 @llvm.vscale.i32()
; CHECKVP-NEXT:    [[TMP7:%.*]] = shl i32 [[TMP6]], 1
; CHECKVP-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECKVP:       vector.body:
; CHECKVP-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECKVP-NEXT:    [[VECTOR_RECUR:%.*]] = phi <vscale x 2 x i32> [ [[VECTOR_RECUR_INIT]], [[VECTOR_PH]] ], [ [[VP_OP_LOAD:%.*]], [[VECTOR_BODY]] ]
; CHECKVP-NEXT:    [[PREV_EVL:%.*]] = phi i32 [ [[TMP7]], [[VECTOR_PH]] ], [ [[TMP12:%.*]], [[VECTOR_BODY]] ]
; CHECKVP-NEXT:    [[TMP8:%.*]] = add nuw nsw i64 [[INDEX]], 1
; CHECKVP-NEXT:    [[TMP9:%.*]] = getelementptr inbounds i32, i32* [[A]], i64 [[TMP8]]
; CHECKVP-NEXT:    [[TMP10:%.*]] = sub i64 [[TMP2]], [[INDEX]]
; CHECKVP-NEXT:    [[TMP11:%.*]] = call i64 @llvm.epi.vsetvl(i64 [[TMP10]], i64 2, i64 0)
; CHECKVP-NEXT:    [[TMP12]] = trunc i64 [[TMP11]] to i32
; CHECKVP-NEXT:    [[TMP13:%.*]] = bitcast i32* [[TMP9]] to <vscale x 2 x i32>*
; CHECKVP-NEXT:    [[VP_OP_LOAD]] = call <vscale x 2 x i32> @llvm.vp.load.nxv2i32.p0nxv2i32(<vscale x 2 x i32>* nonnull [[TMP13]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP12]])
; CHECKVP-NEXT:    [[TMP14:%.*]] = call i32 @llvm.vscale.i32()
; CHECKVP-NEXT:    [[TMP15:%.*]] = shl i32 [[TMP14]], 1
; CHECKVP-NEXT:    [[TMP16:%.*]] = add i32 [[TMP15]], -1
; CHECKVP-NEXT:    [[TMP17:%.*]] = call <vscale x 2 x i32> @llvm.experimental.vp.splice.nxv2i32(<vscale x 2 x i32> [[VECTOR_RECUR]], <vscale x 2 x i32> [[VP_OP_LOAD]], i32 [[TMP16]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[PREV_EVL]], i32 [[TMP12]])
; CHECKVP-NEXT:    [[TMP18:%.*]] = getelementptr inbounds i32, i32* [[B:%.*]], i64 [[INDEX]]
; CHECKVP-NEXT:    [[VP_OP:%.*]] = call <vscale x 2 x i32> @llvm.vp.add.nxv2i32(<vscale x 2 x i32> [[VP_OP_LOAD]], <vscale x 2 x i32> [[TMP17]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP12]])
; CHECKVP-NEXT:    [[TMP19:%.*]] = bitcast i32* [[TMP18]] to <vscale x 2 x i32>*
; CHECKVP-NEXT:    call void @llvm.vp.store.nxv2i32.p0nxv2i32(<vscale x 2 x i32> [[VP_OP]], <vscale x 2 x i32>* [[TMP19]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP12]])
; CHECKVP-NEXT:    [[TMP20:%.*]] = and i64 [[TMP11]], 4294967295
; CHECKVP-NEXT:    [[INDEX_NEXT]] = add i64 [[INDEX]], [[TMP20]]
; CHECKVP-NEXT:    [[TMP21:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[TMP2]]
; CHECKVP-NEXT:    br i1 [[TMP21]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
; CHECKVP:       middle.block:
; CHECKVP-NEXT:    br i1 true, label [[FOR_EXIT:%.*]], label [[SCALAR_PH]]
; CHECKVP:       scalar.ph:
; CHECKVP-NEXT:    br label [[SCALAR_BODY:%.*]]
; CHECKVP:       scalar.body:
; CHECKVP-NEXT:    br i1 undef, label [[FOR_EXIT]], label [[SCALAR_BODY]], !llvm.loop [[LOOP2:![0-9]+]]
; CHECKVP:       for.exit:
; CHECKVP-NEXT:    ret void
;
entry:
  br label %for.preheader

for.preheader:
  %arrayidx.phi.trans.insert = getelementptr inbounds i32, i32* %a, i64 0
  %pre_load = load i32, i32* %arrayidx.phi.trans.insert
  br label %scalar.body

scalar.body:
  %0 = phi i32 [ %pre_load, %for.preheader ], [ %1, %scalar.body ]
  %indvars.iv = phi i64 [ 0, %for.preheader ], [ %indvars.iv.next, %scalar.body ]
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %arrayidx32 = getelementptr inbounds i32, i32* %a, i64 %indvars.iv.next
  %1 = load i32, i32* %arrayidx32
  %arrayidx34 = getelementptr inbounds i32, i32* %b, i64 %indvars.iv
  %add35 = add i32 %1, %0
  store i32 %add35, i32* %arrayidx34
  %lftr.wideiv = trunc i64 %indvars.iv.next to i32
  %exitcond = icmp eq i32 %lftr.wideiv, %n
  br i1 %exitcond, label %for.exit, label %scalar.body

for.exit:
  ret void
}
