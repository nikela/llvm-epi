; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt -mtriple riscv64 -mattr +m,+a,+f,+d,+zepi \
; RUN:    -scalable-vectorization=only -prefer-predicate-over-epilogue=predicate-dont-vectorize \
; RUN:    -S -loop-vectorize -riscv-v-vector-bits-min=64 -instcombine -simplifycfg < %s -o - \
; RUN:    | FileCheck --check-prefix=GATHER %s
; RUN: opt -mtriple riscv64 -mattr +m,+a,+f,+d,+zepi \
; RUN:    -scalable-vectorization=only -prefer-predicate-over-epilogue=predicate-dont-vectorize \
; RUN:    -vectorizer-use-vp-strided-load-store \
; RUN:    -S -loop-vectorize -riscv-v-vector-bits-min=64 -instcombine -simplifycfg < %s -o - \
; RUN:    | FileCheck --check-prefix=STRIDED %s

; ModuleID = 'nostride.c'
source_filename = "nostride.c"
target datalayout = "e-m:e-p:64:64-i64:64-i128:128-n64-S128-v128:128:128-v256:128:128-v512:128:128-v1024:128:128"
target triple = "riscv64-unknown-linux-gnu"

; #include <complex.h>
;
; void foo(int n, int j, float complex temp1, float complex temp2,
;          float complex x[n], float complex y[n], float complex A[n][n]) {
; #pragma clang loop vectorize(assume_safety)
;   for (int i = 0; i <= j - 1; i++) {
;     A[i][j] = A[i][j] + x[i] * temp1 + y[i] * temp2;
;   }
; }

; Function Attrs: nofree norecurse nosync nounwind
define dso_local void @foo(i32 signext %n, i32 signext %j, float %temp1.coerce0, float %temp1.coerce1, float %temp2.coerce0, float %temp2.coerce1, { float, float }* nocapture readonly %x, { float, float }* nocapture readonly %y, { float, float }* nocapture %A) local_unnamed_addr #0 {
; GATHER-LABEL: @foo(
; GATHER-NEXT:  entry:
; GATHER-NEXT:    [[TMP0:%.*]] = zext i32 [[N:%.*]] to i64
; GATHER-NEXT:    [[CMP_NOT_NOT40:%.*]] = icmp sgt i32 [[J:%.*]], 0
; GATHER-NEXT:    br i1 [[CMP_NOT_NOT40]], label [[FOR_BODY_PREHEADER:%.*]], label [[FOR_COND_CLEANUP:%.*]]
; GATHER:       for.body.preheader:
; GATHER-NEXT:    [[IDXPROM1:%.*]] = sext i32 [[J]] to i64
; GATHER-NEXT:    [[WIDE_TRIP_COUNT:%.*]] = zext i32 [[J]] to i64
; GATHER-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[A:%.*]], i64 [[IDXPROM1]]
; GATHER-NEXT:    [[TMP1:%.*]] = xor i64 [[WIDE_TRIP_COUNT]], -1
; GATHER-NEXT:    [[TMP2:%.*]] = call i64 @llvm.vscale.i64()
; GATHER-NEXT:    [[TMP3:%.*]] = shl i64 [[TMP2]], 1
; GATHER-NEXT:    [[TMP4:%.*]] = icmp ugt i64 [[TMP3]], [[TMP1]]
; GATHER-NEXT:    br i1 [[TMP4]], label [[FOR_BODY:%.*]], label [[VECTOR_PH:%.*]]
; GATHER:       vector.ph:
; GATHER-NEXT:    [[TMP5:%.*]] = call <vscale x 2 x i64> @llvm.experimental.stepvector.nxv2i64()
; GATHER-NEXT:    [[TMP6:%.*]] = call i64 @llvm.vscale.i64()
; GATHER-NEXT:    [[TMP7:%.*]] = shl i64 [[TMP6]], 1
; GATHER-NEXT:    [[DOTSPLATINSERT:%.*]] = insertelement <vscale x 2 x i64> poison, i64 [[TMP7]], i64 0
; GATHER-NEXT:    [[DOTSPLAT:%.*]] = shufflevector <vscale x 2 x i64> [[DOTSPLATINSERT]], <vscale x 2 x i64> poison, <vscale x 2 x i32> zeroinitializer
; GATHER-NEXT:    [[BROADCAST_SPLATINSERT1:%.*]] = insertelement <vscale x 2 x i64> poison, i64 [[TMP0]], i64 0
; GATHER-NEXT:    [[BROADCAST_SPLAT2:%.*]] = shufflevector <vscale x 2 x i64> [[BROADCAST_SPLATINSERT1]], <vscale x 2 x i64> poison, <vscale x 2 x i32> zeroinitializer
; GATHER-NEXT:    [[BROADCAST_SPLATINSERT6:%.*]] = insertelement <vscale x 2 x float> poison, float [[TEMP2_COERCE1:%.*]], i64 0
; GATHER-NEXT:    [[BROADCAST_SPLAT7:%.*]] = shufflevector <vscale x 2 x float> [[BROADCAST_SPLATINSERT6]], <vscale x 2 x float> poison, <vscale x 2 x i32> zeroinitializer
; GATHER-NEXT:    [[BROADCAST_SPLATINSERT9:%.*]] = insertelement <vscale x 2 x float> poison, float [[TEMP2_COERCE0:%.*]], i64 0
; GATHER-NEXT:    [[BROADCAST_SPLAT10:%.*]] = shufflevector <vscale x 2 x float> [[BROADCAST_SPLATINSERT9]], <vscale x 2 x float> poison, <vscale x 2 x i32> zeroinitializer
; GATHER-NEXT:    [[BROADCAST_SPLATINSERT15:%.*]] = insertelement <vscale x 2 x float> poison, float [[TEMP1_COERCE1:%.*]], i64 0
; GATHER-NEXT:    [[BROADCAST_SPLAT16:%.*]] = shufflevector <vscale x 2 x float> [[BROADCAST_SPLATINSERT15]], <vscale x 2 x float> poison, <vscale x 2 x i32> zeroinitializer
; GATHER-NEXT:    [[BROADCAST_SPLATINSERT18:%.*]] = insertelement <vscale x 2 x float> poison, float [[TEMP1_COERCE0:%.*]], i64 0
; GATHER-NEXT:    [[BROADCAST_SPLAT19:%.*]] = shufflevector <vscale x 2 x float> [[BROADCAST_SPLATINSERT18]], <vscale x 2 x float> poison, <vscale x 2 x i32> zeroinitializer
; GATHER-NEXT:    br label [[VECTOR_BODY:%.*]]
; GATHER:       vector.body:
; GATHER-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; GATHER-NEXT:    [[VEC_IND:%.*]] = phi <vscale x 2 x i64> [ [[TMP5]], [[VECTOR_PH]] ], [ [[VEC_IND_NEXT:%.*]], [[VECTOR_BODY]] ]
; GATHER-NEXT:    [[TMP8:%.*]] = sub i64 [[WIDE_TRIP_COUNT]], [[INDEX]]
; GATHER-NEXT:    [[TMP9:%.*]] = call i64 @llvm.epi.vsetvl(i64 [[TMP8]], i64 2, i64 0)
; GATHER-NEXT:    [[TMP10:%.*]] = trunc i64 [[TMP9]] to i32
; GATHER-NEXT:    [[VP_OP:%.*]] = call <vscale x 2 x i64> @llvm.vp.mul.nxv2i64(<vscale x 2 x i64> [[VEC_IND]], <vscale x 2 x i64> [[BROADCAST_SPLAT2]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]])
; GATHER-NEXT:    [[TMP11:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[ARRAYIDX]], <vscale x 2 x i64> [[VP_OP]]
; GATHER-NEXT:    [[TMP12:%.*]] = getelementptr inbounds { float, float }, <vscale x 2 x { float, float }*> [[TMP11]], i64 0, i32 0
; GATHER-NEXT:    [[TMP13:%.*]] = getelementptr inbounds { float, float }, <vscale x 2 x { float, float }*> [[TMP11]], i64 0, i32 1
; GATHER-NEXT:    [[TMP14:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[X:%.*]], <vscale x 2 x i64> [[VEC_IND]], i32 0
; GATHER-NEXT:    [[VP_GATHER:%.*]] = call <vscale x 2 x float> @llvm.vp.gather.nxv2f32.nxv2p0f32(<vscale x 2 x float*> [[TMP14]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]])
; GATHER-NEXT:    [[TMP15:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[X]], <vscale x 2 x i64> [[VEC_IND]], i32 1
; GATHER-NEXT:    [[VP_GATHER3:%.*]] = call <vscale x 2 x float> @llvm.vp.gather.nxv2f32.nxv2p0f32(<vscale x 2 x float*> [[TMP15]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]])
; GATHER-NEXT:    [[TMP16:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[Y:%.*]], <vscale x 2 x i64> [[VEC_IND]], i32 0
; GATHER-NEXT:    [[VP_GATHER4:%.*]] = call <vscale x 2 x float> @llvm.vp.gather.nxv2f32.nxv2p0f32(<vscale x 2 x float*> [[TMP16]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]])
; GATHER-NEXT:    [[TMP17:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[Y]], <vscale x 2 x i64> [[VEC_IND]], i32 1
; GATHER-NEXT:    [[VP_GATHER5:%.*]] = call <vscale x 2 x float> @llvm.vp.gather.nxv2f32.nxv2p0f32(<vscale x 2 x float*> [[TMP17]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]])
; GATHER-NEXT:    [[VP_OP8:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fmul.nxv2f32(<vscale x 2 x float> [[VP_GATHER4]], <vscale x 2 x float> [[BROADCAST_SPLAT7]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]])
; GATHER-NEXT:    [[VP_OP11:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fmul.nxv2f32(<vscale x 2 x float> [[VP_GATHER5]], <vscale x 2 x float> [[BROADCAST_SPLAT10]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]])
; GATHER-NEXT:    [[VP_OP12:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fmul.nxv2f32(<vscale x 2 x float> [[VP_GATHER4]], <vscale x 2 x float> [[BROADCAST_SPLAT10]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]])
; GATHER-NEXT:    [[VP_OP13:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fmul.nxv2f32(<vscale x 2 x float> [[VP_GATHER5]], <vscale x 2 x float> [[BROADCAST_SPLAT7]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]])
; GATHER-NEXT:    [[VP_GATHER14:%.*]] = call <vscale x 2 x float> @llvm.vp.gather.nxv2f32.nxv2p0f32(<vscale x 2 x float*> [[TMP13]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]])
; GATHER-NEXT:    [[VP_OP17:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fmul.nxv2f32(<vscale x 2 x float> [[VP_GATHER]], <vscale x 2 x float> [[BROADCAST_SPLAT16]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]])
; GATHER-NEXT:    [[VP_OP20:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fmul.nxv2f32(<vscale x 2 x float> [[VP_GATHER3]], <vscale x 2 x float> [[BROADCAST_SPLAT19]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]])
; GATHER-NEXT:    [[VP_GATHER21:%.*]] = call <vscale x 2 x float> @llvm.vp.gather.nxv2f32.nxv2p0f32(<vscale x 2 x float*> [[TMP12]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]])
; GATHER-NEXT:    [[VP_OP22:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fmul.nxv2f32(<vscale x 2 x float> [[VP_GATHER]], <vscale x 2 x float> [[BROADCAST_SPLAT19]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]])
; GATHER-NEXT:    [[VP_OP23:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fmul.nxv2f32(<vscale x 2 x float> [[VP_GATHER3]], <vscale x 2 x float> [[BROADCAST_SPLAT16]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]])
; GATHER-NEXT:    [[VP_OP24:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fadd.nxv2f32(<vscale x 2 x float> [[VP_OP13]], <vscale x 2 x float> [[VP_OP23]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]])
; GATHER-NEXT:    [[VP_OP25:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fadd.nxv2f32(<vscale x 2 x float> [[VP_OP12]], <vscale x 2 x float> [[VP_OP22]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]])
; GATHER-NEXT:    [[VP_OP26:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fadd.nxv2f32(<vscale x 2 x float> [[VP_OP25]], <vscale x 2 x float> [[VP_GATHER21]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]])
; GATHER-NEXT:    [[VP_OP27:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fsub.nxv2f32(<vscale x 2 x float> [[VP_OP26]], <vscale x 2 x float> [[VP_OP24]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]])
; GATHER-NEXT:    [[VP_OP28:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fadd.nxv2f32(<vscale x 2 x float> [[VP_OP20]], <vscale x 2 x float> [[VP_OP17]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]])
; GATHER-NEXT:    [[VP_OP29:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fadd.nxv2f32(<vscale x 2 x float> [[VP_OP28]], <vscale x 2 x float> [[VP_OP8]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]])
; GATHER-NEXT:    [[VP_OP30:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fadd.nxv2f32(<vscale x 2 x float> [[VP_OP29]], <vscale x 2 x float> [[VP_GATHER14]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]])
; GATHER-NEXT:    [[VP_OP31:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fadd.nxv2f32(<vscale x 2 x float> [[VP_OP30]], <vscale x 2 x float> [[VP_OP11]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]])
; GATHER-NEXT:    call void @llvm.vp.scatter.nxv2f32.nxv2p0f32(<vscale x 2 x float> [[VP_OP27]], <vscale x 2 x float*> [[TMP12]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]]), !llvm.access.group [[ACC_GRP4:![0-9]+]]
; GATHER-NEXT:    call void @llvm.vp.scatter.nxv2f32.nxv2p0f32(<vscale x 2 x float> [[VP_OP31]], <vscale x 2 x float*> [[TMP13]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP10]]), !llvm.access.group [[ACC_GRP4]]
; GATHER-NEXT:    [[TMP18:%.*]] = and i64 [[TMP9]], 4294967295
; GATHER-NEXT:    [[INDEX_NEXT]] = add i64 [[INDEX]], [[TMP18]]
; GATHER-NEXT:    [[VEC_IND_NEXT]] = add <vscale x 2 x i64> [[VEC_IND]], [[DOTSPLAT]]
; GATHER-NEXT:    [[TMP19:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[WIDE_TRIP_COUNT]]
; GATHER-NEXT:    br i1 [[TMP19]], label [[FOR_COND_CLEANUP]], label [[VECTOR_BODY]], !llvm.loop [[LOOP5:![0-9]+]]
; GATHER:       for.cond.cleanup:
; GATHER-NEXT:    ret void
; GATHER:       for.body:
; GATHER-NEXT:    [[INDVARS_IV:%.*]] = phi i64 [ [[INDVARS_IV_NEXT:%.*]], [[FOR_BODY]] ], [ 0, [[FOR_BODY_PREHEADER]] ]
; GATHER-NEXT:    [[TMP20:%.*]] = mul nuw nsw i64 [[INDVARS_IV]], [[TMP0]]
; GATHER-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[ARRAYIDX]], i64 [[TMP20]]
; GATHER-NEXT:    [[ARRAYIDX2_REALP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[ARRAYIDX2]], i64 0, i32 0
; GATHER-NEXT:    [[ARRAYIDX2_IMAGP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[ARRAYIDX2]], i64 0, i32 1
; GATHER-NEXT:    [[ARRAYIDX4_REALP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[X]], i64 [[INDVARS_IV]], i32 0
; GATHER-NEXT:    [[ARRAYIDX4_REAL:%.*]] = load float, float* [[ARRAYIDX4_REALP]], align 4, !llvm.access.group [[ACC_GRP4]]
; GATHER-NEXT:    [[ARRAYIDX4_IMAGP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[X]], i64 [[INDVARS_IV]], i32 1
; GATHER-NEXT:    [[ARRAYIDX4_IMAG:%.*]] = load float, float* [[ARRAYIDX4_IMAGP]], align 4, !llvm.access.group [[ACC_GRP4]]
; GATHER-NEXT:    [[ARRAYIDX7_REALP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[Y]], i64 [[INDVARS_IV]], i32 0
; GATHER-NEXT:    [[ARRAYIDX7_REAL:%.*]] = load float, float* [[ARRAYIDX7_REALP]], align 4, !llvm.access.group [[ACC_GRP4]]
; GATHER-NEXT:    [[ARRAYIDX7_IMAGP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[Y]], i64 [[INDVARS_IV]], i32 1
; GATHER-NEXT:    [[ARRAYIDX7_IMAG:%.*]] = load float, float* [[ARRAYIDX7_IMAGP]], align 4, !llvm.access.group [[ACC_GRP4]]
; GATHER-NEXT:    [[MUL_AD10:%.*]] = fmul fast float [[ARRAYIDX7_REAL]], [[TEMP2_COERCE1]]
; GATHER-NEXT:    [[MUL_BC11:%.*]] = fmul fast float [[ARRAYIDX7_IMAG]], [[TEMP2_COERCE0]]
; GATHER-NEXT:    [[MUL_AC8:%.*]] = fmul fast float [[ARRAYIDX7_REAL]], [[TEMP2_COERCE0]]
; GATHER-NEXT:    [[MUL_BD9_NEG:%.*]] = fmul fast float [[ARRAYIDX7_IMAG]], [[TEMP2_COERCE1]]
; GATHER-NEXT:    [[ARRAYIDX2_IMAG:%.*]] = load float, float* [[ARRAYIDX2_IMAGP]], align 4, !llvm.access.group [[ACC_GRP4]]
; GATHER-NEXT:    [[MUL_AD:%.*]] = fmul fast float [[ARRAYIDX4_REAL]], [[TEMP1_COERCE1]]
; GATHER-NEXT:    [[MUL_BC:%.*]] = fmul fast float [[ARRAYIDX4_IMAG]], [[TEMP1_COERCE0]]
; GATHER-NEXT:    [[ARRAYIDX2_REAL:%.*]] = load float, float* [[ARRAYIDX2_REALP]], align 4, !llvm.access.group [[ACC_GRP4]]
; GATHER-NEXT:    [[MUL_AC:%.*]] = fmul fast float [[ARRAYIDX4_REAL]], [[TEMP1_COERCE0]]
; GATHER-NEXT:    [[MUL_BD_NEG:%.*]] = fmul fast float [[ARRAYIDX4_IMAG]], [[TEMP1_COERCE1]]
; GATHER-NEXT:    [[REASS_ADD:%.*]] = fadd fast float [[MUL_BD9_NEG]], [[MUL_BD_NEG]]
; GATHER-NEXT:    [[ADD_R:%.*]] = fadd fast float [[MUL_AC8]], [[MUL_AC]]
; GATHER-NEXT:    [[MUL_R12:%.*]] = fadd fast float [[ADD_R]], [[ARRAYIDX2_REAL]]
; GATHER-NEXT:    [[ADD_R22:%.*]] = fsub fast float [[MUL_R12]], [[REASS_ADD]]
; GATHER-NEXT:    [[MUL_I13:%.*]] = fadd fast float [[MUL_BC]], [[MUL_AD]]
; GATHER-NEXT:    [[MUL_I:%.*]] = fadd fast float [[MUL_I13]], [[MUL_AD10]]
; GATHER-NEXT:    [[ADD_I:%.*]] = fadd fast float [[MUL_I]], [[ARRAYIDX2_IMAG]]
; GATHER-NEXT:    [[ADD_I23:%.*]] = fadd fast float [[ADD_I]], [[MUL_BC11]]
; GATHER-NEXT:    store float [[ADD_R22]], float* [[ARRAYIDX2_REALP]], align 4, !llvm.access.group [[ACC_GRP4]]
; GATHER-NEXT:    store float [[ADD_I23]], float* [[ARRAYIDX2_IMAGP]], align 4, !llvm.access.group [[ACC_GRP4]]
; GATHER-NEXT:    [[INDVARS_IV_NEXT]] = add nuw nsw i64 [[INDVARS_IV]], 1
; GATHER-NEXT:    [[EXITCOND_NOT:%.*]] = icmp eq i64 [[INDVARS_IV_NEXT]], [[WIDE_TRIP_COUNT]]
; GATHER-NEXT:    br i1 [[EXITCOND_NOT]], label [[FOR_COND_CLEANUP]], label [[FOR_BODY]], !llvm.loop [[LOOP9:![0-9]+]]
;
; STRIDED-LABEL: @foo(
; STRIDED-NEXT:  entry:
; STRIDED-NEXT:    [[TMP0:%.*]] = zext i32 [[N:%.*]] to i64
; STRIDED-NEXT:    [[CMP_NOT_NOT40:%.*]] = icmp sgt i32 [[J:%.*]], 0
; STRIDED-NEXT:    br i1 [[CMP_NOT_NOT40]], label [[FOR_BODY_PREHEADER:%.*]], label [[FOR_COND_CLEANUP:%.*]]
; STRIDED:       for.body.preheader:
; STRIDED-NEXT:    [[IDXPROM1:%.*]] = sext i32 [[J]] to i64
; STRIDED-NEXT:    [[WIDE_TRIP_COUNT:%.*]] = zext i32 [[J]] to i64
; STRIDED-NEXT:    [[ARRAYIDX:%.*]] = getelementptr { float, float }, { float, float }* [[A:%.*]], i64 [[IDXPROM1]]
; STRIDED-NEXT:    [[TMP1:%.*]] = xor i64 [[WIDE_TRIP_COUNT]], -1
; STRIDED-NEXT:    [[TMP2:%.*]] = call i64 @llvm.vscale.i64()
; STRIDED-NEXT:    [[TMP3:%.*]] = shl i64 [[TMP2]], 1
; STRIDED-NEXT:    [[TMP4:%.*]] = icmp ugt i64 [[TMP3]], [[TMP1]]
; STRIDED-NEXT:    br i1 [[TMP4]], label [[FOR_BODY:%.*]], label [[VECTOR_PH:%.*]]
; STRIDED:       vector.ph:
; STRIDED-NEXT:    [[SCEVGEP:%.*]] = getelementptr { float, float }, { float, float }* [[X:%.*]], i64 0, i32 1
; STRIDED-NEXT:    [[SCEVGEP6:%.*]] = getelementptr { float, float }, { float, float }* [[Y:%.*]], i64 0, i32 1
; STRIDED-NEXT:    [[BROADCAST_SPLATINSERT9:%.*]] = insertelement <vscale x 2 x float> poison, float [[TEMP2_COERCE1:%.*]], i64 0
; STRIDED-NEXT:    [[BROADCAST_SPLAT10:%.*]] = shufflevector <vscale x 2 x float> [[BROADCAST_SPLATINSERT9]], <vscale x 2 x float> poison, <vscale x 2 x i32> zeroinitializer
; STRIDED-NEXT:    [[BROADCAST_SPLATINSERT12:%.*]] = insertelement <vscale x 2 x float> poison, float [[TEMP2_COERCE0:%.*]], i64 0
; STRIDED-NEXT:    [[BROADCAST_SPLAT13:%.*]] = shufflevector <vscale x 2 x float> [[BROADCAST_SPLATINSERT12]], <vscale x 2 x float> poison, <vscale x 2 x i32> zeroinitializer
; STRIDED-NEXT:    [[SCEVGEP17:%.*]] = getelementptr { float, float }, { float, float }* [[A]], i64 [[IDXPROM1]], i32 1
; STRIDED-NEXT:    [[TMP5:%.*]] = shl nuw nsw i64 [[TMP0]], 3
; STRIDED-NEXT:    [[BROADCAST_SPLATINSERT20:%.*]] = insertelement <vscale x 2 x float> poison, float [[TEMP1_COERCE1:%.*]], i64 0
; STRIDED-NEXT:    [[BROADCAST_SPLAT21:%.*]] = shufflevector <vscale x 2 x float> [[BROADCAST_SPLATINSERT20]], <vscale x 2 x float> poison, <vscale x 2 x i32> zeroinitializer
; STRIDED-NEXT:    [[BROADCAST_SPLATINSERT23:%.*]] = insertelement <vscale x 2 x float> poison, float [[TEMP1_COERCE0:%.*]], i64 0
; STRIDED-NEXT:    [[BROADCAST_SPLAT24:%.*]] = shufflevector <vscale x 2 x float> [[BROADCAST_SPLATINSERT23]], <vscale x 2 x float> poison, <vscale x 2 x i32> zeroinitializer
; STRIDED-NEXT:    [[TMP6:%.*]] = shl nuw nsw i64 [[TMP0]], 3
; STRIDED-NEXT:    [[TMP7:%.*]] = shl nuw nsw i64 [[TMP0]], 3
; STRIDED-NEXT:    [[SCEVGEP37:%.*]] = getelementptr { float, float }, { float, float }* [[A]], i64 [[IDXPROM1]], i32 1
; STRIDED-NEXT:    [[TMP8:%.*]] = shl nuw nsw i64 [[TMP0]], 3
; STRIDED-NEXT:    br label [[VECTOR_BODY:%.*]]
; STRIDED:       vector.body:
; STRIDED-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; STRIDED-NEXT:    [[TMP9:%.*]] = sub i64 [[WIDE_TRIP_COUNT]], [[INDEX]]
; STRIDED-NEXT:    [[TMP10:%.*]] = call i64 @llvm.epi.vsetvl(i64 [[TMP9]], i64 2, i64 0)
; STRIDED-NEXT:    [[TMP11:%.*]] = trunc i64 [[TMP10]] to i32
; STRIDED-NEXT:    [[TMP12:%.*]] = getelementptr { float, float }, { float, float }* [[X]], i64 [[INDEX]], i32 0
; STRIDED-NEXT:    [[VP_STRIDED_LOAD:%.*]] = call <vscale x 2 x float> @llvm.experimental.vp.strided.load.nxv2f32.p0f32.i64(float* [[TMP12]], i64 8, <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]])
; STRIDED-NEXT:    [[TMP13:%.*]] = shl i64 [[INDEX]], 1
; STRIDED-NEXT:    [[TMP14:%.*]] = getelementptr float, float* [[SCEVGEP]], i64 [[TMP13]]
; STRIDED-NEXT:    [[VP_STRIDED_LOAD4:%.*]] = call <vscale x 2 x float> @llvm.experimental.vp.strided.load.nxv2f32.p0f32.i64(float* [[TMP14]], i64 8, <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]])
; STRIDED-NEXT:    [[TMP15:%.*]] = getelementptr { float, float }, { float, float }* [[Y]], i64 [[INDEX]], i32 0
; STRIDED-NEXT:    [[VP_STRIDED_LOAD5:%.*]] = call <vscale x 2 x float> @llvm.experimental.vp.strided.load.nxv2f32.p0f32.i64(float* [[TMP15]], i64 8, <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]])
; STRIDED-NEXT:    [[TMP16:%.*]] = shl i64 [[INDEX]], 1
; STRIDED-NEXT:    [[TMP17:%.*]] = getelementptr float, float* [[SCEVGEP6]], i64 [[TMP16]]
; STRIDED-NEXT:    [[VP_STRIDED_LOAD8:%.*]] = call <vscale x 2 x float> @llvm.experimental.vp.strided.load.nxv2f32.p0f32.i64(float* [[TMP17]], i64 8, <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]])
; STRIDED-NEXT:    [[VP_OP11:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fmul.nxv2f32(<vscale x 2 x float> [[VP_STRIDED_LOAD5]], <vscale x 2 x float> [[BROADCAST_SPLAT10]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]])
; STRIDED-NEXT:    [[VP_OP14:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fmul.nxv2f32(<vscale x 2 x float> [[VP_STRIDED_LOAD8]], <vscale x 2 x float> [[BROADCAST_SPLAT13]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]])
; STRIDED-NEXT:    [[VP_OP15:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fmul.nxv2f32(<vscale x 2 x float> [[VP_STRIDED_LOAD5]], <vscale x 2 x float> [[BROADCAST_SPLAT13]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]])
; STRIDED-NEXT:    [[VP_OP16:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fmul.nxv2f32(<vscale x 2 x float> [[VP_STRIDED_LOAD8]], <vscale x 2 x float> [[BROADCAST_SPLAT10]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]])
; STRIDED-NEXT:    [[TMP18:%.*]] = mul i64 [[INDEX]], [[TMP5]]
; STRIDED-NEXT:    [[TMP19:%.*]] = bitcast float* [[SCEVGEP17]] to i8*
; STRIDED-NEXT:    [[TMP20:%.*]] = getelementptr i8, i8* [[TMP19]], i64 [[TMP18]]
; STRIDED-NEXT:    [[TMP21:%.*]] = bitcast i8* [[TMP20]] to float*
; STRIDED-NEXT:    [[VP_STRIDED_LOAD19:%.*]] = call <vscale x 2 x float> @llvm.experimental.vp.strided.load.nxv2f32.p0f32.i64(float* [[TMP21]], i64 [[TMP5]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]])
; STRIDED-NEXT:    [[VP_OP22:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fmul.nxv2f32(<vscale x 2 x float> [[VP_STRIDED_LOAD]], <vscale x 2 x float> [[BROADCAST_SPLAT21]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]])
; STRIDED-NEXT:    [[VP_OP25:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fmul.nxv2f32(<vscale x 2 x float> [[VP_STRIDED_LOAD4]], <vscale x 2 x float> [[BROADCAST_SPLAT24]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]])
; STRIDED-NEXT:    [[TMP22:%.*]] = mul i64 [[INDEX]], [[TMP6]]
; STRIDED-NEXT:    [[TMP23:%.*]] = bitcast { float, float }* [[ARRAYIDX]] to i8*
; STRIDED-NEXT:    [[TMP24:%.*]] = getelementptr i8, i8* [[TMP23]], i64 [[TMP22]]
; STRIDED-NEXT:    [[TMP25:%.*]] = bitcast i8* [[TMP24]] to float*
; STRIDED-NEXT:    [[VP_STRIDED_LOAD26:%.*]] = call <vscale x 2 x float> @llvm.experimental.vp.strided.load.nxv2f32.p0f32.i64(float* [[TMP25]], i64 [[TMP6]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]])
; STRIDED-NEXT:    [[VP_OP27:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fmul.nxv2f32(<vscale x 2 x float> [[VP_STRIDED_LOAD]], <vscale x 2 x float> [[BROADCAST_SPLAT24]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]])
; STRIDED-NEXT:    [[VP_OP28:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fmul.nxv2f32(<vscale x 2 x float> [[VP_STRIDED_LOAD4]], <vscale x 2 x float> [[BROADCAST_SPLAT21]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]])
; STRIDED-NEXT:    [[VP_OP29:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fadd.nxv2f32(<vscale x 2 x float> [[VP_OP16]], <vscale x 2 x float> [[VP_OP28]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]])
; STRIDED-NEXT:    [[VP_OP30:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fadd.nxv2f32(<vscale x 2 x float> [[VP_OP15]], <vscale x 2 x float> [[VP_OP27]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]])
; STRIDED-NEXT:    [[VP_OP31:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fadd.nxv2f32(<vscale x 2 x float> [[VP_OP30]], <vscale x 2 x float> [[VP_STRIDED_LOAD26]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]])
; STRIDED-NEXT:    [[VP_OP32:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fsub.nxv2f32(<vscale x 2 x float> [[VP_OP31]], <vscale x 2 x float> [[VP_OP29]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]])
; STRIDED-NEXT:    [[VP_OP33:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fadd.nxv2f32(<vscale x 2 x float> [[VP_OP25]], <vscale x 2 x float> [[VP_OP22]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]])
; STRIDED-NEXT:    [[VP_OP34:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fadd.nxv2f32(<vscale x 2 x float> [[VP_OP33]], <vscale x 2 x float> [[VP_OP11]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]])
; STRIDED-NEXT:    [[VP_OP35:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fadd.nxv2f32(<vscale x 2 x float> [[VP_OP34]], <vscale x 2 x float> [[VP_STRIDED_LOAD19]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]])
; STRIDED-NEXT:    [[VP_OP36:%.*]] = call fast <vscale x 2 x float> @llvm.vp.fadd.nxv2f32(<vscale x 2 x float> [[VP_OP35]], <vscale x 2 x float> [[VP_OP14]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]])
; STRIDED-NEXT:    [[TMP26:%.*]] = mul i64 [[INDEX]], [[TMP7]]
; STRIDED-NEXT:    [[TMP27:%.*]] = bitcast { float, float }* [[ARRAYIDX]] to i8*
; STRIDED-NEXT:    [[TMP28:%.*]] = getelementptr i8, i8* [[TMP27]], i64 [[TMP26]]
; STRIDED-NEXT:    [[TMP29:%.*]] = bitcast i8* [[TMP28]] to float*
; STRIDED-NEXT:    call void @llvm.experimental.vp.strided.store.nxv2f32.p0f32.i64(<vscale x 2 x float> [[VP_OP32]], float* [[TMP29]], i64 [[TMP7]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]]), !llvm.access.group [[ACC_GRP4:![0-9]+]]
; STRIDED-NEXT:    [[TMP30:%.*]] = mul i64 [[INDEX]], [[TMP8]]
; STRIDED-NEXT:    [[TMP31:%.*]] = bitcast float* [[SCEVGEP37]] to i8*
; STRIDED-NEXT:    [[TMP32:%.*]] = getelementptr i8, i8* [[TMP31]], i64 [[TMP30]]
; STRIDED-NEXT:    [[TMP33:%.*]] = bitcast i8* [[TMP32]] to float*
; STRIDED-NEXT:    call void @llvm.experimental.vp.strided.store.nxv2f32.p0f32.i64(<vscale x 2 x float> [[VP_OP36]], float* [[TMP33]], i64 [[TMP8]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i32 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[TMP11]]), !llvm.access.group [[ACC_GRP4]]
; STRIDED-NEXT:    [[TMP34:%.*]] = and i64 [[TMP10]], 4294967295
; STRIDED-NEXT:    [[INDEX_NEXT]] = add i64 [[INDEX]], [[TMP34]]
; STRIDED-NEXT:    [[TMP35:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[WIDE_TRIP_COUNT]]
; STRIDED-NEXT:    br i1 [[TMP35]], label [[FOR_COND_CLEANUP]], label [[VECTOR_BODY]], !llvm.loop [[LOOP5:![0-9]+]]
; STRIDED:       for.cond.cleanup:
; STRIDED-NEXT:    ret void
; STRIDED:       for.body:
; STRIDED-NEXT:    [[INDVARS_IV:%.*]] = phi i64 [ [[INDVARS_IV_NEXT:%.*]], [[FOR_BODY]] ], [ 0, [[FOR_BODY_PREHEADER]] ]
; STRIDED-NEXT:    [[TMP36:%.*]] = mul nuw nsw i64 [[INDVARS_IV]], [[TMP0]]
; STRIDED-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[ARRAYIDX]], i64 [[TMP36]]
; STRIDED-NEXT:    [[ARRAYIDX2_REALP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[ARRAYIDX2]], i64 0, i32 0
; STRIDED-NEXT:    [[ARRAYIDX2_IMAGP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[ARRAYIDX2]], i64 0, i32 1
; STRIDED-NEXT:    [[ARRAYIDX4_REALP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[X]], i64 [[INDVARS_IV]], i32 0
; STRIDED-NEXT:    [[ARRAYIDX4_REAL:%.*]] = load float, float* [[ARRAYIDX4_REALP]], align 4, !llvm.access.group [[ACC_GRP4]]
; STRIDED-NEXT:    [[ARRAYIDX4_IMAGP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[X]], i64 [[INDVARS_IV]], i32 1
; STRIDED-NEXT:    [[ARRAYIDX4_IMAG:%.*]] = load float, float* [[ARRAYIDX4_IMAGP]], align 4, !llvm.access.group [[ACC_GRP4]]
; STRIDED-NEXT:    [[ARRAYIDX7_REALP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[Y]], i64 [[INDVARS_IV]], i32 0
; STRIDED-NEXT:    [[ARRAYIDX7_REAL:%.*]] = load float, float* [[ARRAYIDX7_REALP]], align 4, !llvm.access.group [[ACC_GRP4]]
; STRIDED-NEXT:    [[ARRAYIDX7_IMAGP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[Y]], i64 [[INDVARS_IV]], i32 1
; STRIDED-NEXT:    [[ARRAYIDX7_IMAG:%.*]] = load float, float* [[ARRAYIDX7_IMAGP]], align 4, !llvm.access.group [[ACC_GRP4]]
; STRIDED-NEXT:    [[MUL_AD10:%.*]] = fmul fast float [[ARRAYIDX7_REAL]], [[TEMP2_COERCE1]]
; STRIDED-NEXT:    [[MUL_BC11:%.*]] = fmul fast float [[ARRAYIDX7_IMAG]], [[TEMP2_COERCE0]]
; STRIDED-NEXT:    [[MUL_AC8:%.*]] = fmul fast float [[ARRAYIDX7_REAL]], [[TEMP2_COERCE0]]
; STRIDED-NEXT:    [[MUL_BD9_NEG:%.*]] = fmul fast float [[ARRAYIDX7_IMAG]], [[TEMP2_COERCE1]]
; STRIDED-NEXT:    [[ARRAYIDX2_IMAG:%.*]] = load float, float* [[ARRAYIDX2_IMAGP]], align 4, !llvm.access.group [[ACC_GRP4]]
; STRIDED-NEXT:    [[MUL_AD:%.*]] = fmul fast float [[ARRAYIDX4_REAL]], [[TEMP1_COERCE1]]
; STRIDED-NEXT:    [[MUL_BC:%.*]] = fmul fast float [[ARRAYIDX4_IMAG]], [[TEMP1_COERCE0]]
; STRIDED-NEXT:    [[ARRAYIDX2_REAL:%.*]] = load float, float* [[ARRAYIDX2_REALP]], align 4, !llvm.access.group [[ACC_GRP4]]
; STRIDED-NEXT:    [[MUL_AC:%.*]] = fmul fast float [[ARRAYIDX4_REAL]], [[TEMP1_COERCE0]]
; STRIDED-NEXT:    [[MUL_BD_NEG:%.*]] = fmul fast float [[ARRAYIDX4_IMAG]], [[TEMP1_COERCE1]]
; STRIDED-NEXT:    [[REASS_ADD:%.*]] = fadd fast float [[MUL_BD9_NEG]], [[MUL_BD_NEG]]
; STRIDED-NEXT:    [[ADD_R:%.*]] = fadd fast float [[MUL_AC8]], [[MUL_AC]]
; STRIDED-NEXT:    [[MUL_R12:%.*]] = fadd fast float [[ADD_R]], [[ARRAYIDX2_REAL]]
; STRIDED-NEXT:    [[ADD_R22:%.*]] = fsub fast float [[MUL_R12]], [[REASS_ADD]]
; STRIDED-NEXT:    [[MUL_I13:%.*]] = fadd fast float [[MUL_BC]], [[MUL_AD]]
; STRIDED-NEXT:    [[MUL_I:%.*]] = fadd fast float [[MUL_I13]], [[MUL_AD10]]
; STRIDED-NEXT:    [[ADD_I:%.*]] = fadd fast float [[MUL_I]], [[ARRAYIDX2_IMAG]]
; STRIDED-NEXT:    [[ADD_I23:%.*]] = fadd fast float [[ADD_I]], [[MUL_BC11]]
; STRIDED-NEXT:    store float [[ADD_R22]], float* [[ARRAYIDX2_REALP]], align 4, !llvm.access.group [[ACC_GRP4]]
; STRIDED-NEXT:    store float [[ADD_I23]], float* [[ARRAYIDX2_IMAGP]], align 4, !llvm.access.group [[ACC_GRP4]]
; STRIDED-NEXT:    [[INDVARS_IV_NEXT]] = add nuw nsw i64 [[INDVARS_IV]], 1
; STRIDED-NEXT:    [[EXITCOND_NOT:%.*]] = icmp eq i64 [[INDVARS_IV_NEXT]], [[WIDE_TRIP_COUNT]]
; STRIDED-NEXT:    br i1 [[EXITCOND_NOT]], label [[FOR_COND_CLEANUP]], label [[FOR_BODY]], !llvm.loop [[LOOP9:![0-9]+]]
;
entry:
  %0 = zext i32 %n to i64
  %cmp.not.not40 = icmp sgt i32 %j, 0
  br i1 %cmp.not.not40, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:                               ; preds = %entry
  %idxprom1 = sext i32 %j to i64
  %wide.trip.count = zext i32 %j to i64
  %arrayidx = getelementptr inbounds { float, float }, { float, float }* %A, i64 %idxprom1
  br label %for.body

for.cond.cleanup.loopexit:                        ; preds = %for.body
  br label %for.cond.cleanup

for.cond.cleanup:                                 ; preds = %for.cond.cleanup.loopexit, %entry
  ret void

for.body:                                         ; preds = %for.body.preheader, %for.body
  %indvars.iv = phi i64 [ 0, %for.body.preheader ], [ %indvars.iv.next, %for.body ]
  %1 = mul nuw nsw i64 %indvars.iv, %0
  %arrayidx2 = getelementptr inbounds { float, float }, { float, float }* %arrayidx, i64 %1
  %arrayidx2.realp = getelementptr inbounds { float, float }, { float, float }* %arrayidx2, i64 0, i32 0
  %arrayidx2.imagp = getelementptr inbounds { float, float }, { float, float }* %arrayidx2, i64 0, i32 1
  %arrayidx4.realp = getelementptr inbounds { float, float }, { float, float }* %x, i64 %indvars.iv, i32 0
  %arrayidx4.real = load float, float* %arrayidx4.realp, align 4, !llvm.access.group !4
  %arrayidx4.imagp = getelementptr inbounds { float, float }, { float, float }* %x, i64 %indvars.iv, i32 1
  %arrayidx4.imag = load float, float* %arrayidx4.imagp, align 4, !llvm.access.group !4
  %arrayidx7.realp = getelementptr inbounds { float, float }, { float, float }* %y, i64 %indvars.iv, i32 0
  %arrayidx7.real = load float, float* %arrayidx7.realp, align 4, !llvm.access.group !4
  %arrayidx7.imagp = getelementptr inbounds { float, float }, { float, float }* %y, i64 %indvars.iv, i32 1
  %arrayidx7.imag = load float, float* %arrayidx7.imagp, align 4, !llvm.access.group !4
  %mul_ad10 = fmul fast float %arrayidx7.real, %temp2.coerce1
  %mul_bc11 = fmul fast float %arrayidx7.imag, %temp2.coerce0
  %mul_ac8 = fmul fast float %arrayidx7.real, %temp2.coerce0
  %mul_bd9.neg = fmul fast float %arrayidx7.imag, %temp2.coerce1
  %arrayidx2.imag = load float, float* %arrayidx2.imagp, align 4, !llvm.access.group !4
  %mul_ad = fmul fast float %arrayidx4.real, %temp1.coerce1
  %mul_bc = fmul fast float %arrayidx4.imag, %temp1.coerce0
  %arrayidx2.real = load float, float* %arrayidx2.realp, align 4, !llvm.access.group !4
  %mul_ac = fmul fast float %arrayidx4.real, %temp1.coerce0
  %mul_bd.neg = fmul fast float %arrayidx4.imag, %temp1.coerce1
  %reass.add = fadd fast float %mul_bd9.neg, %mul_bd.neg
  %add.r = fadd fast float %mul_ac8, %mul_ac
  %mul_r12 = fadd fast float %add.r, %arrayidx2.real
  %add.r22 = fsub fast float %mul_r12, %reass.add
  %mul_i13 = fadd fast float %mul_bc, %mul_ad
  %mul_i = fadd fast float %mul_i13, %mul_ad10
  %add.i = fadd fast float %mul_i, %arrayidx2.imag
  %add.i23 = fadd fast float %add.i, %mul_bc11
  store float %add.r22, float* %arrayidx2.realp, align 4, !llvm.access.group !4
  store float %add.i23, float* %arrayidx2.imagp, align 4, !llvm.access.group !4
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, %wide.trip.count
  br i1 %exitcond.not, label %for.cond.cleanup.loopexit, label %for.body, !llvm.loop !5
}

attributes #0 = { nofree norecurse nosync nounwind "frame-pointer"="none" "min-legal-vector-width"="0" "no-infs-fp-math"="true" "no-nans-fp-math"="true" "no-signed-zeros-fp-math"="true" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-features"="+64bit,+a,+c,+d,+zepi,+f,+m,-relax,-save-restore" "unsafe-fp-math"="true" }

!llvm.module.flags = !{!0, !1, !2}
!llvm.ident = !{!3}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 1, !"target-abi", !"lp64d"}
!2 = !{i32 1, !"SmallDataLimit", i32 8}
!3 = !{!"clang version 14.0.0"}
!4 = distinct !{}
!5 = distinct !{!5, !6, !7, !8}
!6 = !{!"llvm.loop.mustprogress"}
!7 = !{!"llvm.loop.parallel_accesses", !4}
!8 = !{!"llvm.loop.vectorize.enable", i1 true}

