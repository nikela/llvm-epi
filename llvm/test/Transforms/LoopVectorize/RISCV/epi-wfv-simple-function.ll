; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt -S --loop-vectorize --simplifycfg --instcombine --simplifycfg %s | FileCheck %s
target datalayout = "e-m:e-p:64:64-i64:64-i128:128-n64-S128"
target triple = "riscv64-unknown-linux-gnu"

define <vscale x 1 x i64> @_ZGVEMk1vv_add(<vscale x 1 x i64> %x, <vscale x 1 x i64> %y, <vscale x 1 x i1> %mask, i32 zeroext %vl) #0 {
; CHECK-LABEL: @_ZGVEMk1vv_add(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[VSCALE:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[ASSUME_COND:%.*]] = icmp uge i32 [[VSCALE]], [[VL:%.*]]
; CHECK-NEXT:    call void @llvm.assume(i1 [[ASSUME_COND]])
; CHECK-NEXT:    [[ZEXT_MASK:%.*]] = zext <vscale x 1 x i1> [[MASK:%.*]] to <vscale x 1 x i64>
; CHECK-NEXT:    [[VEC_MASK:%.*]] = alloca <vscale x 1 x i64>, align 8
; CHECK-NEXT:    call void @llvm.vp.store.nxv1i64.p0(<vscale x 1 x i64> [[ZEXT_MASK]], ptr [[VEC_MASK]], <vscale x 1 x i1> shufflevector (<vscale x 1 x i1> insertelement (<vscale x 1 x i1> poison, i1 true, i32 0), <vscale x 1 x i1> poison, <vscale x 1 x i32> zeroinitializer), i32 [[VL]])
; CHECK-NEXT:    [[VEC_X:%.*]] = alloca <vscale x 1 x i64>, align 8
; CHECK-NEXT:    call void @llvm.vp.store.nxv1i64.p0(<vscale x 1 x i64> [[X:%.*]], ptr [[VEC_X]], <vscale x 1 x i1> [[MASK]], i32 [[VL]])
; CHECK-NEXT:    [[VEC_Y:%.*]] = alloca <vscale x 1 x i64>, align 8
; CHECK-NEXT:    call void @llvm.vp.store.nxv1i64.p0(<vscale x 1 x i64> [[Y:%.*]], ptr [[VEC_Y]], <vscale x 1 x i1> [[MASK]], i32 [[VL]])
; CHECK-NEXT:    [[VEC_RET:%.*]] = alloca <vscale x 1 x i64>, align 8
; CHECK-NEXT:    [[TRIP_COUNT_MINUS_1:%.*]] = add i32 [[VL]], -1
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT:%.*]] = insertelement <vscale x 1 x i32> poison, i32 [[TRIP_COUNT_MINUS_1]], i64 0
; CHECK-NEXT:    [[BROADCAST_SPLAT:%.*]] = shufflevector <vscale x 1 x i32> [[BROADCAST_SPLATINSERT]], <vscale x 1 x i32> poison, <vscale x 1 x i32> zeroinitializer
; CHECK-NEXT:    [[TMP0:%.*]] = call <vscale x 1 x i32> @llvm.experimental.stepvector.nxv1i32()
; CHECK-NEXT:    [[TMP1:%.*]] = icmp ule <vscale x 1 x i32> [[TMP0]], [[BROADCAST_SPLAT]]
; CHECK-NEXT:    [[VP_OP_LOAD:%.*]] = call <vscale x 1 x i64> @llvm.vp.load.nxv1i64.p0(ptr [[VEC_MASK]], <vscale x 1 x i1> shufflevector (<vscale x 1 x i1> insertelement (<vscale x 1 x i1> poison, i1 true, i32 0), <vscale x 1 x i1> poison, <vscale x 1 x i32> zeroinitializer), i32 [[VL]])
; CHECK-NEXT:    [[VP_OP_ICMP:%.*]] = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> [[VP_OP_LOAD]], <vscale x 1 x i64> zeroinitializer, metadata !"ne", <vscale x 1 x i1> shufflevector (<vscale x 1 x i1> insertelement (<vscale x 1 x i1> poison, i1 true, i32 0), <vscale x 1 x i1> poison, <vscale x 1 x i32> zeroinitializer), i32 [[VL]])
; CHECK-NEXT:    [[VP_MASK_SELECT:%.*]] = call <vscale x 1 x i1> @llvm.vp.and.nxv1i1(<vscale x 1 x i1> [[TMP1]], <vscale x 1 x i1> [[VP_OP_ICMP]], <vscale x 1 x i1> shufflevector (<vscale x 1 x i1> insertelement (<vscale x 1 x i1> poison, i1 true, i32 0), <vscale x 1 x i1> poison, <vscale x 1 x i32> zeroinitializer), i32 [[VL]])
; CHECK-NEXT:    [[VP_OP_LOAD4:%.*]] = call <vscale x 1 x i64> @llvm.vp.load.nxv1i64.p0(ptr [[VEC_X]], <vscale x 1 x i1> [[VP_MASK_SELECT]], i32 [[VL]])
; CHECK-NEXT:    [[VP_OP_LOAD5:%.*]] = call <vscale x 1 x i64> @llvm.vp.load.nxv1i64.p0(ptr [[VEC_Y]], <vscale x 1 x i1> [[VP_MASK_SELECT]], i32 [[VL]])
; CHECK-NEXT:    [[VP_OP:%.*]] = call <vscale x 1 x i64> @llvm.vp.add.nxv1i64(<vscale x 1 x i64> [[VP_OP_LOAD5]], <vscale x 1 x i64> [[VP_OP_LOAD4]], <vscale x 1 x i1> [[VP_MASK_SELECT]], i32 [[VL]])
; CHECK-NEXT:    call void @llvm.vp.store.nxv1i64.p0(<vscale x 1 x i64> [[VP_OP]], ptr [[VEC_RET]], <vscale x 1 x i1> [[VP_MASK_SELECT]], i32 [[VL]])
; CHECK-NEXT:    [[VEC_RET1:%.*]] = call <vscale x 1 x i64> @llvm.vp.load.nxv1i64.p0(ptr [[VEC_RET]], <vscale x 1 x i1> [[MASK]], i32 [[VL]])
; CHECK-NEXT:    ret <vscale x 1 x i64> [[VEC_RET1]]
;
entry:
  %vscale = call i32 @llvm.vscale.i32()
  %assume.cond = icmp ule i32 %vl, %vscale
  call void @llvm.assume(i1 %assume.cond)
  %zext.mask = zext <vscale x 1 x i1> %mask to <vscale x 1 x i64>
  %vec.mask = alloca <vscale x 1 x i64>, align 8
  call void @llvm.vp.store.nxv1i64.p0(<vscale x 1 x i64> %zext.mask, ptr %vec.mask, <vscale x 1 x i1> shufflevector (<vscale x 1 x i1> insertelement (<vscale x 1 x i1> poison, i1 true, i32 0), <vscale x 1 x i1> poison, <vscale x 1 x i32> zeroinitializer), i32 %vl)
  %vec.x = alloca <vscale x 1 x i64>, align 8
  call void @llvm.vp.store.nxv1i64.p0(<vscale x 1 x i64> %x, ptr %vec.x, <vscale x 1 x i1> %mask, i32 %vl)
  %vec.y = alloca <vscale x 1 x i64>, align 8
  call void @llvm.vp.store.nxv1i64.p0(<vscale x 1 x i64> %y, ptr %vec.y, <vscale x 1 x i1> %mask, i32 %vl)
  %vec.ret = alloca <vscale x 1 x i64>, align 8
  %vl.check = icmp uge i32 %vl, 0
  br i1 %vl.check, label %simd.loop.preheader, label %return

simd.loop.preheader:                              ; preds = %entry
  br label %simd.loop

simd.loop:                                        ; preds = %simd.loop.preheader, %simd.loop.exit
  %index = phi i32 [ %indvar, %simd.loop.exit ], [ 0, %simd.loop.preheader ]
  %vec.mask.gep = getelementptr i64, ptr %vec.mask, i32 %index
  %mask.parm = load i64, ptr %vec.mask.gep, align 8
  %mask.value = icmp ne i64 %mask.parm, 0
  br i1 %mask.value, label %simd.loop.then, label %simd.loop.exit

simd.loop.then:                                   ; preds = %simd.loop
  %vec.x.gep = getelementptr i64, ptr %vec.x, i32 %index
  %vec.x.elem = load i64, ptr %vec.x.gep, align 8
  %vec.y.gep = getelementptr i64, ptr %vec.y, i32 %index
  %vec.y.elem = load i64, ptr %vec.y.gep, align 8
  %add = add nsw i64 %vec.y.elem, %vec.x.elem
  %vec.ret.gep = getelementptr i64, ptr %vec.ret, i32 %index
  store i64 %add, ptr %vec.ret.gep, align 8
  br label %simd.loop.exit

simd.loop.exit:                                   ; preds = %simd.loop, %simd.loop.then
  %indvar = add nsw i32 %index, 1
  %exit.cond = icmp eq i32 %indvar, %vl
  br i1 %exit.cond, label %return.loopexit, label %simd.loop, !llvm.loop !5

return.loopexit:                                  ; preds = %simd.loop.exit
  br label %return

return:                                           ; preds = %return.loopexit, %entry
  %vec.ret1 = call <vscale x 1 x i64> @llvm.vp.load.nxv1i64.p0(ptr %vec.ret, <vscale x 1 x i1> %mask, i32 %vl)
  ret <vscale x 1 x i64> %vec.ret1
}

define <vscale x 1 x i64> @_ZGVENk1vv_add(<vscale x 1 x i64> %x, <vscale x 1 x i64> %y, i32 zeroext %vl) #0 {
; CHECK-LABEL: @_ZGVENk1vv_add(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[VSCALE:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[ASSUME_COND:%.*]] = icmp uge i32 [[VSCALE]], [[VL:%.*]]
; CHECK-NEXT:    call void @llvm.assume(i1 [[ASSUME_COND]])
; CHECK-NEXT:    [[VEC_X:%.*]] = alloca <vscale x 1 x i64>, align 8
; CHECK-NEXT:    call void @llvm.vp.store.nxv1i64.p0(<vscale x 1 x i64> [[X:%.*]], ptr [[VEC_X]], <vscale x 1 x i1> shufflevector (<vscale x 1 x i1> insertelement (<vscale x 1 x i1> poison, i1 true, i32 0), <vscale x 1 x i1> poison, <vscale x 1 x i32> zeroinitializer), i32 [[VL]])
; CHECK-NEXT:    [[VEC_Y:%.*]] = alloca <vscale x 1 x i64>, align 8
; CHECK-NEXT:    call void @llvm.vp.store.nxv1i64.p0(<vscale x 1 x i64> [[Y:%.*]], ptr [[VEC_Y]], <vscale x 1 x i1> shufflevector (<vscale x 1 x i1> insertelement (<vscale x 1 x i1> poison, i1 true, i32 0), <vscale x 1 x i1> poison, <vscale x 1 x i32> zeroinitializer), i32 [[VL]])
; CHECK-NEXT:    [[VEC_RET:%.*]] = alloca <vscale x 1 x i64>, align 8
; CHECK-NEXT:    [[VP_OP_LOAD:%.*]] = call <vscale x 1 x i64> @llvm.vp.load.nxv1i64.p0(ptr [[VEC_X]], <vscale x 1 x i1> shufflevector (<vscale x 1 x i1> insertelement (<vscale x 1 x i1> poison, i1 true, i32 0), <vscale x 1 x i1> poison, <vscale x 1 x i32> zeroinitializer), i32 [[VL]])
; CHECK-NEXT:    [[VP_OP_LOAD4:%.*]] = call <vscale x 1 x i64> @llvm.vp.load.nxv1i64.p0(ptr [[VEC_Y]], <vscale x 1 x i1> shufflevector (<vscale x 1 x i1> insertelement (<vscale x 1 x i1> poison, i1 true, i32 0), <vscale x 1 x i1> poison, <vscale x 1 x i32> zeroinitializer), i32 [[VL]])
; CHECK-NEXT:    [[VP_OP:%.*]] = call <vscale x 1 x i64> @llvm.vp.add.nxv1i64(<vscale x 1 x i64> [[VP_OP_LOAD4]], <vscale x 1 x i64> [[VP_OP_LOAD]], <vscale x 1 x i1> shufflevector (<vscale x 1 x i1> insertelement (<vscale x 1 x i1> poison, i1 true, i32 0), <vscale x 1 x i1> poison, <vscale x 1 x i32> zeroinitializer), i32 [[VL]])
; CHECK-NEXT:    call void @llvm.vp.store.nxv1i64.p0(<vscale x 1 x i64> [[VP_OP]], ptr [[VEC_RET]], <vscale x 1 x i1> shufflevector (<vscale x 1 x i1> insertelement (<vscale x 1 x i1> poison, i1 true, i32 0), <vscale x 1 x i1> poison, <vscale x 1 x i32> zeroinitializer), i32 [[VL]])
; CHECK-NEXT:    [[VEC_RET1:%.*]] = call <vscale x 1 x i64> @llvm.vp.load.nxv1i64.p0(ptr [[VEC_RET]], <vscale x 1 x i1> shufflevector (<vscale x 1 x i1> insertelement (<vscale x 1 x i1> poison, i1 true, i32 0), <vscale x 1 x i1> poison, <vscale x 1 x i32> zeroinitializer), i32 [[VL]])
; CHECK-NEXT:    ret <vscale x 1 x i64> [[VEC_RET1]]
;
entry:
  %vscale = call i32 @llvm.vscale.i32()
  %assume.cond = icmp ule i32 %vl, %vscale
  call void @llvm.assume(i1 %assume.cond)
  %vec.x = alloca <vscale x 1 x i64>, align 8
  call void @llvm.vp.store.nxv1i64.p0(<vscale x 1 x i64> %x, ptr %vec.x, <vscale x 1 x i1> shufflevector (<vscale x 1 x i1> insertelement (<vscale x 1 x i1> poison, i1 true, i32 0), <vscale x 1 x i1> poison, <vscale x 1 x i32> zeroinitializer), i32 %vl)
  %vec.y = alloca <vscale x 1 x i64>, align 8
  call void @llvm.vp.store.nxv1i64.p0(<vscale x 1 x i64> %y, ptr %vec.y, <vscale x 1 x i1> shufflevector (<vscale x 1 x i1> insertelement (<vscale x 1 x i1> poison, i1 true, i32 0), <vscale x 1 x i1> poison, <vscale x 1 x i32> zeroinitializer), i32 %vl)
  %vec.ret = alloca <vscale x 1 x i64>, align 8
  %vl.check = icmp uge i32 %vl, 0
  br i1 %vl.check, label %simd.loop, label %return

simd.loop:                                        ; preds = %simd.loop.exit, %entry
  %index = phi i32 [ 0, %entry ], [ %indvar, %simd.loop.exit ]
  %vec.x.gep = getelementptr i64, ptr %vec.x, i32 %index
  %vec.x.elem = load i64, ptr %vec.x.gep, align 8
  %vec.y.gep = getelementptr i64, ptr %vec.y, i32 %index
  %vec.y.elem = load i64, ptr %vec.y.gep, align 8
  %add = add nsw i64 %vec.y.elem, %vec.x.elem
  %vec.ret.gep = getelementptr i64, ptr %vec.ret, i32 %index
  store i64 %add, ptr %vec.ret.gep, align 8
  br label %simd.loop.exit

simd.loop.exit:                                   ; preds = %simd.loop
  %indvar = add nsw i32 %index, 1
  %exit.cond = icmp eq i32 %indvar, %vl
  br i1 %exit.cond, label %return, label %simd.loop, !llvm.loop !13

return:                                           ; preds = %simd.loop.exit, %entry
  %vec.ret1 = call <vscale x 1 x i64> @llvm.vp.load.nxv1i64.p0(ptr %vec.ret, <vscale x 1 x i1> shufflevector (<vscale x 1 x i1> insertelement (<vscale x 1 x i1> poison, i1 true, i32 0), <vscale x 1 x i1> poison, <vscale x 1 x i32> zeroinitializer), i32 %vl)
  ret <vscale x 1 x i64> %vec.ret1
}

declare void @llvm.vp.store.nxv1i64.p0(<vscale x 1 x i64>, ptr nocapture, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i64> @llvm.vp.load.nxv1i64.p0(ptr nocapture, <vscale x 1 x i1>, i32)
declare i32 @llvm.vscale.i32()
declare void @llvm.assume(i1 noundef)

attributes #0 = { mustprogress nofree norecurse nosync nounwind readnone willreturn "frame-pointer"="none" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-features"="+64bit,+a,+c,+d,+f,+m,+zepi,+zve32f,+zve32x,+zve64d,+zve64f,+zve64x,+zvl32b,+zvl64b,-relax,-save-restore" }

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 1, !"target-abi", !"lp64d"}
!2 = !{i32 7, !"openmp", i32 50}
!3 = !{i32 1, !"SmallDataLimit", i32 8}
!4 = !{!"clang version 15.0.0"}
!5 = distinct !{!5, !6, !7, !8, !9, !10, !11, !12}
!6 = !{!"llvm.loop.unroll.disable"}
!7 = !{!"llvm.loop.vectorize.enable"}
!8 = !{!"llvm.loop.vectorize.width", i32 1}
!9 = !{!"llvm.loop.vectorize.scalable.enable", i1 true}
!10 = !{!"llvm.loop.vectorize.predicate.enable", i1 true}
!11 = !{!"llvm.loop.single.iteration"}
!12 = !{!"llvm.loop.epilogue.forbid"}
!13 = distinct !{!13, !6, !7, !8, !9, !10, !11, !12}
