; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --include-generated-funcs
; RUN: opt -S --vec-clone-vp %s | FileCheck %s

define i32 @foo(i32 %A, i32 %B, i32 %C) #0 {
entry:
  %add = add nsw i32 %B, %A
  %add1 = add nsw i32 %add, %C
  ret i32 %add1
}

attributes #0 = { "_ZGVEMk1vlu_foo" "_ZGVENk1vlu_foo" }

; CHECK-LABEL: @foo(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 [[B:%.*]], [[A:%.*]]
; CHECK-NEXT:    [[ADD1:%.*]] = add nsw i32 [[ADD]], [[C:%.*]]
; CHECK-NEXT:    ret i32 [[ADD1]]
;
;
; CHECK-LABEL: @_ZGVEMk1vlu_foo(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[VSCALE:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[ASSUME_COND:%.*]] = icmp ule i32 [[VL:%.*]], [[VSCALE]]
; CHECK-NEXT:    call void @llvm.assume(i1 [[ASSUME_COND]])
; CHECK-NEXT:    [[ZEXT_MASK:%.*]] = zext <vscale x 1 x i1> [[MASK:%.*]] to <vscale x 1 x i64>
; CHECK-NEXT:    [[VEC_MASK:%.*]] = alloca <vscale x 1 x i64>, align 8
; CHECK-NEXT:    call void @llvm.vp.store.nxv1i64.p0(<vscale x 1 x i64> [[ZEXT_MASK]], ptr [[VEC_MASK]], <vscale x 1 x i1> shufflevector (<vscale x 1 x i1> insertelement (<vscale x 1 x i1> poison, i1 true, i32 0), <vscale x 1 x i1> poison, <vscale x 1 x i32> zeroinitializer), i32 [[VL]])
; CHECK-NEXT:    [[VEC_A:%.*]] = alloca <vscale x 1 x i32>, align 4
; CHECK-NEXT:    call void @llvm.vp.store.nxv1i32.p0(<vscale x 1 x i32> [[A:%.*]], ptr [[VEC_A]], <vscale x 1 x i1> [[MASK]], i32 [[VL]])
; CHECK-NEXT:    [[VEC_RET:%.*]] = alloca <vscale x 1 x i32>, align 4
; CHECK-NEXT:    [[VL_CHECK:%.*]] = icmp uge i32 [[VL]], 0
; CHECK-NEXT:    br i1 [[VL_CHECK]], label [[SIMD_LOOP:%.*]], label [[RETURN:%.*]]
; CHECK:       simd.loop:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i32 [ 0, [[ENTRY:%.*]] ], [ [[INDVAR:%.*]], [[SIMD_LOOP_EXIT:%.*]] ]
; CHECK-NEXT:    [[VEC_MASK_GEP:%.*]] = getelementptr i64, ptr [[VEC_MASK]], i32 [[INDEX]]
; CHECK-NEXT:    [[MASK_PARM:%.*]] = load i64, ptr [[VEC_MASK_GEP]], align 8
; CHECK-NEXT:    [[MASK_VALUE:%.*]] = icmp ne i64 [[MASK_PARM]], 0
; CHECK-NEXT:    br i1 [[MASK_VALUE]], label [[SIMD_LOOP_THEN:%.*]], label [[SIMD_LOOP_EXIT]]
; CHECK:       simd.loop.then:
; CHECK-NEXT:    [[VEC_A_GEP:%.*]] = getelementptr i32, ptr [[VEC_A]], i32 [[INDEX]]
; CHECK-NEXT:    [[VEC_A_ELEM:%.*]] = load i32, ptr [[VEC_A_GEP]], align 4
; CHECK-NEXT:    [[STRIDE_MUL:%.*]] = mul nsw i32 1, [[INDEX]]
; CHECK-NEXT:    [[STRIDE_ADD:%.*]] = add i32 [[B:%.*]], [[STRIDE_MUL]]
; CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 [[STRIDE_ADD]], [[VEC_A_ELEM]]
; CHECK-NEXT:    [[ADD1:%.*]] = add nsw i32 [[ADD]], [[C:%.*]]
; CHECK-NEXT:    [[VEC_RET_GEP:%.*]] = getelementptr i32, ptr [[VEC_RET]], i32 [[INDEX]]
; CHECK-NEXT:    store i32 [[ADD1]], ptr [[VEC_RET_GEP]], align 4
; CHECK-NEXT:    br label [[SIMD_LOOP_EXIT]]
; CHECK:       simd.loop.exit:
; CHECK-NEXT:    [[INDVAR]] = add nsw i32 [[INDEX]], 1
; CHECK-NEXT:    [[EXIT_COND:%.*]] = icmp eq i32 [[INDVAR]], [[VL]]
; CHECK-NEXT:    br i1 [[EXIT_COND]], label [[RETURN]], label [[SIMD_LOOP]], !llvm.loop [[LOOP0:![0-9]+]]
; CHECK:       return:
; CHECK-NEXT:    [[VEC_RET1:%.*]] = call <vscale x 1 x i32> @llvm.vp.load.nxv1i32.p0(ptr [[VEC_RET]], <vscale x 1 x i1> [[MASK]], i32 [[VL]])
; CHECK-NEXT:    ret <vscale x 1 x i32> [[VEC_RET1]]
;
;
; CHECK-LABEL: @_ZGVENk1vlu_foo(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[VSCALE:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[ASSUME_COND:%.*]] = icmp ule i32 [[VL:%.*]], [[VSCALE]]
; CHECK-NEXT:    call void @llvm.assume(i1 [[ASSUME_COND]])
; CHECK-NEXT:    [[VEC_A:%.*]] = alloca <vscale x 1 x i32>, align 4
; CHECK-NEXT:    call void @llvm.vp.store.nxv1i32.p0(<vscale x 1 x i32> [[A:%.*]], ptr [[VEC_A]], <vscale x 1 x i1> shufflevector (<vscale x 1 x i1> insertelement (<vscale x 1 x i1> poison, i1 true, i32 0), <vscale x 1 x i1> poison, <vscale x 1 x i32> zeroinitializer), i32 [[VL]])
; CHECK-NEXT:    [[VEC_RET:%.*]] = alloca <vscale x 1 x i32>, align 4
; CHECK-NEXT:    [[VL_CHECK:%.*]] = icmp uge i32 [[VL]], 0
; CHECK-NEXT:    br i1 [[VL_CHECK]], label [[SIMD_LOOP:%.*]], label [[RETURN:%.*]]
; CHECK:       simd.loop:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i32 [ 0, [[ENTRY:%.*]] ], [ [[INDVAR:%.*]], [[SIMD_LOOP_EXIT:%.*]] ]
; CHECK-NEXT:    [[VEC_A_GEP:%.*]] = getelementptr i32, ptr [[VEC_A]], i32 [[INDEX]]
; CHECK-NEXT:    [[VEC_A_ELEM:%.*]] = load i32, ptr [[VEC_A_GEP]], align 4
; CHECK-NEXT:    [[STRIDE_MUL:%.*]] = mul nsw i32 1, [[INDEX]]
; CHECK-NEXT:    [[STRIDE_ADD:%.*]] = add i32 [[B:%.*]], [[STRIDE_MUL]]
; CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 [[STRIDE_ADD]], [[VEC_A_ELEM]]
; CHECK-NEXT:    [[ADD1:%.*]] = add nsw i32 [[ADD]], [[C:%.*]]
; CHECK-NEXT:    [[VEC_RET_GEP:%.*]] = getelementptr i32, ptr [[VEC_RET]], i32 [[INDEX]]
; CHECK-NEXT:    store i32 [[ADD1]], ptr [[VEC_RET_GEP]], align 4
; CHECK-NEXT:    br label [[SIMD_LOOP_EXIT]]
; CHECK:       simd.loop.exit:
; CHECK-NEXT:    [[INDVAR]] = add nsw i32 [[INDEX]], 1
; CHECK-NEXT:    [[EXIT_COND:%.*]] = icmp eq i32 [[INDVAR]], [[VL]]
; CHECK-NEXT:    br i1 [[EXIT_COND]], label [[RETURN]], label [[SIMD_LOOP]], !llvm.loop [[LOOP8:![0-9]+]]
; CHECK:       return:
; CHECK-NEXT:    [[VEC_RET1:%.*]] = call <vscale x 1 x i32> @llvm.vp.load.nxv1i32.p0(ptr [[VEC_RET]], <vscale x 1 x i1> shufflevector (<vscale x 1 x i1> insertelement (<vscale x 1 x i1> poison, i1 true, i32 0), <vscale x 1 x i1> poison, <vscale x 1 x i32> zeroinitializer), i32 [[VL]])
; CHECK-NEXT:    ret <vscale x 1 x i32> [[VEC_RET1]]
;
