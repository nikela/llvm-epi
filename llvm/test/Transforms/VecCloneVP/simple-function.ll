; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --include-generated-funcs
; RUN: opt -S -passes=vec-clone-vp %s | FileCheck %s

define i64 @simple_function(i64 %x, i64 %y) #0 {
entry:
  %x.addr = alloca i64, align 8
  %y.addr = alloca i64, align 8
  store i64 %x, ptr %x.addr, align 8
  store i64 %y, ptr %y.addr, align 8
  %0 = load i64, ptr %x.addr, align 8
  %1 = load i64, ptr %y.addr, align 8
  %add = add nsw i64 %0, %1
  ret i64 %add
}

define i64 @simple_function_without_allocas(i64 %x, i64 %y) #1 {
entry:
  %add = add nsw i64 %x, %y
  ret i64 %add
}





attributes #0 = { "_ZGVEMk1vv_simple_function" "_ZGVENk2vv_simple_function" }
attributes #1 = { "_ZGVEMk1vv_simple_function_without_allocas" "_ZGVENk2vv_simple_function_without_allocas" }
; CHECK-LABEL: @simple_function(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[X_ADDR:%.*]] = alloca i64, align 8
; CHECK-NEXT:    [[Y_ADDR:%.*]] = alloca i64, align 8
; CHECK-NEXT:    store i64 [[X:%.*]], ptr [[X_ADDR]], align 8
; CHECK-NEXT:    store i64 [[Y:%.*]], ptr [[Y_ADDR]], align 8
; CHECK-NEXT:    [[TMP0:%.*]] = load i64, ptr [[X_ADDR]], align 8
; CHECK-NEXT:    [[TMP1:%.*]] = load i64, ptr [[Y_ADDR]], align 8
; CHECK-NEXT:    [[ADD:%.*]] = add nsw i64 [[TMP0]], [[TMP1]]
; CHECK-NEXT:    ret i64 [[ADD]]
;
;
; CHECK-LABEL: @simple_function_without_allocas(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[ADD:%.*]] = add nsw i64 [[X:%.*]], [[Y:%.*]]
; CHECK-NEXT:    ret i64 [[ADD]]
;
;
; CHECK-LABEL: @_ZGVEMk1vv_simple_function(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[VSCALE:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[ASSUME_COND:%.*]] = icmp ule i32 [[VL:%.*]], [[VSCALE]]
; CHECK-NEXT:    call void @llvm.assume(i1 [[ASSUME_COND]])
; CHECK-NEXT:    [[ZEXT_MASK:%.*]] = zext <vscale x 1 x i1> [[MASK:%.*]] to <vscale x 1 x i64>
; CHECK-NEXT:    [[VEC_MASK:%.*]] = alloca <vscale x 1 x i64>, align 8
; CHECK-NEXT:    call void @llvm.vp.store.nxv1i64.p0(<vscale x 1 x i64> [[ZEXT_MASK]], ptr [[VEC_MASK]], <vscale x 1 x i1> shufflevector (<vscale x 1 x i1> insertelement (<vscale x 1 x i1> poison, i1 true, i64 0), <vscale x 1 x i1> poison, <vscale x 1 x i32> zeroinitializer), i32 [[VL]])
; CHECK-NEXT:    [[VEC_X:%.*]] = alloca <vscale x 1 x i64>, align 8
; CHECK-NEXT:    call void @llvm.vp.store.nxv1i64.p0(<vscale x 1 x i64> [[X:%.*]], ptr [[VEC_X]], <vscale x 1 x i1> [[MASK]], i32 [[VL]])
; CHECK-NEXT:    [[VEC_Y:%.*]] = alloca <vscale x 1 x i64>, align 8
; CHECK-NEXT:    call void @llvm.vp.store.nxv1i64.p0(<vscale x 1 x i64> [[Y:%.*]], ptr [[VEC_Y]], <vscale x 1 x i1> [[MASK]], i32 [[VL]])
; CHECK-NEXT:    [[VEC_RET:%.*]] = alloca <vscale x 1 x i64>, align 8
; CHECK-NEXT:    [[VL_CHECK:%.*]] = icmp uge i32 [[VL]], 0
; CHECK-NEXT:    br i1 [[VL_CHECK]], label [[SIMD_LOOP:%.*]], label [[RETURN:%.*]]
; CHECK:       simd.loop:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i32 [ 0, [[ENTRY:%.*]] ], [ [[INDVAR:%.*]], [[SIMD_LOOP_EXIT:%.*]] ]
; CHECK-NEXT:    [[VEC_MASK_GEP:%.*]] = getelementptr i64, ptr [[VEC_MASK]], i32 [[INDEX]]
; CHECK-NEXT:    [[MASK_PARM:%.*]] = load i64, ptr [[VEC_MASK_GEP]], align 8
; CHECK-NEXT:    [[MASK_VALUE:%.*]] = icmp ne i64 [[MASK_PARM]], 0
; CHECK-NEXT:    br i1 [[MASK_VALUE]], label [[SIMD_LOOP_THEN:%.*]], label [[SIMD_LOOP_EXIT]]
; CHECK:       simd.loop.then:
; CHECK-NEXT:    [[VEC_X_GEP:%.*]] = getelementptr <vscale x 1 x i64>, ptr [[VEC_X]], i32 0, i32 [[INDEX]]
; CHECK-NEXT:    [[TMP0:%.*]] = load i64, ptr [[VEC_X_GEP]], align 8
; CHECK-NEXT:    [[VEC_Y_GEP:%.*]] = getelementptr <vscale x 1 x i64>, ptr [[VEC_Y]], i32 0, i32 [[INDEX]]
; CHECK-NEXT:    [[TMP1:%.*]] = load i64, ptr [[VEC_Y_GEP]], align 8
; CHECK-NEXT:    [[ADD:%.*]] = add nsw i64 [[TMP0]], [[TMP1]]
; CHECK-NEXT:    [[VEC_RET_GEP:%.*]] = getelementptr i64, ptr [[VEC_RET]], i32 [[INDEX]]
; CHECK-NEXT:    store i64 [[ADD]], ptr [[VEC_RET_GEP]], align 8
; CHECK-NEXT:    br label [[SIMD_LOOP_EXIT]]
; CHECK:       simd.loop.exit:
; CHECK-NEXT:    [[INDVAR]] = add nsw i32 [[INDEX]], 1
; CHECK-NEXT:    [[EXIT_COND:%.*]] = icmp eq i32 [[INDVAR]], [[VL]]
; CHECK-NEXT:    br i1 [[EXIT_COND]], label [[RETURN]], label [[SIMD_LOOP]], !llvm.loop [[LOOP0:![0-9]+]]
; CHECK:       return:
; CHECK-NEXT:    [[VEC_RET1:%.*]] = call <vscale x 1 x i64> @llvm.vp.load.nxv1i64.p0(ptr [[VEC_RET]], <vscale x 1 x i1> [[MASK]], i32 [[VL]])
; CHECK-NEXT:    ret <vscale x 1 x i64> [[VEC_RET1]]
;
;
; CHECK-LABEL: @_ZGVENk2vv_simple_function(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[VSCALE:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[TMP0:%.*]] = mul i32 [[VSCALE]], 2
; CHECK-NEXT:    [[ASSUME_COND:%.*]] = icmp ule i32 [[VL:%.*]], [[TMP0]]
; CHECK-NEXT:    call void @llvm.assume(i1 [[ASSUME_COND]])
; CHECK-NEXT:    [[VEC_X:%.*]] = alloca <vscale x 2 x i64>, align 16
; CHECK-NEXT:    call void @llvm.vp.store.nxv2i64.p0(<vscale x 2 x i64> [[X:%.*]], ptr [[VEC_X]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i64 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[VL]])
; CHECK-NEXT:    [[VEC_Y:%.*]] = alloca <vscale x 2 x i64>, align 16
; CHECK-NEXT:    call void @llvm.vp.store.nxv2i64.p0(<vscale x 2 x i64> [[Y:%.*]], ptr [[VEC_Y]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i64 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[VL]])
; CHECK-NEXT:    [[VEC_RET:%.*]] = alloca <vscale x 2 x i64>, align 16
; CHECK-NEXT:    [[VL_CHECK:%.*]] = icmp uge i32 [[VL]], 0
; CHECK-NEXT:    br i1 [[VL_CHECK]], label [[SIMD_LOOP:%.*]], label [[RETURN:%.*]]
; CHECK:       simd.loop:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i32 [ 0, [[ENTRY:%.*]] ], [ [[INDVAR:%.*]], [[SIMD_LOOP_EXIT:%.*]] ]
; CHECK-NEXT:    [[VEC_X_GEP:%.*]] = getelementptr <vscale x 2 x i64>, ptr [[VEC_X]], i32 0, i32 [[INDEX]]
; CHECK-NEXT:    [[TMP1:%.*]] = load i64, ptr [[VEC_X_GEP]], align 8
; CHECK-NEXT:    [[VEC_Y_GEP:%.*]] = getelementptr <vscale x 2 x i64>, ptr [[VEC_Y]], i32 0, i32 [[INDEX]]
; CHECK-NEXT:    [[TMP2:%.*]] = load i64, ptr [[VEC_Y_GEP]], align 8
; CHECK-NEXT:    [[ADD:%.*]] = add nsw i64 [[TMP1]], [[TMP2]]
; CHECK-NEXT:    [[VEC_RET_GEP:%.*]] = getelementptr i64, ptr [[VEC_RET]], i32 [[INDEX]]
; CHECK-NEXT:    store i64 [[ADD]], ptr [[VEC_RET_GEP]], align 8
; CHECK-NEXT:    br label [[SIMD_LOOP_EXIT]]
; CHECK:       simd.loop.exit:
; CHECK-NEXT:    [[INDVAR]] = add nsw i32 [[INDEX]], 1
; CHECK-NEXT:    [[EXIT_COND:%.*]] = icmp eq i32 [[INDVAR]], [[VL]]
; CHECK-NEXT:    br i1 [[EXIT_COND]], label [[RETURN]], label [[SIMD_LOOP]], !llvm.loop [[LOOP8:![0-9]+]]
; CHECK:       return:
; CHECK-NEXT:    [[VEC_RET1:%.*]] = call <vscale x 2 x i64> @llvm.vp.load.nxv2i64.p0(ptr [[VEC_RET]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i64 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[VL]])
; CHECK-NEXT:    ret <vscale x 2 x i64> [[VEC_RET1]]
;
;
; CHECK-LABEL: @_ZGVEMk1vv_simple_function_without_allocas(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[VSCALE:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[ASSUME_COND:%.*]] = icmp ule i32 [[VL:%.*]], [[VSCALE]]
; CHECK-NEXT:    call void @llvm.assume(i1 [[ASSUME_COND]])
; CHECK-NEXT:    [[ZEXT_MASK:%.*]] = zext <vscale x 1 x i1> [[MASK:%.*]] to <vscale x 1 x i64>
; CHECK-NEXT:    [[VEC_MASK:%.*]] = alloca <vscale x 1 x i64>, align 8
; CHECK-NEXT:    call void @llvm.vp.store.nxv1i64.p0(<vscale x 1 x i64> [[ZEXT_MASK]], ptr [[VEC_MASK]], <vscale x 1 x i1> shufflevector (<vscale x 1 x i1> insertelement (<vscale x 1 x i1> poison, i1 true, i64 0), <vscale x 1 x i1> poison, <vscale x 1 x i32> zeroinitializer), i32 [[VL]])
; CHECK-NEXT:    [[VEC_X:%.*]] = alloca <vscale x 1 x i64>, align 8
; CHECK-NEXT:    call void @llvm.vp.store.nxv1i64.p0(<vscale x 1 x i64> [[X:%.*]], ptr [[VEC_X]], <vscale x 1 x i1> [[MASK]], i32 [[VL]])
; CHECK-NEXT:    [[VEC_Y:%.*]] = alloca <vscale x 1 x i64>, align 8
; CHECK-NEXT:    call void @llvm.vp.store.nxv1i64.p0(<vscale x 1 x i64> [[Y:%.*]], ptr [[VEC_Y]], <vscale x 1 x i1> [[MASK]], i32 [[VL]])
; CHECK-NEXT:    [[VEC_RET:%.*]] = alloca <vscale x 1 x i64>, align 8
; CHECK-NEXT:    [[VL_CHECK:%.*]] = icmp uge i32 [[VL]], 0
; CHECK-NEXT:    br i1 [[VL_CHECK]], label [[SIMD_LOOP:%.*]], label [[RETURN:%.*]]
; CHECK:       simd.loop:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i32 [ 0, [[ENTRY:%.*]] ], [ [[INDVAR:%.*]], [[SIMD_LOOP_EXIT:%.*]] ]
; CHECK-NEXT:    [[VEC_MASK_GEP:%.*]] = getelementptr i64, ptr [[VEC_MASK]], i32 [[INDEX]]
; CHECK-NEXT:    [[MASK_PARM:%.*]] = load i64, ptr [[VEC_MASK_GEP]], align 8
; CHECK-NEXT:    [[MASK_VALUE:%.*]] = icmp ne i64 [[MASK_PARM]], 0
; CHECK-NEXT:    br i1 [[MASK_VALUE]], label [[SIMD_LOOP_THEN:%.*]], label [[SIMD_LOOP_EXIT]]
; CHECK:       simd.loop.then:
; CHECK-NEXT:    [[VEC_X_GEP:%.*]] = getelementptr i64, ptr [[VEC_X]], i32 [[INDEX]]
; CHECK-NEXT:    [[VEC_X_ELEM:%.*]] = load i64, ptr [[VEC_X_GEP]], align 8
; CHECK-NEXT:    [[VEC_Y_GEP:%.*]] = getelementptr i64, ptr [[VEC_Y]], i32 [[INDEX]]
; CHECK-NEXT:    [[VEC_Y_ELEM:%.*]] = load i64, ptr [[VEC_Y_GEP]], align 8
; CHECK-NEXT:    [[ADD:%.*]] = add nsw i64 [[VEC_X_ELEM]], [[VEC_Y_ELEM]]
; CHECK-NEXT:    [[VEC_RET_GEP:%.*]] = getelementptr i64, ptr [[VEC_RET]], i32 [[INDEX]]
; CHECK-NEXT:    store i64 [[ADD]], ptr [[VEC_RET_GEP]], align 8
; CHECK-NEXT:    br label [[SIMD_LOOP_EXIT]]
; CHECK:       simd.loop.exit:
; CHECK-NEXT:    [[INDVAR]] = add nsw i32 [[INDEX]], 1
; CHECK-NEXT:    [[EXIT_COND:%.*]] = icmp eq i32 [[INDVAR]], [[VL]]
; CHECK-NEXT:    br i1 [[EXIT_COND]], label [[RETURN]], label [[SIMD_LOOP]], !llvm.loop [[LOOP10:![0-9]+]]
; CHECK:       return:
; CHECK-NEXT:    [[VEC_RET1:%.*]] = call <vscale x 1 x i64> @llvm.vp.load.nxv1i64.p0(ptr [[VEC_RET]], <vscale x 1 x i1> [[MASK]], i32 [[VL]])
; CHECK-NEXT:    ret <vscale x 1 x i64> [[VEC_RET1]]
;
;
; CHECK-LABEL: @_ZGVENk2vv_simple_function_without_allocas(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[VSCALE:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[TMP0:%.*]] = mul i32 [[VSCALE]], 2
; CHECK-NEXT:    [[ASSUME_COND:%.*]] = icmp ule i32 [[VL:%.*]], [[TMP0]]
; CHECK-NEXT:    call void @llvm.assume(i1 [[ASSUME_COND]])
; CHECK-NEXT:    [[VEC_X:%.*]] = alloca <vscale x 2 x i64>, align 16
; CHECK-NEXT:    call void @llvm.vp.store.nxv2i64.p0(<vscale x 2 x i64> [[X:%.*]], ptr [[VEC_X]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i64 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[VL]])
; CHECK-NEXT:    [[VEC_Y:%.*]] = alloca <vscale x 2 x i64>, align 16
; CHECK-NEXT:    call void @llvm.vp.store.nxv2i64.p0(<vscale x 2 x i64> [[Y:%.*]], ptr [[VEC_Y]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i64 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[VL]])
; CHECK-NEXT:    [[VEC_RET:%.*]] = alloca <vscale x 2 x i64>, align 16
; CHECK-NEXT:    [[VL_CHECK:%.*]] = icmp uge i32 [[VL]], 0
; CHECK-NEXT:    br i1 [[VL_CHECK]], label [[SIMD_LOOP:%.*]], label [[RETURN:%.*]]
; CHECK:       simd.loop:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i32 [ 0, [[ENTRY:%.*]] ], [ [[INDVAR:%.*]], [[SIMD_LOOP_EXIT:%.*]] ]
; CHECK-NEXT:    [[VEC_X_GEP:%.*]] = getelementptr i64, ptr [[VEC_X]], i32 [[INDEX]]
; CHECK-NEXT:    [[VEC_X_ELEM:%.*]] = load i64, ptr [[VEC_X_GEP]], align 8
; CHECK-NEXT:    [[VEC_Y_GEP:%.*]] = getelementptr i64, ptr [[VEC_Y]], i32 [[INDEX]]
; CHECK-NEXT:    [[VEC_Y_ELEM:%.*]] = load i64, ptr [[VEC_Y_GEP]], align 8
; CHECK-NEXT:    [[ADD:%.*]] = add nsw i64 [[VEC_X_ELEM]], [[VEC_Y_ELEM]]
; CHECK-NEXT:    [[VEC_RET_GEP:%.*]] = getelementptr i64, ptr [[VEC_RET]], i32 [[INDEX]]
; CHECK-NEXT:    store i64 [[ADD]], ptr [[VEC_RET_GEP]], align 8
; CHECK-NEXT:    br label [[SIMD_LOOP_EXIT]]
; CHECK:       simd.loop.exit:
; CHECK-NEXT:    [[INDVAR]] = add nsw i32 [[INDEX]], 1
; CHECK-NEXT:    [[EXIT_COND:%.*]] = icmp eq i32 [[INDVAR]], [[VL]]
; CHECK-NEXT:    br i1 [[EXIT_COND]], label [[RETURN]], label [[SIMD_LOOP]], !llvm.loop [[LOOP11:![0-9]+]]
; CHECK:       return:
; CHECK-NEXT:    [[VEC_RET1:%.*]] = call <vscale x 2 x i64> @llvm.vp.load.nxv2i64.p0(ptr [[VEC_RET]], <vscale x 2 x i1> shufflevector (<vscale x 2 x i1> insertelement (<vscale x 2 x i1> poison, i1 true, i64 0), <vscale x 2 x i1> poison, <vscale x 2 x i32> zeroinitializer), i32 [[VL]])
; CHECK-NEXT:    ret <vscale x 2 x i64> [[VEC_RET1]]
;
