; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple riscv64 -mattr=+experimental-v < %s -epi-pipeline | FileCheck %s

@scratch = global i8 0, align 16

define void @nxv1i1(<vscale x 1 x i1> %a, <vscale x 1 x i1> %b) nounwind {
; CHECK-LABEL: nxv1i1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(scratch)
; CHECK-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-NEXT:    vsetvli a1, zero, e8,mf8,ta,mu
; CHECK-NEXT:    vmand.mm v25, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e8,mf8,ta,mu
; CHECK-NEXT:    vmor.mm v25, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e8,mf8,ta,mu
; CHECK-NEXT:    vmxor.mm v25, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 1 x i1>*

  %log_1 = and <vscale x 1 x i1> %a, %b
  store <vscale x 1 x i1> %log_1, <vscale x 1 x i1>* %store_addr

  %log_2 = or <vscale x 1 x i1> %a, %b
  store <vscale x 1 x i1> %log_2, <vscale x 1 x i1>* %store_addr

  %log_3 = xor <vscale x 1 x i1> %a, %b
  store <vscale x 1 x i1> %log_3, <vscale x 1 x i1>* %store_addr

  ret void
}

define void @nxv2i1(<vscale x 2 x i1> %a, <vscale x 2 x i1> %b) nounwind {
; CHECK-LABEL: nxv2i1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(scratch)
; CHECK-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-NEXT:    vsetvli a1, zero, e8,mf4,ta,mu
; CHECK-NEXT:    vmand.mm v25, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e8,mf4,ta,mu
; CHECK-NEXT:    vmor.mm v25, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e8,mf4,ta,mu
; CHECK-NEXT:    vmxor.mm v25, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 2 x i1>*

  %log_1 = and <vscale x 2 x i1> %a, %b
  store <vscale x 2 x i1> %log_1, <vscale x 2 x i1>* %store_addr

  %log_2 = or <vscale x 2 x i1> %a, %b
  store <vscale x 2 x i1> %log_2, <vscale x 2 x i1>* %store_addr

  %log_3 = xor <vscale x 2 x i1> %a, %b
  store <vscale x 2 x i1> %log_3, <vscale x 2 x i1>* %store_addr

  ret void
}

define void @nxv4i1(<vscale x 4 x i1> %a, <vscale x 4 x i1> %b) nounwind {
; CHECK-LABEL: nxv4i1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(scratch)
; CHECK-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-NEXT:    vsetvli a1, zero, e8,mf2,ta,mu
; CHECK-NEXT:    vmand.mm v25, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e8,mf2,ta,mu
; CHECK-NEXT:    vmor.mm v25, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e8,mf2,ta,mu
; CHECK-NEXT:    vmxor.mm v25, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 4 x i1>*

  %log_1 = and <vscale x 4 x i1> %a, %b
  store <vscale x 4 x i1> %log_1, <vscale x 4 x i1>* %store_addr

  %log_2 = or <vscale x 4 x i1> %a, %b
  store <vscale x 4 x i1> %log_2, <vscale x 4 x i1>* %store_addr

  %log_3 = xor <vscale x 4 x i1> %a, %b
  store <vscale x 4 x i1> %log_3, <vscale x 4 x i1>* %store_addr

  ret void
}

define void @nxv8i1(<vscale x 8 x i1> %a, <vscale x 8 x i1> %b) nounwind {
; CHECK-LABEL: nxv8i1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(scratch)
; CHECK-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vmand.mm v25, v0, v16
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vmor.mm v25, v0, v16
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vmxor.mm v25, v0, v16
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 8 x i1>*

  %log_1 = and <vscale x 8 x i1> %a, %b
  store <vscale x 8 x i1> %log_1, <vscale x 8 x i1>* %store_addr

  %log_2 = or <vscale x 8 x i1> %a, %b
  store <vscale x 8 x i1> %log_2, <vscale x 8 x i1>* %store_addr

  %log_3 = xor <vscale x 8 x i1> %a, %b
  store <vscale x 8 x i1> %log_3, <vscale x 8 x i1>* %store_addr

  ret void
}

define void @nxv16i1(<vscale x 16 x i1> %a, <vscale x 16 x i1> %b) nounwind {
; CHECK-LABEL: nxv16i1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(scratch)
; CHECK-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-NEXT:    vsetvli a1, zero, e8,m2,ta,mu
; CHECK-NEXT:    vmand.mm v25, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e8,m2,ta,mu
; CHECK-NEXT:    vmor.mm v25, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e8,m2,ta,mu
; CHECK-NEXT:    vmxor.mm v25, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 16 x i1>*

  %log_1 = and <vscale x 16 x i1> %a, %b
  store <vscale x 16 x i1> %log_1, <vscale x 16 x i1>* %store_addr

  %log_2 = or <vscale x 16 x i1> %a, %b
  store <vscale x 16 x i1> %log_2, <vscale x 16 x i1>* %store_addr

  %log_3 = xor <vscale x 16 x i1> %a, %b
  store <vscale x 16 x i1> %log_3, <vscale x 16 x i1>* %store_addr

  ret void
}

define void @nxv32i1(<vscale x 32 x i1> %a, <vscale x 32 x i1> %b) nounwind {
; CHECK-LABEL: nxv32i1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(scratch)
; CHECK-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-NEXT:    vsetvli a1, zero, e8,m4,ta,mu
; CHECK-NEXT:    vmand.mm v25, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e8,m4,ta,mu
; CHECK-NEXT:    vmor.mm v25, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e8,m4,ta,mu
; CHECK-NEXT:    vmxor.mm v25, v0, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 32 x i1>*

  %log_1 = and <vscale x 32 x i1> %a, %b
  store <vscale x 32 x i1> %log_1, <vscale x 32 x i1>* %store_addr

  %log_2 = or <vscale x 32 x i1> %a, %b
  store <vscale x 32 x i1> %log_2, <vscale x 32 x i1>* %store_addr

  %log_3 = xor <vscale x 32 x i1> %a, %b
  store <vscale x 32 x i1> %log_3, <vscale x 32 x i1>* %store_addr

  ret void
}

; FIXME enable when nxv64i1 is supported

;define void @nxv64i1(<vscale x 64 x i1> %a, <vscale x 64 x i1> %b) nounwind {
;  %store_addr = bitcast i8* @scratch to <vscale x 64 x i1>*
;
;  %log_1 = and <vscale x 64 x i1> %a, %b
;  store <vscale x 64 x i1> %log_1, <vscale x 64 x i1>* %store_addr
;
;  %log_2 = or <vscale x 64 x i1> %a, %b
;  store <vscale x 64 x i1> %log_2, <vscale x 64 x i1>* %store_addr
;
;  %log_3 = xor <vscale x 64 x i1> %a, %b
;  store <vscale x 64 x i1> %log_3, <vscale x 64 x i1>* %store_addr
;
;  ret void
;}
