; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple riscv64 -mattr=+m,+a,+f,+d,+experimental-v < %s \
; RUN:   -epi-pipeline -o - | FileCheck %s

; LMUL = 1
; i64
define <vscale x 1 x i64> @nxv1i64(<vscale x 1 x i64> %va, <vscale x 1 x i64> %vb, i32 %evl1, i32 %evl2, i32 %offset) {
; CHECK-LABEL: nxv1i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a3, a0, e64,m1,ta,mu
; CHECK-NEXT:    vslidedown.vx v25, v8, a2
; CHECK-NEXT:    sub a0, a0, a2
; CHECK-NEXT:    vsetvli a1, a1, e64,m1,ta,mu
; CHECK-NEXT:    vid.v v26
; CHECK-NEXT:    vmsltu.vx v0, v26, a0
; CHECK-NEXT:    vslideup.vx v26, v9, a0
; CHECK-NEXT:    vmerge.vvm v8, v26, v25, v0
; CHECK-NEXT:    ret
  %vc = call <vscale x 1 x i64> @llvm.experimental.vector.vp.slideleftfill.nxv1i64(
    <vscale x 1 x i64> %va,
    <vscale x 1 x i64> %vb,
    i32 %evl1,
    i32 %evl2,
    i32 %offset)
  ret <vscale x 1 x i64> %vc
}

declare <vscale x 1 x i64> @llvm.experimental.vector.vp.slideleftfill.nxv1i64(<vscale x 1 x i64> %va, <vscale x 1 x i64> %vb, i32 %evl1, i32 %evl2, i32 %offset)

; LMUL = 1
; i32
define <vscale x 2 x i32> @nxv2i32(<vscale x 2 x i32> %va, <vscale x 2 x i32> %vb, i32 %evl1, i32 %evl2, i32 %offset) {
; CHECK-LABEL: nxv2i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a3, a0, e32,m1,ta,mu
; CHECK-NEXT:    vslidedown.vx v25, v8, a2
; CHECK-NEXT:    sub a0, a0, a2
; CHECK-NEXT:    vsetvli a1, a1, e32,m1,ta,mu
; CHECK-NEXT:    vid.v v26
; CHECK-NEXT:    vmsltu.vx v0, v26, a0
; CHECK-NEXT:    vslideup.vx v26, v9, a0
; CHECK-NEXT:    vmerge.vvm v8, v26, v25, v0
; CHECK-NEXT:    ret
  %vc = call <vscale x 2 x i32> @llvm.experimental.vector.vp.slideleftfill.nxv2i32(
    <vscale x 2 x i32> %va,
    <vscale x 2 x i32> %vb,
    i32 %evl1,
    i32 %evl2,
    i32 %offset)
  ret <vscale x 2 x i32> %vc
}

declare <vscale x 2 x i32> @llvm.experimental.vector.vp.slideleftfill.nxv2i32(<vscale x 2 x i32> %va, <vscale x 2 x i32> %vb, i32 %evl1, i32 %evl2, i32 %offset)

; LMUL = 1
; double
define <vscale x 1 x double> @nxv1f64(<vscale x 1 x double> %va, <vscale x 1 x double> %vb, i32 %evl1, i32 %evl2, i32 %offset) {
; CHECK-LABEL: nxv1f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a3, a0, e64,m1,ta,mu
; CHECK-NEXT:    vslidedown.vx v25, v8, a2
; CHECK-NEXT:    sub a0, a0, a2
; CHECK-NEXT:    vsetvli a1, a1, e64,m1,ta,mu
; CHECK-NEXT:    vid.v v26
; CHECK-NEXT:    vmsltu.vx v0, v26, a0
; CHECK-NEXT:    vslideup.vx v26, v9, a0
; CHECK-NEXT:    vmerge.vvm v8, v26, v25, v0
; CHECK-NEXT:    ret
  %vc = call <vscale x 1 x double> @llvm.experimental.vector.vp.slideleftfill.nxv1f64(
    <vscale x 1 x double> %va,
    <vscale x 1 x double> %vb,
    i32 %evl1,
    i32 %evl2,
    i32 %offset)
  ret <vscale x 1 x double> %vc
}

declare <vscale x 1 x double> @llvm.experimental.vector.vp.slideleftfill.nxv1f64(<vscale x 1 x double> %va, <vscale x 1 x double> %vb, i32 %evl1, i32 %evl2, i32 %offset)

; LMUL = 1
; float
define <vscale x 2 x float> @nxv2f32(<vscale x 2 x float> %va, <vscale x 2 x float> %vb, i32 %evl1, i32 %evl2, i32 %offset) {
; CHECK-LABEL: nxv2f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a3, a0, e32,m1,ta,mu
; CHECK-NEXT:    vslidedown.vx v25, v8, a2
; CHECK-NEXT:    sub a0, a0, a2
; CHECK-NEXT:    vsetvli a1, a1, e32,m1,ta,mu
; CHECK-NEXT:    vid.v v26
; CHECK-NEXT:    vmsltu.vx v0, v26, a0
; CHECK-NEXT:    vslideup.vx v26, v9, a0
; CHECK-NEXT:    vmerge.vvm v8, v26, v25, v0
; CHECK-NEXT:    ret
  %vc = call <vscale x 2 x float> @llvm.experimental.vector.vp.slideleftfill.nxv2f32(
    <vscale x 2 x float> %va,
    <vscale x 2 x float> %vb,
    i32 %evl1,
    i32 %evl2,
    i32 %offset)
  ret <vscale x 2 x float> %vc
}

declare <vscale x 2 x float> @llvm.experimental.vector.vp.slideleftfill.nxv2f32(<vscale x 2 x float> %va, <vscale x 2 x float> %vb, i32 %evl1, i32 %evl2, i32 %offset)

; LMUL = 2
; i64
define <vscale x 2 x i64> @nxv2i64(<vscale x 2 x i64> %va, <vscale x 2 x i64> %vb, i32 %evl1, i32 %evl2, i32 %offset) {
; CHECK-LABEL: nxv2i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a3, a0, e64,m2,ta,mu
; CHECK-NEXT:    vslidedown.vx v26, v8, a2
; CHECK-NEXT:    sub a0, a0, a2
; CHECK-NEXT:    vsetvli a1, a1, e64,m2,ta,mu
; CHECK-NEXT:    vid.v v28
; CHECK-NEXT:    vmsltu.vx v0, v28, a0
; CHECK-NEXT:    vslideup.vx v28, v10, a0
; CHECK-NEXT:    vmerge.vvm v8, v28, v26, v0
; CHECK-NEXT:    ret
  %vc = call <vscale x 2 x i64> @llvm.experimental.vector.vp.slideleftfill.nxv2i64(
    <vscale x 2 x i64> %va,
    <vscale x 2 x i64> %vb,
    i32 %evl1,
    i32 %evl2,
    i32 %offset)
  ret <vscale x 2 x i64> %vc
}

declare <vscale x 2 x i64> @llvm.experimental.vector.vp.slideleftfill.nxv2i64(<vscale x 2 x i64> %va, <vscale x 2 x i64> %vb, i32 %evl1, i32 %evl2, i32 %offset)
