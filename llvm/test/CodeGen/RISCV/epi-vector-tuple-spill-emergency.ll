; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+m,+a,+f,+d,+c,+experimental-v -o - < %s \
; RUN:     --verify-machineinstrs -epi-pipeline | FileCheck %s

target datalayout = "e-m:e-p:64:64-i64:64-i128:128-n64-S128-v128:128:128-v256:128:128-v512:128:128-v1024:128:128"
target triple = "riscv64-unknown-linux-gnu"

define void @t3fv_16(double* nocapture %ri, double* nocapture readnone %ii, double* nocapture readonly %W, i64 %rs, i64 %mb, i64 %me, i64 %ms) nounwind {
; CHECK-LABEL: t3fv_16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addi sp, sp, -176
; CHECK-NEXT:    sd ra, 168(sp) # 8-byte Folded Spill
; CHECK-NEXT:    sd s0, 160(sp) # 8-byte Folded Spill
; CHECK-NEXT:    sd s1, 152(sp) # 8-byte Folded Spill
; CHECK-NEXT:    sd s2, 144(sp) # 8-byte Folded Spill
; CHECK-NEXT:    sd s3, 136(sp) # 8-byte Folded Spill
; CHECK-NEXT:    sd s4, 128(sp) # 8-byte Folded Spill
; CHECK-NEXT:    sd s5, 120(sp) # 8-byte Folded Spill
; CHECK-NEXT:    sd s6, 112(sp) # 8-byte Folded Spill
; CHECK-NEXT:    sd s7, 104(sp) # 8-byte Folded Spill
; CHECK-NEXT:    sd s8, 96(sp) # 8-byte Folded Spill
; CHECK-NEXT:    sd s9, 88(sp) # 8-byte Folded Spill
; CHECK-NEXT:    sd s10, 80(sp) # 8-byte Folded Spill
; CHECK-NEXT:    sd s11, 72(sp) # 8-byte Folded Spill
; CHECK-NEXT:    csrr a1, vlenb
; CHECK-NEXT:    slli a7, a1, 2
; CHECK-NEXT:    add a1, a1, a7
; CHECK-NEXT:    sub sp, sp, a1
; CHECK-NEXT:    bge a4, a5, .LBB0_3
; CHECK-NEXT:  # %bb.1: # %for.body.lr.ph
; CHECK-NEXT:    slli a1, a4, 6
; CHECK-NEXT:    add a1, a1, a2
; CHECK-NEXT:    addi a2, zero, 2
; CHECK-NEXT:    vsetvli zero, a2, e64, m1, ta, mu
; CHECK-NEXT:    vid.v v25
; CHECK-NEXT:    vand.vi v25, v25, 1
; CHECK-NEXT:    vsetvli a2, zero, e64, m1, ta, mu
; CHECK-NEXT:    vand.vi v25, v25, 1
; CHECK-NEXT:    vmsne.vi v0, v25, 0
; CHECK-NEXT:    slli a2, a3, 3
; CHECK-NEXT:    sd a2, 64(sp) # 8-byte Folded Spill
; CHECK-NEXT:    sub s10, a5, a4
; CHECK-NEXT:    addi a2, zero, 24
; CHECK-NEXT:    mul a2, a3, a2
; CHECK-NEXT:    sd a2, 56(sp) # 8-byte Folded Spill
; CHECK-NEXT:    slli a2, a6, 3
; CHECK-NEXT:    sd a2, 48(sp) # 8-byte Folded Spill
; CHECK-NEXT:    addi a2, zero, 56
; CHECK-NEXT:    mul a2, a3, a2
; CHECK-NEXT:    sd a2, 40(sp) # 8-byte Folded Spill
; CHECK-NEXT:    addi a2, zero, 88
; CHECK-NEXT:    mul a2, a3, a2
; CHECK-NEXT:    sd a2, 32(sp) # 8-byte Folded Spill
; CHECK-NEXT:    addi a2, zero, 120
; CHECK-NEXT:    mul a2, a3, a2
; CHECK-NEXT:    sd a2, 24(sp) # 8-byte Folded Spill
; CHECK-NEXT:    addi a2, zero, 40
; CHECK-NEXT:    mul a2, a3, a2
; CHECK-NEXT:    sd a2, 16(sp) # 8-byte Folded Spill
; CHECK-NEXT:    addi a2, zero, 72
; CHECK-NEXT:    mul t6, a3, a2
; CHECK-NEXT:    addi a2, zero, 104
; CHECK-NEXT:    mul s2, a3, a2
; CHECK-NEXT:    slli s3, a3, 4
; CHECK-NEXT:    addi a2, zero, 48
; CHECK-NEXT:    mul s4, a3, a2
; CHECK-NEXT:    addi a2, zero, 80
; CHECK-NEXT:    mul s5, a3, a2
; CHECK-NEXT:    addi a2, zero, 112
; CHECK-NEXT:    mul s6, a3, a2
; CHECK-NEXT:    addi a2, zero, 96
; CHECK-NEXT:    lui a4, %hi(.LCPI0_0)
; CHECK-NEXT:    fld ft0, %lo(.LCPI0_0)(a4)
; CHECK-NEXT:    lui a4, %hi(.LCPI0_1)
; CHECK-NEXT:    fld ft1, %lo(.LCPI0_1)(a4)
; CHECK-NEXT:    lui a4, %hi(.LCPI0_2)
; CHECK-NEXT:    fld ft2, %lo(.LCPI0_2)(a4)
; CHECK-NEXT:    mul s7, a3, a2
; CHECK-NEXT:    slli s8, a3, 5
; CHECK-NEXT:    slli s9, a3, 6
; CHECK-NEXT:  .LBB0_2: # %for.body
; CHECK-NEXT:    # =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    addi a2, zero, 2
; CHECK-NEXT:    vsetvli zero, a2, e64, m1, ta, mu
; CHECK-NEXT:    vle64.v v25, (a1)
; CHECK-NEXT:    addi a2, a1, 16
; CHECK-NEXT:    vle64.v v26, (a2)
; CHECK-NEXT:    vtrn.vv v3, v25, v25
; CHECK-NEXT:    vfmul.vv v25, v26, v3
; CHECK-NEXT:    vtrn.vv v1, v26, v26
; CHECK-NEXT:    vfneg.v v26, v26, v0.t
; CHECK-NEXT:    vtrn.vv v5, v26, v26
; CHECK-NEXT:    addi a2, a1, 48
; CHECK-NEXT:    vle64.v v28, (a2)
; CHECK-NEXT:    vtrn.vv v5, v6, v26
; CHECK-NEXT:    vmv1r.v v27, v25
; CHECK-NEXT:    vfmacc.vv v27, v4, v5
; CHECK-NEXT:    vfnmsac.vv v25, v4, v5
; CHECK-NEXT:    vfmul.vv v26, v28, v3
; CHECK-NEXT:    vtrn.vv v5, v28, v28
; CHECK-NEXT:    vfneg.v v28, v28, v0.t
; CHECK-NEXT:    addi a2, a1, 32
; CHECK-NEXT:    vle64.v v29, (a2)
; CHECK-NEXT:    vtrn.vv v7, v28, v28
; CHECK-NEXT:    vtrn.vv v7, v8, v28
; CHECK-NEXT:    vfnmsac.vv v26, v4, v7
; CHECK-NEXT:    vfmul.vv v31, v29, v3
; CHECK-NEXT:    vfmul.vv v8, v29, v1
; CHECK-NEXT:    vtrn.vv v9, v27, v27
; CHECK-NEXT:    vfmul.vv v11, v29, v9
; CHECK-NEXT:    vtrn.vv v19, v25, v25
; CHECK-NEXT:    vfmul.vv v18, v29, v19
; CHECK-NEXT:    vtrn.vv v21, v29, v29
; CHECK-NEXT:    vfneg.v v29, v29, v0.t
; CHECK-NEXT:    vtrn.vv v12, v29, v29
; CHECK-NEXT:    vtrn.vv v12, v13, v29
; CHECK-NEXT:    vmv1r.v v25, v31
; CHECK-NEXT:    vfnmsac.vv v25, v4, v12
; CHECK-NEXT:    vmv1r.v v14, v8
; CHECK-NEXT:    addi a2, sp, 72
; CHECK-NEXT:    csrr a3, vlenb
; CHECK-NEXT:    vs1r.v v1, (a2) # Unknown-size Folded Spill
; CHECK-NEXT:    add a2, a2, a3
; CHECK-NEXT:    vs1r.v v2, (a2) # Unknown-size Folded Spill
; CHECK-NEXT:    vfnmsac.vv v14, v2, v12
; CHECK-NEXT:    vfmacc.vv v31, v4, v12
; CHECK-NEXT:    vfmacc.vv v8, v2, v12
; CHECK-NEXT:    vmv1r.v v23, v11
; CHECK-NEXT:    vfnmsac.vv v23, v10, v12
; CHECK-NEXT:    vmv1r.v v16, v18
; CHECK-NEXT:    vfnmsac.vv v16, v20, v12
; CHECK-NEXT:    vfmacc.vv v18, v20, v12
; CHECK-NEXT:    add t5, a0, s9
; CHECK-NEXT:    vle64.v v27, (t5)
; CHECK-NEXT:    vfmacc.vv v11, v10, v12
; CHECK-NEXT:    vle64.v v28, (a0)
; CHECK-NEXT:    csrr a2, vlenb
; CHECK-NEXT:    slli a2, a2, 2
; CHECK-NEXT:    add a2, a2, sp
; CHECK-NEXT:    addi a2, a2, 72
; CHECK-NEXT:    vs1r.v v28, (a2) # Unknown-size Folded Spill
; CHECK-NEXT:    vtrn.vv v12, v25, v25
; CHECK-NEXT:    vfmul.vv v29, v27, v12
; CHECK-NEXT:    vfneg.v v27, v27, v0.t
; CHECK-NEXT:    vtrn.vv v24, v27, v27
; CHECK-NEXT:    add ra, a0, s8
; CHECK-NEXT:    vle64.v v15, (ra)
; CHECK-NEXT:    vtrn.vv v24, v25, v27
; CHECK-NEXT:    vfnmsac.vv v29, v13, v24
; CHECK-NEXT:    vfadd.vv v28, v28, v29
; CHECK-NEXT:    vfmul.vv v30, v15, v9
; CHECK-NEXT:    vfneg.v v15, v15, v0.t
; CHECK-NEXT:    vtrn.vv v12, v15, v15
; CHECK-NEXT:    add s0, a0, s7
; CHECK-NEXT:    vle64.v v27, (s0)
; CHECK-NEXT:    vtrn.vv v12, v13, v15
; CHECK-NEXT:    vfnmsac.vv v30, v10, v12
; CHECK-NEXT:    vtrn.vv v9, v8, v8
; CHECK-NEXT:    vfmul.vv v8, v27, v9
; CHECK-NEXT:    vfneg.v v27, v27, v0.t
; CHECK-NEXT:    vtrn.vv v12, v27, v27
; CHECK-NEXT:    vtrn.vv v12, v13, v27
; CHECK-NEXT:    add a7, a0, s6
; CHECK-NEXT:    vle64.v v15, (a7)
; CHECK-NEXT:    vfnmsac.vv v8, v10, v12
; CHECK-NEXT:    vfadd.vv v27, v30, v8
; CHECK-NEXT:    vtrn.vv v12, v26, v26
; CHECK-NEXT:    vfmul.vv v9, v15, v12
; CHECK-NEXT:    vfneg.v v15, v15, v0.t
; CHECK-NEXT:    vtrn.vv v24, v15, v15
; CHECK-NEXT:    add t0, a0, s5
; CHECK-NEXT:    vle64.v v26, (t0)
; CHECK-NEXT:    vtrn.vv v24, v25, v15
; CHECK-NEXT:    vfnmsac.vv v9, v13, v24
; CHECK-NEXT:    vtrn.vv v12, v31, v31
; CHECK-NEXT:    vfmul.vv v10, v26, v12
; CHECK-NEXT:    vfneg.v v26, v26, v0.t
; CHECK-NEXT:    vtrn.vv v24, v26, v26
; CHECK-NEXT:    add t2, a0, s4
; CHECK-NEXT:    vle64.v v31, (t2)
; CHECK-NEXT:    vtrn.vv v24, v25, v26
; CHECK-NEXT:    vfnmsac.vv v10, v13, v24
; CHECK-NEXT:    vtrn.vv v14, v14, v14
; CHECK-NEXT:    vfmul.vv v13, v31, v14
; CHECK-NEXT:    vfneg.v v31, v31, v0.t
; CHECK-NEXT:    add a6, a0, s3
; CHECK-NEXT:    vle64.v v25, (a6)
; CHECK-NEXT:    vtrn.vv v1, v31, v31
; CHECK-NEXT:    vtrn.vv v1, v2, v31
; CHECK-NEXT:    vfnmsac.vv v13, v15, v1
; CHECK-NEXT:    vfmul.vv v14, v25, v19
; CHECK-NEXT:    vfneg.v v25, v25, v0.t
; CHECK-NEXT:    vtrn.vv v1, v25, v25
; CHECK-NEXT:    vtrn.vv v1, v2, v25
; CHECK-NEXT:    ld a2, 64(sp) # 8-byte Folded Reload
; CHECK-NEXT:    add s11, a0, a2
; CHECK-NEXT:    vle64.v v25, (s11)
; CHECK-NEXT:    vfnmsac.vv v14, v20, v1
; CHECK-NEXT:    vfadd.vv v26, v9, v13
; CHECK-NEXT:    vfadd.vv v7, v14, v10
; CHECK-NEXT:    vfmul.vv v15, v25, v3
; CHECK-NEXT:    vfneg.v v25, v25, v0.t
; CHECK-NEXT:    vtrn.vv v1, v25, v25
; CHECK-NEXT:    add a2, a0, s2
; CHECK-NEXT:    vle64.v v12, (a2)
; CHECK-NEXT:    vtrn.vv v1, v2, v25
; CHECK-NEXT:    vfnmsac.vv v15, v4, v1
; CHECK-NEXT:    vtrn.vv v1, v11, v11
; CHECK-NEXT:    vfmul.vv v17, v12, v1
; CHECK-NEXT:    vfneg.v v12, v12, v0.t
; CHECK-NEXT:    add a3, a0, t6
; CHECK-NEXT:    vle64.v v25, (a3)
; CHECK-NEXT:    vtrn.vv v3, v12, v12
; CHECK-NEXT:    vtrn.vv v3, v4, v12
; CHECK-NEXT:    vfnmsac.vv v17, v2, v3
; CHECK-NEXT:    vfmul.vv v19, v25, v21
; CHECK-NEXT:    vfneg.v v25, v25, v0.t
; CHECK-NEXT:    vtrn.vv v1, v25, v25
; CHECK-NEXT:    ld a4, 16(sp) # 8-byte Folded Reload
; CHECK-NEXT:    add t1, a0, a4
; CHECK-NEXT:    vle64.v v11, (t1)
; CHECK-NEXT:    vtrn.vv v1, v2, v25
; CHECK-NEXT:    vfnmsac.vv v19, v22, v1
; CHECK-NEXT:    vtrn.vv v1, v23, v23
; CHECK-NEXT:    vfmul.vv v25, v11, v1
; CHECK-NEXT:    vfneg.v v11, v11, v0.t
; CHECK-NEXT:    vtrn.vv v3, v11, v11
; CHECK-NEXT:    vtrn.vv v3, v4, v11
; CHECK-NEXT:    vfnmsac.vv v25, v2, v3
; CHECK-NEXT:    ld a4, 24(sp) # 8-byte Folded Reload
; CHECK-NEXT:    add a4, a4, a0
; CHECK-NEXT:    vle64.v v20, (a4)
; CHECK-NEXT:    vfadd.vv v11, v15, v19
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 1
; CHECK-NEXT:    add a5, a5, sp
; CHECK-NEXT:    addi a5, a5, 72
; CHECK-NEXT:    vs1r.v v11, (a5) # Unknown-size Folded Spill
; CHECK-NEXT:    vfadd.vv v31, v25, v17
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli t3, a5, 1
; CHECK-NEXT:    add a5, a5, t3
; CHECK-NEXT:    add a5, a5, sp
; CHECK-NEXT:    addi a5, a5, 72
; CHECK-NEXT:    vs1r.v v31, (a5) # Unknown-size Folded Spill
; CHECK-NEXT:    vfsub.vv v21, v11, v31
; CHECK-NEXT:    vfmul.vv v22, v20, v5
; CHECK-NEXT:    vfneg.v v20, v20, v0.t
; CHECK-NEXT:    vtrn.vv v1, v20, v20
; CHECK-NEXT:    ld a5, 32(sp) # 8-byte Folded Reload
; CHECK-NEXT:    add t3, a0, a5
; CHECK-NEXT:    vle64.v v23, (t3)
; CHECK-NEXT:    vtrn.vv v1, v2, v20
; CHECK-NEXT:    vfnmsac.vv v22, v6, v1
; CHECK-NEXT:    vtrn.vv v1, v18, v18
; CHECK-NEXT:    vfmul.vv v18, v23, v1
; CHECK-NEXT:    vfneg.v v23, v23, v0.t
; CHECK-NEXT:    vtrn.vv v3, v23, v23
; CHECK-NEXT:    ld a5, 40(sp) # 8-byte Folded Reload
; CHECK-NEXT:    add s1, a0, a5
; CHECK-NEXT:    vle64.v v20, (s1)
; CHECK-NEXT:    vtrn.vv v3, v4, v23
; CHECK-NEXT:    vfnmsac.vv v18, v2, v3
; CHECK-NEXT:    vtrn.vv v1, v16, v16
; CHECK-NEXT:    vfmul.vv v16, v20, v1
; CHECK-NEXT:    vfneg.v v20, v20, v0.t
; CHECK-NEXT:    ld a5, 56(sp) # 8-byte Folded Reload
; CHECK-NEXT:    add t4, a0, a5
; CHECK-NEXT:    vle64.v v23, (t4)
; CHECK-NEXT:    vtrn.vv v3, v20, v20
; CHECK-NEXT:    vtrn.vv v3, v4, v20
; CHECK-NEXT:    vfnmsac.vv v16, v2, v3
; CHECK-NEXT:    addi a5, sp, 72
; CHECK-NEXT:    sd a1, 8(sp)
; CHECK-NEXT:    csrr a1, vlenb
; CHECK-NEXT:    vl1r.v v3, (a5) # Unknown-size Folded Reload
; CHECK-NEXT:    add a5, a5, a1
; CHECK-NEXT:    vl1r.v v4, (a5) # Unknown-size Folded Reload
; CHECK-NEXT:    ld a1, 8(sp)
; CHECK-NEXT:    vfmul.vv v20, v23, v3
; CHECK-NEXT:    vfneg.v v23, v23, v0.t
; CHECK-NEXT:    vtrn.vv v1, v23, v23
; CHECK-NEXT:    vtrn.vv v1, v2, v23
; CHECK-NEXT:    vfnmsac.vv v20, v4, v1
; CHECK-NEXT:    vfadd.vv v23, v22, v16
; CHECK-NEXT:    vfadd.vv v24, v20, v18
; CHECK-NEXT:    vfsub.vv v1, v23, v24
; CHECK-NEXT:    vfsub.vv v2, v28, v27
; CHECK-NEXT:    vmv1r.v v12, v27
; CHECK-NEXT:    vmv1r.v v11, v28
; CHECK-NEXT:    vfadd.vv v3, v21, v1
; CHECK-NEXT:    vmv1r.v v4, v2
; CHECK-NEXT:    vfnmsac.vf v4, ft1, v3
; CHECK-NEXT:    vfsub.vv v5, v26, v7
; CHECK-NEXT:    vmv1r.v v31, v26
; CHECK-NEXT:    vmv1r.v v26, v7
; CHECK-NEXT:    vfsub.vv v21, v1, v21
; CHECK-NEXT:    vmv1r.v v1, v5
; CHECK-NEXT:    vfnmsac.vf v1, ft1, v21
; CHECK-NEXT:    vfneg.v v1, v1, v0.t
; CHECK-NEXT:    vtrn.vv v6, v1, v1
; CHECK-NEXT:    vtrn.vv v6, v7, v1
; CHECK-NEXT:    vfsub.vv v1, v4, v6
; CHECK-NEXT:    vse64.v v1, (t2)
; CHECK-NEXT:    vfmacc.vf v2, ft1, v3
; CHECK-NEXT:    vfmacc.vf v5, ft1, v21
; CHECK-NEXT:    vfneg.v v5, v5, v0.t
; CHECK-NEXT:    vtrn.vv v27, v5, v5
; CHECK-NEXT:    vtrn.vv v27, v28, v5
; CHECK-NEXT:    vfadd.vv v21, v2, v27
; CHECK-NEXT:    vse64.v v21, (a6)
; CHECK-NEXT:    vfadd.vv v21, v4, v6
; CHECK-NEXT:    vse64.v v21, (t0)
; CHECK-NEXT:    vfsub.vv v27, v2, v27
; CHECK-NEXT:    vse64.v v27, (a7)
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 2
; CHECK-NEXT:    add a5, a5, sp
; CHECK-NEXT:    addi a5, a5, 72
; CHECK-NEXT:    vl1r.v v27, (a5) # Unknown-size Folded Reload
; CHECK-NEXT:    vfsub.vv v28, v27, v29
; CHECK-NEXT:    vfsub.vv v27, v30, v8
; CHECK-NEXT:    vfsub.vv v29, v14, v10
; CHECK-NEXT:    vfsub.vv v30, v9, v13
; CHECK-NEXT:    vfadd.vv v8, v29, v30
; CHECK-NEXT:    vfsub.vv v29, v30, v29
; CHECK-NEXT:    vfsub.vv v30, v15, v19
; CHECK-NEXT:    vfsub.vv v25, v25, v17
; CHECK-NEXT:    vmv1r.v v9, v30
; CHECK-NEXT:    vfnmsac.vf v9, ft0, v25
; CHECK-NEXT:    vfmacc.vf v25, ft0, v30
; CHECK-NEXT:    vfsub.vv v30, v22, v16
; CHECK-NEXT:    vfsub.vv v10, v18, v20
; CHECK-NEXT:    vmv1r.v v13, v30
; CHECK-NEXT:    vfnmsac.vf v13, ft0, v10
; CHECK-NEXT:    vfmacc.vf v10, ft0, v30
; CHECK-NEXT:    vmv1r.v v30, v28
; CHECK-NEXT:    vfnmsac.vf v30, ft1, v8
; CHECK-NEXT:    vfadd.vv v14, v25, v10
; CHECK-NEXT:    vmv1r.v v15, v30
; CHECK-NEXT:    vfnmsac.vf v15, ft2, v14
; CHECK-NEXT:    vmv1r.v v16, v27
; CHECK-NEXT:    vfmacc.vf v16, ft1, v29
; CHECK-NEXT:    vfsub.vv v17, v13, v9
; CHECK-NEXT:    vmv1r.v v18, v16
; CHECK-NEXT:    vfnmsac.vf v18, ft2, v17
; CHECK-NEXT:    vfneg.v v18, v18, v0.t
; CHECK-NEXT:    vtrn.vv v1, v18, v18
; CHECK-NEXT:    vtrn.vv v1, v2, v18
; CHECK-NEXT:    vfsub.vv v18, v15, v1
; CHECK-NEXT:    vse64.v v18, (t1)
; CHECK-NEXT:    vfmacc.vf v30, ft2, v14
; CHECK-NEXT:    vfmacc.vf v16, ft2, v17
; CHECK-NEXT:    vfneg.v v16, v16, v0.t
; CHECK-NEXT:    vtrn.vv v3, v16, v16
; CHECK-NEXT:    vtrn.vv v3, v4, v16
; CHECK-NEXT:    vfsub.vv v14, v30, v3
; CHECK-NEXT:    vse64.v v14, (a2)
; CHECK-NEXT:    vfadd.vv v14, v15, v1
; CHECK-NEXT:    vse64.v v14, (t3)
; CHECK-NEXT:    vfadd.vv v30, v30, v3
; CHECK-NEXT:    vse64.v v30, (t4)
; CHECK-NEXT:    vfadd.vv v30, v11, v12
; CHECK-NEXT:    vfadd.vv v26, v26, v31
; CHECK-NEXT:    vfadd.vv v31, v30, v26
; CHECK-NEXT:    csrr a2, vlenb
; CHECK-NEXT:    slli a5, a2, 1
; CHECK-NEXT:    add a2, a2, a5
; CHECK-NEXT:    add a2, a2, sp
; CHECK-NEXT:    addi a2, a2, 72
; CHECK-NEXT:    vl1r.v v11, (a2) # Unknown-size Folded Reload
; CHECK-NEXT:    csrr a2, vlenb
; CHECK-NEXT:    slli a2, a2, 1
; CHECK-NEXT:    add a2, a2, sp
; CHECK-NEXT:    addi a2, a2, 72
; CHECK-NEXT:    vl1r.v v12, (a2) # Unknown-size Folded Reload
; CHECK-NEXT:    vfadd.vv v11, v12, v11
; CHECK-NEXT:    vfadd.vv v12, v23, v24
; CHECK-NEXT:    vfadd.vv v14, v11, v12
; CHECK-NEXT:    vfsub.vv v15, v31, v14
; CHECK-NEXT:    vse64.v v15, (t5)
; CHECK-NEXT:    vfsub.vv v26, v30, v26
; CHECK-NEXT:    vfsub.vv v30, v12, v11
; CHECK-NEXT:    vfneg.v v30, v30, v0.t
; CHECK-NEXT:    vtrn.vv v1, v30, v30
; CHECK-NEXT:    vtrn.vv v1, v2, v30
; CHECK-NEXT:    vfadd.vv v30, v26, v1
; CHECK-NEXT:    vse64.v v30, (ra)
; CHECK-NEXT:    vfadd.vv v30, v31, v14
; CHECK-NEXT:    vse64.v v30, (a0)
; CHECK-NEXT:    vfsub.vv v26, v26, v1
; CHECK-NEXT:    vse64.v v26, (s0)
; CHECK-NEXT:    vfmacc.vf v28, ft1, v8
; CHECK-NEXT:    vfadd.vv v26, v9, v13
; CHECK-NEXT:    vmv1r.v v30, v28
; CHECK-NEXT:    vfnmsac.vf v30, ft2, v26
; CHECK-NEXT:    vfnmsac.vf v27, ft1, v29
; CHECK-NEXT:    vfsub.vv v25, v25, v10
; CHECK-NEXT:    vmv1r.v v29, v27
; CHECK-NEXT:    vfnmsac.vf v29, ft2, v25
; CHECK-NEXT:    vfneg.v v29, v29, v0.t
; CHECK-NEXT:    vtrn.vv v1, v29, v29
; CHECK-NEXT:    vtrn.vv v1, v2, v29
; CHECK-NEXT:    vfsub.vv v29, v30, v1
; CHECK-NEXT:    vse64.v v29, (a3)
; CHECK-NEXT:    vfmacc.vf v28, ft2, v26
; CHECK-NEXT:    vfmacc.vf v27, ft2, v25
; CHECK-NEXT:    vfneg.v v27, v27, v0.t
; CHECK-NEXT:    vtrn.vv v3, v27, v27
; CHECK-NEXT:    vtrn.vv v3, v4, v27
; CHECK-NEXT:    vfadd.vv v25, v28, v3
; CHECK-NEXT:    vse64.v v25, (a4)
; CHECK-NEXT:    vfadd.vv v25, v30, v1
; CHECK-NEXT:    vse64.v v25, (s1)
; CHECK-NEXT:    vfsub.vv v25, v28, v3
; CHECK-NEXT:    vse64.v v25, (s11)
; CHECK-NEXT:    addi s10, s10, -1
; CHECK-NEXT:    ld a2, 48(sp) # 8-byte Folded Reload
; CHECK-NEXT:    add a0, a0, a2
; CHECK-NEXT:    addi a1, a1, 64
; CHECK-NEXT:    bnez s10, .LBB0_2
; CHECK-NEXT:  .LBB0_3: # %for.end
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a1, a0, 2
; CHECK-NEXT:    add a0, a0, a1
; CHECK-NEXT:    add sp, sp, a0
; CHECK-NEXT:    ld s11, 72(sp) # 8-byte Folded Reload
; CHECK-NEXT:    ld s10, 80(sp) # 8-byte Folded Reload
; CHECK-NEXT:    ld s9, 88(sp) # 8-byte Folded Reload
; CHECK-NEXT:    ld s8, 96(sp) # 8-byte Folded Reload
; CHECK-NEXT:    ld s7, 104(sp) # 8-byte Folded Reload
; CHECK-NEXT:    ld s6, 112(sp) # 8-byte Folded Reload
; CHECK-NEXT:    ld s5, 120(sp) # 8-byte Folded Reload
; CHECK-NEXT:    ld s4, 128(sp) # 8-byte Folded Reload
; CHECK-NEXT:    ld s3, 136(sp) # 8-byte Folded Reload
; CHECK-NEXT:    ld s2, 144(sp) # 8-byte Folded Reload
; CHECK-NEXT:    ld s1, 152(sp) # 8-byte Folded Reload
; CHECK-NEXT:    ld s0, 160(sp) # 8-byte Folded Reload
; CHECK-NEXT:    ld ra, 168(sp) # 8-byte Folded Reload
; CHECK-NEXT:    addi sp, sp, 176
; CHECK-NEXT:    ret
entry:
  %0 = tail call <vscale x 1 x double> @llvm.epi.vfmv.v.f.nxv1f64.f64(double 0x3FED906BCF328D46, i64 2)
  %1 = tail call <vscale x 1 x double> @llvm.epi.vfmv.v.f.nxv1f64.f64(double 0x3FE6A09E667F3BCD, i64 2)
  %2 = tail call <vscale x 1 x double> @llvm.epi.vfmv.v.f.nxv1f64.f64(double 0x3FDA827999FCEF32, i64 2)
  %cmp326 = icmp slt i64 %mb, %me
  br i1 %cmp326, label %for.body.lr.ph, label %for.end

for.body.lr.ph:                                   ; preds = %entry
  %mul = shl nsw i64 %mb, 3
  %add.ptr = getelementptr inbounds double, double* %W, i64 %mul
  %3 = tail call <vscale x 1 x i64> @llvm.epi.vid.nxv1i64(i64 2) #6
  %4 = tail call <vscale x 1 x i64> @llvm.epi.vmv.v.x.nxv1i64.i64(i64 1, i64 2) #6
  %5 = tail call <vscale x 1 x i64> @llvm.epi.vand.nxv1i64.nxv1i64(<vscale x 1 x i64> %3, <vscale x 1 x i64> %4, i64 2) #6
  %6 = trunc <vscale x 1 x i64> %5 to <vscale x 1 x i1>
  %mul21 = shl nsw i64 %rs, 3
  %mul26 = shl nsw i64 %rs, 2
  %mul31 = mul nsw i64 %rs, 12
  %mul36 = mul nsw i64 %rs, 14
  %mul41 = mul nsw i64 %rs, 10
  %mul46 = mul nsw i64 %rs, 6
  %mul51 = shl nsw i64 %rs, 1
  %mul62 = mul nsw i64 %rs, 13
  %mul68 = mul nsw i64 %rs, 9
  %mul74 = mul nsw i64 %rs, 5
  %mul80 = mul nsw i64 %rs, 15
  %mul86 = mul nsw i64 %rs, 11
  %mul92 = mul nsw i64 %rs, 7
  %mul98 = mul nsw i64 %rs, 3
  br label %for.body

for.body:                                         ; preds = %for.body, %for.body.lr.ph
  %W.addr.0329 = phi double* [ %add.ptr, %for.body.lr.ph ], [ %add.ptr175, %for.body ]
  %m.0328 = phi i64 [ %mb, %for.body.lr.ph ], [ %add, %for.body ]
  %x.0327 = phi double* [ %ri, %for.body.lr.ph ], [ %add.ptr174, %for.body ]
  %7 = bitcast double* %W.addr.0329 to <vscale x 1 x double>*
  %8 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %7, i64 2) #6
  %arrayidx1 = getelementptr inbounds double, double* %W.addr.0329, i64 2
  %9 = bitcast double* %arrayidx1 to <vscale x 1 x double>*
  %10 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* nonnull %9, i64 2) #6
  %11 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %8, <vscale x 1 x double> %8, i64 2) #6
  %12 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %11, 0
  %13 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %11, 1
  %14 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %10, <vscale x 1 x double> %12, i64 2) #6
  %15 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %10, <vscale x 1 x double> %10, <vscale x 1 x double> %10, <vscale x 1 x i1> %6, i64 2) #6
  %16 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %15, <vscale x 1 x double> %15, i64 2) #6
  %17 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %16, 1
  %18 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %17, <vscale x 1 x double> %15, i64 2) #6
  %19 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %18, 0
  %20 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %14, <vscale x 1 x double> %13, <vscale x 1 x double> %19, i64 2) #6
  %21 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %14, <vscale x 1 x double> %13, <vscale x 1 x double> %19, i64 2) #6
  %arrayidx5 = getelementptr inbounds double, double* %W.addr.0329, i64 6
  %22 = bitcast double* %arrayidx5 to <vscale x 1 x double>*
  %23 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* nonnull %22, i64 2) #6
  %24 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %23, <vscale x 1 x double> %12, i64 2) #6
  %25 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %23, <vscale x 1 x double> %23, <vscale x 1 x double> %23, <vscale x 1 x i1> %6, i64 2) #6
  %26 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %25, <vscale x 1 x double> %25, i64 2) #6
  %27 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %26, 1
  %28 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %27, <vscale x 1 x double> %25, i64 2) #6
  %29 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %28, 0
  %30 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %24, <vscale x 1 x double> %13, <vscale x 1 x double> %29, i64 2) #6
  %arrayidx8 = getelementptr inbounds double, double* %W.addr.0329, i64 4
  %31 = bitcast double* %arrayidx8 to <vscale x 1 x double>*
  %32 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* nonnull %31, i64 2) #6
  %33 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %32, <vscale x 1 x double> %12, i64 2) #6
  %34 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %32, <vscale x 1 x double> %32, <vscale x 1 x double> %32, <vscale x 1 x i1> %6, i64 2) #6
  %35 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %34, <vscale x 1 x double> %34, i64 2) #6
  %36 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %35, 1
  %37 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %36, <vscale x 1 x double> %34, i64 2) #6
  %38 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %37, 0
  %39 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %33, <vscale x 1 x double> %13, <vscale x 1 x double> %38, i64 2) #6
  %40 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %10, <vscale x 1 x double> %10, i64 2) #6
  %41 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %40, 0
  %42 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %40, 1
  %43 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %32, <vscale x 1 x double> %41, i64 2) #6
  %44 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %43, <vscale x 1 x double> %42, <vscale x 1 x double> %38, i64 2) #6
  %45 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %33, <vscale x 1 x double> %13, <vscale x 1 x double> %38, i64 2) #6
  %46 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %43, <vscale x 1 x double> %42, <vscale x 1 x double> %38, i64 2) #6
  %47 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %20, <vscale x 1 x double> %20, i64 2) #6
  %48 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %47, 0
  %49 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %47, 1
  %50 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %32, <vscale x 1 x double> %48, i64 2) #6
  %51 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %50, <vscale x 1 x double> %49, <vscale x 1 x double> %38, i64 2) #6
  %52 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %21, <vscale x 1 x double> %21, i64 2) #6
  %53 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %52, 0
  %54 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %52, 1
  %55 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %32, <vscale x 1 x double> %53, i64 2) #6
  %56 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %55, <vscale x 1 x double> %54, <vscale x 1 x double> %38, i64 2) #6
  %57 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %55, <vscale x 1 x double> %54, <vscale x 1 x double> %38, i64 2) #6
  %58 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %50, <vscale x 1 x double> %49, <vscale x 1 x double> %38, i64 2) #6
  %59 = bitcast double* %x.0327 to <vscale x 1 x double>*
  %60 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %59, i64 2) #6
  %arrayidx22 = getelementptr inbounds double, double* %x.0327, i64 %mul21
  %61 = bitcast double* %arrayidx22 to <vscale x 1 x double>*
  %62 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %61, i64 2) #6
  %63 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %39, <vscale x 1 x double> %39, i64 2) #6
  %64 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %63, 0
  %65 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %63, 1
  %66 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %62, <vscale x 1 x double> %64, i64 2) #6
  %67 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %62, <vscale x 1 x double> %62, <vscale x 1 x double> %62, <vscale x 1 x i1> %6, i64 2) #6
  %68 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %67, <vscale x 1 x double> %67, i64 2) #6
  %69 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %68, 1
  %70 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %69, <vscale x 1 x double> %67, i64 2) #6
  %71 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %70, 0
  %72 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %66, <vscale x 1 x double> %65, <vscale x 1 x double> %71, i64 2) #6
  %73 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %60, <vscale x 1 x double> %72, i64 2)
  %74 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %60, <vscale x 1 x double> %72, i64 2)
  %arrayidx27 = getelementptr inbounds double, double* %x.0327, i64 %mul26
  %75 = bitcast double* %arrayidx27 to <vscale x 1 x double>*
  %76 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %75, i64 2) #6
  %77 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %76, <vscale x 1 x double> %48, i64 2) #6
  %78 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %76, <vscale x 1 x double> %76, <vscale x 1 x double> %76, <vscale x 1 x i1> %6, i64 2) #6
  %79 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %78, <vscale x 1 x double> %78, i64 2) #6
  %80 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %79, 1
  %81 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %80, <vscale x 1 x double> %78, i64 2) #6
  %82 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %81, 0
  %83 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %77, <vscale x 1 x double> %49, <vscale x 1 x double> %82, i64 2) #6
  %arrayidx32 = getelementptr inbounds double, double* %x.0327, i64 %mul31
  %84 = bitcast double* %arrayidx32 to <vscale x 1 x double>*
  %85 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %84, i64 2) #6
  %86 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %46, <vscale x 1 x double> %46, i64 2) #6
  %87 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %86, 0
  %88 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %86, 1
  %89 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %85, <vscale x 1 x double> %87, i64 2) #6
  %90 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %85, <vscale x 1 x double> %85, <vscale x 1 x double> %85, <vscale x 1 x i1> %6, i64 2) #6
  %91 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %90, <vscale x 1 x double> %90, i64 2) #6
  %92 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %91, 1
  %93 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %92, <vscale x 1 x double> %90, i64 2) #6
  %94 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %93, 0
  %95 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %89, <vscale x 1 x double> %88, <vscale x 1 x double> %94, i64 2) #6
  %96 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %83, <vscale x 1 x double> %95, i64 2)
  %97 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %83, <vscale x 1 x double> %95, i64 2)
  %arrayidx37 = getelementptr inbounds double, double* %x.0327, i64 %mul36
  %98 = bitcast double* %arrayidx37 to <vscale x 1 x double>*
  %99 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %98, i64 2) #6
  %100 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %30, <vscale x 1 x double> %30, i64 2) #6
  %101 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %100, 0
  %102 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %100, 1
  %103 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %99, <vscale x 1 x double> %101, i64 2) #6
  %104 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %99, <vscale x 1 x double> %99, <vscale x 1 x double> %99, <vscale x 1 x i1> %6, i64 2) #6
  %105 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %104, <vscale x 1 x double> %104, i64 2) #6
  %106 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %105, 1
  %107 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %106, <vscale x 1 x double> %104, i64 2) #6
  %108 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %107, 0
  %109 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %103, <vscale x 1 x double> %102, <vscale x 1 x double> %108, i64 2) #6
  %arrayidx42 = getelementptr inbounds double, double* %x.0327, i64 %mul41
  %110 = bitcast double* %arrayidx42 to <vscale x 1 x double>*
  %111 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %110, i64 2) #6
  %112 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %45, <vscale x 1 x double> %45, i64 2) #6
  %113 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %112, 0
  %114 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %112, 1
  %115 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %111, <vscale x 1 x double> %113, i64 2) #6
  %116 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %111, <vscale x 1 x double> %111, <vscale x 1 x double> %111, <vscale x 1 x i1> %6, i64 2) #6
  %117 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %116, <vscale x 1 x double> %116, i64 2) #6
  %118 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %117, 1
  %119 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %118, <vscale x 1 x double> %116, i64 2) #6
  %120 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %119, 0
  %121 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %115, <vscale x 1 x double> %114, <vscale x 1 x double> %120, i64 2) #6
  %arrayidx47 = getelementptr inbounds double, double* %x.0327, i64 %mul46
  %122 = bitcast double* %arrayidx47 to <vscale x 1 x double>*
  %123 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %122, i64 2) #6
  %124 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %44, <vscale x 1 x double> %44, i64 2) #6
  %125 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %124, 0
  %126 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %124, 1
  %127 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %123, <vscale x 1 x double> %125, i64 2) #6
  %128 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %123, <vscale x 1 x double> %123, <vscale x 1 x double> %123, <vscale x 1 x i1> %6, i64 2) #6
  %129 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %128, <vscale x 1 x double> %128, i64 2) #6
  %130 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %129, 1
  %131 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %130, <vscale x 1 x double> %128, i64 2) #6
  %132 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %131, 0
  %133 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %127, <vscale x 1 x double> %126, <vscale x 1 x double> %132, i64 2) #6
  %arrayidx52 = getelementptr inbounds double, double* %x.0327, i64 %mul51
  %134 = bitcast double* %arrayidx52 to <vscale x 1 x double>*
  %135 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %134, i64 2) #6
  %136 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %135, <vscale x 1 x double> %53, i64 2) #6
  %137 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %135, <vscale x 1 x double> %135, <vscale x 1 x double> %135, <vscale x 1 x i1> %6, i64 2) #6
  %138 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %137, <vscale x 1 x double> %137, i64 2) #6
  %139 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %138, 1
  %140 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %139, <vscale x 1 x double> %137, i64 2) #6
  %141 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %140, 0
  %142 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %136, <vscale x 1 x double> %54, <vscale x 1 x double> %141, i64 2) #6
  %143 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %109, <vscale x 1 x double> %133, i64 2)
  %144 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %142, <vscale x 1 x double> %121, i64 2)
  %145 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %142, <vscale x 1 x double> %121, i64 2)
  %146 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %109, <vscale x 1 x double> %133, i64 2)
  %147 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %145, <vscale x 1 x double> %146, i64 2)
  %148 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %146, <vscale x 1 x double> %145, i64 2)
  %arrayidx57 = getelementptr inbounds double, double* %x.0327, i64 %rs
  %149 = bitcast double* %arrayidx57 to <vscale x 1 x double>*
  %150 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %149, i64 2) #6
  %151 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %150, <vscale x 1 x double> %12, i64 2) #6
  %152 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %150, <vscale x 1 x double> %150, <vscale x 1 x double> %150, <vscale x 1 x i1> %6, i64 2) #6
  %153 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %152, <vscale x 1 x double> %152, i64 2) #6
  %154 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %153, 1
  %155 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %154, <vscale x 1 x double> %152, i64 2) #6
  %156 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %155, 0
  %157 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %151, <vscale x 1 x double> %13, <vscale x 1 x double> %156, i64 2) #6
  %arrayidx63 = getelementptr inbounds double, double* %x.0327, i64 %mul62
  %158 = bitcast double* %arrayidx63 to <vscale x 1 x double>*
  %159 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %158, i64 2) #6
  %160 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %58, <vscale x 1 x double> %58, i64 2) #6
  %161 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %160, 0
  %162 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %160, 1
  %163 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %159, <vscale x 1 x double> %161, i64 2) #6
  %164 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %159, <vscale x 1 x double> %159, <vscale x 1 x double> %159, <vscale x 1 x i1> %6, i64 2) #6
  %165 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %164, <vscale x 1 x double> %164, i64 2) #6
  %166 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %165, 1
  %167 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %166, <vscale x 1 x double> %164, i64 2) #6
  %168 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %167, 0
  %169 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %163, <vscale x 1 x double> %162, <vscale x 1 x double> %168, i64 2) #6
  %arrayidx69 = getelementptr inbounds double, double* %x.0327, i64 %mul68
  %170 = bitcast double* %arrayidx69 to <vscale x 1 x double>*
  %171 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %170, i64 2) #6
  %172 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %32, <vscale x 1 x double> %32, i64 2) #6
  %173 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %172, 0
  %174 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %172, 1
  %175 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %171, <vscale x 1 x double> %173, i64 2) #6
  %176 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %171, <vscale x 1 x double> %171, <vscale x 1 x double> %171, <vscale x 1 x i1> %6, i64 2) #6
  %177 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %176, <vscale x 1 x double> %176, i64 2) #6
  %178 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %177, 1
  %179 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %178, <vscale x 1 x double> %176, i64 2) #6
  %180 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %179, 0
  %181 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %175, <vscale x 1 x double> %174, <vscale x 1 x double> %180, i64 2) #6
  %arrayidx75 = getelementptr inbounds double, double* %x.0327, i64 %mul74
  %182 = bitcast double* %arrayidx75 to <vscale x 1 x double>*
  %183 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %182, i64 2) #6
  %184 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %51, <vscale x 1 x double> %51, i64 2) #6
  %185 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %184, 0
  %186 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %184, 1
  %187 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %183, <vscale x 1 x double> %185, i64 2) #6
  %188 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %183, <vscale x 1 x double> %183, <vscale x 1 x double> %183, <vscale x 1 x i1> %6, i64 2) #6
  %189 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %188, <vscale x 1 x double> %188, i64 2) #6
  %190 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %189, 1
  %191 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %190, <vscale x 1 x double> %188, i64 2) #6
  %192 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %191, 0
  %193 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %187, <vscale x 1 x double> %186, <vscale x 1 x double> %192, i64 2) #6
  %194 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %157, <vscale x 1 x double> %181, i64 2)
  %195 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %193, <vscale x 1 x double> %169, i64 2)
  %196 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %194, <vscale x 1 x double> %195, i64 2)
  %197 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %157, <vscale x 1 x double> %181, i64 2)
  %198 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %193, <vscale x 1 x double> %169, i64 2)
  %199 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %197, <vscale x 1 x double> %2, <vscale x 1 x double> %198, i64 2)
  %200 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %198, <vscale x 1 x double> %2, <vscale x 1 x double> %197, i64 2)
  %arrayidx81 = getelementptr inbounds double, double* %x.0327, i64 %mul80
  %201 = bitcast double* %arrayidx81 to <vscale x 1 x double>*
  %202 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %201, i64 2) #6
  %203 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %23, <vscale x 1 x double> %23, i64 2) #6
  %204 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %203, 0
  %205 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %203, 1
  %206 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %202, <vscale x 1 x double> %204, i64 2) #6
  %207 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %202, <vscale x 1 x double> %202, <vscale x 1 x double> %202, <vscale x 1 x i1> %6, i64 2) #6
  %208 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %207, <vscale x 1 x double> %207, i64 2) #6
  %209 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %208, 1
  %210 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %209, <vscale x 1 x double> %207, i64 2) #6
  %211 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %210, 0
  %212 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %206, <vscale x 1 x double> %205, <vscale x 1 x double> %211, i64 2) #6
  %arrayidx87 = getelementptr inbounds double, double* %x.0327, i64 %mul86
  %213 = bitcast double* %arrayidx87 to <vscale x 1 x double>*
  %214 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %213, i64 2) #6
  %215 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %57, <vscale x 1 x double> %57, i64 2) #6
  %216 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %215, 0
  %217 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %215, 1
  %218 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %214, <vscale x 1 x double> %216, i64 2) #6
  %219 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %214, <vscale x 1 x double> %214, <vscale x 1 x double> %214, <vscale x 1 x i1> %6, i64 2) #6
  %220 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %219, <vscale x 1 x double> %219, i64 2) #6
  %221 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %220, 1
  %222 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %221, <vscale x 1 x double> %219, i64 2) #6
  %223 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %222, 0
  %224 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %218, <vscale x 1 x double> %217, <vscale x 1 x double> %223, i64 2) #6
  %arrayidx93 = getelementptr inbounds double, double* %x.0327, i64 %mul92
  %225 = bitcast double* %arrayidx93 to <vscale x 1 x double>*
  %226 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %225, i64 2) #6
  %227 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %56, <vscale x 1 x double> %56, i64 2) #6
  %228 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %227, 0
  %229 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %227, 1
  %230 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %226, <vscale x 1 x double> %228, i64 2) #6
  %231 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %226, <vscale x 1 x double> %226, <vscale x 1 x double> %226, <vscale x 1 x i1> %6, i64 2) #6
  %232 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %231, <vscale x 1 x double> %231, i64 2) #6
  %233 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %232, 1
  %234 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %233, <vscale x 1 x double> %231, i64 2) #6
  %235 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %234, 0
  %236 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %230, <vscale x 1 x double> %229, <vscale x 1 x double> %235, i64 2) #6
  %arrayidx99 = getelementptr inbounds double, double* %x.0327, i64 %mul98
  %237 = bitcast double* %arrayidx99 to <vscale x 1 x double>*
  %238 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %237, i64 2) #6
  %239 = tail call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double> %238, <vscale x 1 x double> %41, i64 2) #6
  %240 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %238, <vscale x 1 x double> %238, <vscale x 1 x double> %238, <vscale x 1 x i1> %6, i64 2) #6
  %241 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %240, <vscale x 1 x double> %240, i64 2) #6
  %242 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %241, 1
  %243 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %242, <vscale x 1 x double> %240, i64 2) #6
  %244 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %243, 0
  %245 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %239, <vscale x 1 x double> %42, <vscale x 1 x double> %244, i64 2) #6
  %246 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %212, <vscale x 1 x double> %236, i64 2)
  %247 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %245, <vscale x 1 x double> %224, i64 2)
  %248 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %246, <vscale x 1 x double> %247, i64 2)
  %249 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %212, <vscale x 1 x double> %236, i64 2)
  %250 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %224, <vscale x 1 x double> %245, i64 2)
  %251 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %249, <vscale x 1 x double> %2, <vscale x 1 x double> %250, i64 2)
  %252 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %250, <vscale x 1 x double> %2, <vscale x 1 x double> %249, i64 2)
  %253 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %73, <vscale x 1 x double> %96, i64 2)
  %254 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %196, <vscale x 1 x double> %248, i64 2)
  %255 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %253, <vscale x 1 x double> %1, <vscale x 1 x double> %254, i64 2)
  %256 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %253, <vscale x 1 x double> %1, <vscale x 1 x double> %254, i64 2)
  %257 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %143, <vscale x 1 x double> %144, i64 2)
  %258 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %248, <vscale x 1 x double> %196, i64 2)
  %259 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %257, <vscale x 1 x double> %1, <vscale x 1 x double> %258, i64 2)
  %260 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %257, <vscale x 1 x double> %1, <vscale x 1 x double> %258, i64 2)
  %261 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %259, <vscale x 1 x double> %259, <vscale x 1 x double> %259, <vscale x 1 x i1> %6, i64 2) #6
  %262 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %261, <vscale x 1 x double> %261, i64 2) #6
  %263 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %262, 1
  %264 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %263, <vscale x 1 x double> %261, i64 2) #6
  %265 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %264, 0
  %266 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %255, <vscale x 1 x double> %265, i64 2)
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %266, <vscale x 1 x double>* %122, i64 2) #6
  %267 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %260, <vscale x 1 x double> %260, <vscale x 1 x double> %260, <vscale x 1 x i1> %6, i64 2) #6
  %268 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %267, <vscale x 1 x double> %267, i64 2) #6
  %269 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %268, 1
  %270 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %269, <vscale x 1 x double> %267, i64 2) #6
  %271 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %270, 0
  %272 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %256, <vscale x 1 x double> %271, i64 2)
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %272, <vscale x 1 x double>* %134, i64 2) #6
  %273 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %255, <vscale x 1 x double> %265, i64 2)
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %273, <vscale x 1 x double>* %110, i64 2) #6
  %274 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %256, <vscale x 1 x double> %271, i64 2)
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %274, <vscale x 1 x double>* %98, i64 2) #6
  %275 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %74, <vscale x 1 x double> %1, <vscale x 1 x double> %147, i64 2)
  %276 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %200, <vscale x 1 x double> %252, i64 2)
  %277 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %275, <vscale x 1 x double> %0, <vscale x 1 x double> %276, i64 2)
  %278 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %275, <vscale x 1 x double> %0, <vscale x 1 x double> %276, i64 2)
  %279 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %97, <vscale x 1 x double> %1, <vscale x 1 x double> %148, i64 2)
  %280 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %251, <vscale x 1 x double> %199, i64 2)
  %281 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %279, <vscale x 1 x double> %0, <vscale x 1 x double> %280, i64 2)
  %282 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %279, <vscale x 1 x double> %0, <vscale x 1 x double> %280, i64 2)
  %283 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %281, <vscale x 1 x double> %281, <vscale x 1 x double> %281, <vscale x 1 x i1> %6, i64 2) #6
  %284 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %283, <vscale x 1 x double> %283, i64 2) #6
  %285 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %284, 1
  %286 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %285, <vscale x 1 x double> %283, i64 2) #6
  %287 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %286, 0
  %288 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %277, <vscale x 1 x double> %287, i64 2)
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %288, <vscale x 1 x double>* %182, i64 2) #6
  %289 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %282, <vscale x 1 x double> %282, <vscale x 1 x double> %282, <vscale x 1 x i1> %6, i64 2) #6
  %290 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %289, <vscale x 1 x double> %289, i64 2) #6
  %291 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %290, 1
  %292 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %291, <vscale x 1 x double> %289, i64 2) #6
  %293 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %292, 0
  %294 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %278, <vscale x 1 x double> %293, i64 2)
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %294, <vscale x 1 x double>* %158, i64 2) #6
  %295 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %277, <vscale x 1 x double> %287, i64 2)
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %295, <vscale x 1 x double>* %213, i64 2) #6
  %296 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %278, <vscale x 1 x double> %293, i64 2)
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %296, <vscale x 1 x double>* %237, i64 2) #6
  %297 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %73, <vscale x 1 x double> %96, i64 2)
  %298 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %144, <vscale x 1 x double> %143, i64 2)
  %299 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %297, <vscale x 1 x double> %298, i64 2)
  %300 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %297, <vscale x 1 x double> %298, i64 2)
  %301 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %194, <vscale x 1 x double> %195, i64 2)
  %302 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %246, <vscale x 1 x double> %247, i64 2)
  %303 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %301, <vscale x 1 x double> %302, i64 2)
  %304 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %302, <vscale x 1 x double> %301, i64 2)
  %305 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %299, <vscale x 1 x double> %303, i64 2)
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %305, <vscale x 1 x double>* %61, i64 2) #6
  %306 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %304, <vscale x 1 x double> %304, <vscale x 1 x double> %304, <vscale x 1 x i1> %6, i64 2) #6
  %307 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %306, <vscale x 1 x double> %306, i64 2) #6
  %308 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %307, 1
  %309 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %308, <vscale x 1 x double> %306, i64 2) #6
  %310 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %309, 0
  %311 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %300, <vscale x 1 x double> %310, i64 2)
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %311, <vscale x 1 x double>* %75, i64 2) #6
  %312 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %299, <vscale x 1 x double> %303, i64 2)
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %312, <vscale x 1 x double>* %59, i64 2) #6
  %313 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %300, <vscale x 1 x double> %310, i64 2)
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %313, <vscale x 1 x double>* %84, i64 2) #6
  %314 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %74, <vscale x 1 x double> %1, <vscale x 1 x double> %147, i64 2)
  %315 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %199, <vscale x 1 x double> %251, i64 2)
  %316 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %314, <vscale x 1 x double> %0, <vscale x 1 x double> %315, i64 2)
  %317 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %314, <vscale x 1 x double> %0, <vscale x 1 x double> %315, i64 2)
  %318 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %97, <vscale x 1 x double> %1, <vscale x 1 x double> %148, i64 2)
  %319 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %200, <vscale x 1 x double> %252, i64 2)
  %320 = tail call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %318, <vscale x 1 x double> %0, <vscale x 1 x double> %319, i64 2)
  %321 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %318, <vscale x 1 x double> %0, <vscale x 1 x double> %319, i64 2)
  %322 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %320, <vscale x 1 x double> %320, <vscale x 1 x double> %320, <vscale x 1 x i1> %6, i64 2) #6
  %323 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %322, <vscale x 1 x double> %322, i64 2) #6
  %324 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %323, 1
  %325 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %324, <vscale x 1 x double> %322, i64 2) #6
  %326 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %325, 0
  %327 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %316, <vscale x 1 x double> %326, i64 2)
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %327, <vscale x 1 x double>* %170, i64 2) #6
  %328 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %321, <vscale x 1 x double> %321, <vscale x 1 x double> %321, <vscale x 1 x i1> %6, i64 2) #6
  %329 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %328, <vscale x 1 x double> %328, i64 2) #6
  %330 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %329, 1
  %331 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %330, <vscale x 1 x double> %328, i64 2) #6
  %332 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %331, 0
  %333 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %317, <vscale x 1 x double> %332, i64 2)
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %333, <vscale x 1 x double>* %201, i64 2) #6
  %334 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %316, <vscale x 1 x double> %326, i64 2)
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %334, <vscale x 1 x double>* %225, i64 2) #6
  %335 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %317, <vscale x 1 x double> %332, i64 2)
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %335, <vscale x 1 x double>* %149, i64 2) #6
  %add = add i64 %m.0328, 1
  %add.ptr174 = getelementptr inbounds double, double* %x.0327, i64 %ms
  %add.ptr175 = getelementptr inbounds double, double* %W.addr.0329, i64 8
  %exitcond = icmp eq i64 %add, %me
  br i1 %exitcond, label %for.end, label %for.body

for.end:                                          ; preds = %for.body, %entry
  ret void
}

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfmv.v.f.nxv1f64.f64(double, i64) #3

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64)

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64)

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, i64)

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, i64)

; Function Attrs: nounwind readonly
declare <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* nocapture, i64)

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64)

; Function Attrs: nounwind readnone
declare { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64)

; Function Attrs: nounwind writeonly
declare void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>* nocapture, i64)

; Function Attrs: nounwind readnone
declare <vscale x 1 x i64> @llvm.epi.vid.nxv1i64(i64)

; Function Attrs: nounwind readnone
declare <vscale x 1 x i64> @llvm.epi.vmv.v.x.nxv1i64.i64(i64, i64)

; Function Attrs: nounwind readnone
declare <vscale x 1 x i64> @llvm.epi.vand.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, i64)

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>, i64)
