; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+d,+experimental-v -verify-machineinstrs -O0 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=CHECK-O0 %s
; RUN: llc -mtriple=riscv64 -mattr=+d,+experimental-v -verify-machineinstrs -O2 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=CHECK-O2 %s

define void @test_nested_branching(i64 %rvl, i64 %extra1, i64 %extra2, double* %a, double* %b, double* %c) {
; CHECK-O0-LABEL: test_nested_branching:
; CHECK-O0:       # %bb.0: # %entry
; CHECK-O0-NEXT:    addi sp, sp, -80
; CHECK-O0-NEXT:    .cfi_def_cfa_offset 80
; CHECK-O0-NEXT:    sd a5, 32(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a4, 40(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a3, 48(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a2, 56(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a1, 64(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a0, 72(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    li a1, 45
; CHECK-O0-NEXT:    blt a0, a1, .LBB0_5
; CHECK-O0-NEXT:    j .LBB0_1
; CHECK-O0-NEXT:  .LBB0_1: # %if.then
; CHECK-O0-NEXT:    ld a0, 72(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    li a1, 124
; CHECK-O0-NEXT:    blt a0, a1, .LBB0_3
; CHECK-O0-NEXT:    j .LBB0_2
; CHECK-O0-NEXT:  .LBB0_2: # %if.then2
; CHECK-O0-NEXT:    ld a0, 72(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a2, 64(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ori a1, a2, 80
; CHECK-O0-NEXT:    vsetvl a0, a0, a1
; CHECK-O0-NEXT:    sd a0, 24(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    j .LBB0_4
; CHECK-O0-NEXT:  .LBB0_3: # %if.else
; CHECK-O0-NEXT:    ld a0, 72(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    vsetvli a0, a0, e64, m1, ta, mu
; CHECK-O0-NEXT:    sd a0, 24(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    j .LBB0_4
; CHECK-O0-NEXT:  .LBB0_4: # %if.end
; CHECK-O0-NEXT:    ld a1, 48(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a0, 24(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    fld ft0, 0(a1)
; CHECK-O0-NEXT:    lui a2, %hi(.LCPI0_0)
; CHECK-O0-NEXT:    fld ft1, %lo(.LCPI0_0)(a2)
; CHECK-O0-NEXT:    fadd.d ft0, ft0, ft1
; CHECK-O0-NEXT:    fsd ft0, 0(a1)
; CHECK-O0-NEXT:    mv a1, zero
; CHECK-O0-NEXT:    sd a1, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a0, 16(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    j .LBB0_6
; CHECK-O0-NEXT:  .LBB0_5: # %if.else3
; CHECK-O0-NEXT:    ld a1, 56(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a0, 72(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ori a2, a1, 88
; CHECK-O0-NEXT:    vsetvl a0, a0, a2
; CHECK-O0-NEXT:    sd a1, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a0, 16(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    j .LBB0_6
; CHECK-O0-NEXT:  .LBB0_6: # %if.end4
; CHECK-O0-NEXT:    ld a0, 32(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a1, 40(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a2, 48(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a5, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a3, 16(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v9
; CHECK-O0-NEXT:    ori a4, a5, 88
; CHECK-O0-NEXT:    vsetvl a3, a3, a4
; CHECK-O0-NEXT:    vle64.v v9, (a2)
; CHECK-O0-NEXT:    # implicit-def: $v10
; CHECK-O0-NEXT:    vle64.v v10, (a1)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vfadd.vv v8, v9, v10
; CHECK-O0-NEXT:    vse64.v v8, (a0)
; CHECK-O0-NEXT:    addi sp, sp, 80
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_nested_branching:
; CHECK-O2:       # %bb.0: # %entry
; CHECK-O2-NEXT:    li a6, 45
; CHECK-O2-NEXT:    blt a0, a6, .LBB0_3
; CHECK-O2-NEXT:  # %bb.1: # %if.then
; CHECK-O2-NEXT:    li a2, 124
; CHECK-O2-NEXT:    blt a0, a2, .LBB0_4
; CHECK-O2-NEXT:  # %bb.2: # %if.then2
; CHECK-O2-NEXT:    ori a2, a1, 80
; CHECK-O2-NEXT:    vsetvl a0, a0, a2
; CHECK-O2-NEXT:    j .LBB0_5
; CHECK-O2-NEXT:  .LBB0_3: # %if.else3
; CHECK-O2-NEXT:    ori a1, a2, 88
; CHECK-O2-NEXT:    vsetvl a0, a0, a1
; CHECK-O2-NEXT:    j .LBB0_6
; CHECK-O2-NEXT:  .LBB0_4: # %if.else
; CHECK-O2-NEXT:    vsetvli a0, a0, e64, m1, ta, mu
; CHECK-O2-NEXT:  .LBB0_5: # %if.end
; CHECK-O2-NEXT:    fld ft0, 0(a3)
; CHECK-O2-NEXT:    lui a1, %hi(.LCPI0_0)
; CHECK-O2-NEXT:    fld ft1, %lo(.LCPI0_0)(a1)
; CHECK-O2-NEXT:    fadd.d ft0, ft0, ft1
; CHECK-O2-NEXT:    fsd ft0, 0(a3)
; CHECK-O2-NEXT:    mv a2, zero
; CHECK-O2-NEXT:  .LBB0_6: # %if.end4
; CHECK-O2-NEXT:    ori a1, a2, 88
; CHECK-O2-NEXT:    vsetvl a0, a0, a1
; CHECK-O2-NEXT:    vle64.v v8, (a3)
; CHECK-O2-NEXT:    vle64.v v9, (a4)
; CHECK-O2-NEXT:    vfadd.vv v8, v8, v9
; CHECK-O2-NEXT:    vse64.v v8, (a5)
; CHECK-O2-NEXT:    ret
entry:
  %cmp = icmp sgt i64 %rvl, 44
  br i1 %cmp, label %if.then, label %if.else3

if.then:                                          ; preds = %entry
  %cmp1 = icmp sgt i64 %rvl, 123
  br i1 %cmp1, label %if.then2, label %if.else

if.then2:                                         ; preds = %if.then
  %0 = tail call i64 @llvm.epi.vsetvl.ext(i64 %rvl, i64 2, i64 0, i64 %extra1)
  br label %if.end

if.else:                                          ; preds = %if.then
  %1 = tail call i64 @llvm.epi.vsetvl(i64 %rvl, i64 3, i64 0)
  br label %if.end

if.end:                                           ; preds = %if.else, %if.then2
  %gvl.0 = phi i64 [ %0, %if.then2 ], [ %1, %if.else ]
  %2 = load double, double* %a, align 8
  %add = fadd double %2, 1.000000e+00
  store double %add, double* %a, align 8
  br label %if.end4

if.else3:                                         ; preds = %entry
  %3 = tail call i64 @llvm.epi.vsetvl.ext(i64 %rvl, i64 3, i64 0, i64 %extra2)
  br label %if.end4

if.end4:                                          ; preds = %if.else3, %if.end
  %gvl.1 = phi i64 [ %gvl.0, %if.end ], [ %3, %if.else3 ]
  %4 = bitcast double* %a to <vscale x 1 x double>*
  %5 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %4, i64 %gvl.1)
  %6 = bitcast double* %b to <vscale x 1 x double>*
  %7 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %6, i64 %gvl.1)
  %8 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %5, <vscale x 1 x double> %7, i64 %gvl.1)
  %9 = bitcast double* %c to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %8, <vscale x 1 x double>* %9, i64 %gvl.1)
  ret void
}

declare i64 @llvm.epi.vsetvl.ext(i64, i64, i64, i64)
declare i64 @llvm.epi.vsetvl(i64, i64, i64)

declare <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* nocapture, i64)
declare <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64)
declare void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>* nocapture, i64)
