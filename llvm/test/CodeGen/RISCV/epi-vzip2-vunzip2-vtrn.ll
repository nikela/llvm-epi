; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+experimental-v -verify-machineinstrs < %s \
; RUN:    -epi-pipeline | FileCheck %s

declare <vscale x 1 x i64> @llvm.epi.vload.nxv1i64(<vscale x 1 x i64>* nocapture, i64)
declare { <vscale x 1 x i64>, <vscale x 1 x i64> } @llvm.epi.vzip2.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, i64)
declare { <vscale x 1 x i64>, <vscale x 1 x i64> } @llvm.epi.vunzip2.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, i64)
declare { <vscale x 1 x i64>, <vscale x 1 x i64> } @llvm.epi.vtrn.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, i64)
declare void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>* nocapture, i64)

declare <vscale x 2 x i32> @llvm.epi.vload.nxv2i32(<vscale x 2 x i32>* nocapture, i64)
declare { <vscale x 2 x i32>, <vscale x 2 x i32> } @llvm.epi.vzip2.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, i64)
declare { <vscale x 2 x i32>, <vscale x 2 x i32> } @llvm.epi.vunzip2.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, i64)
declare { <vscale x 2 x i32>, <vscale x 2 x i32> } @llvm.epi.vtrn.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, i64)
declare void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>* nocapture, i64)

declare <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* nocapture, i64)
declare { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vzip2.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64)
declare { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vunzip2.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64)
declare { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64)
declare void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>* nocapture, i64)

define void @vzip2.test.nxv1i64(i64* nocapture readonly %in_a0, i64* nocapture readonly %in_a1, i64* nocapture %out_a0, i64* nocapture %out_a1, i64 %gvl) nounwind {
; CHECK-LABEL: vzip2.test.nxv1i64:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vsetvli a4, a4, e64,m1,ta,mu
; CHECK-NEXT:    vle64.v v25, (a0)
; CHECK-NEXT:    vle64.v v26, (a1)
; CHECK-NEXT:    vzip2.vv v1, v25, v26
; CHECK-NEXT:    vse64.v v1, (a2)
; CHECK-NEXT:    vse64.v v2, (a3)
; CHECK-NEXT:    ret
entry:
  %0 = bitcast i64* %in_a0 to <vscale x 1 x i64>*
  %1 = tail call <vscale x 1 x i64> @llvm.epi.vload.nxv1i64(<vscale x 1 x i64>* %0, i64 %gvl)
  %2 = bitcast i64* %in_a1 to <vscale x 1 x i64>*
  %3 = tail call <vscale x 1 x i64> @llvm.epi.vload.nxv1i64(<vscale x 1 x i64>* %2, i64 %gvl)
  %4 = tail call { <vscale x 1 x i64>, <vscale x 1 x i64> } @llvm.epi.vzip2.nxv1i64(<vscale x 1 x i64> %1, <vscale x 1 x i64> %3, i64 %gvl)
  %5 = extractvalue { <vscale x 1 x i64>, <vscale x 1 x i64> } %4, 0
  %6 = extractvalue { <vscale x 1 x i64>, <vscale x 1 x i64> } %4, 1
  %7 = bitcast i64* %out_a0 to <vscale x 1 x i64>*
  tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> %5, <vscale x 1 x i64>* %7, i64 %gvl)
  %8 = bitcast i64* %out_a1 to <vscale x 1 x i64>*
  tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> %6, <vscale x 1 x i64>* %8, i64 %gvl)
  ret void
}

define void @vunzip2.test.nxv1i64(i64* nocapture readonly %in_a0, i64* nocapture readonly %in_a1, i64* nocapture %out_a0, i64* nocapture %out_a1, i64 %gvl) nounwind {
; CHECK-LABEL: vunzip2.test.nxv1i64:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vsetvli a4, a4, e64,m1,ta,mu
; CHECK-NEXT:    vle64.v v25, (a0)
; CHECK-NEXT:    vle64.v v26, (a1)
; CHECK-NEXT:    vunzip2.vv v1, v25, v26
; CHECK-NEXT:    vse64.v v1, (a2)
; CHECK-NEXT:    vse64.v v2, (a3)
; CHECK-NEXT:    ret
entry:
  %0 = bitcast i64* %in_a0 to <vscale x 1 x i64>*
  %1 = tail call <vscale x 1 x i64> @llvm.epi.vload.nxv1i64(<vscale x 1 x i64>* %0, i64 %gvl)
  %2 = bitcast i64* %in_a1 to <vscale x 1 x i64>*
  %3 = tail call <vscale x 1 x i64> @llvm.epi.vload.nxv1i64(<vscale x 1 x i64>* %2, i64 %gvl)
  %4 = tail call { <vscale x 1 x i64>, <vscale x 1 x i64> } @llvm.epi.vunzip2.nxv1i64(<vscale x 1 x i64> %1, <vscale x 1 x i64> %3, i64 %gvl)
  %5 = extractvalue { <vscale x 1 x i64>, <vscale x 1 x i64> } %4, 0
  %6 = extractvalue { <vscale x 1 x i64>, <vscale x 1 x i64> } %4, 1
  %7 = bitcast i64* %out_a0 to <vscale x 1 x i64>*
  tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> %5, <vscale x 1 x i64>* %7, i64 %gvl)
  %8 = bitcast i64* %out_a1 to <vscale x 1 x i64>*
  tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> %6, <vscale x 1 x i64>* %8, i64 %gvl)
  ret void
}

define void @vtrn.test.nxv1i64(i64* nocapture readonly %in_a0, i64* nocapture readonly %in_a1, i64* nocapture %out_a0, i64* nocapture %out_a1, i64 %gvl) nounwind {
; CHECK-LABEL: vtrn.test.nxv1i64:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vsetvli a4, a4, e64,m1,ta,mu
; CHECK-NEXT:    vle64.v v25, (a0)
; CHECK-NEXT:    vle64.v v26, (a1)
; CHECK-NEXT:    vtrn.vv v1, v25, v26
; CHECK-NEXT:    vse64.v v1, (a2)
; CHECK-NEXT:    vse64.v v2, (a3)
; CHECK-NEXT:    ret
entry:
  %0 = bitcast i64* %in_a0 to <vscale x 1 x i64>*
  %1 = tail call <vscale x 1 x i64> @llvm.epi.vload.nxv1i64(<vscale x 1 x i64>* %0, i64 %gvl)
  %2 = bitcast i64* %in_a1 to <vscale x 1 x i64>*
  %3 = tail call <vscale x 1 x i64> @llvm.epi.vload.nxv1i64(<vscale x 1 x i64>* %2, i64 %gvl)
  %4 = tail call { <vscale x 1 x i64>, <vscale x 1 x i64> } @llvm.epi.vtrn.nxv1i64(<vscale x 1 x i64> %1, <vscale x 1 x i64> %3, i64 %gvl)
  %5 = extractvalue { <vscale x 1 x i64>, <vscale x 1 x i64> } %4, 0
  %6 = extractvalue { <vscale x 1 x i64>, <vscale x 1 x i64> } %4, 1
  %7 = bitcast i64* %out_a0 to <vscale x 1 x i64>*
  tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> %5, <vscale x 1 x i64>* %7, i64 %gvl)
  %8 = bitcast i64* %out_a1 to <vscale x 1 x i64>*
  tail call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> %6, <vscale x 1 x i64>* %8, i64 %gvl)
  ret void
}

define void @vzip2.test.nxv2i32(i32* nocapture readonly %in_a0, i32* nocapture readonly %in_a1, i32* nocapture %out_a0, i32* nocapture %out_a1, i64 %gvl) nounwind {
; CHECK-LABEL: vzip2.test.nxv2i32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vsetvli a4, a4, e32,m1,ta,mu
; CHECK-NEXT:    vle32.v v25, (a0)
; CHECK-NEXT:    vle32.v v26, (a1)
; CHECK-NEXT:    vzip2.vv v1, v25, v26
; CHECK-NEXT:    vse32.v v1, (a2)
; CHECK-NEXT:    vse32.v v2, (a3)
; CHECK-NEXT:    ret
entry:
  %0 = bitcast i32* %in_a0 to <vscale x 2 x i32>*
  %1 = tail call <vscale x 2 x i32> @llvm.epi.vload.nxv2i32(<vscale x 2 x i32>* %0, i64 %gvl)
  %2 = bitcast i32* %in_a1 to <vscale x 2 x i32>*
  %3 = tail call <vscale x 2 x i32> @llvm.epi.vload.nxv2i32(<vscale x 2 x i32>* %2, i64 %gvl)
  %4 = tail call { <vscale x 2 x i32>, <vscale x 2 x i32> } @llvm.epi.vzip2.nxv2i32(<vscale x 2 x i32> %1, <vscale x 2 x i32> %3, i64 %gvl)
  %5 = extractvalue { <vscale x 2 x i32>, <vscale x 2 x i32> } %4, 0
  %6 = extractvalue { <vscale x 2 x i32>, <vscale x 2 x i32> } %4, 1
  %7 = bitcast i32* %out_a0 to <vscale x 2 x i32>*
  tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> %5, <vscale x 2 x i32>* %7, i64 %gvl)
  %8 = bitcast i32* %out_a1 to <vscale x 2 x i32>*
  tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> %6, <vscale x 2 x i32>* %8, i64 %gvl)
  ret void
}

define void @vunzip2.test.nxv2i32(i32* nocapture readonly %in_a0, i32* nocapture readonly %in_a1, i32* nocapture %out_a0, i32* nocapture %out_a1, i64 %gvl) nounwind {
; CHECK-LABEL: vunzip2.test.nxv2i32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vsetvli a4, a4, e32,m1,ta,mu
; CHECK-NEXT:    vle32.v v25, (a0)
; CHECK-NEXT:    vle32.v v26, (a1)
; CHECK-NEXT:    vunzip2.vv v1, v25, v26
; CHECK-NEXT:    vse32.v v1, (a2)
; CHECK-NEXT:    vse32.v v2, (a3)
; CHECK-NEXT:    ret
entry:
  %0 = bitcast i32* %in_a0 to <vscale x 2 x i32>*
  %1 = tail call <vscale x 2 x i32> @llvm.epi.vload.nxv2i32(<vscale x 2 x i32>* %0, i64 %gvl)
  %2 = bitcast i32* %in_a1 to <vscale x 2 x i32>*
  %3 = tail call <vscale x 2 x i32> @llvm.epi.vload.nxv2i32(<vscale x 2 x i32>* %2, i64 %gvl)
  %4 = tail call { <vscale x 2 x i32>, <vscale x 2 x i32> } @llvm.epi.vunzip2.nxv2i32(<vscale x 2 x i32> %1, <vscale x 2 x i32> %3, i64 %gvl)
  %5 = extractvalue { <vscale x 2 x i32>, <vscale x 2 x i32> } %4, 0
  %6 = extractvalue { <vscale x 2 x i32>, <vscale x 2 x i32> } %4, 1
  %7 = bitcast i32* %out_a0 to <vscale x 2 x i32>*
  tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> %5, <vscale x 2 x i32>* %7, i64 %gvl)
  %8 = bitcast i32* %out_a1 to <vscale x 2 x i32>*
  tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> %6, <vscale x 2 x i32>* %8, i64 %gvl)
  ret void
}

define void @vtrn.test.nxv2i32(i32* nocapture readonly %in_a0, i32* nocapture readonly %in_a1, i32* nocapture %out_a0, i32* nocapture %out_a1, i64 %gvl) nounwind {
; CHECK-LABEL: vtrn.test.nxv2i32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vsetvli a4, a4, e32,m1,ta,mu
; CHECK-NEXT:    vle32.v v25, (a0)
; CHECK-NEXT:    vle32.v v26, (a1)
; CHECK-NEXT:    vtrn.vv v1, v25, v26
; CHECK-NEXT:    vse32.v v1, (a2)
; CHECK-NEXT:    vse32.v v2, (a3)
; CHECK-NEXT:    ret
entry:
  %0 = bitcast i32* %in_a0 to <vscale x 2 x i32>*
  %1 = tail call <vscale x 2 x i32> @llvm.epi.vload.nxv2i32(<vscale x 2 x i32>* %0, i64 %gvl)
  %2 = bitcast i32* %in_a1 to <vscale x 2 x i32>*
  %3 = tail call <vscale x 2 x i32> @llvm.epi.vload.nxv2i32(<vscale x 2 x i32>* %2, i64 %gvl)
  %4 = tail call { <vscale x 2 x i32>, <vscale x 2 x i32> } @llvm.epi.vtrn.nxv2i32(<vscale x 2 x i32> %1, <vscale x 2 x i32> %3, i64 %gvl)
  %5 = extractvalue { <vscale x 2 x i32>, <vscale x 2 x i32> } %4, 0
  %6 = extractvalue { <vscale x 2 x i32>, <vscale x 2 x i32> } %4, 1
  %7 = bitcast i32* %out_a0 to <vscale x 2 x i32>*
  tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> %5, <vscale x 2 x i32>* %7, i64 %gvl)
  %8 = bitcast i32* %out_a1 to <vscale x 2 x i32>*
  tail call void @llvm.epi.vstore.nxv2i32(<vscale x 2 x i32> %6, <vscale x 2 x i32>* %8, i64 %gvl)
  ret void
}

define void @vzip2.test.nxv1f64(double* nocapture readonly %in_a0, double* nocapture readonly %in_a1, double* nocapture %out_a0, double* nocapture %out_a1, i64 %gvl) nounwind {
; CHECK-LABEL: vzip2.test.nxv1f64:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vsetvli a4, a4, e64,m1,ta,mu
; CHECK-NEXT:    vle64.v v25, (a0)
; CHECK-NEXT:    vle64.v v26, (a1)
; CHECK-NEXT:    vzip2.vv v1, v25, v26
; CHECK-NEXT:    vse64.v v1, (a2)
; CHECK-NEXT:    vse64.v v2, (a3)
; CHECK-NEXT:    ret
entry:
  %0 = bitcast double* %in_a0 to <vscale x 1 x double>*
  %1 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %0, i64 %gvl)
  %2 = bitcast double* %in_a1 to <vscale x 1 x double>*
  %3 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %2, i64 %gvl)
  %4 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vzip2.nxv1f64(<vscale x 1 x double> %1, <vscale x 1 x double> %3, i64 %gvl)
  %5 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %4, 0
  %6 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %4, 1
  %7 = bitcast double* %out_a0 to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %5, <vscale x 1 x double>* %7, i64 %gvl)
  %8 = bitcast double* %out_a1 to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %6, <vscale x 1 x double>* %8, i64 %gvl)
  ret void
}

define void @vunzip2.test.nxv1f64(double* nocapture readonly %in_a0, double* nocapture readonly %in_a1, double* nocapture %out_a0, double* nocapture %out_a1, i64 %gvl) nounwind {
; CHECK-LABEL: vunzip2.test.nxv1f64:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vsetvli a4, a4, e64,m1,ta,mu
; CHECK-NEXT:    vle64.v v25, (a0)
; CHECK-NEXT:    vle64.v v26, (a1)
; CHECK-NEXT:    vunzip2.vv v1, v25, v26
; CHECK-NEXT:    vse64.v v1, (a2)
; CHECK-NEXT:    vse64.v v2, (a3)
; CHECK-NEXT:    ret
entry:
  %0 = bitcast double* %in_a0 to <vscale x 1 x double>*
  %1 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %0, i64 %gvl)
  %2 = bitcast double* %in_a1 to <vscale x 1 x double>*
  %3 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %2, i64 %gvl)
  %4 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vunzip2.nxv1f64(<vscale x 1 x double> %1, <vscale x 1 x double> %3, i64 %gvl)
  %5 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %4, 0
  %6 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %4, 1
  %7 = bitcast double* %out_a0 to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %5, <vscale x 1 x double>* %7, i64 %gvl)
  %8 = bitcast double* %out_a1 to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %6, <vscale x 1 x double>* %8, i64 %gvl)
  ret void
}

define void @vtrn.test.nxv1f64(double* nocapture readonly %in_a0, double* nocapture readonly %in_a1, double* nocapture %out_a0, double* nocapture %out_a1, i64 %gvl) nounwind {
; CHECK-LABEL: vtrn.test.nxv1f64:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vsetvli a4, a4, e64,m1,ta,mu
; CHECK-NEXT:    vle64.v v25, (a0)
; CHECK-NEXT:    vle64.v v26, (a1)
; CHECK-NEXT:    vtrn.vv v1, v25, v26
; CHECK-NEXT:    vse64.v v1, (a2)
; CHECK-NEXT:    vse64.v v2, (a3)
; CHECK-NEXT:    ret
entry:
  %0 = bitcast double* %in_a0 to <vscale x 1 x double>*
  %1 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %0, i64 %gvl)
  %2 = bitcast double* %in_a1 to <vscale x 1 x double>*
  %3 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %2, i64 %gvl)
  %4 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vtrn.nxv1f64(<vscale x 1 x double> %1, <vscale x 1 x double> %3, i64 %gvl)
  %5 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %4, 0
  %6 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %4, 1
  %7 = bitcast double* %out_a0 to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %5, <vscale x 1 x double>* %7, i64 %gvl)
  %8 = bitcast double* %out_a1 to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %6, <vscale x 1 x double>* %8, i64 %gvl)
  ret void
}
