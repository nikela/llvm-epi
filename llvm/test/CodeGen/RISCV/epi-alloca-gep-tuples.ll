; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple riscv64 -mattr +m,+a,+f,+d,+c,+experimental-v -o - %s \
; RUN:     -epi-pipeline | FileCheck %s

%struct.__epi_1xi64x2 = type { <vscale x 1 x i64>, <vscale x 1 x i64> }

define <vscale x 1 x i64> @n1fv6() nounwind {
; CHECK-LABEL: n1fv6:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addi sp, sp, -32
; CHECK-NEXT:    li a0, 2
; CHECK-NEXT:    li a1, 64
; CHECK-NEXT:    vsetvli zero, a0, e64, m1, ta, mu
; CHECK-NEXT:    vmv.v.x v8, a1
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vs1r.v v8, (a0)
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    addi a1, sp, 16
; CHECK-NEXT:    add a0, a0, a1
; CHECK-NEXT:    vs1r.v v8, (a0)
; CHECK-NEXT:    addi sp, sp, 32
; CHECK-NEXT:    ret
entry:
  %KP500000000 = alloca %struct.__epi_1xi64x2, align 8

  %tmp.1.i305 = tail call <vscale x 1 x i64> @llvm.epi.vmv.v.x.nxv1i64.f64(i64 64, i64 2)
  %tmp.6 = getelementptr inbounds %struct.__epi_1xi64x2, %struct.__epi_1xi64x2* %KP500000000, i64 0, i32 0
  store <vscale x 1 x i64> %tmp.1.i305, <vscale x 1 x i64>* %tmp.6, align 8
  %tmp.8 = getelementptr inbounds %struct.__epi_1xi64x2, %struct.__epi_1xi64x2* %KP500000000, i64 0, i32 1
  store <vscale x 1 x i64> %tmp.1.i305, <vscale x 1 x i64>* %tmp.8, align 8

  %tmp.375 = load <vscale x 1 x i64>, <vscale x 1 x i64>* %tmp.8, align 8
  ret <vscale x 1 x i64> %tmp.375
}

%struct.__epi_1xi64x4 = type { <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64> }

define <vscale x 1 x i64> @n1fv6_1() nounwind {
; CHECK-LABEL: n1fv6_1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addi sp, sp, -48
; CHECK-NEXT:    li a0, 2
; CHECK-NEXT:    li a1, 64
; CHECK-NEXT:    vsetvli zero, a0, e64, m1, ta, mu
; CHECK-NEXT:    vmv.v.x v8, a1
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vs1r.v v8, (a0)
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    addi a1, sp, 16
; CHECK-NEXT:    add a2, a1, a0
; CHECK-NEXT:    vs1r.v v8, (a2)
; CHECK-NEXT:    slli a2, a0, 1
; CHECK-NEXT:    add a3, a1, a2
; CHECK-NEXT:    vs1r.v v8, (a3)
; CHECK-NEXT:    add a0, a0, a2
; CHECK-NEXT:    add a0, a0, a1
; CHECK-NEXT:    vs1r.v v8, (a0)
; CHECK-NEXT:    vl1re64.v v8, (a3)
; CHECK-NEXT:    addi sp, sp, 48
; CHECK-NEXT:    ret
entry:
  %KP500000000 = alloca %struct.__epi_1xi64x4, align 8

  %tmp.1.i305 = tail call <vscale x 1 x i64> @llvm.epi.vmv.v.x.nxv1i64.f64(i64 64, i64 2)

  %tmp.6 = getelementptr inbounds %struct.__epi_1xi64x4, %struct.__epi_1xi64x4* %KP500000000, i64 0, i32 0
  store <vscale x 1 x i64> %tmp.1.i305, <vscale x 1 x i64>* %tmp.6, align 8
  %tmp.8 = getelementptr inbounds %struct.__epi_1xi64x4, %struct.__epi_1xi64x4* %KP500000000, i64 0, i32 1
  store <vscale x 1 x i64> %tmp.1.i305, <vscale x 1 x i64>* %tmp.8, align 8
  %tmp.8.2 = getelementptr inbounds %struct.__epi_1xi64x4, %struct.__epi_1xi64x4* %KP500000000, i64 0, i32 2
  store <vscale x 1 x i64> %tmp.1.i305, <vscale x 1 x i64>* %tmp.8.2, align 8
  %tmp.8.3 = getelementptr inbounds %struct.__epi_1xi64x4, %struct.__epi_1xi64x4* %KP500000000, i64 0, i32 3
  store <vscale x 1 x i64> %tmp.1.i305, <vscale x 1 x i64>* %tmp.8.3, align 8

  %tmp.375 = load <vscale x 1 x i64>, <vscale x 1 x i64>* %tmp.8.2, align 8
  ret <vscale x 1 x i64> %tmp.375
}

; Function Attrs: nounwind readnone
declare <vscale x 1 x i64> @llvm.epi.vmv.v.x.nxv1i64.f64(i64, i64)

