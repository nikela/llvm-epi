; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+d,+experimental-v -verify-machineinstrs -O0 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=CHECK-O0 %s
; RUN: llc -mtriple=riscv64 -mattr=+d,+experimental-v -verify-machineinstrs -O2 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=CHECK-O2 %s

define void @test_preserve_extra(i64 %rvl, i64 %extra, i64 %x, <vscale x 1 x double>* %a, <vscale x 1 x double>* %b, <vscale x 1 x double>* %c) {
; CHECK-O0-LABEL: test_preserve_extra:
; CHECK-O0:       # %bb.0: # %entry
; CHECK-O0-NEXT:    mv a2, a1
; CHECK-O0-NEXT:    ori a1, a2, 80
; CHECK-O0-NEXT:    vsetvl a0, a0, a1
; CHECK-O0-NEXT:    # implicit-def: $v9
; CHECK-O0-NEXT:    ori a1, a2, 88
; CHECK-O0-NEXT:    vsetvl a0, a0, a1
; CHECK-O0-NEXT:    vle64.v v9, (a3)
; CHECK-O0-NEXT:    # implicit-def: $v10
; CHECK-O0-NEXT:    vle64.v v10, (a4)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vfadd.vv v8, v9, v10
; CHECK-O0-NEXT:    vse64.v v8, (a5)
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_preserve_extra:
; CHECK-O2:       # %bb.0: # %entry
; CHECK-O2-NEXT:    ori a2, a1, 80
; CHECK-O2-NEXT:    vsetvl a0, a0, a2
; CHECK-O2-NEXT:    ori a2, a1, 88
; CHECK-O2-NEXT:    vsetvl a0, a0, a2
; CHECK-O2-NEXT:    vle64.v v8, (a3)
; CHECK-O2-NEXT:    vle64.v v9, (a4)
; CHECK-O2-NEXT:    vfadd.vv v8, v8, v9
; CHECK-O2-NEXT:    vse64.v v8, (a5)
; CHECK-O2-NEXT:    ret
entry:
  %0 = tail call i64 @llvm.epi.vsetvl.ext(i64 %rvl, i64 2, i64 0, i64 %extra)
  %1 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %a, i64 %0)
  %2 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %b, i64 %0)
  %3 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %1, <vscale x 1 x double> %2, i64 %0)
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %3, <vscale x 1 x double>* %c, i64 %0)
  ret void
}

define void @test_in_if_branching(i64 %rvl, i64 %extra, i64 %x, <vscale x 1 x double>* %a, <vscale x 1 x double>* %b, <vscale x 1 x double>* %c) {
; CHECK-O0-LABEL: test_in_if_branching:
; CHECK-O0:       # %bb.0: # %entry
; CHECK-O0-NEXT:    addi sp, sp, -64
; CHECK-O0-NEXT:    .cfi_def_cfa_offset 64
; CHECK-O0-NEXT:    sd a5, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a4, 16(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a3, 24(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a1, 32(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a0, 40(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    vsetvli a1, a0, e32, m1, ta, mu
; CHECK-O0-NEXT:    li a0, 4
; CHECK-O0-NEXT:    li a3, 0
; CHECK-O0-NEXT:    sd a3, 48(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a1, 56(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    blt a2, a0, .LBB1_2
; CHECK-O0-NEXT:    j .LBB1_1
; CHECK-O0-NEXT:  .LBB1_1: # %if.then
; CHECK-O0-NEXT:    ld a1, 32(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a0, 40(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ori a2, a1, 88
; CHECK-O0-NEXT:    vsetvl a0, a0, a2
; CHECK-O0-NEXT:    sd a1, 48(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a0, 56(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    j .LBB1_2
; CHECK-O0-NEXT:  .LBB1_2: # %if.end
; CHECK-O0-NEXT:    ld a0, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a1, 16(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a2, 24(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a5, 48(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a3, 56(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v9
; CHECK-O0-NEXT:    ori a4, a5, 88
; CHECK-O0-NEXT:    vsetvl a3, a3, a4
; CHECK-O0-NEXT:    vle64.v v9, (a2)
; CHECK-O0-NEXT:    # implicit-def: $v10
; CHECK-O0-NEXT:    vle64.v v10, (a1)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vfadd.vv v8, v9, v10
; CHECK-O0-NEXT:    vse64.v v8, (a0)
; CHECK-O0-NEXT:    addi sp, sp, 64
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_in_if_branching:
; CHECK-O2:       # %bb.0: # %entry
; CHECK-O2-NEXT:    vsetvli a6, a0, e32, m1, ta, mu
; CHECK-O2-NEXT:    li t0, 4
; CHECK-O2-NEXT:    li a7, 0
; CHECK-O2-NEXT:    blt a2, t0, .LBB1_2
; CHECK-O2-NEXT:  # %bb.1: # %if.then
; CHECK-O2-NEXT:    ori a2, a1, 88
; CHECK-O2-NEXT:    vsetvl a6, a0, a2
; CHECK-O2-NEXT:    mv a7, a1
; CHECK-O2-NEXT:  .LBB1_2: # %if.end
; CHECK-O2-NEXT:    ori a1, a7, 88
; CHECK-O2-NEXT:    vsetvl a0, a6, a1
; CHECK-O2-NEXT:    vle64.v v8, (a3)
; CHECK-O2-NEXT:    vle64.v v9, (a4)
; CHECK-O2-NEXT:    vfadd.vv v8, v8, v9
; CHECK-O2-NEXT:    vse64.v v8, (a5)
; CHECK-O2-NEXT:    ret
entry:
  %0 = tail call i64 @llvm.epi.vsetvl(i64 %rvl, i64 2, i64 0)
  %cmp = icmp sgt i64 %x, 3
  br i1 %cmp, label %if.then, label %if.end

if.then:                                          ; preds = %entry
  %1 = tail call i64 @llvm.epi.vsetvl.ext(i64 %rvl, i64 3, i64 0, i64 %extra)
  br label %if.end

if.end:                                           ; preds = %if.then, %entry
  %gvl.0 = phi i64 [ %1, %if.then ], [ %0, %entry ]
  %2 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %a, i64 %gvl.0)
  %3 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %b, i64 %gvl.0)
  %4 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %2, <vscale x 1 x double> %3, i64 %gvl.0)
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %4, <vscale x 1 x double>* %c, i64 %gvl.0)
  ret void
}

define void @test_before_if_branching(i64 %rvl, i64 %extra, i64 %x, <vscale x 1 x double>* %a, <vscale x 1 x double>* %b, <vscale x 1 x double>* %c) {
; CHECK-O0-LABEL: test_before_if_branching:
; CHECK-O0:       # %bb.0: # %entry
; CHECK-O0-NEXT:    addi sp, sp, -48
; CHECK-O0-NEXT:    .cfi_def_cfa_offset 48
; CHECK-O0-NEXT:    sd a5, 0(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a4, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a3, 16(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    mv a3, a1
; CHECK-O0-NEXT:    mv a1, a0
; CHECK-O0-NEXT:    sd a1, 24(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    ori a0, a3, 80
; CHECK-O0-NEXT:    vsetvl a1, a1, a0
; CHECK-O0-NEXT:    li a0, 4
; CHECK-O0-NEXT:    sd a3, 32(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a1, 40(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    blt a2, a0, .LBB2_2
; CHECK-O0-NEXT:    j .LBB2_1
; CHECK-O0-NEXT:  .LBB2_1: # %if.then
; CHECK-O0-NEXT:    ld a0, 24(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    vsetvli a0, a0, e64, m1, ta, mu
; CHECK-O0-NEXT:    li a1, 0
; CHECK-O0-NEXT:    sd a1, 32(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a0, 40(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    j .LBB2_2
; CHECK-O0-NEXT:  .LBB2_2: # %if.end
; CHECK-O0-NEXT:    ld a0, 0(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a1, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a2, 16(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a5, 32(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a3, 40(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v9
; CHECK-O0-NEXT:    ori a4, a5, 88
; CHECK-O0-NEXT:    vsetvl a3, a3, a4
; CHECK-O0-NEXT:    vle64.v v9, (a2)
; CHECK-O0-NEXT:    # implicit-def: $v10
; CHECK-O0-NEXT:    vle64.v v10, (a1)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vfadd.vv v8, v9, v10
; CHECK-O0-NEXT:    vse64.v v8, (a0)
; CHECK-O0-NEXT:    addi sp, sp, 48
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_before_if_branching:
; CHECK-O2:       # %bb.0: # %entry
; CHECK-O2-NEXT:    li a7, 4
; CHECK-O2-NEXT:    ori t0, a1, 80
; CHECK-O2-NEXT:    vsetvl a6, a0, t0
; CHECK-O2-NEXT:    blt a2, a7, .LBB2_2
; CHECK-O2-NEXT:  # %bb.1: # %if.then
; CHECK-O2-NEXT:    vsetvli a6, a0, e64, m1, ta, mu
; CHECK-O2-NEXT:    li a1, 0
; CHECK-O2-NEXT:  .LBB2_2: # %if.end
; CHECK-O2-NEXT:    ori a2, a1, 88
; CHECK-O2-NEXT:    vsetvl a0, a6, a2
; CHECK-O2-NEXT:    vle64.v v8, (a3)
; CHECK-O2-NEXT:    vle64.v v9, (a4)
; CHECK-O2-NEXT:    vfadd.vv v8, v8, v9
; CHECK-O2-NEXT:    vse64.v v8, (a5)
; CHECK-O2-NEXT:    ret
entry:
  %0 = tail call i64 @llvm.epi.vsetvl.ext(i64 %rvl, i64 2, i64 0, i64 %extra)
  %cmp = icmp sgt i64 %x, 3
  br i1 %cmp, label %if.then, label %if.end

if.then:                                          ; preds = %entry
  %1 = tail call i64 @llvm.epi.vsetvl(i64 %rvl, i64 3, i64 0)
  br label %if.end

if.end:                                           ; preds = %if.then, %entry
  %gvl.0 = phi i64 [ %1, %if.then ], [ %0, %entry ]
  %2 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %a, i64 %gvl.0)
  %3 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %b, i64 %gvl.0)
  %4 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %2, <vscale x 1 x double> %3, i64 %gvl.0)
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %4, <vscale x 1 x double>* %c, i64 %gvl.0)
  ret void
}

define void @test_if_else_branching(i64 %rvl, i64 %extra, i64 %x, <vscale x 1 x double>* %a, <vscale x 1 x double>* %b, <vscale x 1 x double>* %c) {
; CHECK-O0-LABEL: test_if_else_branching:
; CHECK-O0:       # %bb.0: # %entry
; CHECK-O0-NEXT:    addi sp, sp, -64
; CHECK-O0-NEXT:    .cfi_def_cfa_offset 64
; CHECK-O0-NEXT:    sd a5, 24(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a4, 32(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a3, 40(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a1, 48(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a0, 56(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    li a0, 4
; CHECK-O0-NEXT:    blt a2, a0, .LBB3_2
; CHECK-O0-NEXT:    j .LBB3_1
; CHECK-O0-NEXT:  .LBB3_1: # %if.then
; CHECK-O0-NEXT:    ld a0, 56(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    vsetvli a0, a0, e64, m1, ta, mu
; CHECK-O0-NEXT:    li a1, 0
; CHECK-O0-NEXT:    sd a1, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a0, 16(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    j .LBB3_3
; CHECK-O0-NEXT:  .LBB3_2: # %if.else
; CHECK-O0-NEXT:    ld a1, 48(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a0, 56(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ori a2, a1, 80
; CHECK-O0-NEXT:    vsetvl a0, a0, a2
; CHECK-O0-NEXT:    sd a1, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a0, 16(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    j .LBB3_3
; CHECK-O0-NEXT:  .LBB3_3: # %if.end
; CHECK-O0-NEXT:    ld a0, 24(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a1, 32(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a2, 40(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a5, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a3, 16(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v9
; CHECK-O0-NEXT:    ori a4, a5, 88
; CHECK-O0-NEXT:    vsetvl a3, a3, a4
; CHECK-O0-NEXT:    vle64.v v9, (a2)
; CHECK-O0-NEXT:    # implicit-def: $v10
; CHECK-O0-NEXT:    vle64.v v10, (a1)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vfadd.vv v8, v9, v10
; CHECK-O0-NEXT:    vse64.v v8, (a0)
; CHECK-O0-NEXT:    addi sp, sp, 64
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_if_else_branching:
; CHECK-O2:       # %bb.0: # %entry
; CHECK-O2-NEXT:    li a6, 4
; CHECK-O2-NEXT:    blt a2, a6, .LBB3_2
; CHECK-O2-NEXT:  # %bb.1: # %if.then
; CHECK-O2-NEXT:    vsetvli a0, a0, e64, m1, ta, mu
; CHECK-O2-NEXT:    li a1, 0
; CHECK-O2-NEXT:    j .LBB3_3
; CHECK-O2-NEXT:  .LBB3_2: # %if.else
; CHECK-O2-NEXT:    ori a2, a1, 80
; CHECK-O2-NEXT:    vsetvl a0, a0, a2
; CHECK-O2-NEXT:  .LBB3_3: # %if.end
; CHECK-O2-NEXT:    ori a2, a1, 88
; CHECK-O2-NEXT:    vsetvl a0, a0, a2
; CHECK-O2-NEXT:    vle64.v v8, (a3)
; CHECK-O2-NEXT:    vle64.v v9, (a4)
; CHECK-O2-NEXT:    vfadd.vv v8, v8, v9
; CHECK-O2-NEXT:    vse64.v v8, (a5)
; CHECK-O2-NEXT:    ret
entry:
  %cmp = icmp sgt i64 %x, 3
  br i1 %cmp, label %if.then, label %if.else

if.then:                                          ; preds = %entry
  %0 = tail call i64 @llvm.epi.vsetvl(i64 %rvl, i64 3, i64 0)
  br label %if.end

if.else:                                          ; preds = %entry
  %1 = tail call i64 @llvm.epi.vsetvl.ext(i64 %rvl, i64 2, i64 0, i64 %extra)
  br label %if.end

if.end:                                           ; preds = %if.else, %if.then
  %gvl.0 = phi i64 [ %0, %if.then ], [ %1, %if.else ]
  %2 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %a, i64 %gvl.0)
  %3 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %b, i64 %gvl.0)
  %4 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %2, <vscale x 1 x double> %3, i64 %gvl.0)
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %4, <vscale x 1 x double>* %c, i64 %gvl.0)
  ret void
}

define void @test_if_else_if_branching(i64 %rvl, i64 %extra, i64 %extra2, i64 %x, <vscale x 1 x double>* %a, <vscale x 1 x double>* %b, <vscale x 1 x double>* %c) {
; CHECK-O0-LABEL: test_if_else_if_branching:
; CHECK-O0:       # %bb.0: # %entry
; CHECK-O0-NEXT:    addi sp, sp, -80
; CHECK-O0-NEXT:    .cfi_def_cfa_offset 80
; CHECK-O0-NEXT:    sd a6, 24(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a5, 32(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a4, 40(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a3, 48(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a2, 56(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a1, 64(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a0, 72(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    li a0, 4
; CHECK-O0-NEXT:    blt a3, a0, .LBB4_2
; CHECK-O0-NEXT:    j .LBB4_1
; CHECK-O0-NEXT:  .LBB4_1: # %if.then
; CHECK-O0-NEXT:    ld a0, 72(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    vsetvli a0, a0, e64, m1, ta, mu
; CHECK-O0-NEXT:    li a1, 0
; CHECK-O0-NEXT:    sd a1, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a0, 16(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    j .LBB4_4
; CHECK-O0-NEXT:  .LBB4_2: # %if.else
; CHECK-O0-NEXT:    ld a1, 48(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a3, 64(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a2, 72(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ori a0, a3, 80
; CHECK-O0-NEXT:    vsetvl a2, a2, a0
; CHECK-O0-NEXT:    li a0, 0
; CHECK-O0-NEXT:    sd a3, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a2, 16(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    blt a0, a1, .LBB4_4
; CHECK-O0-NEXT:    j .LBB4_3
; CHECK-O0-NEXT:  .LBB4_3: # %if.then2
; CHECK-O0-NEXT:    ld a1, 56(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a0, 72(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ori a2, a1, 88
; CHECK-O0-NEXT:    vsetvl a0, a0, a2
; CHECK-O0-NEXT:    sd a1, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a0, 16(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    j .LBB4_4
; CHECK-O0-NEXT:  .LBB4_4: # %if.end3
; CHECK-O0-NEXT:    ld a0, 24(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a1, 32(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a2, 40(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a5, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a3, 16(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v9
; CHECK-O0-NEXT:    ori a4, a5, 88
; CHECK-O0-NEXT:    vsetvl a3, a3, a4
; CHECK-O0-NEXT:    vle64.v v9, (a2)
; CHECK-O0-NEXT:    # implicit-def: $v10
; CHECK-O0-NEXT:    vle64.v v10, (a1)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vfadd.vv v8, v9, v10
; CHECK-O0-NEXT:    vse64.v v8, (a0)
; CHECK-O0-NEXT:    addi sp, sp, 80
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_if_else_if_branching:
; CHECK-O2:       # %bb.0: # %entry
; CHECK-O2-NEXT:    li a7, 4
; CHECK-O2-NEXT:    blt a3, a7, .LBB4_2
; CHECK-O2-NEXT:  # %bb.1: # %if.then
; CHECK-O2-NEXT:    vsetvli a7, a0, e64, m1, ta, mu
; CHECK-O2-NEXT:    li a1, 0
; CHECK-O2-NEXT:    j .LBB4_4
; CHECK-O2-NEXT:  .LBB4_2: # %if.else
; CHECK-O2-NEXT:    ori t0, a1, 80
; CHECK-O2-NEXT:    vsetvl a7, a0, t0
; CHECK-O2-NEXT:    bgtz a3, .LBB4_4
; CHECK-O2-NEXT:  # %bb.3: # %if.then2
; CHECK-O2-NEXT:    ori a1, a2, 88
; CHECK-O2-NEXT:    vsetvl a7, a0, a1
; CHECK-O2-NEXT:    mv a1, a2
; CHECK-O2-NEXT:  .LBB4_4: # %if.end3
; CHECK-O2-NEXT:    ori a2, a1, 88
; CHECK-O2-NEXT:    vsetvl a0, a7, a2
; CHECK-O2-NEXT:    vle64.v v8, (a4)
; CHECK-O2-NEXT:    vle64.v v9, (a5)
; CHECK-O2-NEXT:    vfadd.vv v8, v8, v9
; CHECK-O2-NEXT:    vse64.v v8, (a6)
; CHECK-O2-NEXT:    ret
entry:
  %cmp = icmp sgt i64 %x, 3
  br i1 %cmp, label %if.then, label %if.else

if.then:                                          ; preds = %entry
  %0 = tail call i64 @llvm.epi.vsetvl(i64 %rvl, i64 3, i64 0)
  br label %if.end3

if.else:                                          ; preds = %entry
  %1 = tail call i64 @llvm.epi.vsetvl.ext(i64 %rvl, i64 2, i64 0, i64 %extra)
  %cmp1 = icmp slt i64 %x, 1
  br i1 %cmp1, label %if.then2, label %if.end3

if.then2:                                         ; preds = %if.else
  %2 = tail call i64 @llvm.epi.vsetvl.ext(i64 %rvl, i64 3, i64 0, i64 %extra2)
  br label %if.end3

if.end3:                                          ; preds = %if.else, %if.then2, %if.then
  %gvl.0 = phi i64 [ %0, %if.then ], [ %2, %if.then2 ], [ %1, %if.else ]
  %3 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %a, i64 %gvl.0)
  %4 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %b, i64 %gvl.0)
  %5 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %3, <vscale x 1 x double> %4, i64 %gvl.0)
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %5, <vscale x 1 x double>* %c, i64 %gvl.0)
  ret void
}

declare i64 @llvm.epi.vsetvl.ext(i64, i64, i64, i64)
declare i64 @llvm.epi.vsetvl(i64, i64, i64)

declare <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* nocapture, i64)
declare <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64)
declare void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>* nocapture, i64)
