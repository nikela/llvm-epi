; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+m,+f,+d,+experimental-v -verify-machineinstrs -O0 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=CHECK-O0 %s
; RUN: llc -mtriple=riscv64 -mattr=+m,+f,+d,+experimental-v -verify-machineinstrs -O2 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=CHECK-O2 %s

@scratch = global i8 0, align 16

declare i64 @llvm.epi.vsetvl(
  i64, i64, i64);

declare i64 @llvm.epi.vsetvlmax(
  i64, i64)

declare <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

declare <vscale x 1 x double> @llvm.epi.vload.nxv1f64(
  <vscale x 1 x double>*,
  i64);

declare void @llvm.epi.vstore.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  i64);

declare void @use(i64)

define void @test_vsetvl_chain(<vscale x 1 x double>* %v, i64 %avl) nounwind
; CHECK-O0-LABEL: test_vsetvl_chain:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    # implicit-def: $v9
; CHECK-O0-NEXT:    vle64.v v9, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vfadd.vv v8, v9, v9
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    vse64.v v8, (a0)
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_chain:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vle64.v v8, (a0)
; CHECK-O2-NEXT:    vfadd.vv v8, v8, v8
; CHECK-O2-NEXT:    lui a0, %hi(scratch)
; CHECK-O2-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O2-NEXT:    vse64.v v8, (a0)
; CHECK-O2-NEXT:    ret
{
  %gvl1 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 0)
  %gvl2 = call i64 @llvm.epi.vsetvl(i64 %gvl1, i64 3, i64 0)
  %gvl3 = call i64 @llvm.epi.vsetvl(i64 %gvl2, i64 3, i64 0)
  %gvl4 = call i64 @llvm.epi.vsetvl(i64 %gvl3, i64 3, i64 0)
  %gvl5 = call i64 @llvm.epi.vsetvl(i64 %gvl4, i64 3, i64 0)
  %gvl6 = call i64 @llvm.epi.vsetvl(i64 %gvl5, i64 3, i64 0)
  %gvl7 = call i64 @llvm.epi.vsetvl(i64 %gvl6, i64 3, i64 0)
  %gvl8 = call i64 @llvm.epi.vsetvl(i64 %gvl7, i64 3, i64 0)
  %gvl9 = call i64 @llvm.epi.vsetvl(i64 %gvl8, i64 3, i64 0)
  %gvl10 = call i64 @llvm.epi.vsetvl(i64 %gvl9, i64 3, i64 0)

  %vec = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(
    <vscale x 1 x double>* %v,
    i64 %gvl10)

  %add = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(
    <vscale x 1 x double> %vec,
    <vscale x 1 x double> %vec,
    i64 %gvl10)

  %store_addr = bitcast i8* @scratch to <vscale x 1 x double>*

  call void @llvm.epi.vstore.nxv1f64(
    <vscale x 1 x double> %add,
    <vscale x 1 x double>* %store_addr,
    i64 %gvl10)

  ret void
}

define void @test_vsetvl_chain_2(<vscale x 1 x double>* %v, i64 %avl) nounwind
; CHECK-O0-LABEL: test_vsetvl_chain_2:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    # implicit-def: $v9
; CHECK-O0-NEXT:    vle64.v v9, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vfadd.vv v8, v9, v9
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    vse64.v v8, (a0)
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_chain_2:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vle64.v v8, (a0)
; CHECK-O2-NEXT:    vfadd.vv v8, v8, v8
; CHECK-O2-NEXT:    lui a0, %hi(scratch)
; CHECK-O2-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O2-NEXT:    vse64.v v8, (a0)
; CHECK-O2-NEXT:    ret
{
  %gvl1 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 0)
  %gvl2 = call i64 @llvm.epi.vsetvl(i64 %gvl1, i64 3, i64 0)
  %gvl3 = call i64 @llvm.epi.vsetvl(i64 %gvl1, i64 3, i64 0)
  %gvl4 = call i64 @llvm.epi.vsetvl(i64 %gvl2, i64 3, i64 0)
  %gvl5 = call i64 @llvm.epi.vsetvl(i64 %gvl2, i64 3, i64 0)
  %gvl6 = call i64 @llvm.epi.vsetvl(i64 %gvl3, i64 3, i64 0)
  %gvl7 = call i64 @llvm.epi.vsetvl(i64 %gvl3, i64 3, i64 0)
  %gvl8 = call i64 @llvm.epi.vsetvl(i64 %gvl4, i64 3, i64 0)
  %gvl9 = call i64 @llvm.epi.vsetvl(i64 %gvl4, i64 3, i64 0)
  %gvl10 = call i64 @llvm.epi.vsetvl(i64 %gvl5, i64 3, i64 0)

  %vec = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(
    <vscale x 1 x double>* %v,
    i64 %gvl10)

  %add = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(
    <vscale x 1 x double> %vec,
    <vscale x 1 x double> %vec,
    i64 %gvl10)

  %store_addr = bitcast i8* @scratch to <vscale x 1 x double>*

  call void @llvm.epi.vstore.nxv1f64(
    <vscale x 1 x double> %add,
    <vscale x 1 x double>* %store_addr,
    i64 %gvl10)

  ret void
}

define void @test_vsetvl_chain_3(<vscale x 1 x double>* %v, i64 %avl) nounwind
; CHECK-O0-LABEL: test_vsetvl_chain_3:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    # implicit-def: $v9
; CHECK-O0-NEXT:    vle64.v v9, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vfadd.vv v8, v9, v9
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    vse64.v v8, (a0)
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_chain_3:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vle64.v v8, (a0)
; CHECK-O2-NEXT:    vfadd.vv v8, v8, v8
; CHECK-O2-NEXT:    lui a0, %hi(scratch)
; CHECK-O2-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O2-NEXT:    vse64.v v8, (a0)
; CHECK-O2-NEXT:    ret
{
  %gvl1 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 1, i64 0)
  %gvl2 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 2, i64 0)
  %gvl3 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 0)
  %gvl4 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 1)
  %gvl5 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 2)
  %gvl6 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 3)
  %gvl7 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 0)

  %vec = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(
    <vscale x 1 x double>* %v,
    i64 %gvl7)

  %add = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(
    <vscale x 1 x double> %vec,
    <vscale x 1 x double> %vec,
    i64 %gvl7)

  %store_addr = bitcast i8* @scratch to <vscale x 1 x double>*

  call void @llvm.epi.vstore.nxv1f64(
    <vscale x 1 x double> %add,
    <vscale x 1 x double>* %store_addr,
    i64 %gvl7)

  ret void
}

define void @test_vsetvl_chain_4(<vscale x 1 x double>* %v, i64 %avl) nounwind
; CHECK-O0-LABEL: test_vsetvl_chain_4:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    vsetvli a1, a1, e8, m8, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e16, m8, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e8, m4, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e32, m8, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e16, m4, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e8, m2, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e64, m8, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e32, m4, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e16, m2, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e8, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e64, m4, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e32, m2, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e16, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e64, m2, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e32, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    # implicit-def: $v9
; CHECK-O0-NEXT:    vle64.v v9, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vfadd.vv v8, v9, v9
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    vse64.v v8, (a0)
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_chain_4:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    vsetvli a1, a1, e8, m8, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e16, m8, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e8, m4, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e32, m8, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e16, m4, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e8, m2, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m8, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e32, m4, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e16, m2, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e8, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m4, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e32, m2, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e16, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m2, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e32, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vle64.v v8, (a0)
; CHECK-O2-NEXT:    vfadd.vv v8, v8, v8
; CHECK-O2-NEXT:    lui a0, %hi(scratch)
; CHECK-O2-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O2-NEXT:    vse64.v v8, (a0)
; CHECK-O2-NEXT:    ret
{
  ; FIXME this test case is yet to be optimized
  ; 1/8
  %gvl1 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 0, i64 3)

  ; 1/4
  %gvl2 = call i64 @llvm.epi.vsetvl(i64 %gvl1, i64 1, i64 3)
  %gvl3 = call i64 @llvm.epi.vsetvl(i64 %gvl2, i64 0, i64 2)

  ; 1/2
  %gvl4 = call i64 @llvm.epi.vsetvl(i64 %gvl3, i64 2, i64 3)
  %gvl5 = call i64 @llvm.epi.vsetvl(i64 %gvl4, i64 1, i64 2)
  %gvl6 = call i64 @llvm.epi.vsetvl(i64 %gvl5, i64 0, i64 1)

  ; 1
  %gvl7 = call i64 @llvm.epi.vsetvl(i64 %gvl6, i64 3, i64 3)
  %gvl8 = call i64 @llvm.epi.vsetvl(i64 %gvl7, i64 2, i64 2)
  %gvl9 = call i64 @llvm.epi.vsetvl(i64 %gvl8, i64 1, i64 1)
  %gvl10 = call i64 @llvm.epi.vsetvl(i64 %gvl9, i64 0, i64 0)

  ; 2
  %gvl11 = call i64 @llvm.epi.vsetvl(i64 %gvl10, i64 3, i64 2)
  %gvl12 = call i64 @llvm.epi.vsetvl(i64 %gvl11, i64 2, i64 1)
  %gvl13 = call i64 @llvm.epi.vsetvl(i64 %gvl12, i64 1, i64 0)

  ; 4
  %gvl14 = call i64 @llvm.epi.vsetvl(i64 %gvl13, i64 3, i64 1)
  %gvl15 = call i64 @llvm.epi.vsetvl(i64 %gvl14, i64 2, i64 0)

  ; 8
  %gvl16 = call i64 @llvm.epi.vsetvl(i64 %gvl15, i64 3, i64 0)

  %vec = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(
    <vscale x 1 x double>* %v,
    i64 %gvl16)

  %add = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(
    <vscale x 1 x double> %vec,
    <vscale x 1 x double> %vec,
    i64 %gvl16)

  %store_addr = bitcast i8* @scratch to <vscale x 1 x double>*

  call void @llvm.epi.vstore.nxv1f64(
    <vscale x 1 x double> %add,
    <vscale x 1 x double>* %store_addr,
    i64 %gvl16)

  ret void
}

define void @test_vsetvl_chain_5() nounwind
; CHECK-O0-LABEL: test_vsetvl_chain_5:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_chain_5:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    ret
{
  call i64 @llvm.epi.vsetvl(i64 0, i64 0, i64 0)

  ret void
}

define void @test_vsetvl_chain_6(i64 %avl) nounwind
; CHECK-O0-LABEL: test_vsetvl_chain_6:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -32
; CHECK-O0-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O0-NEXT:    sd a0, 16(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O0-NEXT:    call use@plt
; CHECK-O0-NEXT:    ld a0, 16(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    vsetvli a1, a0, e16, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a1, e32, m2, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e64, m4, ta, mu
; CHECK-O0-NEXT:    sd a1, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    call use@plt
; CHECK-O0-NEXT:    ld a0, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    call use@plt
; CHECK-O0-NEXT:    ld a0, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    vsetvli a0, a0, e64, m2, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e64, m1, ta, mu
; CHECK-O0-NEXT:    call use@plt
; CHECK-O0-NEXT:    ld a0, 16(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e16, m1, ta, mu
; CHECK-O0-NEXT:    call use@plt
; CHECK-O0-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    addi sp, sp, 32
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_chain_6:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    addi sp, sp, -32
; CHECK-O2-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; CHECK-O2-NEXT:    sd s0, 16(sp) # 8-byte Folded Spill
; CHECK-O2-NEXT:    sd s1, 8(sp) # 8-byte Folded Spill
; CHECK-O2-NEXT:    vsetvli s1, a0, e8, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, s1, e8, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O2-NEXT:    call use@plt
; CHECK-O2-NEXT:    vsetvli a1, s1, e16, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a1, e32, m2, ta, mu
; CHECK-O2-NEXT:    vsetvli s0, a1, e64, m4, ta, mu
; CHECK-O2-NEXT:    call use@plt
; CHECK-O2-NEXT:    mv a0, s0
; CHECK-O2-NEXT:    call use@plt
; CHECK-O2-NEXT:    vsetvli a0, s0, e64, m2, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e64, m1, ta, mu
; CHECK-O2-NEXT:    call use@plt
; CHECK-O2-NEXT:    vsetvli a0, s1, e32, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e16, m1, ta, mu
; CHECK-O2-NEXT:    call use@plt
; CHECK-O2-NEXT:    ld s1, 8(sp) # 8-byte Folded Reload
; CHECK-O2-NEXT:    ld s0, 16(sp) # 8-byte Folded Reload
; CHECK-O2-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; CHECK-O2-NEXT:    addi sp, sp, 32
; CHECK-O2-NEXT:    ret
{
  %1 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 0, i64 0)
  %2 = call i64 @llvm.epi.vsetvl(i64 %1, i64 0, i64 0)
  %3 = call i64 @llvm.epi.vsetvl(i64 %2, i64 0, i64 0)
  call void @use(i64 %3)
  %4 = call i64 @llvm.epi.vsetvl(i64 %1, i64 1, i64 0)
  %5 = call i64 @llvm.epi.vsetvl(i64 %4, i64 2, i64 1)
  %6 = call i64 @llvm.epi.vsetvl(i64 %4, i64 3, i64 2)
  call void @use(i64 %5)
  call void @use(i64 %6)
  %7 = call i64 @llvm.epi.vsetvl(i64 %6, i64 3, i64 1)
  %8 = call i64 @llvm.epi.vsetvl(i64 %7, i64 3, i64 0)
  call void @use(i64 %8)
  %9 = call i64 @llvm.epi.vsetvl(i64 %1, i64 2, i64 0)
  %10 = call i64 @llvm.epi.vsetvl(i64 %9, i64 1, i64 0)
  call void @use(i64 %10)
  ret void
}

define void @test_vsetvl_chain_7(i64 %avl) nounwind
; CHECK-O0-LABEL: test_vsetvl_chain_7:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e16, m1, ta, mu
; CHECK-O0-NEXT:    call use@plt
; CHECK-O0-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_chain_7:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    addi sp, sp, -16
; CHECK-O2-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O2-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e16, m1, ta, mu
; CHECK-O2-NEXT:    call use@plt
; CHECK-O2-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O2-NEXT:    addi sp, sp, 16
; CHECK-O2-NEXT:    ret
{
  %1 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 2, i64 0)
  %2 = call i64 @llvm.epi.vsetvl(i64 %1, i64 0, i64 0)
  %3 = call i64 @llvm.epi.vsetvl(i64 %2, i64 1, i64 0)
  call void @use(i64 %3)
  ret void
}

define void @test_vsetvl_chain_8(i64 %avl) nounwind
; CHECK-O0-LABEL: test_vsetvl_chain_8:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    vsetvli a0, a0, e16, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O0-NEXT:    call use@plt
; CHECK-O0-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_chain_8:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    addi sp, sp, -16
; CHECK-O2-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O2-NEXT:    vsetvli a0, a0, e16, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O2-NEXT:    call use@plt
; CHECK-O2-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O2-NEXT:    addi sp, sp, 16
; CHECK-O2-NEXT:    ret
{
  %1 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 1, i64 0)
  %2 = call i64 @llvm.epi.vsetvl(i64 %1, i64 0, i64 0)
  %3 = call i64 @llvm.epi.vsetvl(i64 %2, i64 2, i64 0)
  call void @use(i64 %3)
  ret void
}

define void @test_vsetvl_chain_9(i64 %avl) nounwind
; CHECK-O0-LABEL: test_vsetvl_chain_9:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e16, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O0-NEXT:    call use@plt
; CHECK-O0-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_chain_9:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    addi sp, sp, -16
; CHECK-O2-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O2-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e16, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O2-NEXT:    call use@plt
; CHECK-O2-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O2-NEXT:    addi sp, sp, 16
; CHECK-O2-NEXT:    ret
{
  %1 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 2, i64 0)
  %2 = call i64 @llvm.epi.vsetvl(i64 %1, i64 1, i64 0)
  %3 = call i64 @llvm.epi.vsetvl(i64 %2, i64 0, i64 0)
  %4 = call i64 @llvm.epi.vsetvl(i64 %3, i64 2, i64 0)
  call void @use(i64 %4)
  ret void
}

define void @test_vsetvl_chain_10(i64 %avl) nounwind
; CHECK-O0-LABEL: test_vsetvl_chain_10:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    vsetvli a0, a0, e32, m2, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e16, m2, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e8, m2, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O0-NEXT:    call use@plt
; CHECK-O0-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_chain_10:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    addi sp, sp, -16
; CHECK-O2-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O2-NEXT:    vsetvli a0, a0, e32, m2, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e16, m2, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e8, m2, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O2-NEXT:    call use@plt
; CHECK-O2-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O2-NEXT:    addi sp, sp, 16
; CHECK-O2-NEXT:    ret
{
  %1 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 2, i64 1)
  %2 = call i64 @llvm.epi.vsetvl(i64 %1, i64 1, i64 1)
  %3 = call i64 @llvm.epi.vsetvl(i64 %2, i64 0, i64 1)
  %4 = call i64 @llvm.epi.vsetvl(i64 %3, i64 2, i64 0)
  call void @use(i64 %4)
  ret void
}

define void @test_vsetvl_chain_11(i64 %avl) nounwind
; CHECK-O0-LABEL: test_vsetvl_chain_11:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    vsetvli a0, a0, e64, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e16, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e16, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e64, m1, ta, mu
; CHECK-O0-NEXT:    call use@plt
; CHECK-O0-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_chain_11:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    addi sp, sp, -16
; CHECK-O2-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O2-NEXT:    vsetvli a0, a0, e64, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e16, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e16, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e64, m1, ta, mu
; CHECK-O2-NEXT:    call use@plt
; CHECK-O2-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O2-NEXT:    addi sp, sp, 16
; CHECK-O2-NEXT:    ret
{
  %1 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 0)
  %2 = call i64 @llvm.epi.vsetvl(i64 %1, i64 2, i64 0)
  %3 = call i64 @llvm.epi.vsetvl(i64 %2, i64 1, i64 0)
  %4 = call i64 @llvm.epi.vsetvl(i64 %3, i64 0, i64 0)
  %5 = call i64 @llvm.epi.vsetvl(i64 %4, i64 0, i64 0)
  %6 = call i64 @llvm.epi.vsetvl(i64 %5, i64 1, i64 0)
  %7 = call i64 @llvm.epi.vsetvl(i64 %6, i64 2, i64 0)
  %8 = call i64 @llvm.epi.vsetvl(i64 %7, i64 3, i64 0)
  call void @use(i64 %8)
  ret void
}

define void @test_vsetvl_chain_12(i64 %avl) nounwind
; CHECK-O0-LABEL: test_vsetvl_chain_12:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    vsetvli a0, a0, e64, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e16, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O0-NEXT:    call use@plt
; CHECK-O0-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_chain_12:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    addi sp, sp, -16
; CHECK-O2-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O2-NEXT:    vsetvli a0, a0, e64, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e16, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O2-NEXT:    call use@plt
; CHECK-O2-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O2-NEXT:    addi sp, sp, 16
; CHECK-O2-NEXT:    ret
{
  %1 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 0)
  %2 = call i64 @llvm.epi.vsetvl(i64 %1, i64 2, i64 0)
  %3 = call i64 @llvm.epi.vsetvl(i64 %2, i64 1, i64 0)
  %4 = call i64 @llvm.epi.vsetvl(i64 %3, i64 2, i64 0)
  call void @use(i64 %4)
  ret void
}

define void @test_vsetvl_chain_13(i64 %avl) nounwind
; CHECK-O0-LABEL: test_vsetvl_chain_13:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    vsetvli a0, a0, e64, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O0-NEXT:    call use@plt
; CHECK-O0-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_chain_13:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    addi sp, sp, -16
; CHECK-O2-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O2-NEXT:    vsetvli a0, a0, e64, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O2-NEXT:    call use@plt
; CHECK-O2-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O2-NEXT:    addi sp, sp, 16
; CHECK-O2-NEXT:    ret
{
  %1 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 0)
  %2 = call i64 @llvm.epi.vsetvl(i64 %1, i64 0, i64 0)
  %3 = call i64 @llvm.epi.vsetvl(i64 %2, i64 2, i64 0)
  call void @use(i64 %3)
  ret void
}

define void @test_vsetvl_chain_14(i64 %avl) nounwind
; CHECK-O0-LABEL: test_vsetvl_chain_14:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e16, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e64, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O0-NEXT:    call use@plt
; CHECK-O0-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_chain_14:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    addi sp, sp, -16
; CHECK-O2-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O2-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e16, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e64, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O2-NEXT:    call use@plt
; CHECK-O2-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O2-NEXT:    addi sp, sp, 16
; CHECK-O2-NEXT:    ret
{
  %1 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 2, i64 0)
  %2 = call i64 @llvm.epi.vsetvl(i64 %1, i64 0, i64 0)
  %3 = call i64 @llvm.epi.vsetvl(i64 %2, i64 1, i64 0)
  %4 = call i64 @llvm.epi.vsetvl(i64 %3, i64 3, i64 0)
  %5 = call i64 @llvm.epi.vsetvl(i64 %4, i64 2, i64 0)
  call void @use(i64 %5)
  ret void
}

define void @test_vsetvl_chain_15(i64 %avl) nounwind
; CHECK-O0-LABEL: test_vsetvl_chain_15:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O0-NEXT:    call use@plt
; CHECK-O0-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_chain_15:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    addi sp, sp, -16
; CHECK-O2-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O2-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O2-NEXT:    call use@plt
; CHECK-O2-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O2-NEXT:    addi sp, sp, 16
; CHECK-O2-NEXT:    ret
{
  %1 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 0, i64 0)
  %2 = call i64 @llvm.epi.vsetvl(i64 %1, i64 0, i64 0)
  %3 = call i64 @llvm.epi.vsetvl(i64 %2, i64 0, i64 0)
  %4 = call i64 @llvm.epi.vsetvl(i64 %3, i64 0, i64 0)
  call void @use(i64 %4)
  ret void
}

define void @test_vsetvl_chain_16(i64 %avl) nounwind
; CHECK-O0-LABEL: test_vsetvl_chain_16:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e16, m1, ta, mu
; CHECK-O0-NEXT:    call use@plt
; CHECK-O0-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_chain_16:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    addi sp, sp, -16
; CHECK-O2-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O2-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e16, m1, ta, mu
; CHECK-O2-NEXT:    call use@plt
; CHECK-O2-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O2-NEXT:    addi sp, sp, 16
; CHECK-O2-NEXT:    ret
{
  ; Both instructions compute same effective GVL, having different SEW/LMUL
  ; ratios.
  %1 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 2, i64 0)
  %2 = call i64 @llvm.epi.vsetvl(i64 %1, i64 1, i64 0)
  call void @use(i64 %2)
  ret void
}

define void @test_vsetvl_chain_17(i64 %avl) nounwind
; CHECK-O0-LABEL: test_vsetvl_chain_17:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e64, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e64, m2, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e32, m2, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e16, m2, ta, mu
; CHECK-O0-NEXT:    call use@plt
; CHECK-O0-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_chain_17:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    addi sp, sp, -16
; CHECK-O2-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O2-NEXT:    vsetvli a0, a0, e8, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e64, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e64, m2, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e32, m2, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e16, m2, ta, mu
; CHECK-O2-NEXT:    call use@plt
; CHECK-O2-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O2-NEXT:    addi sp, sp, 16
; CHECK-O2-NEXT:    ret
{
  ; Test AVL propagation when some instruction in the chain are more
  ; restrictive. Ie. e8m1 > e64m1 && e8m1 > e32m2, but e64m1 < e32m2, so e8m1
  ; can't propagate AVL to e32m2.
  %1 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 0, i64 0)
  %2 = call i64 @llvm.epi.vsetvl(i64 %1, i64 3, i64 0)
  %3 = call i64 @llvm.epi.vsetvl(i64 %2, i64 3, i64 1)
  %4 = call i64 @llvm.epi.vsetvl(i64 %3, i64 2, i64 1)
  %5 = call i64 @llvm.epi.vsetvl(i64 %4, i64 1, i64 1)
  call void @use(i64 %5)
  ret void
}

define void @test_vsetvl_chain_18(i64 %avl) nounwind
; CHECK-O0-LABEL: test_vsetvl_chain_18:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    vsetvli a0, a0, e64, m1, ta, mu
; CHECK-O0-NEXT:    sd a0, 0(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    vsetvli a0, a0, e32, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e16, m1, ta, mu
; CHECK-O0-NEXT:    call use@plt
; CHECK-O0-NEXT:    ld a0, 0(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    vsetvli a0, a0, e16, m2, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e8, m2, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e8, m4, ta, mu
; CHECK-O0-NEXT:    call use@plt
; CHECK-O0-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_chain_18:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    addi sp, sp, -16
; CHECK-O2-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; CHECK-O2-NEXT:    sd s0, 0(sp) # 8-byte Folded Spill
; CHECK-O2-NEXT:    vsetvli s0, a0, e64, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, s0, e32, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e16, m1, ta, mu
; CHECK-O2-NEXT:    call use@plt
; CHECK-O2-NEXT:    vsetvli a0, s0, e16, m2, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e8, m2, ta, mu
; CHECK-O2-NEXT:    vsetvli a0, a0, e8, m4, ta, mu
; CHECK-O2-NEXT:    call use@plt
; CHECK-O2-NEXT:    ld s0, 0(sp) # 8-byte Folded Reload
; CHECK-O2-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; CHECK-O2-NEXT:    addi sp, sp, 16
; CHECK-O2-NEXT:    ret
{
  %1 = call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 0)
  %2 = call i64 @llvm.epi.vsetvl(i64 %1, i64 2, i64 0)
  %3 = call i64 @llvm.epi.vsetvl(i64 %2, i64 1, i64 0)
  call void @use(i64 %3)
  %4 = call i64 @llvm.epi.vsetvl(i64 %1, i64 1, i64 1)
  %5 = call i64 @llvm.epi.vsetvl(i64 %4, i64 0, i64 1)
  %6 = call i64 @llvm.epi.vsetvl(i64 %5, i64 0, i64 2)
  call void @use(i64 %6)
  ret void
}

define i64 @test_vsetvl_chain_19() nounwind
; CHECK-O0-LABEL: test_vsetvl_chain_19:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    vsetvli a0, zero, e8, m1, ta, mu
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_chain_19:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    vsetvli a0, zero, e8, m1, ta, mu
; CHECK-O2-NEXT:    ret
{
  %1 = call i64 @llvm.epi.vsetvlmax(i64 0, i64 0)
  %2 = call i64 @llvm.epi.vsetvlmax(i64 1, i64 0)
  %3 = call i64 @llvm.epi.vsetvlmax(i64 1, i64 0)
  %4 = call i64 @llvm.epi.vsetvlmax(i64 0, i64 0)
  ret i64 %4
}
