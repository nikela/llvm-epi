; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+v -verify-machineinstrs < %s | FileCheck %s

declare <vscale x 8 x i16> @llvm.epi.vwredsumu.nxv8i16(<vscale x 8 x i8> %a, <vscale x 8 x i16> %b, i64 %evl)
declare <vscale x 16 x i16> @llvm.epi.vwredsumu.nxv16i16(<vscale x 16 x i8> %a, <vscale x 16 x i16> %b, i64 %evl)
declare <vscale x 32 x i16> @llvm.epi.vwredsumu.nxv32i16(<vscale x 32 x i8> %a, <vscale x 32 x i16> %b, i64 %evl)

declare <vscale x 4 x i32> @llvm.epi.vwredsumu.nxv4i32(<vscale x 4 x i16> %a, <vscale x 4 x i32> %b, i64 %evl)
declare <vscale x 8 x i32> @llvm.epi.vwredsumu.nxv8i32(<vscale x 8 x i16> %a, <vscale x 8 x i32> %b, i64 %evl)
declare <vscale x 16 x i32> @llvm.epi.vwredsumu.nxv16i32(<vscale x 16 x i16> %a, <vscale x 16 x i32> %b, i64 %evl)

declare <vscale x 2 x i64> @llvm.epi.vwredsumu.nxv2i64(<vscale x 2 x i32> %a, <vscale x 2 x i64> %b, i64 %evl)
declare <vscale x 4 x i64> @llvm.epi.vwredsumu.nxv4i64(<vscale x 4 x i32> %a, <vscale x 4 x i64> %b, i64 %evl)
declare <vscale x 8 x i64> @llvm.epi.vwredsumu.nxv8i64(<vscale x 8 x i32> %a, <vscale x 8 x i64> %b, i64 %evl)

declare <vscale x 8 x i16> @llvm.epi.vwredsumu.mask.nxv8i16(<vscale x 8 x i16> %merge, <vscale x 8 x i8> %a, <vscale x 8 x i16> %b, <vscale x 8 x i1> %mask, i64 %evl)
declare <vscale x 16 x i16> @llvm.epi.vwredsumu.mask.nxv16i16(<vscale x 16 x i16> %merge, <vscale x 16 x i8> %a, <vscale x 16 x i16> %b, <vscale x 16 x i1> %mask, i64 %evl)
declare <vscale x 32 x i16> @llvm.epi.vwredsumu.mask.nxv32i16(<vscale x 32 x i16> %merge, <vscale x 32 x i8> %a, <vscale x 32 x i16> %b, <vscale x 32 x i1> %mask, i64 %evl)

declare <vscale x 4 x i32> @llvm.epi.vwredsumu.mask.nxv4i32(<vscale x 4 x i32> %merge, <vscale x 4 x i16> %a, <vscale x 4 x i32> %b, <vscale x 4 x i1> %mask, i64 %evl)
declare <vscale x 8 x i32> @llvm.epi.vwredsumu.mask.nxv8i32(<vscale x 8 x i32> %merge, <vscale x 8 x i16> %a, <vscale x 8 x i32> %b, <vscale x 8 x i1> %mask, i64 %evl)
declare <vscale x 16 x i32> @llvm.epi.vwredsumu.mask.nxv16i32(<vscale x 16 x i32> %merge, <vscale x 16 x i16> %a, <vscale x 16 x i32> %b, <vscale x 16 x i1> %mask, i64 %evl)

declare <vscale x 2 x i64> @llvm.epi.vwredsumu.mask.nxv2i64(<vscale x 2 x i64> %merge, <vscale x 2 x i32> %a, <vscale x 2 x i64> %b, <vscale x 2 x i1> %mask, i64 %evl)
declare <vscale x 4 x i64> @llvm.epi.vwredsumu.mask.nxv4i64(<vscale x 4 x i64> %merge, <vscale x 4 x i32> %a, <vscale x 4 x i64> %b, <vscale x 4 x i1> %mask, i64 %evl)
declare <vscale x 8 x i64> @llvm.epi.vwredsumu.mask.nxv8i64(<vscale x 8 x i64> %merge, <vscale x 8 x i32> %a, <vscale x 8 x i64> %b, <vscale x 8 x i1> %mask, i64 %evl)

define <vscale x 8 x i16> @test_llvm.epi.vwredsumu_nxv8i16(<vscale x 8 x i8> %a, <vscale x 8 x i16> %b, i64 %evl) {
; CHECK-LABEL: test_llvm.epi.vwredsumu_nxv8i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e8, m1, ta, mu
; CHECK-NEXT:    vwredsumu.vs v8, v8, v10
; CHECK-NEXT:    ret
    %1 = call <vscale x 8 x i16> @llvm.epi.vwredsumu.nxv8i16(<vscale x 8 x i8> %a, <vscale x 8 x i16> %b, i64 %evl)
    ret <vscale x 8 x i16> %1
}

define <vscale x 16 x i16> @test_llvm.epi.vwredsumu_nxv16i16(<vscale x 16 x i8> %a, <vscale x 16 x i16> %b, i64 %evl) {
; CHECK-LABEL: test_llvm.epi.vwredsumu_nxv16i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e8, m2, ta, mu
; CHECK-NEXT:    vwredsumu.vs v8, v8, v12
; CHECK-NEXT:    ret
    %1 = call <vscale x 16 x i16> @llvm.epi.vwredsumu.nxv16i16(<vscale x 16 x i8> %a, <vscale x 16 x i16> %b, i64 %evl)
    ret <vscale x 16 x i16> %1
}

define <vscale x 32 x i16> @test_llvm.epi.vwredsumu_nxv32i16(<vscale x 32 x i8> %a, <vscale x 32 x i16> %b, i64 %evl) {
; CHECK-LABEL: test_llvm.epi.vwredsumu_nxv32i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e8, m4, ta, mu
; CHECK-NEXT:    vwredsumu.vs v8, v8, v16
; CHECK-NEXT:    ret
    %1 = call <vscale x 32 x i16> @llvm.epi.vwredsumu.nxv32i16(<vscale x 32 x i8> %a, <vscale x 32 x i16> %b, i64 %evl)
    ret <vscale x 32 x i16> %1
}

define <vscale x 4 x i32> @test_llvm.epi.vwredsumu_nxv4i32(<vscale x 4 x i16> %a, <vscale x 4 x i32> %b, i64 %evl) {
; CHECK-LABEL: test_llvm.epi.vwredsumu_nxv4i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e16, m1, ta, mu
; CHECK-NEXT:    vwredsumu.vs v8, v8, v10
; CHECK-NEXT:    ret
    %1 = call <vscale x 4 x i32> @llvm.epi.vwredsumu.nxv4i32(<vscale x 4 x i16> %a, <vscale x 4 x i32> %b, i64 %evl)
    ret <vscale x 4 x i32> %1
}

define <vscale x 8 x i32> @test_llvm.epi.vwredsumu_nxv8i32(<vscale x 8 x i16> %a, <vscale x 8 x i32> %b, i64 %evl) {
; CHECK-LABEL: test_llvm.epi.vwredsumu_nxv8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e16, m2, ta, mu
; CHECK-NEXT:    vwredsumu.vs v8, v8, v12
; CHECK-NEXT:    ret
    %1 = call <vscale x 8 x i32> @llvm.epi.vwredsumu.nxv8i32(<vscale x 8 x i16> %a, <vscale x 8 x i32> %b, i64 %evl)
    ret <vscale x 8 x i32> %1
}

define <vscale x 16 x i32> @test_llvm.epi.vwredsumu_nxv16i32(<vscale x 16 x i16> %a, <vscale x 16 x i32> %b, i64 %evl) {
; CHECK-LABEL: test_llvm.epi.vwredsumu_nxv16i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e16, m4, ta, mu
; CHECK-NEXT:    vwredsumu.vs v8, v8, v16
; CHECK-NEXT:    ret
    %1 = call <vscale x 16 x i32> @llvm.epi.vwredsumu.nxv16i32(<vscale x 16 x i16> %a, <vscale x 16 x i32> %b, i64 %evl)
    ret <vscale x 16 x i32> %1
}

define <vscale x 2 x i64> @test_llvm.epi.vwredsumu_nxv2i64(<vscale x 2 x i32> %a, <vscale x 2 x i64> %b, i64 %evl) {
; CHECK-LABEL: test_llvm.epi.vwredsumu_nxv2i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e32, m1, ta, mu
; CHECK-NEXT:    vwredsumu.vs v8, v8, v10
; CHECK-NEXT:    ret
    %1 = call <vscale x 2 x i64> @llvm.epi.vwredsumu.nxv2i64(<vscale x 2 x i32> %a, <vscale x 2 x i64> %b, i64 %evl)
    ret <vscale x 2 x i64> %1
}

define <vscale x 4 x i64> @test_llvm.epi.vwredsumu_nxv4i64(<vscale x 4 x i32> %a, <vscale x 4 x i64> %b, i64 %evl) {
; CHECK-LABEL: test_llvm.epi.vwredsumu_nxv4i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e32, m2, ta, mu
; CHECK-NEXT:    vwredsumu.vs v8, v8, v12
; CHECK-NEXT:    ret
    %1 = call <vscale x 4 x i64> @llvm.epi.vwredsumu.nxv4i64(<vscale x 4 x i32> %a, <vscale x 4 x i64> %b, i64 %evl)
    ret <vscale x 4 x i64> %1
}

define <vscale x 8 x i64> @test_llvm.epi.vwredsumu_nxv8i64(<vscale x 8 x i32> %a, <vscale x 8 x i64> %b, i64 %evl) {
; CHECK-LABEL: test_llvm.epi.vwredsumu_nxv8i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e32, m4, ta, mu
; CHECK-NEXT:    vwredsumu.vs v8, v8, v16
; CHECK-NEXT:    ret
    %1 = call <vscale x 8 x i64> @llvm.epi.vwredsumu.nxv8i64(<vscale x 8 x i32> %a, <vscale x 8 x i64> %b, i64 %evl)
    ret <vscale x 8 x i64> %1
}

define <vscale x 8 x i16> @test_llvm.epi.vwredsumu.mask_nxv8i16(<vscale x 8 x i16> %merge, <vscale x 8 x i8> %a, <vscale x 8 x i16> %b, <vscale x 8 x i1> %mask, i64 %evl) {
; CHECK-LABEL: test_llvm.epi.vwredsumu.mask_nxv8i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e8, m1, ta, mu
; CHECK-NEXT:    vwredsumu.vs v8, v10, v12, v0.t
; CHECK-NEXT:    ret
    %1 = call <vscale x 8 x i16> @llvm.epi.vwredsumu.mask.nxv8i16(<vscale x 8 x i16> %merge, <vscale x 8 x i8> %a, <vscale x 8 x i16> %b, <vscale x 8 x i1> %mask, i64 %evl)
    ret <vscale x 8 x i16> %1
}

define <vscale x 16 x i16> @test_llvm.epi.vwredsumu.mask_nxv16i16(<vscale x 16 x i16> %merge, <vscale x 16 x i8> %a, <vscale x 16 x i16> %b, <vscale x 16 x i1> %mask, i64 %evl) {
; CHECK-LABEL: test_llvm.epi.vwredsumu.mask_nxv16i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e8, m2, ta, mu
; CHECK-NEXT:    vwredsumu.vs v8, v12, v16, v0.t
; CHECK-NEXT:    ret
    %1 = call <vscale x 16 x i16> @llvm.epi.vwredsumu.mask.nxv16i16(<vscale x 16 x i16> %merge, <vscale x 16 x i8> %a, <vscale x 16 x i16> %b, <vscale x 16 x i1> %mask, i64 %evl)
    ret <vscale x 16 x i16> %1
}

define <vscale x 32 x i16> @test_llvm.epi.vwredsumu.mask_nxv32i16(<vscale x 32 x i16> %merge, <vscale x 32 x i8> %a, <vscale x 32 x i16> %b, <vscale x 32 x i1> %mask, i64 %evl) {
; CHECK-LABEL: test_llvm.epi.vwredsumu.mask_nxv32i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vl8re16.v v24, (a0)
; CHECK-NEXT:    vsetvli zero, a1, e8, m4, ta, mu
; CHECK-NEXT:    vwredsumu.vs v8, v16, v24, v0.t
; CHECK-NEXT:    ret
    %1 = call <vscale x 32 x i16> @llvm.epi.vwredsumu.mask.nxv32i16(<vscale x 32 x i16> %merge, <vscale x 32 x i8> %a, <vscale x 32 x i16> %b, <vscale x 32 x i1> %mask, i64 %evl)
    ret <vscale x 32 x i16> %1
}

define <vscale x 4 x i32> @test_llvm.epi.vwredsumu.mask_nxv4i32(<vscale x 4 x i32> %merge, <vscale x 4 x i16> %a, <vscale x 4 x i32> %b, <vscale x 4 x i1> %mask, i64 %evl) {
; CHECK-LABEL: test_llvm.epi.vwredsumu.mask_nxv4i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e16, m1, ta, mu
; CHECK-NEXT:    vwredsumu.vs v8, v10, v12, v0.t
; CHECK-NEXT:    ret
    %1 = call <vscale x 4 x i32> @llvm.epi.vwredsumu.mask.nxv4i32(<vscale x 4 x i32> %merge, <vscale x 4 x i16> %a, <vscale x 4 x i32> %b, <vscale x 4 x i1> %mask, i64 %evl)
    ret <vscale x 4 x i32> %1
}

define <vscale x 8 x i32> @test_llvm.epi.vwredsumu.mask_nxv8i32(<vscale x 8 x i32> %merge, <vscale x 8 x i16> %a, <vscale x 8 x i32> %b, <vscale x 8 x i1> %mask, i64 %evl) {
; CHECK-LABEL: test_llvm.epi.vwredsumu.mask_nxv8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e16, m2, ta, mu
; CHECK-NEXT:    vwredsumu.vs v8, v12, v16, v0.t
; CHECK-NEXT:    ret
    %1 = call <vscale x 8 x i32> @llvm.epi.vwredsumu.mask.nxv8i32(<vscale x 8 x i32> %merge, <vscale x 8 x i16> %a, <vscale x 8 x i32> %b, <vscale x 8 x i1> %mask, i64 %evl)
    ret <vscale x 8 x i32> %1
}

define <vscale x 16 x i32> @test_llvm.epi.vwredsumu.mask_nxv16i32(<vscale x 16 x i32> %merge, <vscale x 16 x i16> %a, <vscale x 16 x i32> %b, <vscale x 16 x i1> %mask, i64 %evl) {
; CHECK-LABEL: test_llvm.epi.vwredsumu.mask_nxv16i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vl8re32.v v24, (a0)
; CHECK-NEXT:    vsetvli zero, a1, e16, m4, ta, mu
; CHECK-NEXT:    vwredsumu.vs v8, v16, v24, v0.t
; CHECK-NEXT:    ret
    %1 = call <vscale x 16 x i32> @llvm.epi.vwredsumu.mask.nxv16i32(<vscale x 16 x i32> %merge, <vscale x 16 x i16> %a, <vscale x 16 x i32> %b, <vscale x 16 x i1> %mask, i64 %evl)
    ret <vscale x 16 x i32> %1
}

define <vscale x 2 x i64> @test_llvm.epi.vwredsumu.mask_nxv2i64(<vscale x 2 x i64> %merge, <vscale x 2 x i32> %a, <vscale x 2 x i64> %b, <vscale x 2 x i1> %mask, i64 %evl) {
; CHECK-LABEL: test_llvm.epi.vwredsumu.mask_nxv2i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e32, m1, ta, mu
; CHECK-NEXT:    vwredsumu.vs v8, v10, v12, v0.t
; CHECK-NEXT:    ret
    %1 = call <vscale x 2 x i64> @llvm.epi.vwredsumu.mask.nxv2i64(<vscale x 2 x i64> %merge, <vscale x 2 x i32> %a, <vscale x 2 x i64> %b, <vscale x 2 x i1> %mask, i64 %evl)
    ret <vscale x 2 x i64> %1
}

define <vscale x 4 x i64> @test_llvm.epi.vwredsumu.mask_nxv4i64(<vscale x 4 x i64> %merge, <vscale x 4 x i32> %a, <vscale x 4 x i64> %b, <vscale x 4 x i1> %mask, i64 %evl) {
; CHECK-LABEL: test_llvm.epi.vwredsumu.mask_nxv4i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e32, m2, ta, mu
; CHECK-NEXT:    vwredsumu.vs v8, v12, v16, v0.t
; CHECK-NEXT:    ret
    %1 = call <vscale x 4 x i64> @llvm.epi.vwredsumu.mask.nxv4i64(<vscale x 4 x i64> %merge, <vscale x 4 x i32> %a, <vscale x 4 x i64> %b, <vscale x 4 x i1> %mask, i64 %evl)
    ret <vscale x 4 x i64> %1
}

define <vscale x 8 x i64> @test_llvm.epi.vwredsumu.mask_nxv8i64(<vscale x 8 x i64> %merge, <vscale x 8 x i32> %a, <vscale x 8 x i64> %b, <vscale x 8 x i1> %mask, i64 %evl) {
; CHECK-LABEL: test_llvm.epi.vwredsumu.mask_nxv8i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vl8re64.v v24, (a0)
; CHECK-NEXT:    vsetvli zero, a1, e32, m4, ta, mu
; CHECK-NEXT:    vwredsumu.vs v8, v16, v24, v0.t
; CHECK-NEXT:    ret
    %1 = call <vscale x 8 x i64> @llvm.epi.vwredsumu.mask.nxv8i64(<vscale x 8 x i64> %merge, <vscale x 8 x i32> %a, <vscale x 8 x i64> %b, <vscale x 8 x i1> %mask, i64 %evl)
    ret <vscale x 8 x i64> %1
}
