; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+f,+d,+experimental-v -verify-machineinstrs \
; RUN:    -O0 < %s -epi-pipeline | FileCheck --check-prefix=SPILL-O0 %s
@scratch = global i64 0, align 16

; This test checks whether we have enough GPR emergency spill slots when we
; have to spill a vector and there are no free GPRs available. Currently 3 GPRs
; are required for a vector spill: address handle, old VL and old VType.
; In the output assembler we can see how a2, a3 and a4 are spilled before the
; vector spill ocurs.
define void @foo(i64 %avl) nounwind {
; SPILL-O0-LABEL: foo:
; SPILL-O0:       # %bb.0:
; SPILL-O0-NEXT:    addi sp, sp, -208
; SPILL-O0-NEXT:    sd ra, 200(sp) # 8-byte Folded Spill
; SPILL-O0-NEXT:    sd s0, 192(sp) # 8-byte Folded Spill
; SPILL-O0-NEXT:    sd s1, 184(sp) # 8-byte Folded Spill
; SPILL-O0-NEXT:    sd s2, 176(sp) # 8-byte Folded Spill
; SPILL-O0-NEXT:    sd s3, 168(sp) # 8-byte Folded Spill
; SPILL-O0-NEXT:    sd s4, 160(sp) # 8-byte Folded Spill
; SPILL-O0-NEXT:    sd s5, 152(sp) # 8-byte Folded Spill
; SPILL-O0-NEXT:    sd s6, 144(sp) # 8-byte Folded Spill
; SPILL-O0-NEXT:    sd s7, 136(sp) # 8-byte Folded Spill
; SPILL-O0-NEXT:    sd s8, 128(sp) # 8-byte Folded Spill
; SPILL-O0-NEXT:    sd s9, 120(sp) # 8-byte Folded Spill
; SPILL-O0-NEXT:    sd s10, 112(sp) # 8-byte Folded Spill
; SPILL-O0-NEXT:    sd s11, 104(sp) # 8-byte Folded Spill
; SPILL-O0-NEXT:    addi s0, sp, 208
; SPILL-O0-NEXT:    rdvlenb a1
; SPILL-O0-NEXT:    sub sp, sp, a1
; SPILL-O0-NEXT:    sd sp, -200(s0)
; SPILL-O0-NEXT:    sub sp, sp, a1
; SPILL-O0-NEXT:    sd sp, -208(s0)
; SPILL-O0-NEXT:    sd a0, -184(s0) # 8-byte Folded Spill
; SPILL-O0-NEXT:    lui a0, %hi(scratch)
; SPILL-O0-NEXT:    addi a0, a0, %lo(scratch)
; SPILL-O0-NEXT:    sd a0, -176(s0) # 8-byte Folded Spill
; SPILL-O0-NEXT:    addi a1, a0, 8
; SPILL-O0-NEXT:    sd a1, -168(s0) # 8-byte Folded Spill
; SPILL-O0-NEXT:    addi a1, a0, 16
; SPILL-O0-NEXT:    sd a1, -160(s0) # 8-byte Folded Spill
; SPILL-O0-NEXT:    addi a1, a0, 24
; SPILL-O0-NEXT:    sd a1, -152(s0) # 8-byte Folded Spill
; SPILL-O0-NEXT:    addi a1, a0, 32
; SPILL-O0-NEXT:    sd a1, -144(s0) # 8-byte Folded Spill
; SPILL-O0-NEXT:    addi a1, a0, 40
; SPILL-O0-NEXT:    sd a1, -136(s0) # 8-byte Folded Spill
; SPILL-O0-NEXT:    addi ra, a0, 48
; SPILL-O0-NEXT:    addi s11, a0, 56
; SPILL-O0-NEXT:    addi s10, a0, 64
; SPILL-O0-NEXT:    addi s9, a0, 72
; SPILL-O0-NEXT:    addi s8, a0, 80
; SPILL-O0-NEXT:    addi s7, a0, 88
; SPILL-O0-NEXT:    addi s6, a0, 96
; SPILL-O0-NEXT:    addi s5, a0, 104
; SPILL-O0-NEXT:    addi s4, a0, 112
; SPILL-O0-NEXT:    addi s3, a0, 120
; SPILL-O0-NEXT:    addi s2, a0, 128
; SPILL-O0-NEXT:    addi s1, a0, 136
; SPILL-O0-NEXT:    addi t6, a0, 144
; SPILL-O0-NEXT:    addi t5, a0, 152
; SPILL-O0-NEXT:    addi t4, a0, 160
; SPILL-O0-NEXT:    addi t3, a0, 168
; SPILL-O0-NEXT:    addi t2, a0, 176
; SPILL-O0-NEXT:    addi t1, a0, 184
; SPILL-O0-NEXT:    addi t0, a0, 192
; SPILL-O0-NEXT:    addi a7, a0, 200
; SPILL-O0-NEXT:    addi a6, a0, 208
; SPILL-O0-NEXT:    addi a5, a0, 216
; SPILL-O0-NEXT:    addi a4, a0, 224
; SPILL-O0-NEXT:    addi a3, a0, 232
; SPILL-O0-NEXT:    addi a2, a0, 240
; SPILL-O0-NEXT:    addi a1, a0, 248
; SPILL-O0-NEXT:    addi a0, a0, 256
; SPILL-O0-NEXT:    sd a0, -128(s0) # 8-byte Folded Spill
; SPILL-O0-NEXT:    ld a0, -184(s0) # 8-byte Folded Reload
; SPILL-O0-NEXT:    # implicit-def: $v25
; SPILL-O0-NEXT:    vsetvli a0, a0, e64,m1,ta,mu
; SPILL-O0-NEXT:    ld a0, -176(s0) # 8-byte Folded Reload
; SPILL-O0-NEXT:    vle64.v v25, (a0)
; SPILL-O0-NEXT:    ld a0, -168(s0) # 8-byte Folded Reload
; SPILL-O0-NEXT:    sd s4, -112(s0)
; SPILL-O0-NEXT:    ld s4, -208(s0)
; SPILL-O0-NEXT:    vs1r.v v25, (s4)
; SPILL-O0-NEXT:    ld s4, -112(s0)
; SPILL-O0-NEXT:    # implicit-def: $v7
; SPILL-O0-NEXT:    vle64.v v7, (a0)
; SPILL-O0-NEXT:    ld a0, -160(s0) # 8-byte Folded Reload
; SPILL-O0-NEXT:    # implicit-def: $v6
; SPILL-O0-NEXT:    vle64.v v6, (a0)
; SPILL-O0-NEXT:    ld a0, -152(s0) # 8-byte Folded Reload
; SPILL-O0-NEXT:    # implicit-def: $v5
; SPILL-O0-NEXT:    vle64.v v5, (a0)
; SPILL-O0-NEXT:    ld a0, -144(s0) # 8-byte Folded Reload
; SPILL-O0-NEXT:    # implicit-def: $v4
; SPILL-O0-NEXT:    vle64.v v4, (a0)
; SPILL-O0-NEXT:    ld a0, -136(s0) # 8-byte Folded Reload
; SPILL-O0-NEXT:    # implicit-def: $v3
; SPILL-O0-NEXT:    vle64.v v3, (a0)
; SPILL-O0-NEXT:    ld a0, -128(s0) # 8-byte Folded Reload
; SPILL-O0-NEXT:    # implicit-def: $v2
; SPILL-O0-NEXT:    vle64.v v2, (ra)
; SPILL-O0-NEXT:    # implicit-def: $v1
; SPILL-O0-NEXT:    vle64.v v1, (s11)
; SPILL-O0-NEXT:    # implicit-def: $v0
; SPILL-O0-NEXT:    vle64.v v0, (s10)
; SPILL-O0-NEXT:    # implicit-def: $v24
; SPILL-O0-NEXT:    vle64.v v24, (s9)
; SPILL-O0-NEXT:    # implicit-def: $v23
; SPILL-O0-NEXT:    vle64.v v23, (s8)
; SPILL-O0-NEXT:    # implicit-def: $v22
; SPILL-O0-NEXT:    vle64.v v22, (s7)
; SPILL-O0-NEXT:    # implicit-def: $v21
; SPILL-O0-NEXT:    vle64.v v21, (s6)
; SPILL-O0-NEXT:    # implicit-def: $v20
; SPILL-O0-NEXT:    vle64.v v20, (s5)
; SPILL-O0-NEXT:    # implicit-def: $v19
; SPILL-O0-NEXT:    vle64.v v19, (s4)
; SPILL-O0-NEXT:    # implicit-def: $v18
; SPILL-O0-NEXT:    vle64.v v18, (s3)
; SPILL-O0-NEXT:    # implicit-def: $v17
; SPILL-O0-NEXT:    vle64.v v17, (s2)
; SPILL-O0-NEXT:    # implicit-def: $v16
; SPILL-O0-NEXT:    vle64.v v16, (s1)
; SPILL-O0-NEXT:    # implicit-def: $v15
; SPILL-O0-NEXT:    vle64.v v15, (t6)
; SPILL-O0-NEXT:    # implicit-def: $v14
; SPILL-O0-NEXT:    vle64.v v14, (t5)
; SPILL-O0-NEXT:    # implicit-def: $v13
; SPILL-O0-NEXT:    vle64.v v13, (t4)
; SPILL-O0-NEXT:    # implicit-def: $v12
; SPILL-O0-NEXT:    vle64.v v12, (t3)
; SPILL-O0-NEXT:    # implicit-def: $v11
; SPILL-O0-NEXT:    vle64.v v11, (t2)
; SPILL-O0-NEXT:    # implicit-def: $v10
; SPILL-O0-NEXT:    vle64.v v10, (t1)
; SPILL-O0-NEXT:    # implicit-def: $v9
; SPILL-O0-NEXT:    vle64.v v9, (t0)
; SPILL-O0-NEXT:    # implicit-def: $v8
; SPILL-O0-NEXT:    vle64.v v8, (a7)
; SPILL-O0-NEXT:    # implicit-def: $v31
; SPILL-O0-NEXT:    vle64.v v31, (a6)
; SPILL-O0-NEXT:    # implicit-def: $v30
; SPILL-O0-NEXT:    vle64.v v30, (a5)
; SPILL-O0-NEXT:    # implicit-def: $v29
; SPILL-O0-NEXT:    vle64.v v29, (a4)
; SPILL-O0-NEXT:    # implicit-def: $v28
; SPILL-O0-NEXT:    vle64.v v28, (a3)
; SPILL-O0-NEXT:    # implicit-def: $v27
; SPILL-O0-NEXT:    vle64.v v27, (a2)
; SPILL-O0-NEXT:    # implicit-def: $v26
; SPILL-O0-NEXT:    vle64.v v26, (a1)
; SPILL-O0-NEXT:    # implicit-def: $v25
; SPILL-O0-NEXT:    vle64.v v25, (a0)
; SPILL-O0-NEXT:    ld a0, -176(s0) # 8-byte Folded Reload
; SPILL-O0-NEXT:    sd t4, -112(s0)
; SPILL-O0-NEXT:    ld t4, -200(s0)
; SPILL-O0-NEXT:    vs1r.v v25, (t4)
; SPILL-O0-NEXT:    ld t4, -208(s0)
; SPILL-O0-NEXT:    vl1r.v v25, (t4)
; SPILL-O0-NEXT:    vse64.v v25, (a0)
; SPILL-O0-NEXT:    ld a0, -168(s0) # 8-byte Folded Reload
; SPILL-O0-NEXT:    ld t4, -200(s0)
; SPILL-O0-NEXT:    vl1r.v v25, (t4)
; SPILL-O0-NEXT:    ld t4, -112(s0)
; SPILL-O0-NEXT:    vse64.v v7, (a0)
; SPILL-O0-NEXT:    ld a0, -160(s0) # 8-byte Folded Reload
; SPILL-O0-NEXT:    vse64.v v6, (a0)
; SPILL-O0-NEXT:    ld a0, -152(s0) # 8-byte Folded Reload
; SPILL-O0-NEXT:    vse64.v v5, (a0)
; SPILL-O0-NEXT:    ld a0, -144(s0) # 8-byte Folded Reload
; SPILL-O0-NEXT:    vse64.v v4, (a0)
; SPILL-O0-NEXT:    ld a0, -136(s0) # 8-byte Folded Reload
; SPILL-O0-NEXT:    vse64.v v3, (a0)
; SPILL-O0-NEXT:    ld a0, -128(s0) # 8-byte Folded Reload
; SPILL-O0-NEXT:    vse64.v v2, (ra)
; SPILL-O0-NEXT:    vse64.v v1, (s11)
; SPILL-O0-NEXT:    vse64.v v0, (s10)
; SPILL-O0-NEXT:    vse64.v v24, (s9)
; SPILL-O0-NEXT:    vse64.v v23, (s8)
; SPILL-O0-NEXT:    vse64.v v22, (s7)
; SPILL-O0-NEXT:    vse64.v v21, (s6)
; SPILL-O0-NEXT:    vse64.v v20, (s5)
; SPILL-O0-NEXT:    vse64.v v19, (s4)
; SPILL-O0-NEXT:    vse64.v v18, (s3)
; SPILL-O0-NEXT:    vse64.v v17, (s2)
; SPILL-O0-NEXT:    vse64.v v16, (s1)
; SPILL-O0-NEXT:    vse64.v v15, (t6)
; SPILL-O0-NEXT:    vse64.v v14, (t5)
; SPILL-O0-NEXT:    vse64.v v13, (t4)
; SPILL-O0-NEXT:    vse64.v v12, (t3)
; SPILL-O0-NEXT:    vse64.v v11, (t2)
; SPILL-O0-NEXT:    vse64.v v10, (t1)
; SPILL-O0-NEXT:    vse64.v v9, (t0)
; SPILL-O0-NEXT:    vse64.v v8, (a7)
; SPILL-O0-NEXT:    vse64.v v31, (a6)
; SPILL-O0-NEXT:    vse64.v v30, (a5)
; SPILL-O0-NEXT:    vse64.v v29, (a4)
; SPILL-O0-NEXT:    vse64.v v28, (a3)
; SPILL-O0-NEXT:    vse64.v v27, (a2)
; SPILL-O0-NEXT:    vse64.v v26, (a1)
; SPILL-O0-NEXT:    vse64.v v25, (a0)
; SPILL-O0-NEXT:    addi sp, s0, -208
; SPILL-O0-NEXT:    ld s11, 104(sp) # 8-byte Folded Reload
; SPILL-O0-NEXT:    ld s10, 112(sp) # 8-byte Folded Reload
; SPILL-O0-NEXT:    ld s9, 120(sp) # 8-byte Folded Reload
; SPILL-O0-NEXT:    ld s8, 128(sp) # 8-byte Folded Reload
; SPILL-O0-NEXT:    ld s7, 136(sp) # 8-byte Folded Reload
; SPILL-O0-NEXT:    ld s6, 144(sp) # 8-byte Folded Reload
; SPILL-O0-NEXT:    ld s5, 152(sp) # 8-byte Folded Reload
; SPILL-O0-NEXT:    ld s4, 160(sp) # 8-byte Folded Reload
; SPILL-O0-NEXT:    ld s3, 168(sp) # 8-byte Folded Reload
; SPILL-O0-NEXT:    ld s2, 176(sp) # 8-byte Folded Reload
; SPILL-O0-NEXT:    ld s1, 184(sp) # 8-byte Folded Reload
; SPILL-O0-NEXT:    ld s0, 192(sp) # 8-byte Folded Reload
; SPILL-O0-NEXT:    ld ra, 200(sp) # 8-byte Folded Reload
; SPILL-O0-NEXT:    addi sp, sp, 208
; SPILL-O0-NEXT:    ret
  %p1 = getelementptr inbounds i64, i64* @scratch, i64 1
  %p2 = getelementptr inbounds i64, i64* @scratch, i64 2
  %p3 = getelementptr inbounds i64, i64* @scratch, i64 3
  %p4 = getelementptr inbounds i64, i64* @scratch, i64 4
  %p5 = getelementptr inbounds i64, i64* @scratch, i64 5
  %p6 = getelementptr inbounds i64, i64* @scratch, i64 6
  %p7 = getelementptr inbounds i64, i64* @scratch, i64 7
  %p8 = getelementptr inbounds i64, i64* @scratch, i64 8
  %p9 = getelementptr inbounds i64, i64* @scratch, i64 9
  %p10 = getelementptr inbounds i64, i64* @scratch, i64 10
  %p11 = getelementptr inbounds i64, i64* @scratch, i64 11
  %p12 = getelementptr inbounds i64, i64* @scratch, i64 12
  %p13 = getelementptr inbounds i64, i64* @scratch, i64 13
  %p14 = getelementptr inbounds i64, i64* @scratch, i64 14
  %p15 = getelementptr inbounds i64, i64* @scratch, i64 15
  %p16 = getelementptr inbounds i64, i64* @scratch, i64 16
  %p17 = getelementptr inbounds i64, i64* @scratch, i64 17
  %p18 = getelementptr inbounds i64, i64* @scratch, i64 18
  %p19 = getelementptr inbounds i64, i64* @scratch, i64 19
  %p20 = getelementptr inbounds i64, i64* @scratch, i64 20
  %p21 = getelementptr inbounds i64, i64* @scratch, i64 21
  %p22 = getelementptr inbounds i64, i64* @scratch, i64 22
  %p23 = getelementptr inbounds i64, i64* @scratch, i64 23
  %p24 = getelementptr inbounds i64, i64* @scratch, i64 24
  %p25 = getelementptr inbounds i64, i64* @scratch, i64 25
  %p26 = getelementptr inbounds i64, i64* @scratch, i64 26
  %p27 = getelementptr inbounds i64, i64* @scratch, i64 27
  %p28 = getelementptr inbounds i64, i64* @scratch, i64 28
  %p29 = getelementptr inbounds i64, i64* @scratch, i64 29
  %p30 = getelementptr inbounds i64, i64* @scratch, i64 30
  %p31 = getelementptr inbounds i64, i64* @scratch, i64 31
  %pspill = getelementptr inbounds i64, i64* @scratch, i64 32

  %bc0 = bitcast i64* @scratch to <vscale x 1 x double>*
  %bc1 = bitcast i64* %p1 to <vscale x 1 x double>*
  %bc2 = bitcast i64* %p2 to <vscale x 1 x double>*
  %bc3 = bitcast i64* %p3 to <vscale x 1 x double>*
  %bc4 = bitcast i64* %p4 to <vscale x 1 x double>*
  %bc5 = bitcast i64* %p5 to <vscale x 1 x double>*
  %bc6 = bitcast i64* %p6 to <vscale x 1 x double>*
  %bc7 = bitcast i64* %p7 to <vscale x 1 x double>*
  %bc8 = bitcast i64* %p8 to <vscale x 1 x double>*
  %bc9 = bitcast i64* %p9 to <vscale x 1 x double>*
  %bc10 = bitcast i64* %p10 to <vscale x 1 x double>*
  %bc11 = bitcast i64* %p11 to <vscale x 1 x double>*
  %bc12 = bitcast i64* %p12 to <vscale x 1 x double>*
  %bc13 = bitcast i64* %p13 to <vscale x 1 x double>*
  %bc14 = bitcast i64* %p14 to <vscale x 1 x double>*
  %bc15 = bitcast i64* %p15 to <vscale x 1 x double>*
  %bc16 = bitcast i64* %p16 to <vscale x 1 x double>*
  %bc17 = bitcast i64* %p17 to <vscale x 1 x double>*
  %bc18 = bitcast i64* %p18 to <vscale x 1 x double>*
  %bc19 = bitcast i64* %p19 to <vscale x 1 x double>*
  %bc20 = bitcast i64* %p20 to <vscale x 1 x double>*
  %bc21 = bitcast i64* %p21 to <vscale x 1 x double>*
  %bc22 = bitcast i64* %p22 to <vscale x 1 x double>*
  %bc23 = bitcast i64* %p23 to <vscale x 1 x double>*
  %bc24 = bitcast i64* %p24 to <vscale x 1 x double>*
  %bc25 = bitcast i64* %p25 to <vscale x 1 x double>*
  %bc26 = bitcast i64* %p26 to <vscale x 1 x double>*
  %bc27 = bitcast i64* %p27 to <vscale x 1 x double>*
  %bc28 = bitcast i64* %p28 to <vscale x 1 x double>*
  %bc29 = bitcast i64* %p29 to <vscale x 1 x double>*
  %bc30 = bitcast i64* %p30 to <vscale x 1 x double>*
  %bc31 = bitcast i64* %p31 to <vscale x 1 x double>*
  %bcspill = bitcast i64* %pspill to <vscale x 1 x double>*

  %v0 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc0, i64 %avl)
  %v1 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc1, i64 %avl)
  %v2 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc2, i64 %avl)
  %v3 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc3, i64 %avl)
  %v4 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc4, i64 %avl)
  %v5 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc5, i64 %avl)
  %v6 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc6, i64 %avl)
  %v7 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc7, i64 %avl)
  %v8 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc8, i64 %avl)
  %v9 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc9, i64 %avl)
  %v10 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc10, i64 %avl)
  %v11 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc11, i64 %avl)
  %v12 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc12, i64 %avl)
  %v13 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc13, i64 %avl)
  %v14 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc14, i64 %avl)
  %v15 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc15, i64 %avl)
  %v16 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc16, i64 %avl)
  %v17 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc17, i64 %avl)
  %v18 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc18, i64 %avl)
  %v19 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc19, i64 %avl)
  %v20 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc20, i64 %avl)
  %v21 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc21, i64 %avl)
  %v22 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc22, i64 %avl)
  %v23 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc23, i64 %avl)
  %v24 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc24, i64 %avl)
  %v25 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc25, i64 %avl)
  %v26 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc26, i64 %avl)
  %v27 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc27, i64 %avl)
  %v28 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc28, i64 %avl)
  %v29 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc29, i64 %avl)
  %v30 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc30, i64 %avl)
  %v31 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bc31, i64 %avl)
  %vspill = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %bcspill, i64 %avl)

  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v0, <vscale x 1 x double>* %bc0, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v1, <vscale x 1 x double>* %bc1, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v2, <vscale x 1 x double>* %bc2, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v3, <vscale x 1 x double>* %bc3, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v4, <vscale x 1 x double>* %bc4, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v5, <vscale x 1 x double>* %bc5, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v6, <vscale x 1 x double>* %bc6, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v7, <vscale x 1 x double>* %bc7, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v8, <vscale x 1 x double>* %bc8, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v9, <vscale x 1 x double>* %bc9, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v10, <vscale x 1 x double>* %bc10, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v11, <vscale x 1 x double>* %bc11, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v12, <vscale x 1 x double>* %bc12, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v13, <vscale x 1 x double>* %bc13, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v14, <vscale x 1 x double>* %bc14, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v15, <vscale x 1 x double>* %bc15, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v16, <vscale x 1 x double>* %bc16, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v17, <vscale x 1 x double>* %bc17, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v18, <vscale x 1 x double>* %bc18, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v19, <vscale x 1 x double>* %bc19, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v20, <vscale x 1 x double>* %bc20, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v21, <vscale x 1 x double>* %bc21, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v22, <vscale x 1 x double>* %bc22, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v23, <vscale x 1 x double>* %bc23, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v24, <vscale x 1 x double>* %bc24, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v25, <vscale x 1 x double>* %bc25, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v26, <vscale x 1 x double>* %bc26, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v27, <vscale x 1 x double>* %bc27, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v28, <vscale x 1 x double>* %bc28, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v29, <vscale x 1 x double>* %bc29, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v30, <vscale x 1 x double>* %bc30, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %v31, <vscale x 1 x double>* %bc31, i64 %avl)
  call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %vspill, <vscale x 1 x double>* %bcspill, i64 %avl)

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>*, i64)
declare void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>*, i64)
