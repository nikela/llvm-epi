; NOTE: Tests autogenerated by utils/EPI/generate-intrinsics-tests.py
; RUN: llc -mtriple=riscv64 -mattr=+m,+f,+d,+a,+c,+experimental-v \
; RUN:    -verify-machineinstrs --riscv-no-aliases < %s | FileCheck %s

@scratch = global i8 0, align 16


declare i64 @llvm.epi.vpopc.i64.nxv1i1(
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vpopc_m_i64_nxv1i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vpopc_m_i64_nxv1i1
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vpopc.m a0, {{v[0-9]+}}
  %a = call i64 @llvm.epi.vpopc.i64.nxv1i1(
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p

  ret void
}

declare i64 @llvm.epi.vpopc.mask.i64.nxv1i1(
  <vscale x 1 x i1>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vpopc_mask_m_i64_nxv1i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vpopc_mask_m_i64_nxv1i1
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vpopc.m a0, {{v[0-9]+}}, v0.t
  %a = call i64 @llvm.epi.vpopc.mask.i64.nxv1i1(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p

  ret void
}


declare i64 @llvm.epi.vpopc.i64.nxv2i1(
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vpopc_m_i64_nxv2i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vpopc_m_i64_nxv2i1
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vpopc.m a0, {{v[0-9]+}}
  %a = call i64 @llvm.epi.vpopc.i64.nxv2i1(
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p

  ret void
}

declare i64 @llvm.epi.vpopc.mask.i64.nxv2i1(
  <vscale x 2 x i1>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vpopc_mask_m_i64_nxv2i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vpopc_mask_m_i64_nxv2i1
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vpopc.m a0, {{v[0-9]+}}, v0.t
  %a = call i64 @llvm.epi.vpopc.mask.i64.nxv2i1(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p

  ret void
}


declare i64 @llvm.epi.vpopc.i64.nxv4i1(
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vpopc_m_i64_nxv4i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vpopc_m_i64_nxv4i1
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vpopc.m a0, {{v[0-9]+}}
  %a = call i64 @llvm.epi.vpopc.i64.nxv4i1(
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p

  ret void
}

declare i64 @llvm.epi.vpopc.mask.i64.nxv4i1(
  <vscale x 4 x i1>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vpopc_mask_m_i64_nxv4i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vpopc_mask_m_i64_nxv4i1
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vpopc.m a0, {{v[0-9]+}}, v0.t
  %a = call i64 @llvm.epi.vpopc.mask.i64.nxv4i1(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p

  ret void
}


declare i64 @llvm.epi.vpopc.i64.nxv8i1(
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vpopc_m_i64_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vpopc_m_i64_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vpopc.m a0, {{v[0-9]+}}
  %a = call i64 @llvm.epi.vpopc.i64.nxv8i1(
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p

  ret void
}

declare i64 @llvm.epi.vpopc.mask.i64.nxv8i1(
  <vscale x 8 x i1>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vpopc_mask_m_i64_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vpopc_mask_m_i64_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vpopc.m a0, {{v[0-9]+}}, v0.t
  %a = call i64 @llvm.epi.vpopc.mask.i64.nxv8i1(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p

  ret void
}


declare i64 @llvm.epi.vfirst.i64.nxv1i1(
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfirst_m_i64_nxv1i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfirst_m_i64_nxv1i1
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfirst.m a0, {{v[0-9]+}}
  %a = call i64 @llvm.epi.vfirst.i64.nxv1i1(
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p

  ret void
}

declare i64 @llvm.epi.vfirst.mask.i64.nxv1i1(
  <vscale x 1 x i1>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfirst_mask_m_i64_nxv1i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfirst_mask_m_i64_nxv1i1
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfirst.m a0, {{v[0-9]+}}, v0.t
  %a = call i64 @llvm.epi.vfirst.mask.i64.nxv1i1(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p

  ret void
}


declare i64 @llvm.epi.vfirst.i64.nxv2i1(
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfirst_m_i64_nxv2i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfirst_m_i64_nxv2i1
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfirst.m a0, {{v[0-9]+}}
  %a = call i64 @llvm.epi.vfirst.i64.nxv2i1(
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p

  ret void
}

declare i64 @llvm.epi.vfirst.mask.i64.nxv2i1(
  <vscale x 2 x i1>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfirst_mask_m_i64_nxv2i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfirst_mask_m_i64_nxv2i1
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfirst.m a0, {{v[0-9]+}}, v0.t
  %a = call i64 @llvm.epi.vfirst.mask.i64.nxv2i1(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p

  ret void
}


declare i64 @llvm.epi.vfirst.i64.nxv4i1(
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfirst_m_i64_nxv4i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfirst_m_i64_nxv4i1
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vfirst.m a0, {{v[0-9]+}}
  %a = call i64 @llvm.epi.vfirst.i64.nxv4i1(
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p

  ret void
}

declare i64 @llvm.epi.vfirst.mask.i64.nxv4i1(
  <vscale x 4 x i1>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfirst_mask_m_i64_nxv4i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfirst_mask_m_i64_nxv4i1
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vfirst.m a0, {{v[0-9]+}}, v0.t
  %a = call i64 @llvm.epi.vfirst.mask.i64.nxv4i1(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p

  ret void
}


declare i64 @llvm.epi.vfirst.i64.nxv8i1(
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfirst_m_i64_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfirst_m_i64_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vfirst.m a0, {{v[0-9]+}}
  %a = call i64 @llvm.epi.vfirst.i64.nxv8i1(
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p

  ret void
}

declare i64 @llvm.epi.vfirst.mask.i64.nxv8i1(
  <vscale x 8 x i1>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfirst_mask_m_i64_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfirst_mask_m_i64_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vfirst.m a0, {{v[0-9]+}}, v0.t
  %a = call i64 @llvm.epi.vfirst.mask.i64.nxv8i1(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vload.nxv8i8(
  <vscale x 8 x i8>*,
  i64);

define void @intrinsic_vload_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vle8.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 8 x i8> @llvm.epi.vload.nxv8i8(
    <vscale x 8 x i8>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vload.mask.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload_mask_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_mask_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vle8.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vload.mask.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vload.nxv16i8(
  <vscale x 16 x i8>*,
  i64);

define void @intrinsic_vload_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vle8.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 16 x i8> @llvm.epi.vload.nxv16i8(
    <vscale x 16 x i8>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vload.mask.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>*,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload_mask_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_mask_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vle8.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vload.mask.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8>* undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vload.nxv32i8(
  <vscale x 32 x i8>*,
  i64);

define void @intrinsic_vload_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vle8.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 32 x i8> @llvm.epi.vload.nxv32i8(
    <vscale x 32 x i8>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vload.mask.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>*,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vload_mask_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_mask_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vle8.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vload.mask.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8>* undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vload.nxv4i16(
  <vscale x 4 x i16>*,
  i64);

define void @intrinsic_vload_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vle16.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 4 x i16> @llvm.epi.vload.nxv4i16(
    <vscale x 4 x i16>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vload.mask.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>*,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload_mask_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_mask_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vle16.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vload.mask.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16>* undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vload.nxv8i16(
  <vscale x 8 x i16>*,
  i64);

define void @intrinsic_vload_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vle16.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 8 x i16> @llvm.epi.vload.nxv8i16(
    <vscale x 8 x i16>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vload.mask.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload_mask_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_mask_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vle16.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vload.mask.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vload.nxv16i16(
  <vscale x 16 x i16>*,
  i64);

define void @intrinsic_vload_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vle16.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 16 x i16> @llvm.epi.vload.nxv16i16(
    <vscale x 16 x i16>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vload.mask.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>*,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload_mask_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_mask_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vle16.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vload.mask.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16>* undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vload.nxv32i16(
  <vscale x 32 x i16>*,
  i64);

define void @intrinsic_vload_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vle16.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 32 x i16> @llvm.epi.vload.nxv32i16(
    <vscale x 32 x i16>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vload.mask.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>*,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vload_mask_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_mask_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vle16.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vload.mask.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16>* undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vload.nxv2i32(
  <vscale x 2 x i32>*,
  i64);

define void @intrinsic_vload_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vle32.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 2 x i32> @llvm.epi.vload.nxv2i32(
    <vscale x 2 x i32>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vload.mask.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>*,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload_mask_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_mask_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vle32.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vload.mask.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32>* undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vload.nxv4i32(
  <vscale x 4 x i32>*,
  i64);

define void @intrinsic_vload_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vle32.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 4 x i32> @llvm.epi.vload.nxv4i32(
    <vscale x 4 x i32>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vload.mask.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>*,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload_mask_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_mask_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vle32.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vload.mask.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32>* undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vload.nxv8i32(
  <vscale x 8 x i32>*,
  i64);

define void @intrinsic_vload_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vle32.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 8 x i32> @llvm.epi.vload.nxv8i32(
    <vscale x 8 x i32>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vload.mask.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload_mask_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_mask_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vle32.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vload.mask.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vload.nxv16i32(
  <vscale x 16 x i32>*,
  i64);

define void @intrinsic_vload_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vle32.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 16 x i32> @llvm.epi.vload.nxv16i32(
    <vscale x 16 x i32>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vload.mask.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>*,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload_mask_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_mask_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vle32.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vload.mask.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32>* undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vload.nxv1i64(
  <vscale x 1 x i64>*,
  i64);

define void @intrinsic_vload_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vle64.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 1 x i64> @llvm.epi.vload.nxv1i64(
    <vscale x 1 x i64>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vload.mask.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>*,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vload_mask_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_mask_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vle64.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vload.mask.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64>* undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vload.nxv2i64(
  <vscale x 2 x i64>*,
  i64);

define void @intrinsic_vload_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vle64.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 2 x i64> @llvm.epi.vload.nxv2i64(
    <vscale x 2 x i64>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vload.mask.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>*,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload_mask_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_mask_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vle64.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vload.mask.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64>* undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vload.nxv4i64(
  <vscale x 4 x i64>*,
  i64);

define void @intrinsic_vload_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vle64.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 4 x i64> @llvm.epi.vload.nxv4i64(
    <vscale x 4 x i64>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vload.mask.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>*,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload_mask_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_mask_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vle64.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vload.mask.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64>* undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vload.nxv8i64(
  <vscale x 8 x i64>*,
  i64);

define void @intrinsic_vload_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vle64.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 8 x i64> @llvm.epi.vload.nxv8i64(
    <vscale x 8 x i64>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vload.mask.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload_mask_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_mask_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vle64.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vload.mask.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vload.nxv2f32(
  <vscale x 2 x float>*,
  i64);

define void @intrinsic_vload_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vle32.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 2 x float> @llvm.epi.vload.nxv2f32(
    <vscale x 2 x float>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vload.mask.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>*,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload_mask_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_mask_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vle32.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vload.mask.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float>* undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vload.nxv4f32(
  <vscale x 4 x float>*,
  i64);

define void @intrinsic_vload_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vle32.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 4 x float> @llvm.epi.vload.nxv4f32(
    <vscale x 4 x float>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vload.mask.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>*,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload_mask_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_mask_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vle32.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vload.mask.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float>* undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vload.nxv8f32(
  <vscale x 8 x float>*,
  i64);

define void @intrinsic_vload_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vle32.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 8 x float> @llvm.epi.vload.nxv8f32(
    <vscale x 8 x float>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vload.mask.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload_mask_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_mask_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vle32.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vload.mask.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vload.nxv16f32(
  <vscale x 16 x float>*,
  i64);

define void @intrinsic_vload_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vle32.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 16 x float> @llvm.epi.vload.nxv16f32(
    <vscale x 16 x float>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vload.mask.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>*,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload_mask_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_mask_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vle32.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vload.mask.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float>* undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vload.nxv1f64(
  <vscale x 1 x double>*,
  i64);

define void @intrinsic_vload_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vle64.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(
    <vscale x 1 x double>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vload.mask.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vload_mask_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_mask_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vle64.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vload.mask.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double>* undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vload.nxv2f64(
  <vscale x 2 x double>*,
  i64);

define void @intrinsic_vload_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vle64.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 2 x double> @llvm.epi.vload.nxv2f64(
    <vscale x 2 x double>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vload.mask.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>*,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload_mask_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_mask_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vle64.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vload.mask.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double>* undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vload.nxv4f64(
  <vscale x 4 x double>*,
  i64);

define void @intrinsic_vload_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vle64.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 4 x double> @llvm.epi.vload.nxv4f64(
    <vscale x 4 x double>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vload.mask.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>*,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload_mask_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_mask_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vle64.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vload.mask.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double>* undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vload.nxv8f64(
  <vscale x 8 x double>*,
  i64);

define void @intrinsic_vload_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vle64.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 8 x double> @llvm.epi.vload.nxv8f64(
    <vscale x 8 x double>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vload.mask.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload_mask_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload_mask_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vle64.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vload.mask.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare void @llvm.epi.vstore.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>*,
  i64);

define void @intrinsic_vstore_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vse8.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.mask.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore_mask_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_mask_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vse8.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.mask.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>*,
  i64);

define void @intrinsic_vstore_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vse8.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.mask.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>*,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore_mask_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_mask_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vse8.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.mask.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8>* undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>*,
  i64);

define void @intrinsic_vstore_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vse8.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.mask.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>*,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vstore_mask_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_mask_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vse8.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.mask.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8>* undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>*,
  i64);

define void @intrinsic_vstore_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vse16.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.mask.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>*,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore_mask_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_mask_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vse16.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.mask.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16>* undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>*,
  i64);

define void @intrinsic_vstore_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vse16.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.mask.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore_mask_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_mask_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vse16.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.mask.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>*,
  i64);

define void @intrinsic_vstore_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vse16.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.mask.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>*,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore_mask_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_mask_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vse16.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.mask.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16>* undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>*,
  i64);

define void @intrinsic_vstore_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vse16.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.mask.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>*,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vstore_mask_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_mask_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vse16.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.mask.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16>* undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>*,
  i64);

define void @intrinsic_vstore_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vse32.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.mask.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>*,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore_mask_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_mask_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vse32.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.mask.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32>* undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>*,
  i64);

define void @intrinsic_vstore_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vse32.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.mask.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>*,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore_mask_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_mask_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vse32.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.mask.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32>* undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>*,
  i64);

define void @intrinsic_vstore_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vse32.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.mask.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore_mask_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_mask_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vse32.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.mask.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>*,
  i64);

define void @intrinsic_vstore_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vse32.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.mask.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>*,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore_mask_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_mask_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vse32.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.mask.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32>* undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>*,
  i64);

define void @intrinsic_vstore_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vse64.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.mask.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>*,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vstore_mask_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_mask_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vse64.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.mask.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64>* undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>*,
  i64);

define void @intrinsic_vstore_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vse64.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.mask.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>*,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore_mask_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_mask_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vse64.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.mask.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64>* undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>*,
  i64);

define void @intrinsic_vstore_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vse64.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.mask.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>*,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore_mask_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_mask_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vse64.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.mask.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64>* undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>*,
  i64);

define void @intrinsic_vstore_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vse64.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.mask.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore_mask_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_mask_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vse64.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.mask.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>*,
  i64);

define void @intrinsic_vstore_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vse32.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.mask.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>*,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore_mask_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_mask_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vse32.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.mask.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float>* undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>*,
  i64);

define void @intrinsic_vstore_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vse32.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.mask.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>*,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore_mask_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_mask_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vse32.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.mask.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float>* undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>*,
  i64);

define void @intrinsic_vstore_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vse32.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.mask.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore_mask_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_mask_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vse32.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.mask.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>*,
  i64);

define void @intrinsic_vstore_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vse32.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.mask.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>*,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore_mask_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_mask_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vse32.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.mask.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float>* undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  i64);

define void @intrinsic_vstore_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vse64.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.mask.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vstore_mask_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_mask_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vse64.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.mask.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double>* undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>*,
  i64);

define void @intrinsic_vstore_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vse64.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.mask.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>*,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore_mask_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_mask_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vse64.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.mask.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double>* undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>*,
  i64);

define void @intrinsic_vstore_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vse64.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.mask.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>*,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore_mask_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_mask_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vse64.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.mask.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double>* undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>*,
  i64);

define void @intrinsic_vstore_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vse64.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.mask.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore_mask_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore_mask_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vse64.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.mask.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vload.strided.nxv8i8(
  <vscale x 8 x i8>*,
  i64,
  i64);

define void @intrinsic_vload.strided_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vlse8.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 8 x i8> @llvm.epi.vload.strided.nxv8i8(
    <vscale x 8 x i8>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vload.strided.mask.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.strided_mask_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_mask_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vlse8.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vload.strided.mask.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vload.strided.nxv16i8(
  <vscale x 16 x i8>*,
  i64,
  i64);

define void @intrinsic_vload.strided_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vlse8.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 16 x i8> @llvm.epi.vload.strided.nxv16i8(
    <vscale x 16 x i8>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vload.strided.mask.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>*,
  i64,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload.strided_mask_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_mask_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vlse8.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vload.strided.mask.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8>* undef,
    i64 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vload.strided.nxv32i8(
  <vscale x 32 x i8>*,
  i64,
  i64);

define void @intrinsic_vload.strided_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vlse8.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 32 x i8> @llvm.epi.vload.strided.nxv32i8(
    <vscale x 32 x i8>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vload.strided.mask.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>*,
  i64,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vload.strided_mask_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_mask_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vlse8.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vload.strided.mask.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8>* undef,
    i64 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vload.strided.nxv4i16(
  <vscale x 4 x i16>*,
  i64,
  i64);

define void @intrinsic_vload.strided_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vlse16.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 4 x i16> @llvm.epi.vload.strided.nxv4i16(
    <vscale x 4 x i16>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vload.strided.mask.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>*,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.strided_mask_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_mask_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vlse16.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vload.strided.mask.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16>* undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vload.strided.nxv8i16(
  <vscale x 8 x i16>*,
  i64,
  i64);

define void @intrinsic_vload.strided_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vlse16.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 8 x i16> @llvm.epi.vload.strided.nxv8i16(
    <vscale x 8 x i16>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vload.strided.mask.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.strided_mask_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_mask_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vlse16.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vload.strided.mask.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vload.strided.nxv16i16(
  <vscale x 16 x i16>*,
  i64,
  i64);

define void @intrinsic_vload.strided_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vlse16.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 16 x i16> @llvm.epi.vload.strided.nxv16i16(
    <vscale x 16 x i16>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vload.strided.mask.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>*,
  i64,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload.strided_mask_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_mask_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vlse16.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vload.strided.mask.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16>* undef,
    i64 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vload.strided.nxv32i16(
  <vscale x 32 x i16>*,
  i64,
  i64);

define void @intrinsic_vload.strided_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vlse16.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 32 x i16> @llvm.epi.vload.strided.nxv32i16(
    <vscale x 32 x i16>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vload.strided.mask.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>*,
  i64,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vload.strided_mask_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_mask_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vlse16.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vload.strided.mask.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16>* undef,
    i64 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vload.strided.nxv2i32(
  <vscale x 2 x i32>*,
  i64,
  i64);

define void @intrinsic_vload.strided_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 2 x i32> @llvm.epi.vload.strided.nxv2i32(
    <vscale x 2 x i32>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vload.strided.mask.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>*,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload.strided_mask_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_mask_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vload.strided.mask.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32>* undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vload.strided.nxv4i32(
  <vscale x 4 x i32>*,
  i64,
  i64);

define void @intrinsic_vload.strided_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 4 x i32> @llvm.epi.vload.strided.nxv4i32(
    <vscale x 4 x i32>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vload.strided.mask.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>*,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.strided_mask_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_mask_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vload.strided.mask.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32>* undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vload.strided.nxv8i32(
  <vscale x 8 x i32>*,
  i64,
  i64);

define void @intrinsic_vload.strided_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 8 x i32> @llvm.epi.vload.strided.nxv8i32(
    <vscale x 8 x i32>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vload.strided.mask.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.strided_mask_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_mask_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vload.strided.mask.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vload.strided.nxv16i32(
  <vscale x 16 x i32>*,
  i64,
  i64);

define void @intrinsic_vload.strided_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 16 x i32> @llvm.epi.vload.strided.nxv16i32(
    <vscale x 16 x i32>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vload.strided.mask.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>*,
  i64,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload.strided_mask_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_mask_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vload.strided.mask.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32>* undef,
    i64 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vload.strided.nxv1i64(
  <vscale x 1 x i64>*,
  i64,
  i64);

define void @intrinsic_vload.strided_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 1 x i64> @llvm.epi.vload.strided.nxv1i64(
    <vscale x 1 x i64>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vload.strided.mask.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>*,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vload.strided_mask_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_mask_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vload.strided.mask.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64>* undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vload.strided.nxv2i64(
  <vscale x 2 x i64>*,
  i64,
  i64);

define void @intrinsic_vload.strided_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 2 x i64> @llvm.epi.vload.strided.nxv2i64(
    <vscale x 2 x i64>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vload.strided.mask.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>*,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload.strided_mask_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_mask_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vload.strided.mask.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64>* undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vload.strided.nxv4i64(
  <vscale x 4 x i64>*,
  i64,
  i64);

define void @intrinsic_vload.strided_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 4 x i64> @llvm.epi.vload.strided.nxv4i64(
    <vscale x 4 x i64>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vload.strided.mask.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>*,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.strided_mask_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_mask_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vload.strided.mask.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64>* undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vload.strided.nxv8i64(
  <vscale x 8 x i64>*,
  i64,
  i64);

define void @intrinsic_vload.strided_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 8 x i64> @llvm.epi.vload.strided.nxv8i64(
    <vscale x 8 x i64>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vload.strided.mask.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.strided_mask_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_mask_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vload.strided.mask.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vload.strided.nxv2f32(
  <vscale x 2 x float>*,
  i64,
  i64);

define void @intrinsic_vload.strided_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 2 x float> @llvm.epi.vload.strided.nxv2f32(
    <vscale x 2 x float>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vload.strided.mask.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>*,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload.strided_mask_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_mask_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vload.strided.mask.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float>* undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vload.strided.nxv4f32(
  <vscale x 4 x float>*,
  i64,
  i64);

define void @intrinsic_vload.strided_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 4 x float> @llvm.epi.vload.strided.nxv4f32(
    <vscale x 4 x float>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vload.strided.mask.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>*,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.strided_mask_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_mask_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vload.strided.mask.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float>* undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vload.strided.nxv8f32(
  <vscale x 8 x float>*,
  i64,
  i64);

define void @intrinsic_vload.strided_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 8 x float> @llvm.epi.vload.strided.nxv8f32(
    <vscale x 8 x float>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vload.strided.mask.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.strided_mask_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_mask_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vload.strided.mask.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vload.strided.nxv16f32(
  <vscale x 16 x float>*,
  i64,
  i64);

define void @intrinsic_vload.strided_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 16 x float> @llvm.epi.vload.strided.nxv16f32(
    <vscale x 16 x float>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vload.strided.mask.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>*,
  i64,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload.strided_mask_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_mask_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vload.strided.mask.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float>* undef,
    i64 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vload.strided.nxv1f64(
  <vscale x 1 x double>*,
  i64,
  i64);

define void @intrinsic_vload.strided_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 1 x double> @llvm.epi.vload.strided.nxv1f64(
    <vscale x 1 x double>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vload.strided.mask.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vload.strided_mask_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_mask_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vload.strided.mask.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double>* undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vload.strided.nxv2f64(
  <vscale x 2 x double>*,
  i64,
  i64);

define void @intrinsic_vload.strided_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 2 x double> @llvm.epi.vload.strided.nxv2f64(
    <vscale x 2 x double>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vload.strided.mask.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>*,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload.strided_mask_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_mask_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vload.strided.mask.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double>* undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vload.strided.nxv4f64(
  <vscale x 4 x double>*,
  i64,
  i64);

define void @intrinsic_vload.strided_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 4 x double> @llvm.epi.vload.strided.nxv4f64(
    <vscale x 4 x double>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vload.strided.mask.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>*,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.strided_mask_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_mask_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vload.strided.mask.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double>* undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vload.strided.nxv8f64(
  <vscale x 8 x double>*,
  i64,
  i64);

define void @intrinsic_vload.strided_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 8 x double> @llvm.epi.vload.strided.nxv8f64(
    <vscale x 8 x double>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vload.strided.mask.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.strided_mask_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.strided_mask_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vload.strided.mask.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare void @llvm.epi.vstore.strided.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>*,
  i64,
  i64);

define void @intrinsic_vstore.strided_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsse8.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.strided.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.strided.mask.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.strided_mask_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_mask_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsse8.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.strided.mask.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.strided.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>*,
  i64,
  i64);

define void @intrinsic_vstore.strided_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsse8.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.strided.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.strided.mask.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>*,
  i64,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore.strided_mask_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_mask_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsse8.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.strided.mask.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8>* undef,
    i64 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.strided.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>*,
  i64,
  i64);

define void @intrinsic_vstore.strided_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsse8.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.strided.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.strided.mask.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>*,
  i64,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vstore.strided_mask_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_mask_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsse8.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.strided.mask.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8>* undef,
    i64 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.strided.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>*,
  i64,
  i64);

define void @intrinsic_vstore.strided_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsse16.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.strided.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.strided.mask.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>*,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.strided_mask_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_mask_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsse16.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.strided.mask.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16>* undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.strided.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>*,
  i64,
  i64);

define void @intrinsic_vstore.strided_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsse16.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.strided.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.strided.mask.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.strided_mask_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_mask_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsse16.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.strided.mask.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.strided.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>*,
  i64,
  i64);

define void @intrinsic_vstore.strided_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsse16.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.strided.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.strided.mask.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>*,
  i64,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore.strided_mask_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_mask_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsse16.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.strided.mask.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16>* undef,
    i64 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.strided.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>*,
  i64,
  i64);

define void @intrinsic_vstore.strided_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsse16.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.strided.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.strided.mask.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>*,
  i64,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vstore.strided_mask_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_mask_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsse16.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.strided.mask.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16>* undef,
    i64 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.strided.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>*,
  i64,
  i64);

define void @intrinsic_vstore.strided_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.strided.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.strided.mask.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>*,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore.strided_mask_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_mask_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.strided.mask.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32>* undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.strided.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>*,
  i64,
  i64);

define void @intrinsic_vstore.strided_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.strided.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.strided.mask.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>*,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.strided_mask_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_mask_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.strided.mask.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32>* undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.strided.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>*,
  i64,
  i64);

define void @intrinsic_vstore.strided_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.strided.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.strided.mask.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.strided_mask_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_mask_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.strided.mask.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.strided.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>*,
  i64,
  i64);

define void @intrinsic_vstore.strided_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.strided.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.strided.mask.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>*,
  i64,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore.strided_mask_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_mask_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.strided.mask.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32>* undef,
    i64 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.strided.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>*,
  i64,
  i64);

define void @intrinsic_vstore.strided_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.strided.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.strided.mask.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>*,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vstore.strided_mask_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_mask_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.strided.mask.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64>* undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.strided.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>*,
  i64,
  i64);

define void @intrinsic_vstore.strided_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.strided.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.strided.mask.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>*,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore.strided_mask_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_mask_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.strided.mask.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64>* undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.strided.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>*,
  i64,
  i64);

define void @intrinsic_vstore.strided_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.strided.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.strided.mask.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>*,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.strided_mask_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_mask_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.strided.mask.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64>* undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.strided.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>*,
  i64,
  i64);

define void @intrinsic_vstore.strided_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.strided.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.strided.mask.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.strided_mask_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_mask_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.strided.mask.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.strided.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>*,
  i64,
  i64);

define void @intrinsic_vstore.strided_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.strided.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.strided.mask.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>*,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore.strided_mask_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_mask_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.strided.mask.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float>* undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.strided.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>*,
  i64,
  i64);

define void @intrinsic_vstore.strided_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.strided.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.strided.mask.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>*,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.strided_mask_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_mask_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.strided.mask.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float>* undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.strided.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>*,
  i64,
  i64);

define void @intrinsic_vstore.strided_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.strided.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.strided.mask.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.strided_mask_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_mask_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.strided.mask.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.strided.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>*,
  i64,
  i64);

define void @intrinsic_vstore.strided_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.strided.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.strided.mask.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>*,
  i64,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore.strided_mask_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_mask_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.strided.mask.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float>* undef,
    i64 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.strided.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  i64,
  i64);

define void @intrinsic_vstore.strided_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.strided.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.strided.mask.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vstore.strided_mask_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_mask_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.strided.mask.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double>* undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.strided.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>*,
  i64,
  i64);

define void @intrinsic_vstore.strided_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.strided.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.strided.mask.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>*,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore.strided_mask_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_mask_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.strided.mask.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double>* undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.strided.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>*,
  i64,
  i64);

define void @intrinsic_vstore.strided_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.strided.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.strided.mask.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>*,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.strided_mask_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_mask_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.strided.mask.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double>* undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.strided.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>*,
  i64,
  i64);

define void @intrinsic_vstore.strided_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.strided.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.strided.mask.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.strided_mask_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.strided_mask_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.strided.mask.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vload.indexed.nxv8i8(
  <vscale x 8 x i8>*,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vload.indexed_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vluxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vload.indexed.nxv8i8(
    <vscale x 8 x i8>* undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vload.indexed.mask.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>*,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.indexed_mask_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_mask_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vluxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vload.indexed.mask.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8>* undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vload.indexed.nxv16i8(
  <vscale x 16 x i8>*,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vload.indexed_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vluxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vload.indexed.nxv16i8(
    <vscale x 16 x i8>* undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vload.indexed.mask.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>*,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload.indexed_mask_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_mask_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vluxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vload.indexed.mask.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8>* undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vload.indexed.nxv32i8(
  <vscale x 32 x i8>*,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vload.indexed_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vluxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vload.indexed.nxv32i8(
    <vscale x 32 x i8>* undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vload.indexed.mask.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>*,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vload.indexed_mask_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_mask_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vluxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vload.indexed.mask.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8>* undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vload.indexed.nxv4i16(
  <vscale x 4 x i16>*,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vload.indexed_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vluxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vload.indexed.nxv4i16(
    <vscale x 4 x i16>* undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vload.indexed.mask.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>*,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.indexed_mask_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_mask_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vluxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vload.indexed.mask.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16>* undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vload.indexed.nxv8i16(
  <vscale x 8 x i16>*,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vload.indexed_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vluxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vload.indexed.nxv8i16(
    <vscale x 8 x i16>* undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vload.indexed.mask.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>*,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.indexed_mask_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_mask_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vluxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vload.indexed.mask.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16>* undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vload.indexed.nxv16i16(
  <vscale x 16 x i16>*,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vload.indexed_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vluxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vload.indexed.nxv16i16(
    <vscale x 16 x i16>* undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vload.indexed.mask.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>*,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload.indexed_mask_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_mask_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vluxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vload.indexed.mask.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16>* undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vload.indexed.nxv32i16(
  <vscale x 32 x i16>*,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vload.indexed_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vluxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vload.indexed.nxv32i16(
    <vscale x 32 x i16>* undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vload.indexed.mask.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>*,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vload.indexed_mask_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_mask_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vluxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vload.indexed.mask.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16>* undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vload.indexed.nxv2i32(
  <vscale x 2 x i32>*,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vload.indexed_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vload.indexed.nxv2i32(
    <vscale x 2 x i32>* undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vload.indexed.mask.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>*,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload.indexed_mask_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_mask_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vload.indexed.mask.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32>* undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vload.indexed.nxv4i32(
  <vscale x 4 x i32>*,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vload.indexed_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vload.indexed.nxv4i32(
    <vscale x 4 x i32>* undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vload.indexed.mask.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>*,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.indexed_mask_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_mask_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vload.indexed.mask.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32>* undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vload.indexed.nxv8i32(
  <vscale x 8 x i32>*,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vload.indexed_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vload.indexed.nxv8i32(
    <vscale x 8 x i32>* undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vload.indexed.mask.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>*,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.indexed_mask_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_mask_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vload.indexed.mask.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32>* undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vload.indexed.nxv16i32(
  <vscale x 16 x i32>*,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vload.indexed_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vload.indexed.nxv16i32(
    <vscale x 16 x i32>* undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vload.indexed.mask.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>*,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload.indexed_mask_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_mask_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vload.indexed.mask.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32>* undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vload.indexed.nxv1i64(
  <vscale x 1 x i64>*,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vload.indexed_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vload.indexed.nxv1i64(
    <vscale x 1 x i64>* undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vload.indexed.mask.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>*,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vload.indexed_mask_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_mask_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vload.indexed.mask.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64>* undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vload.indexed.nxv2i64(
  <vscale x 2 x i64>*,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vload.indexed_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vload.indexed.nxv2i64(
    <vscale x 2 x i64>* undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vload.indexed.mask.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>*,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload.indexed_mask_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_mask_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vload.indexed.mask.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64>* undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vload.indexed.nxv4i64(
  <vscale x 4 x i64>*,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vload.indexed_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vload.indexed.nxv4i64(
    <vscale x 4 x i64>* undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vload.indexed.mask.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>*,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.indexed_mask_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_mask_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vload.indexed.mask.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64>* undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vload.indexed.nxv8i64(
  <vscale x 8 x i64>*,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vload.indexed_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vload.indexed.nxv8i64(
    <vscale x 8 x i64>* undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vload.indexed.mask.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>*,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.indexed_mask_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_mask_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vload.indexed.mask.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64>* undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vload.indexed.nxv2f32(
  <vscale x 2 x float>*,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vload.indexed_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 2 x float> @llvm.epi.vload.indexed.nxv2f32(
    <vscale x 2 x float>* undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vload.indexed.mask.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>*,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload.indexed_mask_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_mask_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vload.indexed.mask.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float>* undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vload.indexed.nxv4f32(
  <vscale x 4 x float>*,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vload.indexed_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vload.indexed.nxv4f32(
    <vscale x 4 x float>* undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vload.indexed.mask.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>*,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.indexed_mask_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_mask_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vload.indexed.mask.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float>* undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vload.indexed.nxv8f32(
  <vscale x 8 x float>*,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vload.indexed_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vload.indexed.nxv8f32(
    <vscale x 8 x float>* undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vload.indexed.mask.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>*,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.indexed_mask_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_mask_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vload.indexed.mask.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float>* undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vload.indexed.nxv16f32(
  <vscale x 16 x float>*,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vload.indexed_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 16 x float> @llvm.epi.vload.indexed.nxv16f32(
    <vscale x 16 x float>* undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vload.indexed.mask.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>*,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload.indexed_mask_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_mask_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vload.indexed.mask.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float>* undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64(
  <vscale x 1 x double>*,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vload.indexed_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64(
    <vscale x 1 x double>* undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vload.indexed.mask.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vload.indexed_mask_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_mask_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vload.indexed.mask.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double>* undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vload.indexed.nxv2f64(
  <vscale x 2 x double>*,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vload.indexed_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vload.indexed.nxv2f64(
    <vscale x 2 x double>* undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vload.indexed.mask.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>*,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload.indexed_mask_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_mask_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vload.indexed.mask.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double>* undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vload.indexed.nxv4f64(
  <vscale x 4 x double>*,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vload.indexed_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vload.indexed.nxv4f64(
    <vscale x 4 x double>* undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vload.indexed.mask.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>*,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.indexed_mask_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_mask_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vload.indexed.mask.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double>* undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vload.indexed.nxv8f64(
  <vscale x 8 x double>*,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vload.indexed_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vload.indexed.nxv8f64(
    <vscale x 8 x double>* undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vload.indexed.mask.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>*,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.indexed_mask_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.indexed_mask_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vload.indexed.mask.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double>* undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare void @llvm.epi.vstore.indexed.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>*,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vstore.indexed_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsuxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.indexed.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8>* undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.indexed.mask.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>*,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.indexed_mask_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_mask_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsuxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.indexed.mask.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8>* undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.indexed.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>*,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vstore.indexed_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsuxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.indexed.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8>* undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.indexed.mask.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>*,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore.indexed_mask_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_mask_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsuxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.indexed.mask.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8>* undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.indexed.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>*,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vstore.indexed_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsuxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.indexed.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8>* undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.indexed.mask.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>*,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vstore.indexed_mask_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_mask_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsuxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.indexed.mask.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8>* undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.indexed.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>*,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vstore.indexed_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsuxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.indexed.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16>* undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.indexed.mask.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>*,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.indexed_mask_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_mask_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsuxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.indexed.mask.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16>* undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.indexed.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>*,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vstore.indexed_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsuxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.indexed.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16>* undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.indexed.mask.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>*,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.indexed_mask_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_mask_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsuxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.indexed.mask.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16>* undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.indexed.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>*,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vstore.indexed_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsuxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.indexed.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16>* undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.indexed.mask.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>*,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore.indexed_mask_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_mask_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsuxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.indexed.mask.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16>* undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.indexed.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>*,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vstore.indexed_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsuxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.indexed.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16>* undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.indexed.mask.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>*,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vstore.indexed_mask_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_mask_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsuxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.indexed.mask.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16>* undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.indexed.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>*,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vstore.indexed_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.indexed.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32>* undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.indexed.mask.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>*,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore.indexed_mask_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_mask_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.indexed.mask.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32>* undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.indexed.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>*,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vstore.indexed_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.indexed.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32>* undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.indexed.mask.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>*,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.indexed_mask_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_mask_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.indexed.mask.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32>* undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.indexed.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>*,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vstore.indexed_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.indexed.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32>* undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.indexed.mask.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>*,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.indexed_mask_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_mask_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.indexed.mask.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32>* undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.indexed.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>*,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vstore.indexed_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.indexed.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32>* undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.indexed.mask.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>*,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore.indexed_mask_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_mask_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.indexed.mask.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32>* undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.indexed.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>*,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vstore.indexed_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.indexed.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64>* undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.indexed.mask.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>*,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vstore.indexed_mask_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_mask_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.indexed.mask.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64>* undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.indexed.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>*,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vstore.indexed_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.indexed.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64>* undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.indexed.mask.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>*,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore.indexed_mask_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_mask_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.indexed.mask.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64>* undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.indexed.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>*,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vstore.indexed_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.indexed.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64>* undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.indexed.mask.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>*,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.indexed_mask_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_mask_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.indexed.mask.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64>* undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.indexed.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>*,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vstore.indexed_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.indexed.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64>* undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.indexed.mask.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>*,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.indexed_mask_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_mask_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.indexed.mask.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64>* undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.indexed.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>*,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vstore.indexed_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.indexed.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float>* undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.indexed.mask.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>*,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore.indexed_mask_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_mask_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.indexed.mask.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float>* undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.indexed.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>*,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vstore.indexed_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.indexed.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float>* undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.indexed.mask.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>*,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.indexed_mask_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_mask_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.indexed.mask.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float>* undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.indexed.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>*,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vstore.indexed_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.indexed.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float>* undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.indexed.mask.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>*,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.indexed_mask_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_mask_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.indexed.mask.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float>* undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.indexed.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>*,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vstore.indexed_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.indexed.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float>* undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.indexed.mask.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>*,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore.indexed_mask_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_mask_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.indexed.mask.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float>* undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.indexed.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vstore.indexed_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.indexed.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double>* undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.indexed.mask.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vstore.indexed_mask_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_mask_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.indexed.mask.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double>* undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.indexed.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>*,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vstore.indexed_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.indexed.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double>* undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.indexed.mask.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>*,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore.indexed_mask_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_mask_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.indexed.mask.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double>* undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.indexed.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>*,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vstore.indexed_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.indexed.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double>* undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.indexed.mask.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>*,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.indexed_mask_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_mask_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.indexed.mask.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double>* undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.indexed.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>*,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vstore.indexed_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.indexed.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double>* undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.indexed.mask.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>*,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.indexed_mask_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.indexed_mask_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.indexed.mask.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double>* undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vload.nt.nxv8i8(
  <vscale x 8 x i8>*,
  i64);

define void @intrinsic_vload.nt_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu, nt
; CHECK:       vle8.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 8 x i8> @llvm.epi.vload.nt.nxv8i8(
    <vscale x 8 x i8>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vload.nt.mask.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.nt_mask_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_mask_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu, nt
; CHECK:       vle8.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vload.nt.mask.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vload.nt.nxv16i8(
  <vscale x 16 x i8>*,
  i64);

define void @intrinsic_vload.nt_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu, nt
; CHECK:       vle8.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 16 x i8> @llvm.epi.vload.nt.nxv16i8(
    <vscale x 16 x i8>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vload.nt.mask.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>*,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload.nt_mask_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_mask_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu, nt
; CHECK:       vle8.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vload.nt.mask.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8>* undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vload.nt.nxv32i8(
  <vscale x 32 x i8>*,
  i64);

define void @intrinsic_vload.nt_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu, nt
; CHECK:       vle8.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 32 x i8> @llvm.epi.vload.nt.nxv32i8(
    <vscale x 32 x i8>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vload.nt.mask.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>*,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vload.nt_mask_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_mask_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu, nt
; CHECK:       vle8.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vload.nt.mask.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8>* undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vload.nt.nxv4i16(
  <vscale x 4 x i16>*,
  i64);

define void @intrinsic_vload.nt_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu, nt
; CHECK:       vle16.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 4 x i16> @llvm.epi.vload.nt.nxv4i16(
    <vscale x 4 x i16>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vload.nt.mask.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>*,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.nt_mask_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_mask_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu, nt
; CHECK:       vle16.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vload.nt.mask.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16>* undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vload.nt.nxv8i16(
  <vscale x 8 x i16>*,
  i64);

define void @intrinsic_vload.nt_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu, nt
; CHECK:       vle16.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 8 x i16> @llvm.epi.vload.nt.nxv8i16(
    <vscale x 8 x i16>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vload.nt.mask.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.nt_mask_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_mask_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu, nt
; CHECK:       vle16.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vload.nt.mask.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vload.nt.nxv16i16(
  <vscale x 16 x i16>*,
  i64);

define void @intrinsic_vload.nt_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu, nt
; CHECK:       vle16.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 16 x i16> @llvm.epi.vload.nt.nxv16i16(
    <vscale x 16 x i16>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vload.nt.mask.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>*,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload.nt_mask_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_mask_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu, nt
; CHECK:       vle16.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vload.nt.mask.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16>* undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vload.nt.nxv32i16(
  <vscale x 32 x i16>*,
  i64);

define void @intrinsic_vload.nt_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu, nt
; CHECK:       vle16.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 32 x i16> @llvm.epi.vload.nt.nxv32i16(
    <vscale x 32 x i16>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vload.nt.mask.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>*,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vload.nt_mask_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_mask_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu, nt
; CHECK:       vle16.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vload.nt.mask.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16>* undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vload.nt.nxv2i32(
  <vscale x 2 x i32>*,
  i64);

define void @intrinsic_vload.nt_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vle32.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 2 x i32> @llvm.epi.vload.nt.nxv2i32(
    <vscale x 2 x i32>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vload.nt.mask.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>*,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload.nt_mask_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_mask_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vle32.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vload.nt.mask.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32>* undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vload.nt.nxv4i32(
  <vscale x 4 x i32>*,
  i64);

define void @intrinsic_vload.nt_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vle32.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 4 x i32> @llvm.epi.vload.nt.nxv4i32(
    <vscale x 4 x i32>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vload.nt.mask.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>*,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.nt_mask_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_mask_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vle32.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vload.nt.mask.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32>* undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vload.nt.nxv8i32(
  <vscale x 8 x i32>*,
  i64);

define void @intrinsic_vload.nt_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vle32.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 8 x i32> @llvm.epi.vload.nt.nxv8i32(
    <vscale x 8 x i32>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vload.nt.mask.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.nt_mask_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_mask_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vle32.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vload.nt.mask.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vload.nt.nxv16i32(
  <vscale x 16 x i32>*,
  i64);

define void @intrinsic_vload.nt_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vle32.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 16 x i32> @llvm.epi.vload.nt.nxv16i32(
    <vscale x 16 x i32>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vload.nt.mask.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>*,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload.nt_mask_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_mask_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vle32.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vload.nt.mask.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32>* undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vload.nt.nxv1i64(
  <vscale x 1 x i64>*,
  i64);

define void @intrinsic_vload.nt_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vle64.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 1 x i64> @llvm.epi.vload.nt.nxv1i64(
    <vscale x 1 x i64>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vload.nt.mask.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>*,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vload.nt_mask_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_mask_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vle64.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vload.nt.mask.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64>* undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vload.nt.nxv2i64(
  <vscale x 2 x i64>*,
  i64);

define void @intrinsic_vload.nt_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vle64.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 2 x i64> @llvm.epi.vload.nt.nxv2i64(
    <vscale x 2 x i64>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vload.nt.mask.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>*,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload.nt_mask_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_mask_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vle64.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vload.nt.mask.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64>* undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vload.nt.nxv4i64(
  <vscale x 4 x i64>*,
  i64);

define void @intrinsic_vload.nt_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vle64.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 4 x i64> @llvm.epi.vload.nt.nxv4i64(
    <vscale x 4 x i64>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vload.nt.mask.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>*,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.nt_mask_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_mask_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vle64.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vload.nt.mask.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64>* undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vload.nt.nxv8i64(
  <vscale x 8 x i64>*,
  i64);

define void @intrinsic_vload.nt_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vle64.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 8 x i64> @llvm.epi.vload.nt.nxv8i64(
    <vscale x 8 x i64>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vload.nt.mask.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.nt_mask_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_mask_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vle64.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vload.nt.mask.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vload.nt.nxv2f32(
  <vscale x 2 x float>*,
  i64);

define void @intrinsic_vload.nt_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vle32.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 2 x float> @llvm.epi.vload.nt.nxv2f32(
    <vscale x 2 x float>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vload.nt.mask.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>*,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload.nt_mask_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_mask_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vle32.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vload.nt.mask.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float>* undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vload.nt.nxv4f32(
  <vscale x 4 x float>*,
  i64);

define void @intrinsic_vload.nt_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vle32.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 4 x float> @llvm.epi.vload.nt.nxv4f32(
    <vscale x 4 x float>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vload.nt.mask.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>*,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.nt_mask_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_mask_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vle32.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vload.nt.mask.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float>* undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vload.nt.nxv8f32(
  <vscale x 8 x float>*,
  i64);

define void @intrinsic_vload.nt_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vle32.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 8 x float> @llvm.epi.vload.nt.nxv8f32(
    <vscale x 8 x float>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vload.nt.mask.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.nt_mask_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_mask_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vle32.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vload.nt.mask.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vload.nt.nxv16f32(
  <vscale x 16 x float>*,
  i64);

define void @intrinsic_vload.nt_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vle32.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 16 x float> @llvm.epi.vload.nt.nxv16f32(
    <vscale x 16 x float>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vload.nt.mask.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>*,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload.nt_mask_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_mask_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vle32.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vload.nt.mask.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float>* undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vload.nt.nxv1f64(
  <vscale x 1 x double>*,
  i64);

define void @intrinsic_vload.nt_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vle64.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 1 x double> @llvm.epi.vload.nt.nxv1f64(
    <vscale x 1 x double>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vload.nt.mask.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vload.nt_mask_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_mask_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vle64.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vload.nt.mask.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double>* undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vload.nt.nxv2f64(
  <vscale x 2 x double>*,
  i64);

define void @intrinsic_vload.nt_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vle64.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 2 x double> @llvm.epi.vload.nt.nxv2f64(
    <vscale x 2 x double>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vload.nt.mask.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>*,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload.nt_mask_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_mask_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vle64.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vload.nt.mask.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double>* undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vload.nt.nxv4f64(
  <vscale x 4 x double>*,
  i64);

define void @intrinsic_vload.nt_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vle64.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 4 x double> @llvm.epi.vload.nt.nxv4f64(
    <vscale x 4 x double>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vload.nt.mask.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>*,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.nt_mask_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_mask_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vle64.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vload.nt.mask.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double>* undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vload.nt.nxv8f64(
  <vscale x 8 x double>*,
  i64);

define void @intrinsic_vload.nt_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vle64.v {{v[0-9]+}}, (a0)
  %a = call <vscale x 8 x double> @llvm.epi.vload.nt.nxv8f64(
    <vscale x 8 x double>* undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vload.nt.mask.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.nt_mask_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt_mask_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vle64.v {{v[0-9]+}}, (a0), v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vload.nt.mask.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare void @llvm.epi.vstore.nt.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>*,
  i64);

define void @intrinsic_vstore.nt_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu, nt
; CHECK:       vse8.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nt.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.mask.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.nt_mask_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_mask_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu, nt
; CHECK:       vse8.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.nt.mask.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>*,
  i64);

define void @intrinsic_vstore.nt_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu, nt
; CHECK:       vse8.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nt.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.mask.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>*,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore.nt_mask_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_mask_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu, nt
; CHECK:       vse8.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.nt.mask.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8>* undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>*,
  i64);

define void @intrinsic_vstore.nt_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu, nt
; CHECK:       vse8.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nt.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.mask.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>*,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vstore.nt_mask_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_mask_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu, nt
; CHECK:       vse8.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.nt.mask.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8>* undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>*,
  i64);

define void @intrinsic_vstore.nt_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu, nt
; CHECK:       vse16.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nt.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.mask.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>*,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.nt_mask_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_mask_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu, nt
; CHECK:       vse16.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.nt.mask.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16>* undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>*,
  i64);

define void @intrinsic_vstore.nt_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu, nt
; CHECK:       vse16.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nt.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.mask.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.nt_mask_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_mask_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu, nt
; CHECK:       vse16.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.nt.mask.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>*,
  i64);

define void @intrinsic_vstore.nt_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu, nt
; CHECK:       vse16.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nt.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.mask.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>*,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore.nt_mask_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_mask_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu, nt
; CHECK:       vse16.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.nt.mask.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16>* undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>*,
  i64);

define void @intrinsic_vstore.nt_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu, nt
; CHECK:       vse16.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nt.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.mask.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>*,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vstore.nt_mask_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_mask_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu, nt
; CHECK:       vse16.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.nt.mask.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16>* undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>*,
  i64);

define void @intrinsic_vstore.nt_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vse32.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nt.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.mask.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>*,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore.nt_mask_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_mask_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vse32.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.nt.mask.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32>* undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>*,
  i64);

define void @intrinsic_vstore.nt_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vse32.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nt.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.mask.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>*,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.nt_mask_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_mask_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vse32.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.nt.mask.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32>* undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>*,
  i64);

define void @intrinsic_vstore.nt_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vse32.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nt.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.mask.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.nt_mask_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_mask_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vse32.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.nt.mask.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>*,
  i64);

define void @intrinsic_vstore.nt_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vse32.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nt.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.mask.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>*,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore.nt_mask_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_mask_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vse32.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.nt.mask.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32>* undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>*,
  i64);

define void @intrinsic_vstore.nt_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vse64.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nt.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.mask.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>*,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vstore.nt_mask_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_mask_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vse64.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.nt.mask.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64>* undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>*,
  i64);

define void @intrinsic_vstore.nt_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vse64.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nt.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.mask.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>*,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore.nt_mask_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_mask_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vse64.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.nt.mask.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64>* undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>*,
  i64);

define void @intrinsic_vstore.nt_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vse64.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nt.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.mask.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>*,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.nt_mask_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_mask_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vse64.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.nt.mask.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64>* undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>*,
  i64);

define void @intrinsic_vstore.nt_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vse64.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nt.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.mask.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.nt_mask_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_mask_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vse64.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.nt.mask.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>*,
  i64);

define void @intrinsic_vstore.nt_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vse32.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nt.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.mask.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>*,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore.nt_mask_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_mask_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vse32.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.nt.mask.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float>* undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>*,
  i64);

define void @intrinsic_vstore.nt_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vse32.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nt.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.mask.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>*,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.nt_mask_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_mask_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vse32.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.nt.mask.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float>* undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>*,
  i64);

define void @intrinsic_vstore.nt_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vse32.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nt.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.mask.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.nt_mask_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_mask_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vse32.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.nt.mask.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>*,
  i64);

define void @intrinsic_vstore.nt_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vse32.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nt.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.mask.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>*,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore.nt_mask_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_mask_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vse32.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.nt.mask.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float>* undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  i64);

define void @intrinsic_vstore.nt_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vse64.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nt.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.mask.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vstore.nt_mask_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_mask_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vse64.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.nt.mask.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double>* undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>*,
  i64);

define void @intrinsic_vstore.nt_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vse64.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nt.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.mask.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>*,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore.nt_mask_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_mask_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vse64.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.nt.mask.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double>* undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>*,
  i64);

define void @intrinsic_vstore.nt_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vse64.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nt.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.mask.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>*,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.nt_mask_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_mask_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vse64.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.nt.mask.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double>* undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>*,
  i64);

define void @intrinsic_vstore.nt_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vse64.v {{v[0-9]+}}, (a0)
  call void @llvm.epi.vstore.nt.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double>* undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.mask.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>*,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.nt_mask_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt_mask_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vse64.v {{v[0-9]+}}, (a0), v0.t
  call void @llvm.epi.vstore.nt.mask.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double>* undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vload.nt.strided.nxv8i8(
  <vscale x 8 x i8>*,
  i64,
  i64);

define void @intrinsic_vload.nt.strided_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu, nt
; CHECK:       vlse8.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 8 x i8> @llvm.epi.vload.nt.strided.nxv8i8(
    <vscale x 8 x i8>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vload.nt.strided.mask.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.nt.strided_mask_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_mask_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu, nt
; CHECK:       vlse8.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vload.nt.strided.mask.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vload.nt.strided.nxv16i8(
  <vscale x 16 x i8>*,
  i64,
  i64);

define void @intrinsic_vload.nt.strided_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu, nt
; CHECK:       vlse8.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 16 x i8> @llvm.epi.vload.nt.strided.nxv16i8(
    <vscale x 16 x i8>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vload.nt.strided.mask.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>*,
  i64,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload.nt.strided_mask_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_mask_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu, nt
; CHECK:       vlse8.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vload.nt.strided.mask.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8>* undef,
    i64 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vload.nt.strided.nxv32i8(
  <vscale x 32 x i8>*,
  i64,
  i64);

define void @intrinsic_vload.nt.strided_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu, nt
; CHECK:       vlse8.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 32 x i8> @llvm.epi.vload.nt.strided.nxv32i8(
    <vscale x 32 x i8>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vload.nt.strided.mask.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>*,
  i64,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vload.nt.strided_mask_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_mask_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu, nt
; CHECK:       vlse8.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vload.nt.strided.mask.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8>* undef,
    i64 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vload.nt.strided.nxv4i16(
  <vscale x 4 x i16>*,
  i64,
  i64);

define void @intrinsic_vload.nt.strided_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu, nt
; CHECK:       vlse16.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 4 x i16> @llvm.epi.vload.nt.strided.nxv4i16(
    <vscale x 4 x i16>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vload.nt.strided.mask.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>*,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.nt.strided_mask_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_mask_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu, nt
; CHECK:       vlse16.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vload.nt.strided.mask.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16>* undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vload.nt.strided.nxv8i16(
  <vscale x 8 x i16>*,
  i64,
  i64);

define void @intrinsic_vload.nt.strided_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu, nt
; CHECK:       vlse16.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 8 x i16> @llvm.epi.vload.nt.strided.nxv8i16(
    <vscale x 8 x i16>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vload.nt.strided.mask.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.nt.strided_mask_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_mask_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu, nt
; CHECK:       vlse16.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vload.nt.strided.mask.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vload.nt.strided.nxv16i16(
  <vscale x 16 x i16>*,
  i64,
  i64);

define void @intrinsic_vload.nt.strided_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu, nt
; CHECK:       vlse16.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 16 x i16> @llvm.epi.vload.nt.strided.nxv16i16(
    <vscale x 16 x i16>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vload.nt.strided.mask.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>*,
  i64,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload.nt.strided_mask_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_mask_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu, nt
; CHECK:       vlse16.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vload.nt.strided.mask.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16>* undef,
    i64 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vload.nt.strided.nxv32i16(
  <vscale x 32 x i16>*,
  i64,
  i64);

define void @intrinsic_vload.nt.strided_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu, nt
; CHECK:       vlse16.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 32 x i16> @llvm.epi.vload.nt.strided.nxv32i16(
    <vscale x 32 x i16>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vload.nt.strided.mask.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>*,
  i64,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vload.nt.strided_mask_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_mask_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu, nt
; CHECK:       vlse16.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vload.nt.strided.mask.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16>* undef,
    i64 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vload.nt.strided.nxv2i32(
  <vscale x 2 x i32>*,
  i64,
  i64);

define void @intrinsic_vload.nt.strided_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 2 x i32> @llvm.epi.vload.nt.strided.nxv2i32(
    <vscale x 2 x i32>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vload.nt.strided.mask.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>*,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload.nt.strided_mask_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_mask_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vload.nt.strided.mask.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32>* undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vload.nt.strided.nxv4i32(
  <vscale x 4 x i32>*,
  i64,
  i64);

define void @intrinsic_vload.nt.strided_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 4 x i32> @llvm.epi.vload.nt.strided.nxv4i32(
    <vscale x 4 x i32>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vload.nt.strided.mask.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>*,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.nt.strided_mask_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_mask_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vload.nt.strided.mask.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32>* undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vload.nt.strided.nxv8i32(
  <vscale x 8 x i32>*,
  i64,
  i64);

define void @intrinsic_vload.nt.strided_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 8 x i32> @llvm.epi.vload.nt.strided.nxv8i32(
    <vscale x 8 x i32>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vload.nt.strided.mask.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.nt.strided_mask_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_mask_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vload.nt.strided.mask.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vload.nt.strided.nxv16i32(
  <vscale x 16 x i32>*,
  i64,
  i64);

define void @intrinsic_vload.nt.strided_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 16 x i32> @llvm.epi.vload.nt.strided.nxv16i32(
    <vscale x 16 x i32>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vload.nt.strided.mask.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>*,
  i64,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload.nt.strided_mask_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_mask_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vload.nt.strided.mask.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32>* undef,
    i64 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vload.nt.strided.nxv1i64(
  <vscale x 1 x i64>*,
  i64,
  i64);

define void @intrinsic_vload.nt.strided_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 1 x i64> @llvm.epi.vload.nt.strided.nxv1i64(
    <vscale x 1 x i64>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vload.nt.strided.mask.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>*,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vload.nt.strided_mask_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_mask_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vload.nt.strided.mask.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64>* undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vload.nt.strided.nxv2i64(
  <vscale x 2 x i64>*,
  i64,
  i64);

define void @intrinsic_vload.nt.strided_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 2 x i64> @llvm.epi.vload.nt.strided.nxv2i64(
    <vscale x 2 x i64>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vload.nt.strided.mask.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>*,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload.nt.strided_mask_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_mask_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vload.nt.strided.mask.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64>* undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vload.nt.strided.nxv4i64(
  <vscale x 4 x i64>*,
  i64,
  i64);

define void @intrinsic_vload.nt.strided_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 4 x i64> @llvm.epi.vload.nt.strided.nxv4i64(
    <vscale x 4 x i64>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vload.nt.strided.mask.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>*,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.nt.strided_mask_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_mask_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vload.nt.strided.mask.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64>* undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vload.nt.strided.nxv8i64(
  <vscale x 8 x i64>*,
  i64,
  i64);

define void @intrinsic_vload.nt.strided_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 8 x i64> @llvm.epi.vload.nt.strided.nxv8i64(
    <vscale x 8 x i64>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vload.nt.strided.mask.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.nt.strided_mask_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_mask_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vload.nt.strided.mask.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vload.nt.strided.nxv2f32(
  <vscale x 2 x float>*,
  i64,
  i64);

define void @intrinsic_vload.nt.strided_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 2 x float> @llvm.epi.vload.nt.strided.nxv2f32(
    <vscale x 2 x float>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vload.nt.strided.mask.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>*,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload.nt.strided_mask_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_mask_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vload.nt.strided.mask.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float>* undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vload.nt.strided.nxv4f32(
  <vscale x 4 x float>*,
  i64,
  i64);

define void @intrinsic_vload.nt.strided_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 4 x float> @llvm.epi.vload.nt.strided.nxv4f32(
    <vscale x 4 x float>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vload.nt.strided.mask.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>*,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.nt.strided_mask_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_mask_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vload.nt.strided.mask.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float>* undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vload.nt.strided.nxv8f32(
  <vscale x 8 x float>*,
  i64,
  i64);

define void @intrinsic_vload.nt.strided_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 8 x float> @llvm.epi.vload.nt.strided.nxv8f32(
    <vscale x 8 x float>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vload.nt.strided.mask.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.nt.strided_mask_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_mask_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vload.nt.strided.mask.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vload.nt.strided.nxv16f32(
  <vscale x 16 x float>*,
  i64,
  i64);

define void @intrinsic_vload.nt.strided_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 16 x float> @llvm.epi.vload.nt.strided.nxv16f32(
    <vscale x 16 x float>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vload.nt.strided.mask.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>*,
  i64,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload.nt.strided_mask_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_mask_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vlse32.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vload.nt.strided.mask.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float>* undef,
    i64 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vload.nt.strided.nxv1f64(
  <vscale x 1 x double>*,
  i64,
  i64);

define void @intrinsic_vload.nt.strided_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 1 x double> @llvm.epi.vload.nt.strided.nxv1f64(
    <vscale x 1 x double>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vload.nt.strided.mask.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vload.nt.strided_mask_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_mask_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vload.nt.strided.mask.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double>* undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vload.nt.strided.nxv2f64(
  <vscale x 2 x double>*,
  i64,
  i64);

define void @intrinsic_vload.nt.strided_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 2 x double> @llvm.epi.vload.nt.strided.nxv2f64(
    <vscale x 2 x double>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vload.nt.strided.mask.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>*,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload.nt.strided_mask_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_mask_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vload.nt.strided.mask.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double>* undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vload.nt.strided.nxv4f64(
  <vscale x 4 x double>*,
  i64,
  i64);

define void @intrinsic_vload.nt.strided_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 4 x double> @llvm.epi.vload.nt.strided.nxv4f64(
    <vscale x 4 x double>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vload.nt.strided.mask.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>*,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.nt.strided_mask_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_mask_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vload.nt.strided.mask.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double>* undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vload.nt.strided.nxv8f64(
  <vscale x 8 x double>*,
  i64,
  i64);

define void @intrinsic_vload.nt.strided_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0
  %a = call <vscale x 8 x double> @llvm.epi.vload.nt.strided.nxv8f64(
    <vscale x 8 x double>* undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vload.nt.strided.mask.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.nt.strided_mask_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.strided_mask_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vlse64.v {{v[0-9]+}}, (a0), a0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vload.nt.strided.mask.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare void @llvm.epi.vstore.nt.strided.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>*,
  i64,
  i64);

define void @intrinsic_vstore.nt.strided_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu, nt
; CHECK:       vsse8.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.nt.strided.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.strided.mask.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.nt.strided_mask_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_mask_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu, nt
; CHECK:       vsse8.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.nt.strided.mask.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.strided.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>*,
  i64,
  i64);

define void @intrinsic_vstore.nt.strided_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu, nt
; CHECK:       vsse8.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.nt.strided.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.strided.mask.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>*,
  i64,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore.nt.strided_mask_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_mask_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu, nt
; CHECK:       vsse8.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.nt.strided.mask.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8>* undef,
    i64 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.strided.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>*,
  i64,
  i64);

define void @intrinsic_vstore.nt.strided_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu, nt
; CHECK:       vsse8.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.nt.strided.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.strided.mask.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>*,
  i64,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vstore.nt.strided_mask_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_mask_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu, nt
; CHECK:       vsse8.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.nt.strided.mask.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8>* undef,
    i64 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.strided.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>*,
  i64,
  i64);

define void @intrinsic_vstore.nt.strided_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu, nt
; CHECK:       vsse16.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.nt.strided.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.strided.mask.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>*,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.nt.strided_mask_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_mask_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu, nt
; CHECK:       vsse16.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.nt.strided.mask.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16>* undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.strided.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>*,
  i64,
  i64);

define void @intrinsic_vstore.nt.strided_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu, nt
; CHECK:       vsse16.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.nt.strided.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.strided.mask.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.nt.strided_mask_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_mask_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu, nt
; CHECK:       vsse16.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.nt.strided.mask.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.strided.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>*,
  i64,
  i64);

define void @intrinsic_vstore.nt.strided_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu, nt
; CHECK:       vsse16.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.nt.strided.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.strided.mask.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>*,
  i64,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore.nt.strided_mask_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_mask_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu, nt
; CHECK:       vsse16.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.nt.strided.mask.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16>* undef,
    i64 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.strided.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>*,
  i64,
  i64);

define void @intrinsic_vstore.nt.strided_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu, nt
; CHECK:       vsse16.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.nt.strided.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.strided.mask.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>*,
  i64,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vstore.nt.strided_mask_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_mask_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu, nt
; CHECK:       vsse16.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.nt.strided.mask.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16>* undef,
    i64 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.strided.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>*,
  i64,
  i64);

define void @intrinsic_vstore.nt.strided_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.nt.strided.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.strided.mask.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>*,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore.nt.strided_mask_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_mask_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.nt.strided.mask.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32>* undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.strided.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>*,
  i64,
  i64);

define void @intrinsic_vstore.nt.strided_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.nt.strided.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.strided.mask.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>*,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.nt.strided_mask_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_mask_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.nt.strided.mask.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32>* undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.strided.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>*,
  i64,
  i64);

define void @intrinsic_vstore.nt.strided_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.nt.strided.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.strided.mask.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.nt.strided_mask_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_mask_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.nt.strided.mask.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.strided.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>*,
  i64,
  i64);

define void @intrinsic_vstore.nt.strided_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.nt.strided.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.strided.mask.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>*,
  i64,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore.nt.strided_mask_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_mask_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.nt.strided.mask.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32>* undef,
    i64 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.strided.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>*,
  i64,
  i64);

define void @intrinsic_vstore.nt.strided_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.nt.strided.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.strided.mask.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>*,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vstore.nt.strided_mask_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_mask_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.nt.strided.mask.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64>* undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.strided.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>*,
  i64,
  i64);

define void @intrinsic_vstore.nt.strided_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.nt.strided.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.strided.mask.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>*,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore.nt.strided_mask_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_mask_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.nt.strided.mask.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64>* undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.strided.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>*,
  i64,
  i64);

define void @intrinsic_vstore.nt.strided_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.nt.strided.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.strided.mask.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>*,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.nt.strided_mask_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_mask_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.nt.strided.mask.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64>* undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.strided.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>*,
  i64,
  i64);

define void @intrinsic_vstore.nt.strided_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.nt.strided.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.strided.mask.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.nt.strided_mask_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_mask_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.nt.strided.mask.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.strided.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>*,
  i64,
  i64);

define void @intrinsic_vstore.nt.strided_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.nt.strided.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.strided.mask.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>*,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore.nt.strided_mask_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_mask_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.nt.strided.mask.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float>* undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.strided.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>*,
  i64,
  i64);

define void @intrinsic_vstore.nt.strided_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.nt.strided.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.strided.mask.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>*,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.nt.strided_mask_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_mask_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.nt.strided.mask.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float>* undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.strided.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>*,
  i64,
  i64);

define void @intrinsic_vstore.nt.strided_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.nt.strided.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.strided.mask.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.nt.strided_mask_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_mask_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.nt.strided.mask.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.strided.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>*,
  i64,
  i64);

define void @intrinsic_vstore.nt.strided_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.nt.strided.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.strided.mask.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>*,
  i64,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore.nt.strided_mask_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_mask_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vsse32.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.nt.strided.mask.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float>* undef,
    i64 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.strided.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  i64,
  i64);

define void @intrinsic_vstore.nt.strided_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.nt.strided.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.strided.mask.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vstore.nt.strided_mask_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_mask_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.nt.strided.mask.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double>* undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.strided.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>*,
  i64,
  i64);

define void @intrinsic_vstore.nt.strided_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.nt.strided.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.strided.mask.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>*,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore.nt.strided_mask_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_mask_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.nt.strided.mask.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double>* undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.strided.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>*,
  i64,
  i64);

define void @intrinsic_vstore.nt.strided_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.nt.strided.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.strided.mask.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>*,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.nt.strided_mask_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_mask_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.nt.strided.mask.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double>* undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.strided.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>*,
  i64,
  i64);

define void @intrinsic_vstore.nt.strided_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0
  call void @llvm.epi.vstore.nt.strided.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double>* undef,
    i64 undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.strided.mask.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>*,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.nt.strided_mask_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.strided_mask_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vsse64.v {{v[0-9]+}}, (a0), a0, v0.t
  call void @llvm.epi.vstore.nt.strided.mask.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double>* undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vload.nt.indexed.nxv8i8(
  <vscale x 8 x i8>*,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vload.nt.indexed_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu, nt
; CHECK:       vluxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vload.nt.indexed.nxv8i8(
    <vscale x 8 x i8>* undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vload.nt.indexed.mask.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>*,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.nt.indexed_mask_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_mask_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu, nt
; CHECK:       vluxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vload.nt.indexed.mask.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8>* undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vload.nt.indexed.nxv16i8(
  <vscale x 16 x i8>*,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vload.nt.indexed_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu, nt
; CHECK:       vluxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vload.nt.indexed.nxv16i8(
    <vscale x 16 x i8>* undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vload.nt.indexed.mask.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>*,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload.nt.indexed_mask_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_mask_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu, nt
; CHECK:       vluxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vload.nt.indexed.mask.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8>* undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vload.nt.indexed.nxv32i8(
  <vscale x 32 x i8>*,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vload.nt.indexed_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu, nt
; CHECK:       vluxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vload.nt.indexed.nxv32i8(
    <vscale x 32 x i8>* undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vload.nt.indexed.mask.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>*,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vload.nt.indexed_mask_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_mask_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu, nt
; CHECK:       vluxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vload.nt.indexed.mask.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8>* undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vload.nt.indexed.nxv4i16(
  <vscale x 4 x i16>*,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vload.nt.indexed_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu, nt
; CHECK:       vluxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vload.nt.indexed.nxv4i16(
    <vscale x 4 x i16>* undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vload.nt.indexed.mask.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>*,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.nt.indexed_mask_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_mask_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu, nt
; CHECK:       vluxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vload.nt.indexed.mask.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16>* undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vload.nt.indexed.nxv8i16(
  <vscale x 8 x i16>*,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vload.nt.indexed_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu, nt
; CHECK:       vluxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vload.nt.indexed.nxv8i16(
    <vscale x 8 x i16>* undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vload.nt.indexed.mask.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>*,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.nt.indexed_mask_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_mask_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu, nt
; CHECK:       vluxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vload.nt.indexed.mask.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16>* undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vload.nt.indexed.nxv16i16(
  <vscale x 16 x i16>*,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vload.nt.indexed_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu, nt
; CHECK:       vluxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vload.nt.indexed.nxv16i16(
    <vscale x 16 x i16>* undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vload.nt.indexed.mask.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>*,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload.nt.indexed_mask_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_mask_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu, nt
; CHECK:       vluxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vload.nt.indexed.mask.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16>* undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vload.nt.indexed.nxv32i16(
  <vscale x 32 x i16>*,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vload.nt.indexed_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu, nt
; CHECK:       vluxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vload.nt.indexed.nxv32i16(
    <vscale x 32 x i16>* undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vload.nt.indexed.mask.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>*,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vload.nt.indexed_mask_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_mask_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu, nt
; CHECK:       vluxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vload.nt.indexed.mask.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16>* undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vload.nt.indexed.nxv2i32(
  <vscale x 2 x i32>*,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vload.nt.indexed_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vload.nt.indexed.nxv2i32(
    <vscale x 2 x i32>* undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vload.nt.indexed.mask.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>*,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload.nt.indexed_mask_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_mask_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vload.nt.indexed.mask.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32>* undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vload.nt.indexed.nxv4i32(
  <vscale x 4 x i32>*,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vload.nt.indexed_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vload.nt.indexed.nxv4i32(
    <vscale x 4 x i32>* undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vload.nt.indexed.mask.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>*,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.nt.indexed_mask_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_mask_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vload.nt.indexed.mask.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32>* undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vload.nt.indexed.nxv8i32(
  <vscale x 8 x i32>*,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vload.nt.indexed_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vload.nt.indexed.nxv8i32(
    <vscale x 8 x i32>* undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vload.nt.indexed.mask.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>*,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.nt.indexed_mask_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_mask_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vload.nt.indexed.mask.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32>* undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vload.nt.indexed.nxv16i32(
  <vscale x 16 x i32>*,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vload.nt.indexed_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vload.nt.indexed.nxv16i32(
    <vscale x 16 x i32>* undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vload.nt.indexed.mask.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>*,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload.nt.indexed_mask_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_mask_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vload.nt.indexed.mask.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32>* undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vload.nt.indexed.nxv1i64(
  <vscale x 1 x i64>*,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vload.nt.indexed_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vload.nt.indexed.nxv1i64(
    <vscale x 1 x i64>* undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vload.nt.indexed.mask.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>*,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vload.nt.indexed_mask_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_mask_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vload.nt.indexed.mask.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64>* undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vload.nt.indexed.nxv2i64(
  <vscale x 2 x i64>*,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vload.nt.indexed_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vload.nt.indexed.nxv2i64(
    <vscale x 2 x i64>* undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vload.nt.indexed.mask.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>*,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload.nt.indexed_mask_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_mask_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vload.nt.indexed.mask.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64>* undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vload.nt.indexed.nxv4i64(
  <vscale x 4 x i64>*,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vload.nt.indexed_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vload.nt.indexed.nxv4i64(
    <vscale x 4 x i64>* undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vload.nt.indexed.mask.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>*,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.nt.indexed_mask_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_mask_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vload.nt.indexed.mask.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64>* undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vload.nt.indexed.nxv8i64(
  <vscale x 8 x i64>*,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vload.nt.indexed_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vload.nt.indexed.nxv8i64(
    <vscale x 8 x i64>* undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vload.nt.indexed.mask.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>*,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.nt.indexed_mask_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_mask_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vload.nt.indexed.mask.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64>* undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vload.nt.indexed.nxv2f32(
  <vscale x 2 x float>*,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vload.nt.indexed_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 2 x float> @llvm.epi.vload.nt.indexed.nxv2f32(
    <vscale x 2 x float>* undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vload.nt.indexed.mask.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>*,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload.nt.indexed_mask_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_mask_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vload.nt.indexed.mask.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float>* undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vload.nt.indexed.nxv4f32(
  <vscale x 4 x float>*,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vload.nt.indexed_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vload.nt.indexed.nxv4f32(
    <vscale x 4 x float>* undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vload.nt.indexed.mask.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>*,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.nt.indexed_mask_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_mask_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vload.nt.indexed.mask.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float>* undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vload.nt.indexed.nxv8f32(
  <vscale x 8 x float>*,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vload.nt.indexed_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vload.nt.indexed.nxv8f32(
    <vscale x 8 x float>* undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vload.nt.indexed.mask.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>*,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.nt.indexed_mask_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_mask_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vload.nt.indexed.mask.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float>* undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vload.nt.indexed.nxv16f32(
  <vscale x 16 x float>*,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vload.nt.indexed_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 16 x float> @llvm.epi.vload.nt.indexed.nxv16f32(
    <vscale x 16 x float>* undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vload.nt.indexed.mask.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>*,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vload.nt.indexed_mask_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_mask_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vluxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vload.nt.indexed.mask.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float>* undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vload.nt.indexed.nxv1f64(
  <vscale x 1 x double>*,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vload.nt.indexed_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 1 x double> @llvm.epi.vload.nt.indexed.nxv1f64(
    <vscale x 1 x double>* undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vload.nt.indexed.mask.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vload.nt.indexed_mask_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_mask_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vload.nt.indexed.mask.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double>* undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vload.nt.indexed.nxv2f64(
  <vscale x 2 x double>*,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vload.nt.indexed_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vload.nt.indexed.nxv2f64(
    <vscale x 2 x double>* undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vload.nt.indexed.mask.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>*,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vload.nt.indexed_mask_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_mask_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vload.nt.indexed.mask.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double>* undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vload.nt.indexed.nxv4f64(
  <vscale x 4 x double>*,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vload.nt.indexed_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vload.nt.indexed.nxv4f64(
    <vscale x 4 x double>* undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vload.nt.indexed.mask.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>*,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vload.nt.indexed_mask_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_mask_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vload.nt.indexed.mask.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double>* undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vload.nt.indexed.nxv8f64(
  <vscale x 8 x double>*,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vload.nt.indexed_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vload.nt.indexed.nxv8f64(
    <vscale x 8 x double>* undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vload.nt.indexed.mask.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>*,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vload.nt.indexed_mask_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vload.nt.indexed_mask_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vluxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vload.nt.indexed.mask.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double>* undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare void @llvm.epi.vstore.nt.indexed.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>*,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vstore.nt.indexed_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu, nt
; CHECK:       vsuxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.nt.indexed.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8>* undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.indexed.mask.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>*,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.nt.indexed_mask_v_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_mask_v_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu, nt
; CHECK:       vsuxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.nt.indexed.mask.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8>* undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.indexed.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>*,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vstore.nt.indexed_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu, nt
; CHECK:       vsuxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.nt.indexed.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8>* undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.indexed.mask.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>*,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore.nt.indexed_mask_v_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_mask_v_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu, nt
; CHECK:       vsuxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.nt.indexed.mask.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8>* undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.indexed.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>*,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vstore.nt.indexed_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu, nt
; CHECK:       vsuxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.nt.indexed.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8>* undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.indexed.mask.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>*,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vstore.nt.indexed_mask_v_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_mask_v_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu, nt
; CHECK:       vsuxei8.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.nt.indexed.mask.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8>* undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.indexed.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>*,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vstore.nt.indexed_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu, nt
; CHECK:       vsuxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.nt.indexed.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16>* undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.indexed.mask.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>*,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.nt.indexed_mask_v_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_mask_v_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu, nt
; CHECK:       vsuxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.nt.indexed.mask.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16>* undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.indexed.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>*,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vstore.nt.indexed_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu, nt
; CHECK:       vsuxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.nt.indexed.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16>* undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.indexed.mask.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>*,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.nt.indexed_mask_v_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_mask_v_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu, nt
; CHECK:       vsuxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.nt.indexed.mask.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16>* undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.indexed.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>*,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vstore.nt.indexed_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu, nt
; CHECK:       vsuxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.nt.indexed.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16>* undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.indexed.mask.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>*,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore.nt.indexed_mask_v_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_mask_v_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu, nt
; CHECK:       vsuxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.nt.indexed.mask.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16>* undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.indexed.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>*,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vstore.nt.indexed_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu, nt
; CHECK:       vsuxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.nt.indexed.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16>* undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.indexed.mask.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>*,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vstore.nt.indexed_mask_v_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_mask_v_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu, nt
; CHECK:       vsuxei16.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.nt.indexed.mask.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16>* undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.indexed.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>*,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vstore.nt.indexed_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.nt.indexed.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32>* undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.indexed.mask.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>*,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore.nt.indexed_mask_v_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_mask_v_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.nt.indexed.mask.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32>* undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.indexed.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>*,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vstore.nt.indexed_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.nt.indexed.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32>* undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.indexed.mask.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>*,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.nt.indexed_mask_v_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_mask_v_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.nt.indexed.mask.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32>* undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.indexed.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>*,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vstore.nt.indexed_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.nt.indexed.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32>* undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.indexed.mask.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>*,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.nt.indexed_mask_v_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_mask_v_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.nt.indexed.mask.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32>* undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.indexed.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>*,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vstore.nt.indexed_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.nt.indexed.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32>* undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.indexed.mask.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>*,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore.nt.indexed_mask_v_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_mask_v_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.nt.indexed.mask.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32>* undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.indexed.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>*,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vstore.nt.indexed_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.nt.indexed.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64>* undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.indexed.mask.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>*,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vstore.nt.indexed_mask_v_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_mask_v_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.nt.indexed.mask.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64>* undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.indexed.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>*,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vstore.nt.indexed_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.nt.indexed.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64>* undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.indexed.mask.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>*,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore.nt.indexed_mask_v_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_mask_v_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.nt.indexed.mask.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64>* undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.indexed.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>*,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vstore.nt.indexed_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.nt.indexed.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64>* undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.indexed.mask.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>*,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.nt.indexed_mask_v_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_mask_v_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.nt.indexed.mask.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64>* undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.indexed.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>*,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vstore.nt.indexed_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.nt.indexed.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64>* undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.indexed.mask.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>*,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.nt.indexed_mask_v_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_mask_v_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.nt.indexed.mask.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64>* undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.indexed.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>*,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vstore.nt.indexed_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.nt.indexed.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float>* undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.indexed.mask.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>*,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore.nt.indexed_mask_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_mask_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu, nt
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.nt.indexed.mask.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float>* undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.indexed.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>*,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vstore.nt.indexed_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.nt.indexed.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float>* undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.indexed.mask.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>*,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.nt.indexed_mask_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_mask_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu, nt
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.nt.indexed.mask.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float>* undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.indexed.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>*,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vstore.nt.indexed_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.nt.indexed.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float>* undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.indexed.mask.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>*,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.nt.indexed_mask_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_mask_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu, nt
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.nt.indexed.mask.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float>* undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.indexed.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>*,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vstore.nt.indexed_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.nt.indexed.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float>* undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.indexed.mask.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>*,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vstore.nt.indexed_mask_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_mask_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu, nt
; CHECK:       vsuxei32.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.nt.indexed.mask.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float>* undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.indexed.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vstore.nt.indexed_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.nt.indexed.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double>* undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.indexed.mask.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vstore.nt.indexed_mask_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_mask_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu, nt
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.nt.indexed.mask.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double>* undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.indexed.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>*,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vstore.nt.indexed_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.nt.indexed.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double>* undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.indexed.mask.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>*,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vstore.nt.indexed_mask_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_mask_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu, nt
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.nt.indexed.mask.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double>* undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.indexed.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>*,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vstore.nt.indexed_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.nt.indexed.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double>* undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.indexed.mask.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>*,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vstore.nt.indexed_mask_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_mask_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu, nt
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.nt.indexed.mask.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double>* undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  ret void
}


declare void @llvm.epi.vstore.nt.indexed.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>*,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vstore.nt.indexed_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}
  call void @llvm.epi.vstore.nt.indexed.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double>* undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  ret void
}

declare void @llvm.epi.vstore.nt.indexed.mask.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>*,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vstore.nt.indexed_mask_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vstore.nt.indexed_mask_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu, nt
; CHECK:       vsuxei64.v {{v[0-9]+}}, (a0), {{v[0-9]+}}, v0.t
  call void @llvm.epi.vstore.nt.indexed.mask.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double>* undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfsqrt.nxv2f32(
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfsqrt_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsqrt_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfsqrt.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x float> @llvm.epi.vfsqrt.nxv2f32(
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfsqrt.mask.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsqrt_mask_v_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsqrt_mask_v_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfsqrt.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfsqrt.mask.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfsqrt.nxv4f32(
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfsqrt_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsqrt_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfsqrt.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vfsqrt.nxv4f32(
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfsqrt.mask.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsqrt_mask_v_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsqrt_mask_v_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfsqrt.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfsqrt.mask.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfsqrt.nxv8f32(
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfsqrt_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsqrt_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfsqrt.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vfsqrt.nxv8f32(
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfsqrt.mask.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsqrt_mask_v_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsqrt_mask_v_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfsqrt.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfsqrt.mask.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfsqrt.nxv16f32(
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfsqrt_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsqrt_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfsqrt.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x float> @llvm.epi.vfsqrt.nxv16f32(
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfsqrt.mask.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfsqrt_mask_v_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsqrt_mask_v_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfsqrt.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfsqrt.mask.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsqrt.nxv1f64(
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfsqrt_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsqrt_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfsqrt.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x double> @llvm.epi.vfsqrt.nxv1f64(
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsqrt.mask.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfsqrt_mask_v_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsqrt_mask_v_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfsqrt.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfsqrt.mask.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfsqrt.nxv2f64(
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfsqrt_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsqrt_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfsqrt.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfsqrt.nxv2f64(
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfsqrt.mask.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsqrt_mask_v_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsqrt_mask_v_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfsqrt.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfsqrt.mask.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfsqrt.nxv4f64(
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfsqrt_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsqrt_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfsqrt.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfsqrt.nxv4f64(
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfsqrt.mask.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsqrt_mask_v_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsqrt_mask_v_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfsqrt.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfsqrt.mask.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfsqrt.nxv8f64(
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfsqrt_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsqrt_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfsqrt.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfsqrt.nxv8f64(
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfsqrt.mask.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsqrt_mask_v_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsqrt_mask_v_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfsqrt.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfsqrt.mask.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmv.v.x.nxv8i8.i8(
  i8,
  i64);

define void @intrinsic_vmv.v.x_x_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.v.x_x_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmv.v.x {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vmv.v.x.nxv8i8.i8(
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmv.v.x.nxv16i8.i8(
  i8,
  i64);

define void @intrinsic_vmv.v.x_x_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.v.x_x_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmv.v.x {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vmv.v.x.nxv16i8.i8(
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmv.v.x.nxv32i8.i8(
  i8,
  i64);

define void @intrinsic_vmv.v.x_x_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.v.x_x_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmv.v.x {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vmv.v.x.nxv32i8.i8(
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmv.v.x.nxv4i16.i16(
  i16,
  i64);

define void @intrinsic_vmv.v.x_x_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.v.x_x_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmv.v.x {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vmv.v.x.nxv4i16.i16(
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmv.v.x.nxv8i16.i16(
  i16,
  i64);

define void @intrinsic_vmv.v.x_x_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.v.x_x_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmv.v.x {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vmv.v.x.nxv8i16.i16(
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmv.v.x.nxv16i16.i16(
  i16,
  i64);

define void @intrinsic_vmv.v.x_x_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.v.x_x_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmv.v.x {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vmv.v.x.nxv16i16.i16(
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmv.v.x.nxv32i16.i16(
  i16,
  i64);

define void @intrinsic_vmv.v.x_x_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.v.x_x_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmv.v.x {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vmv.v.x.nxv32i16.i16(
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmv.v.x.nxv2i32.i32(
  i32,
  i64);

define void @intrinsic_vmv.v.x_x_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.v.x_x_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmv.v.x {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vmv.v.x.nxv2i32.i32(
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmv.v.x.nxv4i32.i32(
  i32,
  i64);

define void @intrinsic_vmv.v.x_x_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.v.x_x_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmv.v.x {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vmv.v.x.nxv4i32.i32(
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmv.v.x.nxv8i32.i32(
  i32,
  i64);

define void @intrinsic_vmv.v.x_x_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.v.x_x_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmv.v.x {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vmv.v.x.nxv8i32.i32(
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmv.v.x.nxv16i32.i32(
  i32,
  i64);

define void @intrinsic_vmv.v.x_x_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.v.x_x_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmv.v.x {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vmv.v.x.nxv16i32.i32(
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmv.v.x.nxv1i64.i64(
  i64,
  i64);

define void @intrinsic_vmv.v.x_x_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.v.x_x_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmv.v.x {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vmv.v.x.nxv1i64.i64(
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmv.v.x.nxv2i64.i64(
  i64,
  i64);

define void @intrinsic_vmv.v.x_x_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.v.x_x_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmv.v.x {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vmv.v.x.nxv2i64.i64(
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmv.v.x.nxv4i64.i64(
  i64,
  i64);

define void @intrinsic_vmv.v.x_x_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.v.x_x_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmv.v.x {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vmv.v.x.nxv4i64.i64(
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmv.v.x.nxv8i64.i64(
  i64,
  i64);

define void @intrinsic_vmv.v.x_x_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.v.x_x_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmv.v.x {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vmv.v.x.nxv8i64.i64(
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmv.v.f.nxv2f32.f32(
  float,
  i64);

define void @intrinsic_vfmv.v.f_f_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.v.f_f_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmv.v.f {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x float> @llvm.epi.vfmv.v.f.nxv2f32.f32(
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmv.v.f.nxv4f32.f32(
  float,
  i64);

define void @intrinsic_vfmv.v.f_f_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.v.f_f_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmv.v.f {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x float> @llvm.epi.vfmv.v.f.nxv4f32.f32(
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmv.v.f.nxv8f32.f32(
  float,
  i64);

define void @intrinsic_vfmv.v.f_f_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.v.f_f_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmv.v.f {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x float> @llvm.epi.vfmv.v.f.nxv8f32.f32(
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmv.v.f.nxv16f32.f32(
  float,
  i64);

define void @intrinsic_vfmv.v.f_f_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.v.f_f_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmv.v.f {{v[0-9]+}}, ft0
  %a = call <vscale x 16 x float> @llvm.epi.vfmv.v.f.nxv16f32.f32(
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmv.v.f.nxv1f64.f64(
  double,
  i64);

define void @intrinsic_vfmv.v.f_f_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.v.f_f_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmv.v.f {{v[0-9]+}}, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vfmv.v.f.nxv1f64.f64(
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmv.v.f.nxv2f64.f64(
  double,
  i64);

define void @intrinsic_vfmv.v.f_f_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.v.f_f_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmv.v.f {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfmv.v.f.nxv2f64.f64(
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmv.v.f.nxv4f64.f64(
  double,
  i64);

define void @intrinsic_vfmv.v.f_f_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.v.f_f_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmv.v.f {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfmv.v.f.nxv4f64.f64(
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmv.v.f.nxv8f64.f64(
  double,
  i64);

define void @intrinsic_vfmv.v.f_f_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.v.f_f_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmv.v.f {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfmv.v.f.nxv8f64.f64(
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare i8 @llvm.epi.vmv.x.s.i8.nxv8i8(
  <vscale x 8 x i8>);

define void @intrinsic_vmv.x.s_s_i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.x.s_s_i8_nxv8i8
; CHECK:       vsetvli zero, zero, e8, m1, ta, mu
; CHECK:       vmv.x.s a0, {{v[0-9]+}}
  %a = call i8 @llvm.epi.vmv.x.s.i8.nxv8i8(
    <vscale x 8 x i8> undef)

  %p = bitcast i8* @scratch to i8*
  store i8 %a, i8* %p

  ret void
}


declare i8 @llvm.epi.vmv.x.s.i8.nxv16i8(
  <vscale x 16 x i8>);

define void @intrinsic_vmv.x.s_s_i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.x.s_s_i8_nxv16i8
; CHECK:       vsetvli zero, zero, e8, m2, ta, mu
; CHECK:       vmv.x.s a0, {{v[0-9]+}}
  %a = call i8 @llvm.epi.vmv.x.s.i8.nxv16i8(
    <vscale x 16 x i8> undef)

  %p = bitcast i8* @scratch to i8*
  store i8 %a, i8* %p

  ret void
}


declare i8 @llvm.epi.vmv.x.s.i8.nxv32i8(
  <vscale x 32 x i8>);

define void @intrinsic_vmv.x.s_s_i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.x.s_s_i8_nxv32i8
; CHECK:       vsetvli zero, zero, e8, m4, ta, mu
; CHECK:       vmv.x.s a0, {{v[0-9]+}}
  %a = call i8 @llvm.epi.vmv.x.s.i8.nxv32i8(
    <vscale x 32 x i8> undef)

  %p = bitcast i8* @scratch to i8*
  store i8 %a, i8* %p

  ret void
}


declare i16 @llvm.epi.vmv.x.s.i16.nxv4i16(
  <vscale x 4 x i16>);

define void @intrinsic_vmv.x.s_s_i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.x.s_s_i16_nxv4i16
; CHECK:       vsetvli zero, zero, e16, m1, ta, mu
; CHECK:       vmv.x.s a0, {{v[0-9]+}}
  %a = call i16 @llvm.epi.vmv.x.s.i16.nxv4i16(
    <vscale x 4 x i16> undef)

  %p = bitcast i8* @scratch to i16*
  store i16 %a, i16* %p

  ret void
}


declare i16 @llvm.epi.vmv.x.s.i16.nxv8i16(
  <vscale x 8 x i16>);

define void @intrinsic_vmv.x.s_s_i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.x.s_s_i16_nxv8i16
; CHECK:       vsetvli zero, zero, e16, m2, ta, mu
; CHECK:       vmv.x.s a0, {{v[0-9]+}}
  %a = call i16 @llvm.epi.vmv.x.s.i16.nxv8i16(
    <vscale x 8 x i16> undef)

  %p = bitcast i8* @scratch to i16*
  store i16 %a, i16* %p

  ret void
}


declare i16 @llvm.epi.vmv.x.s.i16.nxv16i16(
  <vscale x 16 x i16>);

define void @intrinsic_vmv.x.s_s_i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.x.s_s_i16_nxv16i16
; CHECK:       vsetvli zero, zero, e16, m4, ta, mu
; CHECK:       vmv.x.s a0, {{v[0-9]+}}
  %a = call i16 @llvm.epi.vmv.x.s.i16.nxv16i16(
    <vscale x 16 x i16> undef)

  %p = bitcast i8* @scratch to i16*
  store i16 %a, i16* %p

  ret void
}


declare i16 @llvm.epi.vmv.x.s.i16.nxv32i16(
  <vscale x 32 x i16>);

define void @intrinsic_vmv.x.s_s_i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.x.s_s_i16_nxv32i16
; CHECK:       vsetvli zero, zero, e16, m8, ta, mu
; CHECK:       vmv.x.s a0, {{v[0-9]+}}
  %a = call i16 @llvm.epi.vmv.x.s.i16.nxv32i16(
    <vscale x 32 x i16> undef)

  %p = bitcast i8* @scratch to i16*
  store i16 %a, i16* %p

  ret void
}


declare i32 @llvm.epi.vmv.x.s.i32.nxv2i32(
  <vscale x 2 x i32>);

define void @intrinsic_vmv.x.s_s_i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.x.s_s_i32_nxv2i32
; CHECK:       vsetvli zero, zero, e32, m1, ta, mu
; CHECK:       vmv.x.s a0, {{v[0-9]+}}
  %a = call i32 @llvm.epi.vmv.x.s.i32.nxv2i32(
    <vscale x 2 x i32> undef)

  %p = bitcast i8* @scratch to i32*
  store i32 %a, i32* %p

  ret void
}


declare i32 @llvm.epi.vmv.x.s.i32.nxv4i32(
  <vscale x 4 x i32>);

define void @intrinsic_vmv.x.s_s_i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.x.s_s_i32_nxv4i32
; CHECK:       vsetvli zero, zero, e32, m2, ta, mu
; CHECK:       vmv.x.s a0, {{v[0-9]+}}
  %a = call i32 @llvm.epi.vmv.x.s.i32.nxv4i32(
    <vscale x 4 x i32> undef)

  %p = bitcast i8* @scratch to i32*
  store i32 %a, i32* %p

  ret void
}


declare i32 @llvm.epi.vmv.x.s.i32.nxv8i32(
  <vscale x 8 x i32>);

define void @intrinsic_vmv.x.s_s_i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.x.s_s_i32_nxv8i32
; CHECK:       vsetvli zero, zero, e32, m4, ta, mu
; CHECK:       vmv.x.s a0, {{v[0-9]+}}
  %a = call i32 @llvm.epi.vmv.x.s.i32.nxv8i32(
    <vscale x 8 x i32> undef)

  %p = bitcast i8* @scratch to i32*
  store i32 %a, i32* %p

  ret void
}


declare i32 @llvm.epi.vmv.x.s.i32.nxv16i32(
  <vscale x 16 x i32>);

define void @intrinsic_vmv.x.s_s_i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.x.s_s_i32_nxv16i32
; CHECK:       vsetvli zero, zero, e32, m8, ta, mu
; CHECK:       vmv.x.s a0, {{v[0-9]+}}
  %a = call i32 @llvm.epi.vmv.x.s.i32.nxv16i32(
    <vscale x 16 x i32> undef)

  %p = bitcast i8* @scratch to i32*
  store i32 %a, i32* %p

  ret void
}


declare i64 @llvm.epi.vmv.x.s.i64.nxv1i64(
  <vscale x 1 x i64>);

define void @intrinsic_vmv.x.s_s_i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.x.s_s_i64_nxv1i64
; CHECK:       vsetvli zero, zero, e64, m1, ta, mu
; CHECK:       vmv.x.s a0, {{v[0-9]+}}
  %a = call i64 @llvm.epi.vmv.x.s.i64.nxv1i64(
    <vscale x 1 x i64> undef)

  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p

  ret void
}


declare i64 @llvm.epi.vmv.x.s.i64.nxv2i64(
  <vscale x 2 x i64>);

define void @intrinsic_vmv.x.s_s_i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.x.s_s_i64_nxv2i64
; CHECK:       vsetvli zero, zero, e64, m2, ta, mu
; CHECK:       vmv.x.s a0, {{v[0-9]+}}
  %a = call i64 @llvm.epi.vmv.x.s.i64.nxv2i64(
    <vscale x 2 x i64> undef)

  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p

  ret void
}


declare i64 @llvm.epi.vmv.x.s.i64.nxv4i64(
  <vscale x 4 x i64>);

define void @intrinsic_vmv.x.s_s_i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.x.s_s_i64_nxv4i64
; CHECK:       vsetvli zero, zero, e64, m4, ta, mu
; CHECK:       vmv.x.s a0, {{v[0-9]+}}
  %a = call i64 @llvm.epi.vmv.x.s.i64.nxv4i64(
    <vscale x 4 x i64> undef)

  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p

  ret void
}


declare i64 @llvm.epi.vmv.x.s.i64.nxv8i64(
  <vscale x 8 x i64>);

define void @intrinsic_vmv.x.s_s_i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.x.s_s_i64_nxv8i64
; CHECK:       vsetvli zero, zero, e64, m8, ta, mu
; CHECK:       vmv.x.s a0, {{v[0-9]+}}
  %a = call i64 @llvm.epi.vmv.x.s.i64.nxv8i64(
    <vscale x 8 x i64> undef)

  %p = bitcast i8* @scratch to i64*
  store i64 %a, i64* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmv.s.x.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmv.s.x_x_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.s.x_x_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmv.s.x {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vmv.s.x.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmv.s.x.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmv.s.x_x_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.s.x_x_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmv.s.x {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vmv.s.x.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmv.s.x.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmv.s.x_x_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.s.x_x_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmv.s.x {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vmv.s.x.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmv.s.x.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmv.s.x_x_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.s.x_x_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmv.s.x {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vmv.s.x.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmv.s.x.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmv.s.x_x_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.s.x_x_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmv.s.x {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vmv.s.x.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmv.s.x.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmv.s.x_x_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.s.x_x_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmv.s.x {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vmv.s.x.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmv.s.x.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmv.s.x_x_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.s.x_x_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmv.s.x {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vmv.s.x.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmv.s.x.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmv.s.x_x_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.s.x_x_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmv.s.x {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vmv.s.x.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmv.s.x.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmv.s.x_x_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.s.x_x_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmv.s.x {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vmv.s.x.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmv.s.x.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmv.s.x_x_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.s.x_x_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmv.s.x {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vmv.s.x.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmv.s.x.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmv.s.x_x_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.s.x_x_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmv.s.x {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vmv.s.x.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmv.s.x.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmv.s.x_x_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.s.x_x_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmv.s.x {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vmv.s.x.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmv.s.x.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmv.s.x_x_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.s.x_x_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmv.s.x {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vmv.s.x.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmv.s.x.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmv.s.x_x_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.s.x_x_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmv.s.x {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vmv.s.x.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmv.s.x.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmv.s.x_x_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmv.s.x_x_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmv.s.x {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vmv.s.x.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmv.s.f.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfmv.s.f_f_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.s.f_f_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmv.s.f {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x float> @llvm.epi.vfmv.s.f.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmv.s.f.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfmv.s.f_f_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.s.f_f_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmv.s.f {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x float> @llvm.epi.vfmv.s.f.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmv.s.f.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfmv.s.f_f_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.s.f_f_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmv.s.f {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x float> @llvm.epi.vfmv.s.f.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmv.s.f.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfmv.s.f_f_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.s.f_f_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmv.s.f {{v[0-9]+}}, ft0
  %a = call <vscale x 16 x float> @llvm.epi.vfmv.s.f.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmv.s.f.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfmv.s.f_f_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.s.f_f_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmv.s.f {{v[0-9]+}}, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vfmv.s.f.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmv.s.f.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfmv.s.f_f_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.s.f_f_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmv.s.f {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfmv.s.f.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmv.s.f.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfmv.s.f_f_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.s.f_f_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmv.s.f {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfmv.s.f.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmv.s.f.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfmv.s.f_f_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.s.f_f_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmv.s.f {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfmv.s.f.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.viota.nxv8i8(
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_viota_m_nxv8i8_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_m_nxv8i8_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.viota.nxv8i8(
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.viota.mask.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_viota_mask_m_nxv8i8_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_mask_m_nxv8i8_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.viota.mask.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.viota.nxv16i8(
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_viota_m_nxv16i8_nxv16i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_m_nxv16i8_nxv16i1
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.viota.nxv16i8(
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.viota.mask.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_viota_mask_m_nxv16i8_nxv16i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_mask_m_nxv16i8_nxv16i1
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.viota.mask.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.viota.nxv32i8(
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_viota_m_nxv32i8_nxv32i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_m_nxv32i8_nxv32i1
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.viota.nxv32i8(
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.viota.mask.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_viota_mask_m_nxv32i8_nxv32i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_mask_m_nxv32i8_nxv32i1
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.viota.mask.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.viota.nxv4i16(
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_viota_m_nxv4i16_nxv4i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_m_nxv4i16_nxv4i1
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.viota.nxv4i16(
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.viota.mask.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_viota_mask_m_nxv4i16_nxv4i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_mask_m_nxv4i16_nxv4i1
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.viota.mask.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.viota.nxv8i16(
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_viota_m_nxv8i16_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_m_nxv8i16_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.viota.nxv8i16(
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.viota.mask.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_viota_mask_m_nxv8i16_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_mask_m_nxv8i16_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.viota.mask.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.viota.nxv16i16(
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_viota_m_nxv16i16_nxv16i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_m_nxv16i16_nxv16i1
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.viota.nxv16i16(
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.viota.mask.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_viota_mask_m_nxv16i16_nxv16i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_mask_m_nxv16i16_nxv16i1
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.viota.mask.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.viota.nxv32i16(
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_viota_m_nxv32i16_nxv32i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_m_nxv32i16_nxv32i1
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.viota.nxv32i16(
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.viota.mask.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_viota_mask_m_nxv32i16_nxv32i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_mask_m_nxv32i16_nxv32i1
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.viota.mask.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.viota.nxv2i32(
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_viota_m_nxv2i32_nxv2i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_m_nxv2i32_nxv2i1
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.viota.nxv2i32(
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.viota.mask.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_viota_mask_m_nxv2i32_nxv2i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_mask_m_nxv2i32_nxv2i1
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.viota.mask.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.viota.nxv4i32(
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_viota_m_nxv4i32_nxv4i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_m_nxv4i32_nxv4i1
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.viota.nxv4i32(
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.viota.mask.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_viota_mask_m_nxv4i32_nxv4i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_mask_m_nxv4i32_nxv4i1
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.viota.mask.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.viota.nxv8i32(
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_viota_m_nxv8i32_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_m_nxv8i32_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.viota.nxv8i32(
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.viota.mask.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_viota_mask_m_nxv8i32_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_mask_m_nxv8i32_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.viota.mask.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.viota.nxv16i32(
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_viota_m_nxv16i32_nxv16i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_m_nxv16i32_nxv16i1
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.viota.nxv16i32(
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.viota.mask.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_viota_mask_m_nxv16i32_nxv16i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_mask_m_nxv16i32_nxv16i1
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.viota.mask.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.viota.nxv1i64(
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_viota_m_nxv1i64_nxv1i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_m_nxv1i64_nxv1i1
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.viota.nxv1i64(
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.viota.mask.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_viota_mask_m_nxv1i64_nxv1i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_mask_m_nxv1i64_nxv1i1
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.viota.mask.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.viota.nxv2i64(
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_viota_m_nxv2i64_nxv2i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_m_nxv2i64_nxv2i1
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.viota.nxv2i64(
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.viota.mask.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_viota_mask_m_nxv2i64_nxv2i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_mask_m_nxv2i64_nxv2i1
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.viota.mask.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.viota.nxv4i64(
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_viota_m_nxv4i64_nxv4i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_m_nxv4i64_nxv4i1
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.viota.nxv4i64(
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.viota.mask.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_viota_mask_m_nxv4i64_nxv4i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_mask_m_nxv4i64_nxv4i1
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.viota.mask.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.viota.nxv8i64(
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_viota_m_nxv8i64_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_m_nxv8i64_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.viota.nxv8i64(
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.viota.mask.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_viota_mask_m_nxv8i64_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_viota_mask_m_nxv8i64_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       viota.m {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.viota.mask.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare float @llvm.epi.vfmv.f.s.f32.nxv2f32(
  <vscale x 2 x float>);

define void @intrinsic_vfmv.f.s_s_f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.f.s_s_f32_nxv2f32
; CHECK:       vsetvli zero, zero, e32, m1, ta, mu
; CHECK:       vfmv.f.s ft0, {{v[0-9]+}}
  %a = call float @llvm.epi.vfmv.f.s.f32.nxv2f32(
    <vscale x 2 x float> undef)

  %p = bitcast i8* @scratch to float*
  store float %a, float* %p

  ret void
}


declare float @llvm.epi.vfmv.f.s.f32.nxv4f32(
  <vscale x 4 x float>);

define void @intrinsic_vfmv.f.s_s_f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.f.s_s_f32_nxv4f32
; CHECK:       vsetvli zero, zero, e32, m2, ta, mu
; CHECK:       vfmv.f.s ft0, {{v[0-9]+}}
  %a = call float @llvm.epi.vfmv.f.s.f32.nxv4f32(
    <vscale x 4 x float> undef)

  %p = bitcast i8* @scratch to float*
  store float %a, float* %p

  ret void
}


declare float @llvm.epi.vfmv.f.s.f32.nxv8f32(
  <vscale x 8 x float>);

define void @intrinsic_vfmv.f.s_s_f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.f.s_s_f32_nxv8f32
; CHECK:       vsetvli zero, zero, e32, m4, ta, mu
; CHECK:       vfmv.f.s ft0, {{v[0-9]+}}
  %a = call float @llvm.epi.vfmv.f.s.f32.nxv8f32(
    <vscale x 8 x float> undef)

  %p = bitcast i8* @scratch to float*
  store float %a, float* %p

  ret void
}


declare float @llvm.epi.vfmv.f.s.f32.nxv16f32(
  <vscale x 16 x float>);

define void @intrinsic_vfmv.f.s_s_f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.f.s_s_f32_nxv16f32
; CHECK:       vsetvli zero, zero, e32, m8, ta, mu
; CHECK:       vfmv.f.s ft0, {{v[0-9]+}}
  %a = call float @llvm.epi.vfmv.f.s.f32.nxv16f32(
    <vscale x 16 x float> undef)

  %p = bitcast i8* @scratch to float*
  store float %a, float* %p

  ret void
}


declare double @llvm.epi.vfmv.f.s.f64.nxv1f64(
  <vscale x 1 x double>);

define void @intrinsic_vfmv.f.s_s_f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.f.s_s_f64_nxv1f64
; CHECK:       vsetvli zero, zero, e64, m1, ta, mu
; CHECK:       vfmv.f.s ft0, {{v[0-9]+}}
  %a = call double @llvm.epi.vfmv.f.s.f64.nxv1f64(
    <vscale x 1 x double> undef)

  %p = bitcast i8* @scratch to double*
  store double %a, double* %p

  ret void
}


declare double @llvm.epi.vfmv.f.s.f64.nxv2f64(
  <vscale x 2 x double>);

define void @intrinsic_vfmv.f.s_s_f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.f.s_s_f64_nxv2f64
; CHECK:       vsetvli zero, zero, e64, m2, ta, mu
; CHECK:       vfmv.f.s ft0, {{v[0-9]+}}
  %a = call double @llvm.epi.vfmv.f.s.f64.nxv2f64(
    <vscale x 2 x double> undef)

  %p = bitcast i8* @scratch to double*
  store double %a, double* %p

  ret void
}


declare double @llvm.epi.vfmv.f.s.f64.nxv4f64(
  <vscale x 4 x double>);

define void @intrinsic_vfmv.f.s_s_f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.f.s_s_f64_nxv4f64
; CHECK:       vsetvli zero, zero, e64, m4, ta, mu
; CHECK:       vfmv.f.s ft0, {{v[0-9]+}}
  %a = call double @llvm.epi.vfmv.f.s.f64.nxv4f64(
    <vscale x 4 x double> undef)

  %p = bitcast i8* @scratch to double*
  store double %a, double* %p

  ret void
}


declare double @llvm.epi.vfmv.f.s.f64.nxv8f64(
  <vscale x 8 x double>);

define void @intrinsic_vfmv.f.s_s_f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmv.f.s_s_f64_nxv8f64
; CHECK:       vsetvli zero, zero, e64, m8, ta, mu
; CHECK:       vfmv.f.s ft0, {{v[0-9]+}}
  %a = call double @llvm.epi.vfmv.f.s.f64.nxv8f64(
    <vscale x 8 x double> undef)

  %p = bitcast i8* @scratch to double*
  store double %a, double* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vid.nxv8i8(
  i64);

define void @intrinsic_vid_v_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_v_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vid.v {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vid.nxv8i8(
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vid.mask.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vid_mask_v_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_mask_v_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vid.v {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vid.mask.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vid.nxv16i8(
  i64);

define void @intrinsic_vid_v_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_v_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vid.v {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vid.nxv16i8(
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vid.mask.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vid_mask_v_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_mask_v_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vid.v {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vid.mask.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vid.nxv32i8(
  i64);

define void @intrinsic_vid_v_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_v_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vid.v {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vid.nxv32i8(
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vid.mask.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vid_mask_v_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_mask_v_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vid.v {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vid.mask.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vid.nxv4i16(
  i64);

define void @intrinsic_vid_v_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_v_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vid.v {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vid.nxv4i16(
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vid.mask.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vid_mask_v_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_mask_v_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vid.v {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vid.mask.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vid.nxv8i16(
  i64);

define void @intrinsic_vid_v_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_v_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vid.v {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vid.nxv8i16(
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vid.mask.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vid_mask_v_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_mask_v_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vid.v {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vid.mask.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vid.nxv16i16(
  i64);

define void @intrinsic_vid_v_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_v_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vid.v {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vid.nxv16i16(
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vid.mask.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vid_mask_v_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_mask_v_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vid.v {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vid.mask.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vid.nxv32i16(
  i64);

define void @intrinsic_vid_v_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_v_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vid.v {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vid.nxv32i16(
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vid.mask.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vid_mask_v_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_mask_v_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vid.v {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vid.mask.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vid.nxv2i32(
  i64);

define void @intrinsic_vid_v_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_v_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vid.v {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vid.nxv2i32(
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vid.mask.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vid_mask_v_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_mask_v_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vid.v {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vid.mask.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vid.nxv4i32(
  i64);

define void @intrinsic_vid_v_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_v_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vid.v {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vid.nxv4i32(
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vid.mask.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vid_mask_v_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_mask_v_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vid.v {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vid.mask.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vid.nxv8i32(
  i64);

define void @intrinsic_vid_v_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_v_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vid.v {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vid.nxv8i32(
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vid.mask.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vid_mask_v_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_mask_v_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vid.v {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vid.mask.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vid.nxv16i32(
  i64);

define void @intrinsic_vid_v_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_v_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vid.v {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vid.nxv16i32(
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vid.mask.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vid_mask_v_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_mask_v_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vid.v {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vid.mask.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vid.nxv1i64(
  i64);

define void @intrinsic_vid_v_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_v_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vid.v {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vid.nxv1i64(
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vid.mask.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vid_mask_v_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_mask_v_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vid.v {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vid.mask.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vid.nxv2i64(
  i64);

define void @intrinsic_vid_v_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_v_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vid.v {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vid.nxv2i64(
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vid.mask.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vid_mask_v_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_mask_v_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vid.v {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vid.mask.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vid.nxv4i64(
  i64);

define void @intrinsic_vid_v_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_v_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vid.v {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vid.nxv4i64(
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vid.mask.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vid_mask_v_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_mask_v_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vid.v {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vid.mask.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vid.nxv8i64(
  i64);

define void @intrinsic_vid_v_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_v_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vid.v {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vid.nxv8i64(
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vid.mask.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vid_mask_v_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vid_mask_v_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vid.v {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vid.mask.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vadd.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vadd_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vadd.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vadd.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vadd.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vadd.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vadd_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vadd.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vadd.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vadd.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vadd.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vadd_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vadd.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vadd.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vadd.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vadd.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vadd_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vadd.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vadd.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vadd.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vadd.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vadd_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vadd.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vadd.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vadd.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vadd.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vadd_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vadd.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vadd.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vadd.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vadd.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vadd_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vadd.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vadd.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vadd.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vadd.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vadd_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vadd.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vadd.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vadd.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vadd.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vadd_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vadd.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vadd.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vadd.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vadd.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vadd_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vadd.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vadd.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vadd.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vadd.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vadd_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vadd.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vadd.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vadd.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vadd.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vadd_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vadd.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vadd.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vadd.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vadd.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vadd_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vadd.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vadd.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vadd.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vadd.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vadd_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vadd.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vadd.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vadd.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vadd.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vadd_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vadd.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vadd.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadd_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vadd.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vadd.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vadd_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vadd.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vadd.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vadd.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vadd.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vadd_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vadd.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vadd.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vadd.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vadd.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vadd_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vadd.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vadd.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vadd.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vadd.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vadd_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vadd.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vadd.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vadd.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vadd.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vadd_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vadd.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vadd.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vadd.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vadd.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vadd_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vadd.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vadd.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vadd.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vadd.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vadd_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vadd.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vadd.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vadd.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vadd.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vadd_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vadd.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vadd.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vadd.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vadd.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vadd_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vadd.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vadd.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vadd.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vadd.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vadd_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vadd.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vadd.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vadd.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vadd.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vadd_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vadd.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vadd.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vadd.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vadd.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vadd_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vadd.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vadd.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vadd.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vadd.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vadd_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vadd.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vadd.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vadd.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vadd.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vadd_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vadd.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vadd.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vadd.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vadd.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vadd_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vadd.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vadd.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadd_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vadd.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vadd.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vadd.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vadd.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vadd.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vadd.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vadd.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vadd.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vadd.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vadd.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vadd.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vadd.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vadd.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vadd.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vadd.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vadd.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vadd.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vadd.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vadd.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vadd.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vadd.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vadd.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vadd.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vadd.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vadd.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vadd.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vadd.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vadd.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vadd.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vadd_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vadd.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vadd_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadd_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vadd.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsub.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vsub_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vsub.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsub.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsub.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsub.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vsub_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vsub.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsub.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsub.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsub.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vsub_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vsub.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsub.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsub.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsub.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vsub_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vsub.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsub.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsub.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsub.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vsub_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vsub.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsub.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsub.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsub.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vsub_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vsub.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsub.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsub.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsub.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vsub_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vsub.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsub.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsub.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsub.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vsub_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vsub.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsub.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsub.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsub.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vsub_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vsub.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsub.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsub.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsub.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vsub_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vsub.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsub.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsub.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsub.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vsub_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vsub.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsub.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsub.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsub.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vsub_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vsub.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsub.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsub.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsub.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vsub_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vsub.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsub.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsub.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsub.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vsub_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vsub.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsub.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsub.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsub.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vsub_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vsub.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsub.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsub_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsub.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsub.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vsub_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vsub.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsub.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsub.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsub.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vsub_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vsub.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsub.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsub.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsub.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vsub_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vsub.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsub.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsub.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsub.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vsub_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vsub.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsub.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsub.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsub.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vsub_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vsub.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsub.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsub.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsub.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vsub_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vsub.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsub.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsub.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsub.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vsub_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vsub.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsub.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsub.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsub.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vsub_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vsub.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsub.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsub.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsub.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vsub_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vsub.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsub.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsub.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsub.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vsub_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vsub.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsub.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsub.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsub.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vsub_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vsub.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsub.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsub.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsub.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vsub_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vsub.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsub.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsub.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsub.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vsub_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vsub.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsub.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsub.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsub.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vsub_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vsub.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsub.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsub.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsub.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vsub_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vsub.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsub.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsub_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsub_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsub.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vrsub.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vrsub_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vrsub.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vrsub.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vrsub.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vrsub.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vrsub_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vrsub.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vrsub.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vrsub.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vrsub.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vrsub_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vrsub.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vrsub.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vrsub.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vrsub.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vrsub_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vrsub.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vrsub.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vrsub.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vrsub.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vrsub_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vrsub.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vrsub.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vrsub.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vrsub.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vrsub_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vrsub.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vrsub.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vrsub.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vrsub.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vrsub_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vrsub.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vrsub.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vrsub.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vrsub.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vrsub_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vrsub.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vrsub.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vrsub.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vrsub.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vrsub_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vrsub.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vrsub.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vrsub.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vrsub.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vrsub_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vrsub.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vrsub.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vrsub.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vrsub.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vrsub_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vrsub.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vrsub.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vrsub.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vrsub.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vrsub_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vrsub.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vrsub.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vrsub.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vrsub.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vrsub_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vrsub.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vrsub.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vrsub.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vrsub.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vrsub_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vrsub.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vrsub.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vrsub.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vrsub.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vrsub_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vrsub.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vrsub.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrsub_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vrsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vrsub.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vrsub.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vrsub.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vrsub.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vrsub.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vrsub.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vrsub.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vrsub.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vrsub.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vrsub.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vrsub.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vrsub.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vrsub.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vrsub.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vrsub.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vrsub.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vrsub.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vrsub.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vrsub.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vrsub.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vrsub.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vrsub.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vrsub.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vrsub.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vrsub.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vrsub.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vrsub.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vrsub.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vrsub.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vrsub_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vrsub.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vrsub_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrsub_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vrsub.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vrsub.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwaddu.nxv8i16.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vwaddu_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vwaddu.nxv8i16.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwaddu.mask.nxv8i16.nxv8i8.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwaddu.mask.nxv8i16.nxv8i8.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwaddu.nxv16i16.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vwaddu_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vwaddu.nxv16i16.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwaddu.mask.nxv16i16.nxv16i8.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwaddu.mask.nxv16i16.nxv16i8.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwaddu.nxv32i16.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vwaddu_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vwaddu.nxv32i16.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwaddu.mask.nxv32i16.nxv32i8.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwaddu.mask.nxv32i16.nxv32i8.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwaddu.nxv4i32.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vwaddu_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vwaddu.nxv4i32.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwaddu.mask.nxv4i32.nxv4i16.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwaddu.mask.nxv4i32.nxv4i16.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwaddu.nxv8i32.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vwaddu_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vwaddu.nxv8i32.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwaddu.mask.nxv8i32.nxv8i16.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwaddu.mask.nxv8i32.nxv8i16.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwaddu.nxv16i32.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vwaddu_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vwaddu.nxv16i32.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwaddu.mask.nxv16i32.nxv16i16.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwaddu.mask.nxv16i32.nxv16i16.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwaddu.nxv2i64.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vwaddu_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vwaddu.nxv2i64.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwaddu.mask.nxv2i64.nxv2i32.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwaddu.mask.nxv2i64.nxv2i32.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwaddu.nxv4i64.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vwaddu_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vwaddu.nxv4i64.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwaddu.mask.nxv4i64.nxv4i32.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwaddu.mask.nxv4i64.nxv4i32.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwaddu.nxv8i64.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vwaddu_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vwaddu.nxv8i64.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwaddu.mask.nxv8i64.nxv8i32.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwaddu.mask.nxv8i64.nxv8i32.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwaddu.nxv8i16.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vwaddu_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vwaddu.nxv8i16.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwaddu.mask.nxv8i16.nxv8i8.i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwaddu.mask.nxv8i16.nxv8i8.i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwaddu.nxv16i16.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vwaddu_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vwaddu.nxv16i16.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwaddu.mask.nxv16i16.nxv16i8.i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwaddu.mask.nxv16i16.nxv16i8.i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwaddu.nxv32i16.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vwaddu_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vwaddu.nxv32i16.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwaddu.mask.nxv32i16.nxv32i8.i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwaddu.mask.nxv32i16.nxv32i8.i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwaddu.nxv4i32.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vwaddu_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vwaddu.nxv4i32.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwaddu.mask.nxv4i32.nxv4i16.i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwaddu.mask.nxv4i32.nxv4i16.i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwaddu.nxv8i32.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vwaddu_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vwaddu.nxv8i32.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwaddu.mask.nxv8i32.nxv8i16.i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwaddu.mask.nxv8i32.nxv8i16.i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwaddu.nxv16i32.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vwaddu_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vwaddu.nxv16i32.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwaddu.mask.nxv16i32.nxv16i16.i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwaddu.mask.nxv16i32.nxv16i16.i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwaddu.nxv2i64.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vwaddu_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vwaddu.nxv2i64.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwaddu.mask.nxv2i64.nxv2i32.i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwaddu.mask.nxv2i64.nxv2i32.i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwaddu.nxv4i64.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vwaddu_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vwaddu.nxv4i64.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwaddu.mask.nxv4i64.nxv4i32.i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwaddu.mask.nxv4i64.nxv4i32.i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwaddu.nxv8i64.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vwaddu_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vwaddu.nxv8i64.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwaddu.mask.nxv8i64.nxv8i32.i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwaddu_mask_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu_mask_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwaddu.mask.nxv8i64.nxv8i32.i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwadd.nxv8i16.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vwadd_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vwadd.nxv8i16.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwadd.mask.nxv8i16.nxv8i8.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwadd.mask.nxv8i16.nxv8i8.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwadd.nxv16i16.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vwadd_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vwadd.nxv16i16.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwadd.mask.nxv16i16.nxv16i8.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwadd.mask.nxv16i16.nxv16i8.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwadd.nxv32i16.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vwadd_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vwadd.nxv32i16.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwadd.mask.nxv32i16.nxv32i8.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwadd.mask.nxv32i16.nxv32i8.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwadd.nxv4i32.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vwadd_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vwadd.nxv4i32.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwadd.mask.nxv4i32.nxv4i16.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwadd.mask.nxv4i32.nxv4i16.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwadd.nxv8i32.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vwadd_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vwadd.nxv8i32.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwadd.mask.nxv8i32.nxv8i16.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwadd.mask.nxv8i32.nxv8i16.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwadd.nxv16i32.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vwadd_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vwadd.nxv16i32.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwadd.mask.nxv16i32.nxv16i16.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwadd.mask.nxv16i32.nxv16i16.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwadd.nxv2i64.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vwadd_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vwadd.nxv2i64.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwadd.mask.nxv2i64.nxv2i32.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwadd.mask.nxv2i64.nxv2i32.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwadd.nxv4i64.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vwadd_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vwadd.nxv4i64.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwadd.mask.nxv4i64.nxv4i32.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwadd.mask.nxv4i64.nxv4i32.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwadd.nxv8i64.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vwadd_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vwadd.nxv8i64.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwadd.mask.nxv8i64.nxv8i32.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwadd.mask.nxv8i64.nxv8i32.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwadd.nxv8i16.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vwadd_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vwadd.nxv8i16.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwadd.mask.nxv8i16.nxv8i8.i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwadd.mask.nxv8i16.nxv8i8.i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwadd.nxv16i16.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vwadd_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vwadd.nxv16i16.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwadd.mask.nxv16i16.nxv16i8.i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwadd.mask.nxv16i16.nxv16i8.i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwadd.nxv32i16.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vwadd_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vwadd.nxv32i16.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwadd.mask.nxv32i16.nxv32i8.i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwadd.mask.nxv32i16.nxv32i8.i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwadd.nxv4i32.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vwadd_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vwadd.nxv4i32.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwadd.mask.nxv4i32.nxv4i16.i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwadd.mask.nxv4i32.nxv4i16.i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwadd.nxv8i32.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vwadd_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vwadd.nxv8i32.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwadd.mask.nxv8i32.nxv8i16.i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwadd.mask.nxv8i32.nxv8i16.i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwadd.nxv16i32.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vwadd_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vwadd.nxv16i32.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwadd.mask.nxv16i32.nxv16i16.i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwadd.mask.nxv16i32.nxv16i16.i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwadd.nxv2i64.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vwadd_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vwadd.nxv2i64.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwadd.mask.nxv2i64.nxv2i32.i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwadd.mask.nxv2i64.nxv2i32.i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwadd.nxv4i64.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vwadd_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vwadd.nxv4i64.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwadd.mask.nxv4i64.nxv4i32.i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwadd.mask.nxv4i64.nxv4i32.i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwadd.nxv8i64.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vwadd_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vwadd.nxv8i64.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwadd.mask.nxv8i64.nxv8i32.i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwadd_mask_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd_mask_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwadd.mask.nxv8i64.nxv8i32.i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwaddu.w.nxv8i16.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vwaddu.w_wv_nxv8i16_nxv8i16_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_wv_nxv8i16_nxv8i16_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwaddu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vwaddu.w.nxv8i16.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwaddu.w.mask.nxv8i16.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwaddu.w_mask_wv_nxv8i16_nxv8i16_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_mask_wv_nxv8i16_nxv8i16_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwaddu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwaddu.w.mask.nxv8i16.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwaddu.w.nxv16i16.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vwaddu.w_wv_nxv16i16_nxv16i16_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_wv_nxv16i16_nxv16i16_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwaddu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vwaddu.w.nxv16i16.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwaddu.w.mask.nxv16i16.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwaddu.w_mask_wv_nxv16i16_nxv16i16_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_mask_wv_nxv16i16_nxv16i16_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwaddu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwaddu.w.mask.nxv16i16.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwaddu.w.nxv32i16.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vwaddu.w_wv_nxv32i16_nxv32i16_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_wv_nxv32i16_nxv32i16_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwaddu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vwaddu.w.nxv32i16.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwaddu.w.mask.nxv32i16.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwaddu.w_mask_wv_nxv32i16_nxv32i16_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_mask_wv_nxv32i16_nxv32i16_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwaddu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwaddu.w.mask.nxv32i16.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwaddu.w.nxv4i32.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vwaddu.w_wv_nxv4i32_nxv4i32_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_wv_nxv4i32_nxv4i32_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwaddu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vwaddu.w.nxv4i32.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwaddu.w.mask.nxv4i32.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwaddu.w_mask_wv_nxv4i32_nxv4i32_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_mask_wv_nxv4i32_nxv4i32_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwaddu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwaddu.w.mask.nxv4i32.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwaddu.w.nxv8i32.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vwaddu.w_wv_nxv8i32_nxv8i32_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_wv_nxv8i32_nxv8i32_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwaddu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vwaddu.w.nxv8i32.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwaddu.w.mask.nxv8i32.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwaddu.w_mask_wv_nxv8i32_nxv8i32_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_mask_wv_nxv8i32_nxv8i32_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwaddu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwaddu.w.mask.nxv8i32.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwaddu.w.nxv16i32.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vwaddu.w_wv_nxv16i32_nxv16i32_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_wv_nxv16i32_nxv16i32_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwaddu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vwaddu.w.nxv16i32.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwaddu.w.mask.nxv16i32.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwaddu.w_mask_wv_nxv16i32_nxv16i32_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_mask_wv_nxv16i32_nxv16i32_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwaddu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwaddu.w.mask.nxv16i32.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwaddu.w.nxv2i64.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vwaddu.w_wv_nxv2i64_nxv2i64_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_wv_nxv2i64_nxv2i64_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwaddu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vwaddu.w.nxv2i64.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwaddu.w.mask.nxv2i64.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwaddu.w_mask_wv_nxv2i64_nxv2i64_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_mask_wv_nxv2i64_nxv2i64_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwaddu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwaddu.w.mask.nxv2i64.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwaddu.w.nxv4i64.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vwaddu.w_wv_nxv4i64_nxv4i64_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_wv_nxv4i64_nxv4i64_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwaddu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vwaddu.w.nxv4i64.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwaddu.w.mask.nxv4i64.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwaddu.w_mask_wv_nxv4i64_nxv4i64_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_mask_wv_nxv4i64_nxv4i64_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwaddu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwaddu.w.mask.nxv4i64.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwaddu.w.nxv8i64.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vwaddu.w_wv_nxv8i64_nxv8i64_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_wv_nxv8i64_nxv8i64_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwaddu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vwaddu.w.nxv8i64.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwaddu.w.mask.nxv8i64.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwaddu.w_mask_wv_nxv8i64_nxv8i64_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_mask_wv_nxv8i64_nxv8i64_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwaddu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwaddu.w.mask.nxv8i64.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwaddu.w.nxv8i16.i8(
  <vscale x 8 x i16>,
  i8,
  i64);

define void @intrinsic_vwaddu.w_wx_nxv8i16_nxv8i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_wx_nxv8i16_nxv8i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwaddu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vwaddu.w.nxv8i16.i8(
    <vscale x 8 x i16> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwaddu.w.mask.nxv8i16.i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwaddu.w_mask_wx_nxv8i16_nxv8i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_mask_wx_nxv8i16_nxv8i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwaddu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwaddu.w.mask.nxv8i16.i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwaddu.w.nxv16i16.i8(
  <vscale x 16 x i16>,
  i8,
  i64);

define void @intrinsic_vwaddu.w_wx_nxv16i16_nxv16i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_wx_nxv16i16_nxv16i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwaddu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vwaddu.w.nxv16i16.i8(
    <vscale x 16 x i16> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwaddu.w.mask.nxv16i16.i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwaddu.w_mask_wx_nxv16i16_nxv16i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_mask_wx_nxv16i16_nxv16i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwaddu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwaddu.w.mask.nxv16i16.i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwaddu.w.nxv32i16.i8(
  <vscale x 32 x i16>,
  i8,
  i64);

define void @intrinsic_vwaddu.w_wx_nxv32i16_nxv32i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_wx_nxv32i16_nxv32i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwaddu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vwaddu.w.nxv32i16.i8(
    <vscale x 32 x i16> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwaddu.w.mask.nxv32i16.i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwaddu.w_mask_wx_nxv32i16_nxv32i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_mask_wx_nxv32i16_nxv32i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwaddu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwaddu.w.mask.nxv32i16.i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwaddu.w.nxv4i32.i16(
  <vscale x 4 x i32>,
  i16,
  i64);

define void @intrinsic_vwaddu.w_wx_nxv4i32_nxv4i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_wx_nxv4i32_nxv4i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwaddu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vwaddu.w.nxv4i32.i16(
    <vscale x 4 x i32> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwaddu.w.mask.nxv4i32.i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwaddu.w_mask_wx_nxv4i32_nxv4i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_mask_wx_nxv4i32_nxv4i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwaddu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwaddu.w.mask.nxv4i32.i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwaddu.w.nxv8i32.i16(
  <vscale x 8 x i32>,
  i16,
  i64);

define void @intrinsic_vwaddu.w_wx_nxv8i32_nxv8i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_wx_nxv8i32_nxv8i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwaddu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vwaddu.w.nxv8i32.i16(
    <vscale x 8 x i32> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwaddu.w.mask.nxv8i32.i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwaddu.w_mask_wx_nxv8i32_nxv8i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_mask_wx_nxv8i32_nxv8i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwaddu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwaddu.w.mask.nxv8i32.i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwaddu.w.nxv16i32.i16(
  <vscale x 16 x i32>,
  i16,
  i64);

define void @intrinsic_vwaddu.w_wx_nxv16i32_nxv16i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_wx_nxv16i32_nxv16i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwaddu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vwaddu.w.nxv16i32.i16(
    <vscale x 16 x i32> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwaddu.w.mask.nxv16i32.i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwaddu.w_mask_wx_nxv16i32_nxv16i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_mask_wx_nxv16i32_nxv16i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwaddu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwaddu.w.mask.nxv16i32.i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwaddu.w.nxv2i64.i32(
  <vscale x 2 x i64>,
  i32,
  i64);

define void @intrinsic_vwaddu.w_wx_nxv2i64_nxv2i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_wx_nxv2i64_nxv2i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwaddu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vwaddu.w.nxv2i64.i32(
    <vscale x 2 x i64> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwaddu.w.mask.nxv2i64.i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwaddu.w_mask_wx_nxv2i64_nxv2i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_mask_wx_nxv2i64_nxv2i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwaddu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwaddu.w.mask.nxv2i64.i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwaddu.w.nxv4i64.i32(
  <vscale x 4 x i64>,
  i32,
  i64);

define void @intrinsic_vwaddu.w_wx_nxv4i64_nxv4i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_wx_nxv4i64_nxv4i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwaddu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vwaddu.w.nxv4i64.i32(
    <vscale x 4 x i64> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwaddu.w.mask.nxv4i64.i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwaddu.w_mask_wx_nxv4i64_nxv4i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_mask_wx_nxv4i64_nxv4i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwaddu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwaddu.w.mask.nxv4i64.i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwaddu.w.nxv8i64.i32(
  <vscale x 8 x i64>,
  i32,
  i64);

define void @intrinsic_vwaddu.w_wx_nxv8i64_nxv8i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_wx_nxv8i64_nxv8i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwaddu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vwaddu.w.nxv8i64.i32(
    <vscale x 8 x i64> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwaddu.w.mask.nxv8i64.i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwaddu.w_mask_wx_nxv8i64_nxv8i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwaddu.w_mask_wx_nxv8i64_nxv8i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwaddu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwaddu.w.mask.nxv8i64.i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwadd.w.nxv8i16.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vwadd.w_wv_nxv8i16_nxv8i16_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_wv_nxv8i16_nxv8i16_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vwadd.w.nxv8i16.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwadd.w.mask.nxv8i16.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwadd.w_mask_wv_nxv8i16_nxv8i16_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_mask_wv_nxv8i16_nxv8i16_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwadd.w.mask.nxv8i16.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwadd.w.nxv16i16.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vwadd.w_wv_nxv16i16_nxv16i16_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_wv_nxv16i16_nxv16i16_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vwadd.w.nxv16i16.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwadd.w.mask.nxv16i16.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwadd.w_mask_wv_nxv16i16_nxv16i16_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_mask_wv_nxv16i16_nxv16i16_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwadd.w.mask.nxv16i16.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwadd.w.nxv32i16.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vwadd.w_wv_nxv32i16_nxv32i16_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_wv_nxv32i16_nxv32i16_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vwadd.w.nxv32i16.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwadd.w.mask.nxv32i16.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwadd.w_mask_wv_nxv32i16_nxv32i16_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_mask_wv_nxv32i16_nxv32i16_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwadd.w.mask.nxv32i16.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwadd.w.nxv4i32.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vwadd.w_wv_nxv4i32_nxv4i32_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_wv_nxv4i32_nxv4i32_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vwadd.w.nxv4i32.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwadd.w.mask.nxv4i32.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwadd.w_mask_wv_nxv4i32_nxv4i32_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_mask_wv_nxv4i32_nxv4i32_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwadd.w.mask.nxv4i32.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwadd.w.nxv8i32.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vwadd.w_wv_nxv8i32_nxv8i32_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_wv_nxv8i32_nxv8i32_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vwadd.w.nxv8i32.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwadd.w.mask.nxv8i32.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwadd.w_mask_wv_nxv8i32_nxv8i32_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_mask_wv_nxv8i32_nxv8i32_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwadd.w.mask.nxv8i32.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwadd.w.nxv16i32.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vwadd.w_wv_nxv16i32_nxv16i32_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_wv_nxv16i32_nxv16i32_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vwadd.w.nxv16i32.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwadd.w.mask.nxv16i32.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwadd.w_mask_wv_nxv16i32_nxv16i32_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_mask_wv_nxv16i32_nxv16i32_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwadd.w.mask.nxv16i32.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwadd.w.nxv2i64.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vwadd.w_wv_nxv2i64_nxv2i64_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_wv_nxv2i64_nxv2i64_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vwadd.w.nxv2i64.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwadd.w.mask.nxv2i64.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwadd.w_mask_wv_nxv2i64_nxv2i64_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_mask_wv_nxv2i64_nxv2i64_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwadd.w.mask.nxv2i64.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwadd.w.nxv4i64.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vwadd.w_wv_nxv4i64_nxv4i64_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_wv_nxv4i64_nxv4i64_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vwadd.w.nxv4i64.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwadd.w.mask.nxv4i64.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwadd.w_mask_wv_nxv4i64_nxv4i64_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_mask_wv_nxv4i64_nxv4i64_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwadd.w.mask.nxv4i64.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwadd.w.nxv8i64.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vwadd.w_wv_nxv8i64_nxv8i64_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_wv_nxv8i64_nxv8i64_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vwadd.w.nxv8i64.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwadd.w.mask.nxv8i64.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwadd.w_mask_wv_nxv8i64_nxv8i64_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_mask_wv_nxv8i64_nxv8i64_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwadd.w.mask.nxv8i64.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwadd.w.nxv8i16.i8(
  <vscale x 8 x i16>,
  i8,
  i64);

define void @intrinsic_vwadd.w_wx_nxv8i16_nxv8i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_wx_nxv8i16_nxv8i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwadd.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vwadd.w.nxv8i16.i8(
    <vscale x 8 x i16> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwadd.w.mask.nxv8i16.i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwadd.w_mask_wx_nxv8i16_nxv8i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_mask_wx_nxv8i16_nxv8i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwadd.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwadd.w.mask.nxv8i16.i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwadd.w.nxv16i16.i8(
  <vscale x 16 x i16>,
  i8,
  i64);

define void @intrinsic_vwadd.w_wx_nxv16i16_nxv16i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_wx_nxv16i16_nxv16i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwadd.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vwadd.w.nxv16i16.i8(
    <vscale x 16 x i16> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwadd.w.mask.nxv16i16.i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwadd.w_mask_wx_nxv16i16_nxv16i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_mask_wx_nxv16i16_nxv16i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwadd.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwadd.w.mask.nxv16i16.i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwadd.w.nxv32i16.i8(
  <vscale x 32 x i16>,
  i8,
  i64);

define void @intrinsic_vwadd.w_wx_nxv32i16_nxv32i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_wx_nxv32i16_nxv32i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwadd.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vwadd.w.nxv32i16.i8(
    <vscale x 32 x i16> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwadd.w.mask.nxv32i16.i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwadd.w_mask_wx_nxv32i16_nxv32i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_mask_wx_nxv32i16_nxv32i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwadd.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwadd.w.mask.nxv32i16.i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwadd.w.nxv4i32.i16(
  <vscale x 4 x i32>,
  i16,
  i64);

define void @intrinsic_vwadd.w_wx_nxv4i32_nxv4i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_wx_nxv4i32_nxv4i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwadd.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vwadd.w.nxv4i32.i16(
    <vscale x 4 x i32> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwadd.w.mask.nxv4i32.i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwadd.w_mask_wx_nxv4i32_nxv4i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_mask_wx_nxv4i32_nxv4i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwadd.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwadd.w.mask.nxv4i32.i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwadd.w.nxv8i32.i16(
  <vscale x 8 x i32>,
  i16,
  i64);

define void @intrinsic_vwadd.w_wx_nxv8i32_nxv8i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_wx_nxv8i32_nxv8i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwadd.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vwadd.w.nxv8i32.i16(
    <vscale x 8 x i32> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwadd.w.mask.nxv8i32.i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwadd.w_mask_wx_nxv8i32_nxv8i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_mask_wx_nxv8i32_nxv8i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwadd.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwadd.w.mask.nxv8i32.i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwadd.w.nxv16i32.i16(
  <vscale x 16 x i32>,
  i16,
  i64);

define void @intrinsic_vwadd.w_wx_nxv16i32_nxv16i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_wx_nxv16i32_nxv16i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwadd.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vwadd.w.nxv16i32.i16(
    <vscale x 16 x i32> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwadd.w.mask.nxv16i32.i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwadd.w_mask_wx_nxv16i32_nxv16i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_mask_wx_nxv16i32_nxv16i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwadd.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwadd.w.mask.nxv16i32.i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwadd.w.nxv2i64.i32(
  <vscale x 2 x i64>,
  i32,
  i64);

define void @intrinsic_vwadd.w_wx_nxv2i64_nxv2i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_wx_nxv2i64_nxv2i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwadd.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vwadd.w.nxv2i64.i32(
    <vscale x 2 x i64> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwadd.w.mask.nxv2i64.i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwadd.w_mask_wx_nxv2i64_nxv2i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_mask_wx_nxv2i64_nxv2i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwadd.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwadd.w.mask.nxv2i64.i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwadd.w.nxv4i64.i32(
  <vscale x 4 x i64>,
  i32,
  i64);

define void @intrinsic_vwadd.w_wx_nxv4i64_nxv4i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_wx_nxv4i64_nxv4i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwadd.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vwadd.w.nxv4i64.i32(
    <vscale x 4 x i64> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwadd.w.mask.nxv4i64.i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwadd.w_mask_wx_nxv4i64_nxv4i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_mask_wx_nxv4i64_nxv4i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwadd.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwadd.w.mask.nxv4i64.i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwadd.w.nxv8i64.i32(
  <vscale x 8 x i64>,
  i32,
  i64);

define void @intrinsic_vwadd.w_wx_nxv8i64_nxv8i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_wx_nxv8i64_nxv8i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwadd.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vwadd.w.nxv8i64.i32(
    <vscale x 8 x i64> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwadd.w.mask.nxv8i64.i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwadd.w_mask_wx_nxv8i64_nxv8i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwadd.w_mask_wx_nxv8i64_nxv8i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwadd.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwadd.w.mask.nxv8i64.i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwsubu.nxv8i16.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vwsubu_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwsubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vwsubu.nxv8i16.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwsubu.mask.nxv8i16.nxv8i8.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwsubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwsubu.mask.nxv8i16.nxv8i8.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwsubu.nxv16i16.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vwsubu_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwsubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vwsubu.nxv16i16.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwsubu.mask.nxv16i16.nxv16i8.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwsubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwsubu.mask.nxv16i16.nxv16i8.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwsubu.nxv32i16.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vwsubu_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwsubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vwsubu.nxv32i16.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwsubu.mask.nxv32i16.nxv32i8.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwsubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwsubu.mask.nxv32i16.nxv32i8.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwsubu.nxv4i32.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vwsubu_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwsubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vwsubu.nxv4i32.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwsubu.mask.nxv4i32.nxv4i16.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwsubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwsubu.mask.nxv4i32.nxv4i16.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwsubu.nxv8i32.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vwsubu_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwsubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vwsubu.nxv8i32.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwsubu.mask.nxv8i32.nxv8i16.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwsubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwsubu.mask.nxv8i32.nxv8i16.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwsubu.nxv16i32.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vwsubu_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwsubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vwsubu.nxv16i32.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwsubu.mask.nxv16i32.nxv16i16.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwsubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwsubu.mask.nxv16i32.nxv16i16.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwsubu.nxv2i64.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vwsubu_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwsubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vwsubu.nxv2i64.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwsubu.mask.nxv2i64.nxv2i32.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwsubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwsubu.mask.nxv2i64.nxv2i32.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwsubu.nxv4i64.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vwsubu_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwsubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vwsubu.nxv4i64.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwsubu.mask.nxv4i64.nxv4i32.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwsubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwsubu.mask.nxv4i64.nxv4i32.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwsubu.nxv8i64.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vwsubu_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwsubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vwsubu.nxv8i64.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwsubu.mask.nxv8i64.nxv8i32.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwsubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwsubu.mask.nxv8i64.nxv8i32.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwsubu.nxv8i16.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vwsubu_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwsubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vwsubu.nxv8i16.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwsubu.mask.nxv8i16.nxv8i8.i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwsubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwsubu.mask.nxv8i16.nxv8i8.i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwsubu.nxv16i16.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vwsubu_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwsubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vwsubu.nxv16i16.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwsubu.mask.nxv16i16.nxv16i8.i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwsubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwsubu.mask.nxv16i16.nxv16i8.i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwsubu.nxv32i16.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vwsubu_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwsubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vwsubu.nxv32i16.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwsubu.mask.nxv32i16.nxv32i8.i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwsubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwsubu.mask.nxv32i16.nxv32i8.i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwsubu.nxv4i32.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vwsubu_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwsubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vwsubu.nxv4i32.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwsubu.mask.nxv4i32.nxv4i16.i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwsubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwsubu.mask.nxv4i32.nxv4i16.i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwsubu.nxv8i32.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vwsubu_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwsubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vwsubu.nxv8i32.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwsubu.mask.nxv8i32.nxv8i16.i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwsubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwsubu.mask.nxv8i32.nxv8i16.i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwsubu.nxv16i32.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vwsubu_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwsubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vwsubu.nxv16i32.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwsubu.mask.nxv16i32.nxv16i16.i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwsubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwsubu.mask.nxv16i32.nxv16i16.i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwsubu.nxv2i64.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vwsubu_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwsubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vwsubu.nxv2i64.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwsubu.mask.nxv2i64.nxv2i32.i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwsubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwsubu.mask.nxv2i64.nxv2i32.i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwsubu.nxv4i64.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vwsubu_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwsubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vwsubu.nxv4i64.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwsubu.mask.nxv4i64.nxv4i32.i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwsubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwsubu.mask.nxv4i64.nxv4i32.i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwsubu.nxv8i64.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vwsubu_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwsubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vwsubu.nxv8i64.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwsubu.mask.nxv8i64.nxv8i32.i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsubu_mask_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu_mask_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwsubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwsubu.mask.nxv8i64.nxv8i32.i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwsub.nxv8i16.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vwsub_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vwsub.nxv8i16.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwsub.mask.nxv8i16.nxv8i8.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwsub.mask.nxv8i16.nxv8i8.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwsub.nxv16i16.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vwsub_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vwsub.nxv16i16.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwsub.mask.nxv16i16.nxv16i8.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwsub.mask.nxv16i16.nxv16i8.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwsub.nxv32i16.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vwsub_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vwsub.nxv32i16.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwsub.mask.nxv32i16.nxv32i8.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwsub.mask.nxv32i16.nxv32i8.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwsub.nxv4i32.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vwsub_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vwsub.nxv4i32.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwsub.mask.nxv4i32.nxv4i16.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwsub.mask.nxv4i32.nxv4i16.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwsub.nxv8i32.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vwsub_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vwsub.nxv8i32.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwsub.mask.nxv8i32.nxv8i16.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwsub.mask.nxv8i32.nxv8i16.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwsub.nxv16i32.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vwsub_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vwsub.nxv16i32.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwsub.mask.nxv16i32.nxv16i16.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwsub.mask.nxv16i32.nxv16i16.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwsub.nxv2i64.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vwsub_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vwsub.nxv2i64.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwsub.mask.nxv2i64.nxv2i32.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwsub.mask.nxv2i64.nxv2i32.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwsub.nxv4i64.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vwsub_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vwsub.nxv4i64.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwsub.mask.nxv4i64.nxv4i32.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwsub.mask.nxv4i64.nxv4i32.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwsub.nxv8i64.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vwsub_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vwsub.nxv8i64.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwsub.mask.nxv8i64.nxv8i32.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwsub.mask.nxv8i64.nxv8i32.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwsub.nxv8i16.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vwsub_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vwsub.nxv8i16.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwsub.mask.nxv8i16.nxv8i8.i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwsub.mask.nxv8i16.nxv8i8.i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwsub.nxv16i16.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vwsub_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vwsub.nxv16i16.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwsub.mask.nxv16i16.nxv16i8.i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwsub.mask.nxv16i16.nxv16i8.i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwsub.nxv32i16.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vwsub_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vwsub.nxv32i16.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwsub.mask.nxv32i16.nxv32i8.i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwsub.mask.nxv32i16.nxv32i8.i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwsub.nxv4i32.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vwsub_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vwsub.nxv4i32.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwsub.mask.nxv4i32.nxv4i16.i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwsub.mask.nxv4i32.nxv4i16.i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwsub.nxv8i32.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vwsub_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vwsub.nxv8i32.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwsub.mask.nxv8i32.nxv8i16.i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwsub.mask.nxv8i32.nxv8i16.i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwsub.nxv16i32.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vwsub_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vwsub.nxv16i32.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwsub.mask.nxv16i32.nxv16i16.i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwsub.mask.nxv16i32.nxv16i16.i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwsub.nxv2i64.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vwsub_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vwsub.nxv2i64.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwsub.mask.nxv2i64.nxv2i32.i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwsub.mask.nxv2i64.nxv2i32.i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwsub.nxv4i64.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vwsub_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vwsub.nxv4i64.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwsub.mask.nxv4i64.nxv4i32.i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwsub.mask.nxv4i64.nxv4i32.i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwsub.nxv8i64.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vwsub_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vwsub.nxv8i64.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwsub.mask.nxv8i64.nxv8i32.i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsub_mask_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub_mask_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwsub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwsub.mask.nxv8i64.nxv8i32.i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwsubu.w.nxv8i16.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vwsubu.w_wv_nxv8i16_nxv8i16_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_wv_nxv8i16_nxv8i16_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwsubu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vwsubu.w.nxv8i16.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwsubu.w.mask.nxv8i16.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsubu.w_mask_wv_nxv8i16_nxv8i16_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_mask_wv_nxv8i16_nxv8i16_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwsubu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwsubu.w.mask.nxv8i16.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwsubu.w.nxv16i16.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vwsubu.w_wv_nxv16i16_nxv16i16_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_wv_nxv16i16_nxv16i16_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwsubu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vwsubu.w.nxv16i16.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwsubu.w.mask.nxv16i16.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsubu.w_mask_wv_nxv16i16_nxv16i16_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_mask_wv_nxv16i16_nxv16i16_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwsubu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwsubu.w.mask.nxv16i16.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwsubu.w.nxv32i16.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vwsubu.w_wv_nxv32i16_nxv32i16_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_wv_nxv32i16_nxv32i16_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwsubu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vwsubu.w.nxv32i16.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwsubu.w.mask.nxv32i16.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwsubu.w_mask_wv_nxv32i16_nxv32i16_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_mask_wv_nxv32i16_nxv32i16_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwsubu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwsubu.w.mask.nxv32i16.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwsubu.w.nxv4i32.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vwsubu.w_wv_nxv4i32_nxv4i32_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_wv_nxv4i32_nxv4i32_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwsubu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vwsubu.w.nxv4i32.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwsubu.w.mask.nxv4i32.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsubu.w_mask_wv_nxv4i32_nxv4i32_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_mask_wv_nxv4i32_nxv4i32_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwsubu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwsubu.w.mask.nxv4i32.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwsubu.w.nxv8i32.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vwsubu.w_wv_nxv8i32_nxv8i32_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_wv_nxv8i32_nxv8i32_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwsubu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vwsubu.w.nxv8i32.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwsubu.w.mask.nxv8i32.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsubu.w_mask_wv_nxv8i32_nxv8i32_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_mask_wv_nxv8i32_nxv8i32_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwsubu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwsubu.w.mask.nxv8i32.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwsubu.w.nxv16i32.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vwsubu.w_wv_nxv16i32_nxv16i32_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_wv_nxv16i32_nxv16i32_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwsubu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vwsubu.w.nxv16i32.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwsubu.w.mask.nxv16i32.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsubu.w_mask_wv_nxv16i32_nxv16i32_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_mask_wv_nxv16i32_nxv16i32_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwsubu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwsubu.w.mask.nxv16i32.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwsubu.w.nxv2i64.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vwsubu.w_wv_nxv2i64_nxv2i64_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_wv_nxv2i64_nxv2i64_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwsubu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vwsubu.w.nxv2i64.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwsubu.w.mask.nxv2i64.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwsubu.w_mask_wv_nxv2i64_nxv2i64_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_mask_wv_nxv2i64_nxv2i64_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwsubu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwsubu.w.mask.nxv2i64.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwsubu.w.nxv4i64.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vwsubu.w_wv_nxv4i64_nxv4i64_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_wv_nxv4i64_nxv4i64_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwsubu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vwsubu.w.nxv4i64.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwsubu.w.mask.nxv4i64.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsubu.w_mask_wv_nxv4i64_nxv4i64_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_mask_wv_nxv4i64_nxv4i64_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwsubu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwsubu.w.mask.nxv4i64.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwsubu.w.nxv8i64.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vwsubu.w_wv_nxv8i64_nxv8i64_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_wv_nxv8i64_nxv8i64_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwsubu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vwsubu.w.nxv8i64.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwsubu.w.mask.nxv8i64.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsubu.w_mask_wv_nxv8i64_nxv8i64_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_mask_wv_nxv8i64_nxv8i64_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwsubu.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwsubu.w.mask.nxv8i64.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwsubu.w.nxv8i16.i8(
  <vscale x 8 x i16>,
  i8,
  i64);

define void @intrinsic_vwsubu.w_wx_nxv8i16_nxv8i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_wx_nxv8i16_nxv8i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwsubu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vwsubu.w.nxv8i16.i8(
    <vscale x 8 x i16> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwsubu.w.mask.nxv8i16.i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsubu.w_mask_wx_nxv8i16_nxv8i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_mask_wx_nxv8i16_nxv8i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwsubu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwsubu.w.mask.nxv8i16.i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwsubu.w.nxv16i16.i8(
  <vscale x 16 x i16>,
  i8,
  i64);

define void @intrinsic_vwsubu.w_wx_nxv16i16_nxv16i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_wx_nxv16i16_nxv16i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwsubu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vwsubu.w.nxv16i16.i8(
    <vscale x 16 x i16> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwsubu.w.mask.nxv16i16.i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsubu.w_mask_wx_nxv16i16_nxv16i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_mask_wx_nxv16i16_nxv16i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwsubu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwsubu.w.mask.nxv16i16.i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwsubu.w.nxv32i16.i8(
  <vscale x 32 x i16>,
  i8,
  i64);

define void @intrinsic_vwsubu.w_wx_nxv32i16_nxv32i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_wx_nxv32i16_nxv32i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwsubu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vwsubu.w.nxv32i16.i8(
    <vscale x 32 x i16> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwsubu.w.mask.nxv32i16.i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwsubu.w_mask_wx_nxv32i16_nxv32i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_mask_wx_nxv32i16_nxv32i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwsubu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwsubu.w.mask.nxv32i16.i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwsubu.w.nxv4i32.i16(
  <vscale x 4 x i32>,
  i16,
  i64);

define void @intrinsic_vwsubu.w_wx_nxv4i32_nxv4i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_wx_nxv4i32_nxv4i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwsubu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vwsubu.w.nxv4i32.i16(
    <vscale x 4 x i32> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwsubu.w.mask.nxv4i32.i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsubu.w_mask_wx_nxv4i32_nxv4i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_mask_wx_nxv4i32_nxv4i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwsubu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwsubu.w.mask.nxv4i32.i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwsubu.w.nxv8i32.i16(
  <vscale x 8 x i32>,
  i16,
  i64);

define void @intrinsic_vwsubu.w_wx_nxv8i32_nxv8i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_wx_nxv8i32_nxv8i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwsubu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vwsubu.w.nxv8i32.i16(
    <vscale x 8 x i32> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwsubu.w.mask.nxv8i32.i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsubu.w_mask_wx_nxv8i32_nxv8i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_mask_wx_nxv8i32_nxv8i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwsubu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwsubu.w.mask.nxv8i32.i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwsubu.w.nxv16i32.i16(
  <vscale x 16 x i32>,
  i16,
  i64);

define void @intrinsic_vwsubu.w_wx_nxv16i32_nxv16i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_wx_nxv16i32_nxv16i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwsubu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vwsubu.w.nxv16i32.i16(
    <vscale x 16 x i32> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwsubu.w.mask.nxv16i32.i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsubu.w_mask_wx_nxv16i32_nxv16i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_mask_wx_nxv16i32_nxv16i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwsubu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwsubu.w.mask.nxv16i32.i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwsubu.w.nxv2i64.i32(
  <vscale x 2 x i64>,
  i32,
  i64);

define void @intrinsic_vwsubu.w_wx_nxv2i64_nxv2i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_wx_nxv2i64_nxv2i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwsubu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vwsubu.w.nxv2i64.i32(
    <vscale x 2 x i64> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwsubu.w.mask.nxv2i64.i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwsubu.w_mask_wx_nxv2i64_nxv2i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_mask_wx_nxv2i64_nxv2i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwsubu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwsubu.w.mask.nxv2i64.i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwsubu.w.nxv4i64.i32(
  <vscale x 4 x i64>,
  i32,
  i64);

define void @intrinsic_vwsubu.w_wx_nxv4i64_nxv4i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_wx_nxv4i64_nxv4i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwsubu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vwsubu.w.nxv4i64.i32(
    <vscale x 4 x i64> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwsubu.w.mask.nxv4i64.i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsubu.w_mask_wx_nxv4i64_nxv4i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_mask_wx_nxv4i64_nxv4i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwsubu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwsubu.w.mask.nxv4i64.i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwsubu.w.nxv8i64.i32(
  <vscale x 8 x i64>,
  i32,
  i64);

define void @intrinsic_vwsubu.w_wx_nxv8i64_nxv8i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_wx_nxv8i64_nxv8i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwsubu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vwsubu.w.nxv8i64.i32(
    <vscale x 8 x i64> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwsubu.w.mask.nxv8i64.i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsubu.w_mask_wx_nxv8i64_nxv8i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsubu.w_mask_wx_nxv8i64_nxv8i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwsubu.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwsubu.w.mask.nxv8i64.i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwsub.w.nxv8i16.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vwsub.w_wv_nxv8i16_nxv8i16_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_wv_nxv8i16_nxv8i16_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vwsub.w.nxv8i16.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwsub.w.mask.nxv8i16.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsub.w_mask_wv_nxv8i16_nxv8i16_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_mask_wv_nxv8i16_nxv8i16_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwsub.w.mask.nxv8i16.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwsub.w.nxv16i16.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vwsub.w_wv_nxv16i16_nxv16i16_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_wv_nxv16i16_nxv16i16_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vwsub.w.nxv16i16.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwsub.w.mask.nxv16i16.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsub.w_mask_wv_nxv16i16_nxv16i16_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_mask_wv_nxv16i16_nxv16i16_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwsub.w.mask.nxv16i16.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwsub.w.nxv32i16.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vwsub.w_wv_nxv32i16_nxv32i16_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_wv_nxv32i16_nxv32i16_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vwsub.w.nxv32i16.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwsub.w.mask.nxv32i16.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwsub.w_mask_wv_nxv32i16_nxv32i16_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_mask_wv_nxv32i16_nxv32i16_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwsub.w.mask.nxv32i16.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwsub.w.nxv4i32.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vwsub.w_wv_nxv4i32_nxv4i32_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_wv_nxv4i32_nxv4i32_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vwsub.w.nxv4i32.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwsub.w.mask.nxv4i32.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsub.w_mask_wv_nxv4i32_nxv4i32_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_mask_wv_nxv4i32_nxv4i32_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwsub.w.mask.nxv4i32.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwsub.w.nxv8i32.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vwsub.w_wv_nxv8i32_nxv8i32_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_wv_nxv8i32_nxv8i32_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vwsub.w.nxv8i32.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwsub.w.mask.nxv8i32.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsub.w_mask_wv_nxv8i32_nxv8i32_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_mask_wv_nxv8i32_nxv8i32_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwsub.w.mask.nxv8i32.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwsub.w.nxv16i32.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vwsub.w_wv_nxv16i32_nxv16i32_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_wv_nxv16i32_nxv16i32_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vwsub.w.nxv16i32.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwsub.w.mask.nxv16i32.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsub.w_mask_wv_nxv16i32_nxv16i32_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_mask_wv_nxv16i32_nxv16i32_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwsub.w.mask.nxv16i32.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwsub.w.nxv2i64.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vwsub.w_wv_nxv2i64_nxv2i64_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_wv_nxv2i64_nxv2i64_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vwsub.w.nxv2i64.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwsub.w.mask.nxv2i64.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwsub.w_mask_wv_nxv2i64_nxv2i64_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_mask_wv_nxv2i64_nxv2i64_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwsub.w.mask.nxv2i64.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwsub.w.nxv4i64.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vwsub.w_wv_nxv4i64_nxv4i64_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_wv_nxv4i64_nxv4i64_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vwsub.w.nxv4i64.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwsub.w.mask.nxv4i64.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsub.w_mask_wv_nxv4i64_nxv4i64_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_mask_wv_nxv4i64_nxv4i64_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwsub.w.mask.nxv4i64.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwsub.w.nxv8i64.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vwsub.w_wv_nxv8i64_nxv8i64_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_wv_nxv8i64_nxv8i64_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vwsub.w.nxv8i64.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwsub.w.mask.nxv8i64.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsub.w_mask_wv_nxv8i64_nxv8i64_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_mask_wv_nxv8i64_nxv8i64_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwsub.w.mask.nxv8i64.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwsub.w.nxv8i16.i8(
  <vscale x 8 x i16>,
  i8,
  i64);

define void @intrinsic_vwsub.w_wx_nxv8i16_nxv8i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_wx_nxv8i16_nxv8i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwsub.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vwsub.w.nxv8i16.i8(
    <vscale x 8 x i16> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwsub.w.mask.nxv8i16.i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsub.w_mask_wx_nxv8i16_nxv8i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_mask_wx_nxv8i16_nxv8i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwsub.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwsub.w.mask.nxv8i16.i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwsub.w.nxv16i16.i8(
  <vscale x 16 x i16>,
  i8,
  i64);

define void @intrinsic_vwsub.w_wx_nxv16i16_nxv16i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_wx_nxv16i16_nxv16i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwsub.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vwsub.w.nxv16i16.i8(
    <vscale x 16 x i16> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwsub.w.mask.nxv16i16.i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsub.w_mask_wx_nxv16i16_nxv16i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_mask_wx_nxv16i16_nxv16i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwsub.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwsub.w.mask.nxv16i16.i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwsub.w.nxv32i16.i8(
  <vscale x 32 x i16>,
  i8,
  i64);

define void @intrinsic_vwsub.w_wx_nxv32i16_nxv32i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_wx_nxv32i16_nxv32i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwsub.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vwsub.w.nxv32i16.i8(
    <vscale x 32 x i16> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwsub.w.mask.nxv32i16.i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwsub.w_mask_wx_nxv32i16_nxv32i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_mask_wx_nxv32i16_nxv32i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwsub.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwsub.w.mask.nxv32i16.i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwsub.w.nxv4i32.i16(
  <vscale x 4 x i32>,
  i16,
  i64);

define void @intrinsic_vwsub.w_wx_nxv4i32_nxv4i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_wx_nxv4i32_nxv4i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwsub.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vwsub.w.nxv4i32.i16(
    <vscale x 4 x i32> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwsub.w.mask.nxv4i32.i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsub.w_mask_wx_nxv4i32_nxv4i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_mask_wx_nxv4i32_nxv4i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwsub.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwsub.w.mask.nxv4i32.i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwsub.w.nxv8i32.i16(
  <vscale x 8 x i32>,
  i16,
  i64);

define void @intrinsic_vwsub.w_wx_nxv8i32_nxv8i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_wx_nxv8i32_nxv8i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwsub.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vwsub.w.nxv8i32.i16(
    <vscale x 8 x i32> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwsub.w.mask.nxv8i32.i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsub.w_mask_wx_nxv8i32_nxv8i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_mask_wx_nxv8i32_nxv8i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwsub.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwsub.w.mask.nxv8i32.i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwsub.w.nxv16i32.i16(
  <vscale x 16 x i32>,
  i16,
  i64);

define void @intrinsic_vwsub.w_wx_nxv16i32_nxv16i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_wx_nxv16i32_nxv16i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwsub.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vwsub.w.nxv16i32.i16(
    <vscale x 16 x i32> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwsub.w.mask.nxv16i32.i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwsub.w_mask_wx_nxv16i32_nxv16i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_mask_wx_nxv16i32_nxv16i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwsub.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwsub.w.mask.nxv16i32.i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwsub.w.nxv2i64.i32(
  <vscale x 2 x i64>,
  i32,
  i64);

define void @intrinsic_vwsub.w_wx_nxv2i64_nxv2i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_wx_nxv2i64_nxv2i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwsub.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vwsub.w.nxv2i64.i32(
    <vscale x 2 x i64> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwsub.w.mask.nxv2i64.i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwsub.w_mask_wx_nxv2i64_nxv2i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_mask_wx_nxv2i64_nxv2i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwsub.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwsub.w.mask.nxv2i64.i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwsub.w.nxv4i64.i32(
  <vscale x 4 x i64>,
  i32,
  i64);

define void @intrinsic_vwsub.w_wx_nxv4i64_nxv4i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_wx_nxv4i64_nxv4i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwsub.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vwsub.w.nxv4i64.i32(
    <vscale x 4 x i64> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwsub.w.mask.nxv4i64.i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwsub.w_mask_wx_nxv4i64_nxv4i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_mask_wx_nxv4i64_nxv4i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwsub.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwsub.w.mask.nxv4i64.i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwsub.w.nxv8i64.i32(
  <vscale x 8 x i64>,
  i32,
  i64);

define void @intrinsic_vwsub.w_wx_nxv8i64_nxv8i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_wx_nxv8i64_nxv8i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwsub.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vwsub.w.nxv8i64.i32(
    <vscale x 8 x i64> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwsub.w.mask.nxv8i64.i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwsub.w_mask_wx_nxv8i64_nxv8i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwsub.w_mask_wx_nxv8i64_nxv8i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwsub.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwsub.w.mask.nxv8i64.i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vand.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vand_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vand.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vand.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vand.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vand.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vand_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vand.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vand.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vand.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vand.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vand_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vand.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vand.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vand.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vand.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vand_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vand.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vand.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vand.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vand.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vand_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vand.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vand.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vand.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vand.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vand_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vand.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vand.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vand.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vand.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vand_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vand.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vand.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vand.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vand.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vand_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vand.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vand.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vand.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vand.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vand_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vand.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vand.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vand.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vand.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vand_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vand.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vand.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vand.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vand.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vand_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vand.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vand.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vand.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vand.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vand_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vand.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vand.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vand.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vand.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vand_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vand.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vand.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vand.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vand.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vand_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vand.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vand.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vand.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vand.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vand_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vand.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vand.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vand_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vand.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vand.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vand.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vand_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vand.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vand.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vand.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vand.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vand_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vand.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vand.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vand.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vand.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vand_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vand.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vand.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vand.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vand.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vand_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vand.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vand.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vand.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vand.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vand_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vand.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vand.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vand.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vand.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vand_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vand.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vand.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vand.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vand.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vand_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vand.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vand.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vand.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vand.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vand_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vand.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vand.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vand.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vand.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vand_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vand.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vand.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vand.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vand.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vand_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vand.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vand.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vand.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vand.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vand_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vand.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vand.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vand.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vand.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vand_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vand.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vand.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vand.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vand.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vand_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vand.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vand.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vand.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vand.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vand_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vand.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vand.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vand.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vand.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vand_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vand.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vand.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vand_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vand.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vand.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vand.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vand.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vand.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vand.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vand.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vand.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vand.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vand.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vand.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vand.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vand.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vand.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vand.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vand.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vand.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vand.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vand.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vand.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vand.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vand.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vand.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vand.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vand.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vand.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vand.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vand.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vand.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vand.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vand_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vand.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vand_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vand_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vand.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vand.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vor.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vor_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vor.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vor.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vor.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vor.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vor_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vor.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vor.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vor.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vor.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vor_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vor.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vor.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vor.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vor.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vor_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vor.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vor.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vor.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vor.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vor_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vor.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vor.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vor.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vor.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vor_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vor.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vor.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vor.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vor.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vor_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vor.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vor.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vor.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vor.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vor_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vor.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vor.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vor.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vor.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vor_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vor.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vor.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vor.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vor.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vor_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vor.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vor.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vor.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vor.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vor_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vor.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vor.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vor.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vor.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vor_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vor.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vor.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vor.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vor.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vor_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vor.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vor.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vor.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vor.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vor_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vor.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vor.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vor.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vor.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vor_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vor.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vor.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vor_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vor.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vor.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vor_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vor.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vor.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vor.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vor.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vor_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vor.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vor.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vor.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vor.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vor_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vor.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vor.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vor.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vor.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vor_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vor.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vor.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vor.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vor.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vor_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vor.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vor.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vor.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vor.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vor_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vor.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vor.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vor.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vor.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vor_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vor.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vor.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vor.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vor.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vor_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vor.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vor.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vor.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vor.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vor_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vor.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vor.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vor.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vor.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vor_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vor.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vor.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vor.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vor.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vor_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vor.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vor.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vor.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vor.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vor_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vor.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vor.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vor.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vor.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vor_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vor.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vor.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vor.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vor.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vor_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vor.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vor.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vor.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vor.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vor_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vor.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vor.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vor_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vor.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vor.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vor.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vor.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vor.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vor.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vor.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vor.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vor.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vor.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vor.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vor.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vor.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vor.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vor.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vor.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vor.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vor.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vor.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vor.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vor.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vor.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vor.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vor.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vor.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vor.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vor.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vor.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vor.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vor_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vor.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vor_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vor_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vor.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vxor.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vxor_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vxor.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vxor.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vxor.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vxor.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vxor_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vxor.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vxor.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vxor.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vxor.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vxor_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vxor.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vxor.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vxor.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vxor.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vxor_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vxor.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vxor.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vxor.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vxor.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vxor_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vxor.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vxor.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vxor.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vxor.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vxor_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vxor.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vxor.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vxor.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vxor.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vxor_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vxor.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vxor.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vxor.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vxor.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vxor_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vxor.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vxor.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vxor.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vxor.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vxor_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vxor.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vxor.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vxor.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vxor.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vxor_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vxor.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vxor.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vxor.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vxor.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vxor_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vxor.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vxor.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vxor.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vxor.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vxor_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vxor.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vxor.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vxor.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vxor.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vxor_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vxor.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vxor.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vxor.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vxor.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vxor_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vxor.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vxor.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vxor.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vxor.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vxor_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vxor.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vxor.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vxor_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vxor.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vxor.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vxor.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vxor_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vxor.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vxor.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vxor.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vxor.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vxor_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vxor.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vxor.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vxor.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vxor.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vxor_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vxor.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vxor.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vxor.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vxor.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vxor_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vxor.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vxor.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vxor.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vxor.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vxor_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vxor.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vxor.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vxor.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vxor.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vxor_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vxor.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vxor.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vxor.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vxor.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vxor_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vxor.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vxor.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vxor.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vxor.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vxor_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vxor.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vxor.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vxor.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vxor.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vxor_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vxor.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vxor.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vxor.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vxor.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vxor_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vxor.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vxor.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vxor.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vxor.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vxor_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vxor.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vxor.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vxor.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vxor.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vxor_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vxor.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vxor.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vxor.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vxor.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vxor_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vxor.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vxor.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vxor.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vxor.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vxor_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vxor.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vxor.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vxor.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vxor.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vxor_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vxor.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vxor.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vxor_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vxor.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vxor.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vxor.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vxor.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vxor.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vxor.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vxor.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vxor.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vxor.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vxor.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vxor.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vxor.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vxor.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vxor.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vxor.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vxor.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vxor.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vxor.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vxor.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vxor.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vxor.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vxor.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vxor.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vxor.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vxor.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vxor.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vxor.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vxor.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vxor.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vxor.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vxor_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vxor.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vxor_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vxor_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vxor.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vxor.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsll.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vsll_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vsll.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsll.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsll.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsll.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vsll_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vsll.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsll.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsll.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsll.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vsll_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vsll.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsll.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsll.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsll.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vsll_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vsll.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsll.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsll.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsll.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vsll_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vsll.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsll.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsll.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsll.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vsll_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vsll.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsll.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsll.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsll.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vsll_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vsll.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsll.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsll.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsll.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vsll_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vsll.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsll.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsll.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsll.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vsll_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vsll.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsll.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsll.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsll.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vsll_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vsll.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsll.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsll.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsll.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vsll_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vsll.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsll.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsll.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsll.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vsll_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vsll.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsll.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsll.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsll.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vsll_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vsll.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsll.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsll.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsll.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vsll_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vsll.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsll.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsll.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsll.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vsll_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vsll.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsll.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsll_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsll.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsll.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsll.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vsll_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vsll.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsll.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsll.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsll.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vsll_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vsll.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsll.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsll.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsll.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vsll_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vsll.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsll.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsll.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsll.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vsll_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vsll.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsll.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsll.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsll.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vsll_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vsll.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsll.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsll.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsll.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vsll_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vsll.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsll.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsll.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsll.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vsll_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vsll.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsll.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsll.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsll.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vsll_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vsll.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsll.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsll.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsll.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vsll_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vsll.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsll.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsll.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsll.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vsll_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vsll.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsll.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsll.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsll.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vsll_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vsll.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsll.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsll.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsll.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vsll_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vsll.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsll.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsll.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsll.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vsll_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vsll.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsll.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsll.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsll.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vsll_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vsll.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsll.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsll.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsll.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vsll_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vsll.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsll.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsll_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsll.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsll.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vsll.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsll.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vsll.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsll.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vsll.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsll.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vsll.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsll.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vsll.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsll.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vsll.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsll.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vsll.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsll.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vsll.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsll.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vsll.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsll.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vsll.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsll.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vsll.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsll.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vsll.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsll.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vsll.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsll.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vsll.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsll.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vsll_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vsll.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vsll_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsll_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsll.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsll.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsrl.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vsrl_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vsrl.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsrl.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsrl.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsrl.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vsrl_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vsrl.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsrl.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsrl.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsrl.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vsrl_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vsrl.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsrl.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsrl.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsrl.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vsrl_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vsrl.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsrl.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsrl.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsrl.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vsrl_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vsrl.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsrl.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsrl.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsrl.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vsrl_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vsrl.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsrl.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsrl.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsrl.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vsrl_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vsrl.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsrl.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsrl.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsrl.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vsrl_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vsrl.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsrl.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsrl.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsrl.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vsrl_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vsrl.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsrl.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsrl.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsrl.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vsrl_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vsrl.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsrl.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsrl.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsrl.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vsrl_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vsrl.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsrl.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsrl.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vsrl_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsrl.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsrl.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsrl.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vsrl_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vsrl.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsrl.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsrl.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsrl.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vsrl_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vsrl.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsrl.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsrl.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsrl.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vsrl_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vsrl.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsrl.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsrl.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsrl.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vsrl_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vsrl.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsrl.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsrl.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsrl.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vsrl_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vsrl.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsrl.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsrl.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsrl.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vsrl_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vsrl.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsrl.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsrl.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsrl.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vsrl_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vsrl.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsrl.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsrl.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsrl.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vsrl_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vsrl.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsrl.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsrl.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsrl.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vsrl_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vsrl.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsrl.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsrl.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsrl.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vsrl_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vsrl.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsrl.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsrl.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsrl.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vsrl_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vsrl.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsrl.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsrl.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsrl.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vsrl_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vsrl.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsrl.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsrl.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsrl.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vsrl_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vsrl.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsrl.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsrl.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsrl.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vsrl_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vsrl.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsrl.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsrl.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vsrl_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsrl.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsrl.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsrl.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vsrl_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vsrl.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsrl.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsrl.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsrl.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vsrl_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vsrl.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsrl.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsrl.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsrl.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vsrl_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vsrl.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsrl.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsrl_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsrl.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vsrl.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsrl.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vsrl.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsrl.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vsrl.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsrl.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vsrl.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsrl.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vsrl.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsrl.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vsrl.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsrl.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vsrl.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsrl.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vsrl.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsrl.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vsrl.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsrl.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vsrl.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsrl.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vsrl.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsrl.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsrl.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vsrl.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsrl.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vsrl.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsrl.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vsrl_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vsrl.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vsrl_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsrl_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsrl.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsra.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vsra_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vsra.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsra.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsra.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsra.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vsra_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vsra.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsra.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsra.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsra.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vsra_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vsra.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsra.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsra.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsra.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vsra_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vsra.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsra.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsra.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsra.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vsra_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vsra.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsra.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsra.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsra.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vsra_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vsra.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsra.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsra.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsra.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vsra_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vsra.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsra.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsra.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsra.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vsra_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vsra.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsra.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsra.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsra.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vsra_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vsra.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsra.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsra.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsra.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vsra_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vsra.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsra.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsra.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsra.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vsra_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vsra.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsra.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsra.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsra.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vsra_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vsra.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsra.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsra.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsra.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vsra_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vsra.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsra.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsra.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsra.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vsra_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vsra.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsra.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsra.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsra.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vsra_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vsra.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsra.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsra_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsra.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsra.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vsra_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vsra.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsra.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsra.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsra.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vsra_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vsra.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsra.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsra.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsra.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vsra_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vsra.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsra.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsra.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsra.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vsra_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vsra.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsra.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsra.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsra.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vsra_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vsra.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsra.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsra.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsra.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vsra_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vsra.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsra.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsra.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsra.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vsra_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vsra.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsra.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsra.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsra.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vsra_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vsra.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsra.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsra.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsra.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vsra_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vsra.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsra.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsra.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsra.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vsra_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vsra.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsra.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsra.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsra.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vsra_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vsra.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsra.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsra.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsra.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vsra_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vsra.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsra.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsra.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsra.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vsra_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vsra.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsra.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsra.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsra.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vsra_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vsra.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsra.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsra.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsra.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vsra_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vsra.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsra.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsra_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsra.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vsra.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsra.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vsra.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsra.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vsra.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsra.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vsra.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsra.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vsra.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsra.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vsra.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsra.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vsra.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsra.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vsra.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsra.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vsra.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsra.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vsra.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsra.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vsra.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsra.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vsra.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsra.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vsra.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsra.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vsra.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsra.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vsra_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vsra.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vsra_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsra_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsra.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vnsrl.nxv8i8.nxv8i16.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vnsrl_wv_nxv8i8_nxv8i16_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wv_nxv8i8_nxv8i16_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vnsrl.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vnsrl.nxv8i8.nxv8i16.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vnsrl.mask.nxv8i8.nxv8i16.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnsrl_mask_wv_nxv8i8_nxv8i16_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wv_nxv8i8_nxv8i16_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vnsrl.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vnsrl.mask.nxv8i8.nxv8i16.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vnsrl.nxv16i8.nxv16i16.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vnsrl_wv_nxv16i8_nxv16i16_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wv_nxv16i8_nxv16i16_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vnsrl.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vnsrl.nxv16i8.nxv16i16.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vnsrl.mask.nxv16i8.nxv16i16.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vnsrl_mask_wv_nxv16i8_nxv16i16_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wv_nxv16i8_nxv16i16_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vnsrl.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vnsrl.mask.nxv16i8.nxv16i16.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vnsrl.nxv32i8.nxv32i16.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vnsrl_wv_nxv32i8_nxv32i16_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wv_nxv32i8_nxv32i16_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vnsrl.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vnsrl.nxv32i8.nxv32i16.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vnsrl.mask.nxv32i8.nxv32i16.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vnsrl_mask_wv_nxv32i8_nxv32i16_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wv_nxv32i8_nxv32i16_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vnsrl.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vnsrl.mask.nxv32i8.nxv32i16.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vnsrl.nxv4i16.nxv4i32.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vnsrl_wv_nxv4i16_nxv4i32_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wv_nxv4i16_nxv4i32_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vnsrl.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vnsrl.nxv4i16.nxv4i32.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vnsrl.mask.nxv4i16.nxv4i32.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vnsrl_mask_wv_nxv4i16_nxv4i32_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wv_nxv4i16_nxv4i32_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vnsrl.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vnsrl.mask.nxv4i16.nxv4i32.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vnsrl.nxv8i16.nxv8i32.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vnsrl_wv_nxv8i16_nxv8i32_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wv_nxv8i16_nxv8i32_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vnsrl.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vnsrl.nxv8i16.nxv8i32.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vnsrl.mask.nxv8i16.nxv8i32.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnsrl_mask_wv_nxv8i16_nxv8i32_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wv_nxv8i16_nxv8i32_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vnsrl.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vnsrl.mask.nxv8i16.nxv8i32.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vnsrl.nxv16i16.nxv16i32.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vnsrl_wv_nxv16i16_nxv16i32_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wv_nxv16i16_nxv16i32_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vnsrl.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vnsrl.nxv16i16.nxv16i32.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vnsrl.mask.nxv16i16.nxv16i32.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vnsrl_mask_wv_nxv16i16_nxv16i32_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wv_nxv16i16_nxv16i32_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vnsrl.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vnsrl.mask.nxv16i16.nxv16i32.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vnsrl.nxv2i32.nxv2i64.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vnsrl_wv_nxv2i32_nxv2i64_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wv_nxv2i32_nxv2i64_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vnsrl.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vnsrl.nxv2i32.nxv2i64.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vnsrl.mask.nxv2i32.nxv2i64.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vnsrl_mask_wv_nxv2i32_nxv2i64_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wv_nxv2i32_nxv2i64_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vnsrl.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vnsrl.mask.nxv2i32.nxv2i64.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vnsrl.nxv4i32.nxv4i64.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vnsrl_wv_nxv4i32_nxv4i64_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wv_nxv4i32_nxv4i64_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vnsrl.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vnsrl.nxv4i32.nxv4i64.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vnsrl.mask.nxv4i32.nxv4i64.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vnsrl_mask_wv_nxv4i32_nxv4i64_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wv_nxv4i32_nxv4i64_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vnsrl.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vnsrl.mask.nxv4i32.nxv4i64.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vnsrl.nxv8i32.nxv8i64.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vnsrl_wv_nxv8i32_nxv8i64_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wv_nxv8i32_nxv8i64_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vnsrl.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vnsrl.nxv8i32.nxv8i64.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vnsrl.mask.nxv8i32.nxv8i64.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnsrl_mask_wv_nxv8i32_nxv8i64_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wv_nxv8i32_nxv8i64_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vnsrl.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vnsrl.mask.nxv8i32.nxv8i64.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vnsrl.nxv8i8.nxv8i16.i8(
  <vscale x 8 x i16>,
  i8,
  i64);

define void @intrinsic_vnsrl_wx_nxv8i8_nxv8i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wx_nxv8i8_nxv8i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vnsrl.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vnsrl.nxv8i8.nxv8i16.i8(
    <vscale x 8 x i16> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vnsrl.mask.nxv8i8.nxv8i16.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i16>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnsrl_mask_wx_nxv8i8_nxv8i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wx_nxv8i8_nxv8i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vnsrl.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vnsrl.mask.nxv8i8.nxv8i16.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i16> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vnsrl.nxv16i8.nxv16i16.i8(
  <vscale x 16 x i16>,
  i8,
  i64);

define void @intrinsic_vnsrl_wx_nxv16i8_nxv16i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wx_nxv16i8_nxv16i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vnsrl.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vnsrl.nxv16i8.nxv16i16.i8(
    <vscale x 16 x i16> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vnsrl.mask.nxv16i8.nxv16i16.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i16>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vnsrl_mask_wx_nxv16i8_nxv16i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wx_nxv16i8_nxv16i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vnsrl.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vnsrl.mask.nxv16i8.nxv16i16.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i16> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vnsrl.nxv32i8.nxv32i16.i8(
  <vscale x 32 x i16>,
  i8,
  i64);

define void @intrinsic_vnsrl_wx_nxv32i8_nxv32i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wx_nxv32i8_nxv32i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vnsrl.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vnsrl.nxv32i8.nxv32i16.i8(
    <vscale x 32 x i16> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vnsrl.mask.nxv32i8.nxv32i16.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i16>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vnsrl_mask_wx_nxv32i8_nxv32i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wx_nxv32i8_nxv32i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vnsrl.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vnsrl.mask.nxv32i8.nxv32i16.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i16> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vnsrl.nxv4i16.nxv4i32.i16(
  <vscale x 4 x i32>,
  i16,
  i64);

define void @intrinsic_vnsrl_wx_nxv4i16_nxv4i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wx_nxv4i16_nxv4i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vnsrl.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vnsrl.nxv4i16.nxv4i32.i16(
    <vscale x 4 x i32> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vnsrl.mask.nxv4i16.nxv4i32.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i32>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vnsrl_mask_wx_nxv4i16_nxv4i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wx_nxv4i16_nxv4i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vnsrl.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vnsrl.mask.nxv4i16.nxv4i32.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i32> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vnsrl.nxv8i16.nxv8i32.i16(
  <vscale x 8 x i32>,
  i16,
  i64);

define void @intrinsic_vnsrl_wx_nxv8i16_nxv8i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wx_nxv8i16_nxv8i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vnsrl.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vnsrl.nxv8i16.nxv8i32.i16(
    <vscale x 8 x i32> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vnsrl.mask.nxv8i16.nxv8i32.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i32>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnsrl_mask_wx_nxv8i16_nxv8i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wx_nxv8i16_nxv8i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vnsrl.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vnsrl.mask.nxv8i16.nxv8i32.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i32> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vnsrl.nxv16i16.nxv16i32.i16(
  <vscale x 16 x i32>,
  i16,
  i64);

define void @intrinsic_vnsrl_wx_nxv16i16_nxv16i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wx_nxv16i16_nxv16i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vnsrl.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vnsrl.nxv16i16.nxv16i32.i16(
    <vscale x 16 x i32> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vnsrl.mask.nxv16i16.nxv16i32.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i32>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vnsrl_mask_wx_nxv16i16_nxv16i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wx_nxv16i16_nxv16i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vnsrl.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vnsrl.mask.nxv16i16.nxv16i32.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i32> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vnsrl.nxv2i32.nxv2i64.i32(
  <vscale x 2 x i64>,
  i32,
  i64);

define void @intrinsic_vnsrl_wx_nxv2i32_nxv2i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wx_nxv2i32_nxv2i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vnsrl.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vnsrl.nxv2i32.nxv2i64.i32(
    <vscale x 2 x i64> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vnsrl.mask.nxv2i32.nxv2i64.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i64>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vnsrl_mask_wx_nxv2i32_nxv2i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wx_nxv2i32_nxv2i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vnsrl.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vnsrl.mask.nxv2i32.nxv2i64.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i64> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vnsrl.nxv4i32.nxv4i64.i32(
  <vscale x 4 x i64>,
  i32,
  i64);

define void @intrinsic_vnsrl_wx_nxv4i32_nxv4i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wx_nxv4i32_nxv4i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vnsrl.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vnsrl.nxv4i32.nxv4i64.i32(
    <vscale x 4 x i64> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vnsrl.mask.nxv4i32.nxv4i64.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i64>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vnsrl_mask_wx_nxv4i32_nxv4i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wx_nxv4i32_nxv4i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vnsrl.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vnsrl.mask.nxv4i32.nxv4i64.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i64> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vnsrl.nxv8i32.nxv8i64.i32(
  <vscale x 8 x i64>,
  i32,
  i64);

define void @intrinsic_vnsrl_wx_nxv8i32_nxv8i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wx_nxv8i32_nxv8i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vnsrl.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vnsrl.nxv8i32.nxv8i64.i32(
    <vscale x 8 x i64> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vnsrl.mask.nxv8i32.nxv8i64.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i64>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnsrl_mask_wx_nxv8i32_nxv8i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wx_nxv8i32_nxv8i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vnsrl.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vnsrl.mask.nxv8i32.nxv8i64.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i64> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vnsrl_wi_nxv8i8_nxv8i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wi_nxv8i8_nxv8i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vnsrl.wi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vnsrl.nxv8i8.nxv8i16.i8(
    <vscale x 8 x i16> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vnsrl_mask_wi_nxv8i8_nxv8i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wi_nxv8i8_nxv8i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vnsrl.wi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vnsrl.mask.nxv8i8.nxv8i16.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i16> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vnsrl_wi_nxv16i8_nxv16i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wi_nxv16i8_nxv16i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vnsrl.wi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vnsrl.nxv16i8.nxv16i16.i8(
    <vscale x 16 x i16> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vnsrl_mask_wi_nxv16i8_nxv16i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wi_nxv16i8_nxv16i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vnsrl.wi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vnsrl.mask.nxv16i8.nxv16i16.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i16> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vnsrl_wi_nxv32i8_nxv32i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wi_nxv32i8_nxv32i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vnsrl.wi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vnsrl.nxv32i8.nxv32i16.i8(
    <vscale x 32 x i16> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vnsrl_mask_wi_nxv32i8_nxv32i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wi_nxv32i8_nxv32i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vnsrl.wi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vnsrl.mask.nxv32i8.nxv32i16.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i16> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vnsrl_wi_nxv4i16_nxv4i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wi_nxv4i16_nxv4i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vnsrl.wi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vnsrl.nxv4i16.nxv4i32.i16(
    <vscale x 4 x i32> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vnsrl_mask_wi_nxv4i16_nxv4i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wi_nxv4i16_nxv4i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vnsrl.wi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vnsrl.mask.nxv4i16.nxv4i32.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i32> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vnsrl_wi_nxv8i16_nxv8i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wi_nxv8i16_nxv8i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vnsrl.wi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vnsrl.nxv8i16.nxv8i32.i16(
    <vscale x 8 x i32> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vnsrl_mask_wi_nxv8i16_nxv8i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wi_nxv8i16_nxv8i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vnsrl.wi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vnsrl.mask.nxv8i16.nxv8i32.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i32> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vnsrl_wi_nxv16i16_nxv16i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wi_nxv16i16_nxv16i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vnsrl.wi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vnsrl.nxv16i16.nxv16i32.i16(
    <vscale x 16 x i32> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vnsrl_mask_wi_nxv16i16_nxv16i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wi_nxv16i16_nxv16i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vnsrl.wi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vnsrl.mask.nxv16i16.nxv16i32.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i32> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vnsrl_wi_nxv2i32_nxv2i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wi_nxv2i32_nxv2i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vnsrl.wi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vnsrl.nxv2i32.nxv2i64.i32(
    <vscale x 2 x i64> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vnsrl_mask_wi_nxv2i32_nxv2i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wi_nxv2i32_nxv2i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vnsrl.wi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vnsrl.mask.nxv2i32.nxv2i64.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i64> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vnsrl_wi_nxv4i32_nxv4i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wi_nxv4i32_nxv4i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vnsrl.wi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vnsrl.nxv4i32.nxv4i64.i32(
    <vscale x 4 x i64> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vnsrl_mask_wi_nxv4i32_nxv4i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wi_nxv4i32_nxv4i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vnsrl.wi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vnsrl.mask.nxv4i32.nxv4i64.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i64> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vnsrl_wi_nxv8i32_nxv8i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_wi_nxv8i32_nxv8i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vnsrl.wi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vnsrl.nxv8i32.nxv8i64.i32(
    <vscale x 8 x i64> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vnsrl_mask_wi_nxv8i32_nxv8i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsrl_mask_wi_nxv8i32_nxv8i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vnsrl.wi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vnsrl.mask.nxv8i32.nxv8i64.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i64> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vnsra.nxv8i8.nxv8i16.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vnsra_wv_nxv8i8_nxv8i16_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wv_nxv8i8_nxv8i16_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vnsra.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vnsra.nxv8i8.nxv8i16.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vnsra.mask.nxv8i8.nxv8i16.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnsra_mask_wv_nxv8i8_nxv8i16_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wv_nxv8i8_nxv8i16_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vnsra.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vnsra.mask.nxv8i8.nxv8i16.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vnsra.nxv16i8.nxv16i16.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vnsra_wv_nxv16i8_nxv16i16_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wv_nxv16i8_nxv16i16_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vnsra.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vnsra.nxv16i8.nxv16i16.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vnsra.mask.nxv16i8.nxv16i16.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vnsra_mask_wv_nxv16i8_nxv16i16_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wv_nxv16i8_nxv16i16_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vnsra.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vnsra.mask.nxv16i8.nxv16i16.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vnsra.nxv32i8.nxv32i16.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vnsra_wv_nxv32i8_nxv32i16_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wv_nxv32i8_nxv32i16_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vnsra.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vnsra.nxv32i8.nxv32i16.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vnsra.mask.nxv32i8.nxv32i16.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vnsra_mask_wv_nxv32i8_nxv32i16_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wv_nxv32i8_nxv32i16_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vnsra.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vnsra.mask.nxv32i8.nxv32i16.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vnsra.nxv4i16.nxv4i32.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vnsra_wv_nxv4i16_nxv4i32_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wv_nxv4i16_nxv4i32_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vnsra.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vnsra.nxv4i16.nxv4i32.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vnsra.mask.nxv4i16.nxv4i32.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vnsra_mask_wv_nxv4i16_nxv4i32_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wv_nxv4i16_nxv4i32_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vnsra.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vnsra.mask.nxv4i16.nxv4i32.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vnsra.nxv8i16.nxv8i32.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vnsra_wv_nxv8i16_nxv8i32_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wv_nxv8i16_nxv8i32_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vnsra.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vnsra.nxv8i16.nxv8i32.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vnsra.mask.nxv8i16.nxv8i32.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnsra_mask_wv_nxv8i16_nxv8i32_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wv_nxv8i16_nxv8i32_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vnsra.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vnsra.mask.nxv8i16.nxv8i32.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vnsra.nxv16i16.nxv16i32.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vnsra_wv_nxv16i16_nxv16i32_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wv_nxv16i16_nxv16i32_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vnsra.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vnsra.nxv16i16.nxv16i32.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vnsra.mask.nxv16i16.nxv16i32.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vnsra_mask_wv_nxv16i16_nxv16i32_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wv_nxv16i16_nxv16i32_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vnsra.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vnsra.mask.nxv16i16.nxv16i32.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vnsra.nxv2i32.nxv2i64.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vnsra_wv_nxv2i32_nxv2i64_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wv_nxv2i32_nxv2i64_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vnsra.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vnsra.nxv2i32.nxv2i64.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vnsra.mask.nxv2i32.nxv2i64.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vnsra_mask_wv_nxv2i32_nxv2i64_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wv_nxv2i32_nxv2i64_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vnsra.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vnsra.mask.nxv2i32.nxv2i64.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vnsra.nxv4i32.nxv4i64.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vnsra_wv_nxv4i32_nxv4i64_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wv_nxv4i32_nxv4i64_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vnsra.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vnsra.nxv4i32.nxv4i64.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vnsra.mask.nxv4i32.nxv4i64.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vnsra_mask_wv_nxv4i32_nxv4i64_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wv_nxv4i32_nxv4i64_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vnsra.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vnsra.mask.nxv4i32.nxv4i64.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vnsra.nxv8i32.nxv8i64.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vnsra_wv_nxv8i32_nxv8i64_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wv_nxv8i32_nxv8i64_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vnsra.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vnsra.nxv8i32.nxv8i64.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vnsra.mask.nxv8i32.nxv8i64.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnsra_mask_wv_nxv8i32_nxv8i64_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wv_nxv8i32_nxv8i64_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vnsra.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vnsra.mask.nxv8i32.nxv8i64.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vnsra.nxv8i8.nxv8i16.i8(
  <vscale x 8 x i16>,
  i8,
  i64);

define void @intrinsic_vnsra_wx_nxv8i8_nxv8i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wx_nxv8i8_nxv8i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vnsra.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vnsra.nxv8i8.nxv8i16.i8(
    <vscale x 8 x i16> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vnsra.mask.nxv8i8.nxv8i16.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i16>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnsra_mask_wx_nxv8i8_nxv8i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wx_nxv8i8_nxv8i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vnsra.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vnsra.mask.nxv8i8.nxv8i16.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i16> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vnsra.nxv16i8.nxv16i16.i8(
  <vscale x 16 x i16>,
  i8,
  i64);

define void @intrinsic_vnsra_wx_nxv16i8_nxv16i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wx_nxv16i8_nxv16i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vnsra.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vnsra.nxv16i8.nxv16i16.i8(
    <vscale x 16 x i16> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vnsra.mask.nxv16i8.nxv16i16.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i16>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vnsra_mask_wx_nxv16i8_nxv16i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wx_nxv16i8_nxv16i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vnsra.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vnsra.mask.nxv16i8.nxv16i16.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i16> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vnsra.nxv32i8.nxv32i16.i8(
  <vscale x 32 x i16>,
  i8,
  i64);

define void @intrinsic_vnsra_wx_nxv32i8_nxv32i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wx_nxv32i8_nxv32i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vnsra.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vnsra.nxv32i8.nxv32i16.i8(
    <vscale x 32 x i16> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vnsra.mask.nxv32i8.nxv32i16.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i16>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vnsra_mask_wx_nxv32i8_nxv32i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wx_nxv32i8_nxv32i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vnsra.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vnsra.mask.nxv32i8.nxv32i16.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i16> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vnsra.nxv4i16.nxv4i32.i16(
  <vscale x 4 x i32>,
  i16,
  i64);

define void @intrinsic_vnsra_wx_nxv4i16_nxv4i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wx_nxv4i16_nxv4i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vnsra.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vnsra.nxv4i16.nxv4i32.i16(
    <vscale x 4 x i32> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vnsra.mask.nxv4i16.nxv4i32.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i32>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vnsra_mask_wx_nxv4i16_nxv4i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wx_nxv4i16_nxv4i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vnsra.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vnsra.mask.nxv4i16.nxv4i32.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i32> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vnsra.nxv8i16.nxv8i32.i16(
  <vscale x 8 x i32>,
  i16,
  i64);

define void @intrinsic_vnsra_wx_nxv8i16_nxv8i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wx_nxv8i16_nxv8i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vnsra.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vnsra.nxv8i16.nxv8i32.i16(
    <vscale x 8 x i32> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vnsra.mask.nxv8i16.nxv8i32.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i32>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnsra_mask_wx_nxv8i16_nxv8i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wx_nxv8i16_nxv8i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vnsra.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vnsra.mask.nxv8i16.nxv8i32.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i32> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vnsra.nxv16i16.nxv16i32.i16(
  <vscale x 16 x i32>,
  i16,
  i64);

define void @intrinsic_vnsra_wx_nxv16i16_nxv16i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wx_nxv16i16_nxv16i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vnsra.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vnsra.nxv16i16.nxv16i32.i16(
    <vscale x 16 x i32> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vnsra.mask.nxv16i16.nxv16i32.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i32>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vnsra_mask_wx_nxv16i16_nxv16i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wx_nxv16i16_nxv16i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vnsra.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vnsra.mask.nxv16i16.nxv16i32.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i32> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vnsra.nxv2i32.nxv2i64.i32(
  <vscale x 2 x i64>,
  i32,
  i64);

define void @intrinsic_vnsra_wx_nxv2i32_nxv2i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wx_nxv2i32_nxv2i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vnsra.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vnsra.nxv2i32.nxv2i64.i32(
    <vscale x 2 x i64> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vnsra.mask.nxv2i32.nxv2i64.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i64>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vnsra_mask_wx_nxv2i32_nxv2i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wx_nxv2i32_nxv2i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vnsra.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vnsra.mask.nxv2i32.nxv2i64.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i64> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vnsra.nxv4i32.nxv4i64.i32(
  <vscale x 4 x i64>,
  i32,
  i64);

define void @intrinsic_vnsra_wx_nxv4i32_nxv4i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wx_nxv4i32_nxv4i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vnsra.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vnsra.nxv4i32.nxv4i64.i32(
    <vscale x 4 x i64> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vnsra.mask.nxv4i32.nxv4i64.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i64>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vnsra_mask_wx_nxv4i32_nxv4i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wx_nxv4i32_nxv4i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vnsra.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vnsra.mask.nxv4i32.nxv4i64.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i64> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vnsra.nxv8i32.nxv8i64.i32(
  <vscale x 8 x i64>,
  i32,
  i64);

define void @intrinsic_vnsra_wx_nxv8i32_nxv8i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wx_nxv8i32_nxv8i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vnsra.wx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vnsra.nxv8i32.nxv8i64.i32(
    <vscale x 8 x i64> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vnsra.mask.nxv8i32.nxv8i64.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i64>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnsra_mask_wx_nxv8i32_nxv8i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wx_nxv8i32_nxv8i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vnsra.wx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vnsra.mask.nxv8i32.nxv8i64.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i64> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vnsra_wi_nxv8i8_nxv8i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wi_nxv8i8_nxv8i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vnsra.wi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vnsra.nxv8i8.nxv8i16.i8(
    <vscale x 8 x i16> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vnsra_mask_wi_nxv8i8_nxv8i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wi_nxv8i8_nxv8i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vnsra.wi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vnsra.mask.nxv8i8.nxv8i16.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i16> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vnsra_wi_nxv16i8_nxv16i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wi_nxv16i8_nxv16i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vnsra.wi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vnsra.nxv16i8.nxv16i16.i8(
    <vscale x 16 x i16> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vnsra_mask_wi_nxv16i8_nxv16i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wi_nxv16i8_nxv16i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vnsra.wi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vnsra.mask.nxv16i8.nxv16i16.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i16> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vnsra_wi_nxv32i8_nxv32i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wi_nxv32i8_nxv32i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vnsra.wi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vnsra.nxv32i8.nxv32i16.i8(
    <vscale x 32 x i16> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vnsra_mask_wi_nxv32i8_nxv32i16_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wi_nxv32i8_nxv32i16_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vnsra.wi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vnsra.mask.nxv32i8.nxv32i16.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i16> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vnsra_wi_nxv4i16_nxv4i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wi_nxv4i16_nxv4i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vnsra.wi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vnsra.nxv4i16.nxv4i32.i16(
    <vscale x 4 x i32> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vnsra_mask_wi_nxv4i16_nxv4i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wi_nxv4i16_nxv4i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vnsra.wi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vnsra.mask.nxv4i16.nxv4i32.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i32> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vnsra_wi_nxv8i16_nxv8i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wi_nxv8i16_nxv8i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vnsra.wi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vnsra.nxv8i16.nxv8i32.i16(
    <vscale x 8 x i32> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vnsra_mask_wi_nxv8i16_nxv8i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wi_nxv8i16_nxv8i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vnsra.wi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vnsra.mask.nxv8i16.nxv8i32.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i32> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vnsra_wi_nxv16i16_nxv16i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wi_nxv16i16_nxv16i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vnsra.wi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vnsra.nxv16i16.nxv16i32.i16(
    <vscale x 16 x i32> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vnsra_mask_wi_nxv16i16_nxv16i32_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wi_nxv16i16_nxv16i32_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vnsra.wi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vnsra.mask.nxv16i16.nxv16i32.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i32> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vnsra_wi_nxv2i32_nxv2i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wi_nxv2i32_nxv2i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vnsra.wi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vnsra.nxv2i32.nxv2i64.i32(
    <vscale x 2 x i64> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vnsra_mask_wi_nxv2i32_nxv2i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wi_nxv2i32_nxv2i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vnsra.wi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vnsra.mask.nxv2i32.nxv2i64.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i64> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vnsra_wi_nxv4i32_nxv4i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wi_nxv4i32_nxv4i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vnsra.wi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vnsra.nxv4i32.nxv4i64.i32(
    <vscale x 4 x i64> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vnsra_mask_wi_nxv4i32_nxv4i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wi_nxv4i32_nxv4i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vnsra.wi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vnsra.mask.nxv4i32.nxv4i64.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i64> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vnsra_wi_nxv8i32_nxv8i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_wi_nxv8i32_nxv8i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vnsra.wi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vnsra.nxv8i32.nxv8i64.i32(
    <vscale x 8 x i64> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vnsra_mask_wi_nxv8i32_nxv8i64_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnsra_mask_wi_nxv8i32_nxv8i64_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vnsra.wi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vnsra.mask.nxv8i32.nxv8i64.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i64> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmseq.nxv8i1.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmseq_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.nxv8i1.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmseq.mask.nxv8i1.nxv8i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.mask.nxv8i1.nxv8i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmseq.nxv16i1.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmseq_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmseq.nxv16i1.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmseq.mask.nxv16i1.nxv16i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmseq.mask.nxv16i1.nxv16i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmseq.nxv32i1.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmseq_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i1> @llvm.epi.vmseq.nxv32i1.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmseq.mask.nxv32i1.nxv32i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmseq.mask.nxv32i1.nxv32i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmseq.nxv4i1.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmseq_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmseq.nxv4i1.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmseq.mask.nxv4i1.nxv4i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmseq.mask.nxv4i1.nxv4i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmseq.nxv8i1.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmseq_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.nxv8i1.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmseq.mask.nxv8i1.nxv8i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.mask.nxv8i1.nxv8i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmseq.nxv16i1.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmseq_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmseq.nxv16i1.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmseq.mask.nxv16i1.nxv16i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmseq.mask.nxv16i1.nxv16i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmseq.nxv32i1.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmseq_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i1> @llvm.epi.vmseq.nxv32i1.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmseq.mask.nxv32i1.nxv32i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmseq.mask.nxv32i1.nxv32i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmseq.nxv2i1.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmseq_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmseq.nxv2i1.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmseq.mask.nxv2i1.nxv2i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmseq.mask.nxv2i1.nxv2i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmseq.nxv4i1.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmseq_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmseq.nxv4i1.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmseq.mask.nxv4i1.nxv4i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmseq.mask.nxv4i1.nxv4i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmseq.nxv8i1.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmseq_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.nxv8i1.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmseq.mask.nxv8i1.nxv8i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.mask.nxv8i1.nxv8i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmseq.nxv16i1.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmseq_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmseq.nxv16i1.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmseq.mask.nxv16i1.nxv16i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmseq.mask.nxv16i1.nxv16i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmseq.nxv1i1.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmseq_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmseq.nxv1i1.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmseq.mask.nxv1i1.nxv1i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmseq.mask.nxv1i1.nxv1i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmseq.nxv2i1.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmseq_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmseq.nxv2i1.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmseq.mask.nxv2i1.nxv2i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmseq.mask.nxv2i1.nxv2i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmseq.nxv4i1.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmseq_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmseq.nxv4i1.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmseq.mask.nxv4i1.nxv4i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmseq.mask.nxv4i1.nxv4i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmseq.nxv8i1.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmseq_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.nxv8i1.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmseq.mask.nxv8i1.nxv8i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmseq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.mask.nxv8i1.nxv8i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmseq.nxv8i1.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmseq_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmseq.mask.nxv8i1.i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmseq.nxv16i1.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmseq_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmseq.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmseq.mask.nxv16i1.i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmseq.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmseq.nxv32i1.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmseq_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vmseq.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmseq.mask.nxv32i1.i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmseq.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmseq.nxv4i1.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmseq_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmseq.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmseq.mask.nxv4i1.i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmseq.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmseq.nxv8i1.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmseq_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmseq.mask.nxv8i1.i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmseq.nxv16i1.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmseq_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmseq.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmseq.mask.nxv16i1.i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmseq.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmseq.nxv32i1.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmseq_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vmseq.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmseq.mask.nxv32i1.i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmseq.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmseq.nxv2i1.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmseq_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vmseq.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmseq.mask.nxv2i1.i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmseq.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmseq.nxv4i1.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmseq_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmseq.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmseq.mask.nxv4i1.i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmseq.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmseq.nxv8i1.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmseq_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmseq.mask.nxv8i1.i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmseq.nxv16i1.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmseq_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmseq.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmseq.mask.nxv16i1.i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmseq.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmseq.nxv1i1.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmseq_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i1> @llvm.epi.vmseq.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmseq.mask.nxv1i1.i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmseq.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmseq.nxv2i1.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmseq_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vmseq.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmseq.mask.nxv2i1.i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmseq.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmseq.nxv4i1.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmseq_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmseq.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmseq.mask.nxv4i1.i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmseq.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmseq.nxv8i1.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmseq_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmseq.mask.nxv8i1.i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmseq_mask_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmseq.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmseq_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmseq_mask_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmseq_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vmseq.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vmseq_mask_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmseq.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmseq_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vmseq.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vmseq_mask_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmseq.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vmseq_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vmseq.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vmseq_mask_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmseq.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmseq_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmseq_mask_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmseq_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vmseq.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vmseq_mask_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmseq.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmseq_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vmseq.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vmseq_mask_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmseq.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vmseq_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vmseq.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vmseq_mask_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmseq.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vmseq_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vmseq.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vmseq_mask_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmseq.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmseq_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmseq_mask_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmseq_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vmseq.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vmseq_mask_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmseq.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmseq_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x i1> @llvm.epi.vmseq.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

define void @intrinsic_vmseq_mask_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmseq.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


define void @intrinsic_vmseq_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vmseq.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vmseq_mask_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmseq.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vmseq_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vmseq.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vmseq_mask_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmseq.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmseq_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmseq_mask_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmseq_mask_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmseq.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmseq.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsne.nxv8i1.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmsne_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.nxv8i1.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsne.mask.nxv8i1.nxv8i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.mask.nxv8i1.nxv8i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsne.nxv16i1.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmsne_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmsne.nxv16i1.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsne.mask.nxv16i1.nxv16i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsne.mask.nxv16i1.nxv16i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsne.nxv32i1.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmsne_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i1> @llvm.epi.vmsne.nxv32i1.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsne.mask.nxv32i1.nxv32i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsne.mask.nxv32i1.nxv32i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsne.nxv4i1.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmsne_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmsne.nxv4i1.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsne.mask.nxv4i1.nxv4i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsne.mask.nxv4i1.nxv4i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsne.nxv8i1.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmsne_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.nxv8i1.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsne.mask.nxv8i1.nxv8i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.mask.nxv8i1.nxv8i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsne.nxv16i1.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmsne_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmsne.nxv16i1.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsne.mask.nxv16i1.nxv16i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsne.mask.nxv16i1.nxv16i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsne.nxv32i1.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmsne_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i1> @llvm.epi.vmsne.nxv32i1.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsne.mask.nxv32i1.nxv32i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsne.mask.nxv32i1.nxv32i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsne.nxv2i1.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmsne_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmsne.nxv2i1.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsne.mask.nxv2i1.nxv2i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsne.mask.nxv2i1.nxv2i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsne.nxv4i1.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmsne_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmsne.nxv4i1.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsne.mask.nxv4i1.nxv4i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsne.mask.nxv4i1.nxv4i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsne.nxv8i1.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmsne_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.nxv8i1.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsne.mask.nxv8i1.nxv8i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.mask.nxv8i1.nxv8i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsne.nxv16i1.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmsne_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmsne.nxv16i1.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsne.mask.nxv16i1.nxv16i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsne.mask.nxv16i1.nxv16i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmsne.nxv1i1.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmsne_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmsne.nxv1i1.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmsne.mask.nxv1i1.nxv1i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmsne.mask.nxv1i1.nxv1i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsne.nxv2i1.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmsne_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmsne.nxv2i1.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsne.mask.nxv2i1.nxv2i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsne.mask.nxv2i1.nxv2i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsne.nxv4i1.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmsne_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmsne.nxv4i1.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsne.mask.nxv4i1.nxv4i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsne.mask.nxv4i1.nxv4i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsne.nxv8i1.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmsne_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.nxv8i1.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsne.mask.nxv8i1.nxv8i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.mask.nxv8i1.nxv8i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsne.nxv8i1.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmsne_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsne.mask.nxv8i1.i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsne.nxv16i1.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmsne_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsne.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsne.mask.nxv16i1.i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsne.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsne.nxv32i1.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmsne_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vmsne.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsne.mask.nxv32i1.i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsne.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsne.nxv4i1.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmsne_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsne.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsne.mask.nxv4i1.i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsne.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsne.nxv8i1.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmsne_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsne.mask.nxv8i1.i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsne.nxv16i1.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmsne_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsne.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsne.mask.nxv16i1.i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsne.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsne.nxv32i1.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmsne_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vmsne.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsne.mask.nxv32i1.i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsne.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsne.nxv2i1.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmsne_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vmsne.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsne.mask.nxv2i1.i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsne.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsne.nxv4i1.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmsne_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsne.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsne.mask.nxv4i1.i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsne.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsne.nxv8i1.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmsne_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsne.mask.nxv8i1.i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsne.nxv16i1.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmsne_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsne.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsne.mask.nxv16i1.i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsne.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmsne.nxv1i1.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmsne_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i1> @llvm.epi.vmsne.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmsne.mask.nxv1i1.i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmsne.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsne.nxv2i1.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmsne_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vmsne.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsne.mask.nxv2i1.i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsne.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsne.nxv4i1.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmsne_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsne.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsne.mask.nxv4i1.i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsne.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsne.nxv8i1.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmsne_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsne.mask.nxv8i1.i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsne_mask_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsne.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmsne_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmsne_mask_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmsne_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vmsne.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vmsne_mask_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsne.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmsne_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vmsne.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vmsne_mask_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsne.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vmsne_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vmsne.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vmsne_mask_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsne.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmsne_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmsne_mask_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmsne_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vmsne.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vmsne_mask_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsne.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmsne_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vmsne.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vmsne_mask_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsne.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vmsne_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vmsne.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vmsne_mask_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsne.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vmsne_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vmsne.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vmsne_mask_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsne.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmsne_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmsne_mask_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmsne_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vmsne.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vmsne_mask_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsne.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmsne_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x i1> @llvm.epi.vmsne.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

define void @intrinsic_vmsne_mask_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmsne.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


define void @intrinsic_vmsne_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vmsne.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vmsne_mask_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsne.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vmsne_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vmsne.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vmsne_mask_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsne.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmsne_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmsne_mask_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsne_mask_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsne.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsne.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsltu.nxv8i1.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmsltu_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsltu.nxv8i1.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsltu.mask.nxv8i1.nxv8i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsltu.mask.nxv8i1.nxv8i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsltu.nxv16i1.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmsltu_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmsltu.nxv16i1.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsltu.mask.nxv16i1.nxv16i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsltu.mask.nxv16i1.nxv16i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsltu.nxv32i1.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmsltu_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i1> @llvm.epi.vmsltu.nxv32i1.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsltu.mask.nxv32i1.nxv32i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsltu.mask.nxv32i1.nxv32i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsltu.nxv4i1.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmsltu_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmsltu.nxv4i1.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsltu.mask.nxv4i1.nxv4i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsltu.mask.nxv4i1.nxv4i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsltu.nxv8i1.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmsltu_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsltu.nxv8i1.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsltu.mask.nxv8i1.nxv8i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsltu.mask.nxv8i1.nxv8i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsltu.nxv16i1.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmsltu_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmsltu.nxv16i1.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsltu.mask.nxv16i1.nxv16i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsltu.mask.nxv16i1.nxv16i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsltu.nxv32i1.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmsltu_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i1> @llvm.epi.vmsltu.nxv32i1.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsltu.mask.nxv32i1.nxv32i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsltu.mask.nxv32i1.nxv32i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsltu.nxv2i1.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmsltu_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmsltu.nxv2i1.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsltu.mask.nxv2i1.nxv2i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsltu.mask.nxv2i1.nxv2i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsltu.nxv4i1.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmsltu_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmsltu.nxv4i1.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsltu.mask.nxv4i1.nxv4i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsltu.mask.nxv4i1.nxv4i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsltu.nxv8i1.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmsltu_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsltu.nxv8i1.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsltu.mask.nxv8i1.nxv8i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsltu.mask.nxv8i1.nxv8i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsltu.nxv16i1.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmsltu_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmsltu.nxv16i1.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsltu.mask.nxv16i1.nxv16i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsltu.mask.nxv16i1.nxv16i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmsltu.nxv1i1.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmsltu_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmsltu.nxv1i1.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmsltu.mask.nxv1i1.nxv1i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmsltu.mask.nxv1i1.nxv1i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsltu.nxv2i1.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmsltu_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmsltu.nxv2i1.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsltu.mask.nxv2i1.nxv2i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsltu.mask.nxv2i1.nxv2i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsltu.nxv4i1.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmsltu_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmsltu.nxv4i1.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsltu.mask.nxv4i1.nxv4i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsltu.mask.nxv4i1.nxv4i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsltu.nxv8i1.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmsltu_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsltu.nxv8i1.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsltu.mask.nxv8i1.nxv8i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsltu.mask.nxv8i1.nxv8i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsltu.nxv8i1.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmsltu_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsltu.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsltu.mask.nxv8i1.i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsltu.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsltu.nxv16i1.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmsltu_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsltu.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsltu.mask.nxv16i1.i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsltu.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsltu.nxv32i1.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmsltu_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vmsltu.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsltu.mask.nxv32i1.i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsltu.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsltu.nxv4i1.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmsltu_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsltu.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsltu.mask.nxv4i1.i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsltu.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsltu.nxv8i1.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmsltu_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsltu.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsltu.mask.nxv8i1.i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsltu.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsltu.nxv16i1.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmsltu_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsltu.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsltu.mask.nxv16i1.i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsltu.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsltu.nxv32i1.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmsltu_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vmsltu.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsltu.mask.nxv32i1.i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsltu.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsltu.nxv2i1.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmsltu_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vmsltu.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsltu.mask.nxv2i1.i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsltu.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsltu.nxv4i1.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmsltu_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsltu.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsltu.mask.nxv4i1.i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsltu.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsltu.nxv8i1.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmsltu_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsltu.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsltu.mask.nxv8i1.i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsltu.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsltu.nxv16i1.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmsltu_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsltu.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsltu.mask.nxv16i1.i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsltu.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmsltu.nxv1i1.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmsltu_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i1> @llvm.epi.vmsltu.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmsltu.mask.nxv1i1.i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmsltu.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsltu.nxv2i1.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmsltu_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vmsltu.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsltu.mask.nxv2i1.i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsltu.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsltu.nxv4i1.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmsltu_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsltu.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsltu.mask.nxv4i1.i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsltu.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsltu.nxv8i1.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmsltu_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsltu.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsltu.mask.nxv8i1.i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsltu_mask_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsltu_mask_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsltu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsltu.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmslt.nxv8i1.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmslt_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmslt.nxv8i1.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmslt.mask.nxv8i1.nxv8i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmslt.mask.nxv8i1.nxv8i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmslt.nxv16i1.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmslt_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmslt.nxv16i1.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmslt.mask.nxv16i1.nxv16i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmslt.mask.nxv16i1.nxv16i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmslt.nxv32i1.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmslt_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i1> @llvm.epi.vmslt.nxv32i1.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmslt.mask.nxv32i1.nxv32i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmslt.mask.nxv32i1.nxv32i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmslt.nxv4i1.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmslt_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmslt.nxv4i1.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmslt.mask.nxv4i1.nxv4i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmslt.mask.nxv4i1.nxv4i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmslt.nxv8i1.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmslt_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmslt.nxv8i1.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmslt.mask.nxv8i1.nxv8i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmslt.mask.nxv8i1.nxv8i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmslt.nxv16i1.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmslt_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmslt.nxv16i1.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmslt.mask.nxv16i1.nxv16i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmslt.mask.nxv16i1.nxv16i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmslt.nxv32i1.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmslt_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i1> @llvm.epi.vmslt.nxv32i1.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmslt.mask.nxv32i1.nxv32i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmslt.mask.nxv32i1.nxv32i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmslt.nxv2i1.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmslt_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmslt.nxv2i1.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmslt.mask.nxv2i1.nxv2i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmslt.mask.nxv2i1.nxv2i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmslt.nxv4i1.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmslt_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmslt.nxv4i1.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmslt.mask.nxv4i1.nxv4i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmslt.mask.nxv4i1.nxv4i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmslt.nxv8i1.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmslt_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmslt.nxv8i1.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmslt.mask.nxv8i1.nxv8i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmslt.mask.nxv8i1.nxv8i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmslt.nxv16i1.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmslt_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmslt.nxv16i1.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmslt.mask.nxv16i1.nxv16i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmslt.mask.nxv16i1.nxv16i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmslt.nxv1i1.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmslt_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmslt.nxv1i1.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmslt.mask.nxv1i1.nxv1i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmslt.mask.nxv1i1.nxv1i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmslt.nxv2i1.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmslt_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmslt.nxv2i1.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmslt.mask.nxv2i1.nxv2i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmslt.mask.nxv2i1.nxv2i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmslt.nxv4i1.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmslt_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmslt.nxv4i1.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmslt.mask.nxv4i1.nxv4i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmslt.mask.nxv4i1.nxv4i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmslt.nxv8i1.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmslt_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmslt.nxv8i1.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmslt.mask.nxv8i1.nxv8i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmslt.mask.nxv8i1.nxv8i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmslt.nxv8i1.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmslt_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmslt.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmslt.mask.nxv8i1.i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmslt.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmslt.nxv16i1.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmslt_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmslt.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmslt.mask.nxv16i1.i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmslt.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmslt.nxv32i1.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmslt_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vmslt.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmslt.mask.nxv32i1.i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmslt.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmslt.nxv4i1.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmslt_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmslt.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmslt.mask.nxv4i1.i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmslt.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmslt.nxv8i1.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmslt_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmslt.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmslt.mask.nxv8i1.i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmslt.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmslt.nxv16i1.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmslt_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmslt.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmslt.mask.nxv16i1.i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmslt.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmslt.nxv32i1.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmslt_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vmslt.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmslt.mask.nxv32i1.i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmslt.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmslt.nxv2i1.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmslt_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vmslt.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmslt.mask.nxv2i1.i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmslt.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmslt.nxv4i1.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmslt_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmslt.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmslt.mask.nxv4i1.i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmslt.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmslt.nxv8i1.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmslt_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmslt.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmslt.mask.nxv8i1.i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmslt.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmslt.nxv16i1.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmslt_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmslt.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmslt.mask.nxv16i1.i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmslt.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmslt.nxv1i1.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmslt_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i1> @llvm.epi.vmslt.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmslt.mask.nxv1i1.i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmslt.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmslt.nxv2i1.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmslt_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vmslt.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmslt.mask.nxv2i1.i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmslt.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmslt.nxv4i1.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmslt_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmslt.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmslt.mask.nxv4i1.i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmslt.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmslt.nxv8i1.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmslt_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmslt.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmslt.mask.nxv8i1.i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmslt_mask_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmslt_mask_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmslt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmslt.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsleu.nxv8i1.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmsleu_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.nxv8i1.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsleu.mask.nxv8i1.nxv8i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.mask.nxv8i1.nxv8i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsleu.nxv16i1.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmsleu_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmsleu.nxv16i1.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsleu.mask.nxv16i1.nxv16i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsleu.mask.nxv16i1.nxv16i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsleu.nxv32i1.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmsleu_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i1> @llvm.epi.vmsleu.nxv32i1.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsleu.mask.nxv32i1.nxv32i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsleu.mask.nxv32i1.nxv32i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsleu.nxv4i1.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmsleu_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmsleu.nxv4i1.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsleu.mask.nxv4i1.nxv4i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsleu.mask.nxv4i1.nxv4i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsleu.nxv8i1.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmsleu_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.nxv8i1.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsleu.mask.nxv8i1.nxv8i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.mask.nxv8i1.nxv8i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsleu.nxv16i1.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmsleu_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmsleu.nxv16i1.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsleu.mask.nxv16i1.nxv16i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsleu.mask.nxv16i1.nxv16i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsleu.nxv32i1.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmsleu_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i1> @llvm.epi.vmsleu.nxv32i1.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsleu.mask.nxv32i1.nxv32i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsleu.mask.nxv32i1.nxv32i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsleu.nxv2i1.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmsleu_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmsleu.nxv2i1.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsleu.mask.nxv2i1.nxv2i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsleu.mask.nxv2i1.nxv2i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsleu.nxv4i1.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmsleu_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmsleu.nxv4i1.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsleu.mask.nxv4i1.nxv4i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsleu.mask.nxv4i1.nxv4i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsleu.nxv8i1.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmsleu_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.nxv8i1.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsleu.mask.nxv8i1.nxv8i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.mask.nxv8i1.nxv8i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsleu.nxv16i1.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmsleu_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmsleu.nxv16i1.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsleu.mask.nxv16i1.nxv16i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsleu.mask.nxv16i1.nxv16i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmsleu.nxv1i1.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmsleu_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmsleu.nxv1i1.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmsleu.mask.nxv1i1.nxv1i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmsleu.mask.nxv1i1.nxv1i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsleu.nxv2i1.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmsleu_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmsleu.nxv2i1.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsleu.mask.nxv2i1.nxv2i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsleu.mask.nxv2i1.nxv2i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsleu.nxv4i1.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmsleu_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmsleu.nxv4i1.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsleu.mask.nxv4i1.nxv4i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsleu.mask.nxv4i1.nxv4i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsleu.nxv8i1.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmsleu_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.nxv8i1.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsleu.mask.nxv8i1.nxv8i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsleu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.mask.nxv8i1.nxv8i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsleu.nxv8i1.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmsleu_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsleu.mask.nxv8i1.i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsleu.nxv16i1.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmsleu_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsleu.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsleu.mask.nxv16i1.i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsleu.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsleu.nxv32i1.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmsleu_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vmsleu.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsleu.mask.nxv32i1.i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsleu.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsleu.nxv4i1.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmsleu_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsleu.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsleu.mask.nxv4i1.i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsleu.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsleu.nxv8i1.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmsleu_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsleu.mask.nxv8i1.i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsleu.nxv16i1.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmsleu_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsleu.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsleu.mask.nxv16i1.i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsleu.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsleu.nxv32i1.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmsleu_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vmsleu.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsleu.mask.nxv32i1.i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsleu.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsleu.nxv2i1.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmsleu_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vmsleu.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsleu.mask.nxv2i1.i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsleu.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsleu.nxv4i1.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmsleu_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsleu.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsleu.mask.nxv4i1.i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsleu.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsleu.nxv8i1.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmsleu_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsleu.mask.nxv8i1.i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsleu.nxv16i1.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmsleu_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsleu.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsleu.mask.nxv16i1.i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsleu.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmsleu.nxv1i1.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmsleu_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i1> @llvm.epi.vmsleu.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmsleu.mask.nxv1i1.i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmsleu.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsleu.nxv2i1.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmsleu_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vmsleu.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsleu.mask.nxv2i1.i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsleu.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsleu.nxv4i1.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmsleu_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsleu.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsleu.mask.nxv4i1.i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsleu.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsleu.nxv8i1.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmsleu_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsleu.mask.nxv8i1.i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsleu_mask_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsleu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmsleu_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmsleu_mask_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmsleu_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vmsleu.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vmsleu_mask_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsleu.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmsleu_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vmsleu.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vmsleu_mask_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsleu.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vmsleu_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vmsleu.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vmsleu_mask_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsleu.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmsleu_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmsleu_mask_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmsleu_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vmsleu.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vmsleu_mask_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsleu.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmsleu_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vmsleu.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vmsleu_mask_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsleu.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vmsleu_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vmsleu.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vmsleu_mask_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsleu.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vmsleu_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vmsleu.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vmsleu_mask_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsleu.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmsleu_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmsleu_mask_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmsleu_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vmsleu.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vmsleu_mask_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsleu.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmsleu_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x i1> @llvm.epi.vmsleu.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

define void @intrinsic_vmsleu_mask_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmsleu.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


define void @intrinsic_vmsleu_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vmsleu.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vmsleu_mask_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsleu.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vmsleu_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vmsleu.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vmsleu_mask_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsleu.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmsleu_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmsleu_mask_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsleu_mask_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsleu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsleu.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsle.nxv8i1.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmsle_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.nxv8i1.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsle.mask.nxv8i1.nxv8i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.mask.nxv8i1.nxv8i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsle.nxv16i1.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmsle_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmsle.nxv16i1.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsle.mask.nxv16i1.nxv16i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsle.mask.nxv16i1.nxv16i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsle.nxv32i1.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmsle_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i1> @llvm.epi.vmsle.nxv32i1.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsle.mask.nxv32i1.nxv32i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsle.mask.nxv32i1.nxv32i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsle.nxv4i1.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmsle_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmsle.nxv4i1.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsle.mask.nxv4i1.nxv4i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsle.mask.nxv4i1.nxv4i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsle.nxv8i1.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmsle_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.nxv8i1.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsle.mask.nxv8i1.nxv8i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.mask.nxv8i1.nxv8i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsle.nxv16i1.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmsle_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmsle.nxv16i1.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsle.mask.nxv16i1.nxv16i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsle.mask.nxv16i1.nxv16i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsle.nxv32i1.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmsle_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i1> @llvm.epi.vmsle.nxv32i1.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsle.mask.nxv32i1.nxv32i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsle.mask.nxv32i1.nxv32i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsle.nxv2i1.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmsle_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmsle.nxv2i1.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsle.mask.nxv2i1.nxv2i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsle.mask.nxv2i1.nxv2i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsle.nxv4i1.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmsle_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmsle.nxv4i1.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsle.mask.nxv4i1.nxv4i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsle.mask.nxv4i1.nxv4i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsle.nxv8i1.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmsle_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.nxv8i1.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsle.mask.nxv8i1.nxv8i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.mask.nxv8i1.nxv8i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsle.nxv16i1.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmsle_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmsle.nxv16i1.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsle.mask.nxv16i1.nxv16i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsle.mask.nxv16i1.nxv16i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmsle.nxv1i1.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmsle_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmsle.nxv1i1.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmsle.mask.nxv1i1.nxv1i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmsle.mask.nxv1i1.nxv1i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsle.nxv2i1.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmsle_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmsle.nxv2i1.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsle.mask.nxv2i1.nxv2i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsle.mask.nxv2i1.nxv2i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsle.nxv4i1.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmsle_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmsle.nxv4i1.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsle.mask.nxv4i1.nxv4i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsle.mask.nxv4i1.nxv4i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsle.nxv8i1.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmsle_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.nxv8i1.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsle.mask.nxv8i1.nxv8i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.mask.nxv8i1.nxv8i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsle.nxv8i1.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmsle_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsle.mask.nxv8i1.i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsle.nxv16i1.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmsle_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsle.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsle.mask.nxv16i1.i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsle.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsle.nxv32i1.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmsle_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vmsle.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsle.mask.nxv32i1.i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsle.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsle.nxv4i1.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmsle_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsle.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsle.mask.nxv4i1.i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsle.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsle.nxv8i1.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmsle_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsle.mask.nxv8i1.i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsle.nxv16i1.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmsle_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsle.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsle.mask.nxv16i1.i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsle.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsle.nxv32i1.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmsle_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vmsle.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsle.mask.nxv32i1.i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsle.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsle.nxv2i1.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmsle_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vmsle.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsle.mask.nxv2i1.i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsle.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsle.nxv4i1.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmsle_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsle.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsle.mask.nxv4i1.i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsle.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsle.nxv8i1.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmsle_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsle.mask.nxv8i1.i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsle.nxv16i1.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmsle_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsle.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsle.mask.nxv16i1.i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsle.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmsle.nxv1i1.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmsle_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i1> @llvm.epi.vmsle.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmsle.mask.nxv1i1.i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmsle.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsle.nxv2i1.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmsle_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vmsle.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsle.mask.nxv2i1.i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsle.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsle.nxv4i1.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmsle_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsle.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsle.mask.nxv4i1.i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsle.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsle.nxv8i1.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmsle_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsle.mask.nxv8i1.i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsle_mask_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsle.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmsle_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmsle_mask_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmsle_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vmsle.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vmsle_mask_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsle.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmsle_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vmsle.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vmsle_mask_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsle.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vmsle_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vmsle.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vmsle_mask_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsle.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmsle_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmsle_mask_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmsle_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vmsle.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vmsle_mask_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsle.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmsle_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vmsle.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vmsle_mask_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsle.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vmsle_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vmsle.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vmsle_mask_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsle.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vmsle_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vmsle.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vmsle_mask_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsle.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmsle_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmsle_mask_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmsle_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vmsle.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vmsle_mask_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsle.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmsle_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x i1> @llvm.epi.vmsle.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

define void @intrinsic_vmsle_mask_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmsle.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


define void @intrinsic_vmsle_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vmsle.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vmsle_mask_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsle.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vmsle_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vmsle.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vmsle_mask_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsle.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmsle_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmsle_mask_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsle_mask_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsle.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsle.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsgtu.nxv8i1.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmsgtu_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.nxv8i1.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsgtu.mask.nxv8i1.nxv8i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.mask.nxv8i1.nxv8i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsgtu.nxv16i1.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmsgtu_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgtu.nxv16i1.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsgtu.mask.nxv16i1.nxv16i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgtu.mask.nxv16i1.nxv16i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsgtu.nxv32i1.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmsgtu_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgtu.nxv32i1.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsgtu.mask.nxv32i1.nxv32i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgtu.mask.nxv32i1.nxv32i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsgtu.nxv4i1.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmsgtu_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgtu.nxv4i1.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsgtu.mask.nxv4i1.nxv4i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgtu.mask.nxv4i1.nxv4i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsgtu.nxv8i1.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmsgtu_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.nxv8i1.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsgtu.mask.nxv8i1.nxv8i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.mask.nxv8i1.nxv8i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsgtu.nxv16i1.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmsgtu_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgtu.nxv16i1.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsgtu.mask.nxv16i1.nxv16i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgtu.mask.nxv16i1.nxv16i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsgtu.nxv32i1.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmsgtu_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgtu.nxv32i1.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsgtu.mask.nxv32i1.nxv32i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgtu.mask.nxv32i1.nxv32i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsgtu.nxv2i1.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmsgtu_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgtu.nxv2i1.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsgtu.mask.nxv2i1.nxv2i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgtu.mask.nxv2i1.nxv2i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsgtu.nxv4i1.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmsgtu_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgtu.nxv4i1.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsgtu.mask.nxv4i1.nxv4i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgtu.mask.nxv4i1.nxv4i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsgtu.nxv8i1.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmsgtu_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.nxv8i1.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsgtu.mask.nxv8i1.nxv8i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.mask.nxv8i1.nxv8i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsgtu.nxv16i1.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmsgtu_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgtu.nxv16i1.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsgtu.mask.nxv16i1.nxv16i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgtu.mask.nxv16i1.nxv16i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmsgtu.nxv1i1.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmsgtu_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmsgtu.nxv1i1.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmsgtu.mask.nxv1i1.nxv1i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmsgtu.mask.nxv1i1.nxv1i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsgtu.nxv2i1.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmsgtu_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgtu.nxv2i1.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsgtu.mask.nxv2i1.nxv2i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgtu.mask.nxv2i1.nxv2i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsgtu.nxv4i1.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmsgtu_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgtu.nxv4i1.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsgtu.mask.nxv4i1.nxv4i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgtu.mask.nxv4i1.nxv4i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsgtu.nxv8i1.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmsgtu_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.nxv8i1.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsgtu.mask.nxv8i1.nxv8i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsltu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.mask.nxv8i1.nxv8i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsgtu.nxv8i1.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmsgtu_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsgtu.mask.nxv8i1.i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsgtu.nxv16i1.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmsgtu_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgtu.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsgtu.mask.nxv16i1.i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgtu.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsgtu.nxv32i1.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmsgtu_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgtu.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsgtu.mask.nxv32i1.i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgtu.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsgtu.nxv4i1.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmsgtu_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgtu.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsgtu.mask.nxv4i1.i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgtu.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsgtu.nxv8i1.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmsgtu_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsgtu.mask.nxv8i1.i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsgtu.nxv16i1.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmsgtu_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgtu.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsgtu.mask.nxv16i1.i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgtu.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsgtu.nxv32i1.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmsgtu_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgtu.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsgtu.mask.nxv32i1.i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgtu.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsgtu.nxv2i1.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmsgtu_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgtu.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsgtu.mask.nxv2i1.i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgtu.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsgtu.nxv4i1.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmsgtu_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgtu.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsgtu.mask.nxv4i1.i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgtu.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsgtu.nxv8i1.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmsgtu_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsgtu.mask.nxv8i1.i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsgtu.nxv16i1.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmsgtu_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgtu.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsgtu.mask.nxv16i1.i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgtu.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmsgtu.nxv1i1.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmsgtu_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i1> @llvm.epi.vmsgtu.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmsgtu.mask.nxv1i1.i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmsgtu.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsgtu.nxv2i1.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmsgtu_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgtu.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsgtu.mask.nxv2i1.i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgtu.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsgtu.nxv4i1.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmsgtu_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgtu.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsgtu.mask.nxv4i1.i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgtu.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsgtu.nxv8i1.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmsgtu_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsgtu.mask.nxv8i1.i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsgtu_mask_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsgtu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmsgtu_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmsgtu_mask_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmsgtu_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgtu.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vmsgtu_mask_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgtu.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmsgtu_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgtu.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vmsgtu_mask_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgtu.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vmsgtu_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgtu.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vmsgtu_mask_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgtu.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmsgtu_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmsgtu_mask_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmsgtu_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgtu.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vmsgtu_mask_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgtu.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmsgtu_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgtu.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vmsgtu_mask_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgtu.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vmsgtu_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgtu.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vmsgtu_mask_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgtu.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vmsgtu_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgtu.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vmsgtu_mask_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgtu.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmsgtu_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmsgtu_mask_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmsgtu_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgtu.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vmsgtu_mask_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgtu.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmsgtu_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x i1> @llvm.epi.vmsgtu.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

define void @intrinsic_vmsgtu_mask_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmsgtu.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


define void @intrinsic_vmsgtu_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgtu.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vmsgtu_mask_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgtu.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vmsgtu_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgtu.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vmsgtu_mask_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgtu.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmsgtu_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmsgtu_mask_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgtu_mask_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsgtu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgtu.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsgt.nxv8i1.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmsgt_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.nxv8i1.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsgt.mask.nxv8i1.nxv8i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.mask.nxv8i1.nxv8i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsgt.nxv16i1.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmsgt_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgt.nxv16i1.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsgt.mask.nxv16i1.nxv16i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgt.mask.nxv16i1.nxv16i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsgt.nxv32i1.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmsgt_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgt.nxv32i1.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsgt.mask.nxv32i1.nxv32i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgt.mask.nxv32i1.nxv32i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsgt.nxv4i1.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmsgt_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgt.nxv4i1.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsgt.mask.nxv4i1.nxv4i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgt.mask.nxv4i1.nxv4i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsgt.nxv8i1.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmsgt_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.nxv8i1.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsgt.mask.nxv8i1.nxv8i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.mask.nxv8i1.nxv8i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsgt.nxv16i1.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmsgt_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgt.nxv16i1.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsgt.mask.nxv16i1.nxv16i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgt.mask.nxv16i1.nxv16i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsgt.nxv32i1.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmsgt_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgt.nxv32i1.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsgt.mask.nxv32i1.nxv32i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgt.mask.nxv32i1.nxv32i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsgt.nxv2i1.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmsgt_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgt.nxv2i1.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsgt.mask.nxv2i1.nxv2i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgt.mask.nxv2i1.nxv2i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsgt.nxv4i1.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmsgt_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgt.nxv4i1.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsgt.mask.nxv4i1.nxv4i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgt.mask.nxv4i1.nxv4i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsgt.nxv8i1.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmsgt_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.nxv8i1.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsgt.mask.nxv8i1.nxv8i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.mask.nxv8i1.nxv8i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsgt.nxv16i1.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmsgt_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgt.nxv16i1.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsgt.mask.nxv16i1.nxv16i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgt.mask.nxv16i1.nxv16i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmsgt.nxv1i1.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmsgt_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmsgt.nxv1i1.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmsgt.mask.nxv1i1.nxv1i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmsgt.mask.nxv1i1.nxv1i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsgt.nxv2i1.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmsgt_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgt.nxv2i1.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsgt.mask.nxv2i1.nxv2i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgt.mask.nxv2i1.nxv2i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsgt.nxv4i1.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmsgt_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgt.nxv4i1.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsgt.mask.nxv4i1.nxv4i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgt.mask.nxv4i1.nxv4i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsgt.nxv8i1.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmsgt_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.nxv8i1.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsgt.mask.nxv8i1.nxv8i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmslt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.mask.nxv8i1.nxv8i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsgt.nxv8i1.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmsgt_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsgt.mask.nxv8i1.i8(
  <vscale x 8 x i1>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsgt.nxv16i1.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmsgt_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgt.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsgt.mask.nxv16i1.i8(
  <vscale x 16 x i1>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgt.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsgt.nxv32i1.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmsgt_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgt.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsgt.mask.nxv32i1.i8(
  <vscale x 32 x i1>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgt.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsgt.nxv4i1.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmsgt_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgt.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsgt.mask.nxv4i1.i16(
  <vscale x 4 x i1>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgt.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsgt.nxv8i1.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmsgt_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsgt.mask.nxv8i1.i16(
  <vscale x 8 x i1>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsgt.nxv16i1.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmsgt_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgt.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsgt.mask.nxv16i1.i16(
  <vscale x 16 x i1>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgt.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsgt.nxv32i1.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmsgt_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgt.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

declare <vscale x 32 x i1> @llvm.epi.vmsgt.mask.nxv32i1.i16(
  <vscale x 32 x i1>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgt.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsgt.nxv2i1.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmsgt_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgt.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsgt.mask.nxv2i1.i32(
  <vscale x 2 x i1>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgt.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsgt.nxv4i1.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmsgt_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgt.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsgt.mask.nxv4i1.i32(
  <vscale x 4 x i1>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgt.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsgt.nxv8i1.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmsgt_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsgt.mask.nxv8i1.i32(
  <vscale x 8 x i1>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsgt.nxv16i1.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmsgt_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgt.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmsgt.mask.nxv16i1.i32(
  <vscale x 16 x i1>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgt.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmsgt.nxv1i1.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmsgt_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i1> @llvm.epi.vmsgt.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmsgt.mask.nxv1i1.i64(
  <vscale x 1 x i1>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmsgt.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsgt.nxv2i1.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmsgt_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgt.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmsgt.mask.nxv2i1.i64(
  <vscale x 2 x i1>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgt.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsgt.nxv4i1.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmsgt_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgt.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmsgt.mask.nxv4i1.i64(
  <vscale x 4 x i1>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgt.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsgt.nxv8i1.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmsgt_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmsgt.mask.nxv8i1.i64(
  <vscale x 8 x i1>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsgt_mask_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsgt.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmsgt_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmsgt_mask_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.mask.nxv8i1.i8(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmsgt_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgt.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vmsgt_mask_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgt.mask.nxv16i1.i8(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmsgt_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgt.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vmsgt_mask_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgt.mask.nxv32i1.i8(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vmsgt_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgt.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vmsgt_mask_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgt.mask.nxv4i1.i16(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmsgt_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmsgt_mask_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.mask.nxv8i1.i16(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmsgt_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgt.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vmsgt_mask_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgt.mask.nxv16i1.i16(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmsgt_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgt.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}

define void @intrinsic_vmsgt_mask_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i1> @llvm.epi.vmsgt.mask.nxv32i1.i16(
    <vscale x 32 x i1> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vmsgt_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgt.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vmsgt_mask_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgt.mask.nxv2i1.i32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vmsgt_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgt.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vmsgt_mask_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgt.mask.nxv4i1.i32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmsgt_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmsgt_mask_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.mask.nxv8i1.i32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmsgt_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgt.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

define void @intrinsic_vmsgt_mask_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmsgt.mask.nxv16i1.i32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmsgt_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x i1> @llvm.epi.vmsgt.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

define void @intrinsic_vmsgt_mask_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmsgt.mask.nxv1i1.i64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


define void @intrinsic_vmsgt_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgt.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

define void @intrinsic_vmsgt_mask_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmsgt.mask.nxv2i1.i64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vmsgt_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgt.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

define void @intrinsic_vmsgt_mask_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmsgt.mask.nxv4i1.i64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmsgt_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

define void @intrinsic_vmsgt_mask_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsgt_mask_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsgt.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmsgt.mask.nxv8i1.i64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vminu.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vminu_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vminu.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vminu.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vminu.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vminu.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vminu_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vminu.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vminu.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vminu.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vminu.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vminu_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vminu.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vminu.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vminu.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vminu.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vminu_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vminu.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vminu.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vminu.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vminu.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vminu_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vminu.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vminu.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vminu.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vminu.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vminu_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vminu.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vminu.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vminu.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vminu.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vminu_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vminu.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vminu.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vminu.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vminu.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vminu_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vminu.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vminu.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vminu.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vminu.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vminu_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vminu.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vminu.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vminu.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vminu.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vminu_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vminu.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vminu.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vminu.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vminu.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vminu_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vminu.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vminu.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vminu.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vminu.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vminu_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vminu.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vminu.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vminu.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vminu.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vminu_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vminu.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vminu.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vminu.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vminu.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vminu_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vminu.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vminu.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vminu.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vminu.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vminu_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vminu.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vminu.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vminu_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vminu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vminu.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vminu.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vminu_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vminu.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vminu.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vminu.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vminu.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vminu_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vminu.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vminu.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vminu.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vminu.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vminu_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vminu.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vminu.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vminu.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vminu.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vminu_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vminu.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vminu.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vminu.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vminu.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vminu_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vminu.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vminu.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vminu.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vminu.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vminu_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vminu.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vminu.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vminu.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vminu.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vminu_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vminu.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vminu.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vminu.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vminu.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vminu_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vminu.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vminu.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vminu.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vminu.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vminu_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vminu.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vminu.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vminu.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vminu.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vminu_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vminu.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vminu.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vminu.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vminu.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vminu_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vminu.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vminu.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vminu.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vminu.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vminu_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vminu.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vminu.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vminu.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vminu.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vminu_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vminu.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vminu.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vminu.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vminu.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vminu_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vminu.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vminu.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vminu.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vminu.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vminu_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vminu.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vminu.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vminu_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vminu_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vminu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vminu.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmin.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmin_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vmin.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmin.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmin.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmin.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmin_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vmin.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmin.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmin.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmin.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmin_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vmin.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmin.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmin.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmin.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmin_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vmin.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmin.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmin.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmin.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmin_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vmin.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmin.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmin.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmin.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmin_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vmin.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmin.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmin.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmin.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmin_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vmin.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmin.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmin.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmin.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmin_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vmin.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmin.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmin.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmin.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmin_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vmin.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmin.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmin.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmin.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmin_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vmin.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmin.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmin.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmin.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmin_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vmin.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmin.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmin.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmin.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmin_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vmin.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmin.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmin.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmin.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmin_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vmin.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmin.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmin.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmin.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmin_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vmin.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmin.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmin.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmin.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmin_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vmin.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmin.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmin_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmin.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmin.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmin_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vmin.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmin.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmin.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmin.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmin_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vmin.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmin.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmin.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmin.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmin_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vmin.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmin.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmin.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmin.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmin_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vmin.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmin.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmin.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmin.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmin_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vmin.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmin.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmin.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmin.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmin_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vmin.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmin.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmin.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmin.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmin_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vmin.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmin.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmin.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmin.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmin_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vmin.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmin.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmin.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmin.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmin_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vmin.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmin.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmin.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmin.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmin_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vmin.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmin.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmin.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmin.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmin_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vmin.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmin.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmin.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmin.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmin_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vmin.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmin.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmin.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmin.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmin_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vmin.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmin.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmin.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmin.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmin_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vmin.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmin.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmin.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmin.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmin_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vmin.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmin.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmin_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmin_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmin.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmin.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmaxu.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmaxu_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vmaxu.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmaxu.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmaxu.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmaxu.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmaxu_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vmaxu.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmaxu.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmaxu.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmaxu.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmaxu_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vmaxu.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmaxu.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmaxu.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmaxu.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmaxu_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vmaxu.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmaxu.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmaxu.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmaxu.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmaxu_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vmaxu.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmaxu.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmaxu.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmaxu.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmaxu_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vmaxu.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmaxu.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmaxu.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmaxu.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmaxu_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vmaxu.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmaxu.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmaxu.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmaxu.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmaxu_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vmaxu.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmaxu.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmaxu.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmaxu.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmaxu_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vmaxu.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmaxu.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmaxu.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmaxu.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmaxu_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vmaxu.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmaxu.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmaxu.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmaxu.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmaxu_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vmaxu.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmaxu.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmaxu.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmaxu.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmaxu_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vmaxu.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmaxu.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmaxu.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmaxu.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmaxu_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vmaxu.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmaxu.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmaxu.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmaxu.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmaxu_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vmaxu.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmaxu.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmaxu.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmaxu.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmaxu_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vmaxu.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmaxu.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmaxu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmaxu.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmaxu.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmaxu_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vmaxu.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmaxu.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmaxu.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmaxu.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmaxu_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vmaxu.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmaxu.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmaxu.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmaxu.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmaxu_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vmaxu.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmaxu.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmaxu.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmaxu.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmaxu_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vmaxu.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmaxu.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmaxu.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmaxu.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmaxu_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vmaxu.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmaxu.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmaxu.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmaxu.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmaxu_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vmaxu.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmaxu.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmaxu.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmaxu.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmaxu_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vmaxu.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmaxu.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmaxu.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmaxu.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmaxu_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vmaxu.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmaxu.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmaxu.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmaxu.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmaxu_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vmaxu.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmaxu.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmaxu.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmaxu.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmaxu_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vmaxu.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmaxu.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmaxu.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmaxu.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmaxu_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vmaxu.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmaxu.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmaxu.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmaxu.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmaxu_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vmaxu.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmaxu.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmaxu.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmaxu.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmaxu_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vmaxu.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmaxu.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmaxu.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmaxu.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmaxu_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vmaxu.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmaxu.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmaxu.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmaxu.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmaxu_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vmaxu.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmaxu.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmaxu_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmaxu_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmaxu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmaxu.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmax.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmax_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vmax.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmax.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmax.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmax.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmax_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vmax.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmax.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmax.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmax.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmax_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vmax.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmax.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmax.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmax.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmax_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vmax.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmax.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmax.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmax.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmax_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vmax.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmax.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmax.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmax.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmax_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vmax.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmax.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmax.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmax.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmax_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vmax.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmax.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmax.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmax.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmax_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vmax.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmax.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmax.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmax.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmax_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vmax.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmax.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmax.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmax.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmax_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vmax.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmax.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmax.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmax.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmax_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vmax.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmax.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmax.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmax.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmax_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vmax.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmax.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmax.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmax.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmax_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vmax.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmax.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmax.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmax.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmax_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vmax.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmax.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmax.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmax.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmax_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vmax.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmax.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmax_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmax.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmax.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmax_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vmax.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmax.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmax.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmax.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmax_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vmax.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmax.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmax.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmax.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmax_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vmax.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmax.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmax.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmax.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmax_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vmax.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmax.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmax.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmax.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmax_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vmax.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmax.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmax.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmax.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmax_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vmax.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmax.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmax.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmax.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmax_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vmax.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmax.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmax.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmax.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmax_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vmax.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmax.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmax.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmax.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmax_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vmax.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmax.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmax.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmax.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmax_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vmax.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmax.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmax.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmax.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmax_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vmax.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmax.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmax.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmax.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmax_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vmax.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmax.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmax.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmax.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmax_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vmax.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmax.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmax.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmax.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmax_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vmax.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmax.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmax.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmax.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmax_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vmax.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmax.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmax_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmax_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmax.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmax.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmul.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmul_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vmul.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmul.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmul.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmul.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmul_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vmul.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmul.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmul.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmul.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmul_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vmul.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmul.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmul.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmul.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmul_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vmul.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmul.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmul.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmul.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmul_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vmul.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmul.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmul.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmul.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmul_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vmul.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmul.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmul.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmul.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmul_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vmul.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmul.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmul.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmul.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmul_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vmul.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmul.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmul.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmul.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmul_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vmul.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmul.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmul.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmul.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmul_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vmul.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmul.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmul.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmul.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmul_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vmul.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmul.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmul.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmul.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmul_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vmul.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmul.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmul.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmul.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmul_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vmul.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmul.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmul.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmul.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmul_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vmul.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmul.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmul.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmul.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmul_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vmul.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmul.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmul_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmul.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmul.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmul_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vmul.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmul.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmul.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmul.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmul_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vmul.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmul.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmul.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmul.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmul_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vmul.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmul.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmul.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmul.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmul_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vmul.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmul.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmul.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmul.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmul_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vmul.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmul.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmul.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmul.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmul_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vmul.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmul.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmul.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmul.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmul_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vmul.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmul.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmul.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmul.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmul_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vmul.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmul.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmul.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmul.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmul_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vmul.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmul.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmul.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmul.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmul_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vmul.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmul.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmul.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmul.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmul_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vmul.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmul.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmul.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmul.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmul_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vmul.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmul.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmul.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmul.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmul_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vmul.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmul.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmul.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmul.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmul_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vmul.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmul.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmul.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmul.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmul_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vmul.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmul.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmul_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmul_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmul.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmulh.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmulh_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vmulh.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmulh.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmulh.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmulh.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmulh_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vmulh.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmulh.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmulh.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmulh.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmulh_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vmulh.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmulh.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmulh.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmulh.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmulh_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vmulh.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmulh.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmulh.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmulh.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmulh_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vmulh.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmulh.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmulh.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmulh.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmulh_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vmulh.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmulh.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmulh.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmulh.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmulh_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vmulh.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmulh.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmulh.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmulh.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmulh_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vmulh.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmulh.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmulh.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmulh.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmulh_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vmulh.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmulh.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmulh.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmulh.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmulh_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vmulh.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmulh.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmulh.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmulh.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmulh_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vmulh.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmulh.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmulh.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmulh.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmulh_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vmulh.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmulh.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmulh.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmulh.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmulh_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vmulh.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmulh.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmulh.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmulh.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmulh_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vmulh.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmulh.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmulh.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmulh.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmulh_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vmulh.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmulh.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmulh.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmulh.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmulh.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmulh_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vmulh.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmulh.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmulh.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmulh.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmulh_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vmulh.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmulh.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmulh.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmulh.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmulh_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vmulh.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmulh.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmulh.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmulh.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmulh_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vmulh.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmulh.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmulh.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmulh.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmulh_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vmulh.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmulh.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmulh.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmulh.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmulh_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vmulh.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmulh.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmulh.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmulh.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmulh_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vmulh.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmulh.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmulh.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmulh.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmulh_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vmulh.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmulh.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmulh.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmulh.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmulh_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vmulh.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmulh.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmulh.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmulh.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmulh_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vmulh.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmulh.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmulh.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmulh.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmulh_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vmulh.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmulh.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmulh.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmulh.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmulh_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vmulh.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmulh.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmulh.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmulh.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmulh_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vmulh.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmulh.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmulh.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmulh.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmulh_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vmulh.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmulh.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmulh.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmulh.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmulh_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vmulh.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmulh.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulh_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulh_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmulh.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmulh.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmulhu.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmulhu_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vmulhu.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmulhu.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmulhu.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmulhu.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmulhu_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vmulhu.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmulhu.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmulhu.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmulhu.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmulhu_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vmulhu.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmulhu.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmulhu.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmulhu.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmulhu_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vmulhu.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmulhu.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmulhu.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmulhu.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmulhu_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vmulhu.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmulhu.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmulhu.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmulhu.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmulhu_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vmulhu.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmulhu.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmulhu.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmulhu.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmulhu_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vmulhu.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmulhu.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmulhu.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmulhu.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmulhu_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vmulhu.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmulhu.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmulhu.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmulhu.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmulhu_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vmulhu.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmulhu.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmulhu.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmulhu.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmulhu_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vmulhu.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmulhu.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmulhu.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmulhu.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmulhu_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vmulhu.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmulhu.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmulhu.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmulhu.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmulhu_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vmulhu.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmulhu.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmulhu.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmulhu.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmulhu_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vmulhu.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmulhu.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmulhu.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmulhu.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmulhu_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vmulhu.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmulhu.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmulhu.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmulhu.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmulhu_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vmulhu.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmulhu.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmulhu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmulhu.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmulhu.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmulhu_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vmulhu.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmulhu.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmulhu.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmulhu.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmulhu_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vmulhu.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmulhu.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmulhu.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmulhu.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmulhu_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vmulhu.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmulhu.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmulhu.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmulhu.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmulhu_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vmulhu.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmulhu.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmulhu.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmulhu.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmulhu_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vmulhu.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmulhu.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmulhu.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmulhu.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmulhu_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vmulhu.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmulhu.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmulhu.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmulhu.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmulhu_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vmulhu.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmulhu.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmulhu.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmulhu.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmulhu_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vmulhu.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmulhu.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmulhu.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmulhu.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmulhu_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vmulhu.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmulhu.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmulhu.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmulhu.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmulhu_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vmulhu.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmulhu.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmulhu.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmulhu.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmulhu_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vmulhu.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmulhu.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmulhu.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmulhu.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmulhu_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vmulhu.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmulhu.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmulhu.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmulhu.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmulhu_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vmulhu.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmulhu.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmulhu.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmulhu.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmulhu_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vmulhu.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmulhu.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmulhu.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmulhu.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmulhu_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vmulhu.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmulhu.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhu_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhu_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmulhu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmulhu.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmulhsu.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vmulhsu.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmulhsu.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmulhsu.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmulhsu.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vmulhsu.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmulhsu.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmulhsu.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmulhsu.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vmulhsu.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmulhsu.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmulhsu.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmulhsu.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vmulhsu.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmulhsu.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmulhsu.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmulhsu.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vmulhsu.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmulhsu.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmulhsu.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmulhsu.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vmulhsu.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmulhsu.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmulhsu.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmulhsu.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vmulhsu.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmulhsu.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmulhsu.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmulhsu.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vmulhsu.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmulhsu.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmulhsu.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmulhsu.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vmulhsu.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmulhsu.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmulhsu.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmulhsu.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vmulhsu.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmulhsu.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmulhsu.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmulhsu.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vmulhsu.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmulhsu.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmulhsu.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmulhsu.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vmulhsu.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmulhsu.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmulhsu.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmulhsu.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vmulhsu.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmulhsu.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmulhsu.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmulhsu.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vmulhsu.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmulhsu.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmulhsu.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmulhsu.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmulhsu_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vmulhsu.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmulhsu.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmulhsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmulhsu.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmulhsu.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmulhsu_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vmulhsu.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmulhsu.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmulhsu.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmulhsu.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmulhsu_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vmulhsu.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmulhsu.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmulhsu.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmulhsu.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmulhsu_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vmulhsu.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmulhsu.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmulhsu.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmulhsu.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmulhsu_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vmulhsu.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmulhsu.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmulhsu.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmulhsu.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmulhsu_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vmulhsu.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmulhsu.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmulhsu.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmulhsu.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmulhsu_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vmulhsu.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmulhsu.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmulhsu.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmulhsu.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmulhsu_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vmulhsu.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmulhsu.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmulhsu.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmulhsu.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmulhsu_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vmulhsu.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmulhsu.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmulhsu.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmulhsu.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmulhsu_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vmulhsu.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmulhsu.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmulhsu.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmulhsu.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmulhsu_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vmulhsu.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmulhsu.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmulhsu.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmulhsu.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmulhsu_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vmulhsu.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmulhsu.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmulhsu.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmulhsu.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmulhsu_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vmulhsu.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmulhsu.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmulhsu.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmulhsu.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmulhsu_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vmulhsu.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmulhsu.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmulhsu.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmulhsu.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmulhsu_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vmulhsu.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmulhsu.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmulhsu.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmulhsu.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmulhsu_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vmulhsu.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmulhsu.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmulhsu_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmulhsu_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmulhsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmulhsu.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwmul.nxv8i16.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vwmul_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vwmul.nxv8i16.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwmul.mask.nxv8i16.nxv8i8.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwmul.mask.nxv8i16.nxv8i8.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwmul.nxv16i16.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vwmul_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vwmul.nxv16i16.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwmul.mask.nxv16i16.nxv16i8.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwmul.mask.nxv16i16.nxv16i8.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwmul.nxv32i16.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vwmul_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vwmul.nxv32i16.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwmul.mask.nxv32i16.nxv32i8.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwmul.mask.nxv32i16.nxv32i8.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwmul.nxv4i32.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vwmul_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vwmul.nxv4i32.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwmul.mask.nxv4i32.nxv4i16.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwmul.mask.nxv4i32.nxv4i16.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwmul.nxv8i32.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vwmul_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vwmul.nxv8i32.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwmul.mask.nxv8i32.nxv8i16.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwmul.mask.nxv8i32.nxv8i16.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwmul.nxv16i32.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vwmul_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vwmul.nxv16i32.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwmul.mask.nxv16i32.nxv16i16.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwmul.mask.nxv16i32.nxv16i16.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwmul.nxv2i64.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vwmul_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vwmul.nxv2i64.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwmul.mask.nxv2i64.nxv2i32.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwmul.mask.nxv2i64.nxv2i32.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwmul.nxv4i64.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vwmul_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vwmul.nxv4i64.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwmul.mask.nxv4i64.nxv4i32.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwmul.mask.nxv4i64.nxv4i32.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwmul.nxv8i64.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vwmul_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vwmul.nxv8i64.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwmul.mask.nxv8i64.nxv8i32.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwmul.mask.nxv8i64.nxv8i32.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwmul.nxv8i16.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vwmul_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vwmul.nxv8i16.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwmul.mask.nxv8i16.nxv8i8.i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwmul.mask.nxv8i16.nxv8i8.i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwmul.nxv16i16.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vwmul_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vwmul.nxv16i16.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwmul.mask.nxv16i16.nxv16i8.i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwmul.mask.nxv16i16.nxv16i8.i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwmul.nxv32i16.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vwmul_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vwmul.nxv32i16.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwmul.mask.nxv32i16.nxv32i8.i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwmul.mask.nxv32i16.nxv32i8.i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwmul.nxv4i32.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vwmul_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vwmul.nxv4i32.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwmul.mask.nxv4i32.nxv4i16.i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwmul.mask.nxv4i32.nxv4i16.i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwmul.nxv8i32.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vwmul_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vwmul.nxv8i32.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwmul.mask.nxv8i32.nxv8i16.i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwmul.mask.nxv8i32.nxv8i16.i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwmul.nxv16i32.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vwmul_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vwmul.nxv16i32.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwmul.mask.nxv16i32.nxv16i16.i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwmul.mask.nxv16i32.nxv16i16.i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwmul.nxv2i64.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vwmul_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vwmul.nxv2i64.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwmul.mask.nxv2i64.nxv2i32.i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwmul.mask.nxv2i64.nxv2i32.i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwmul.nxv4i64.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vwmul_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vwmul.nxv4i64.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwmul.mask.nxv4i64.nxv4i32.i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwmul.mask.nxv4i64.nxv4i32.i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwmul.nxv8i64.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vwmul_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vwmul.nxv8i64.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwmul.mask.nxv8i64.nxv8i32.i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmul_mask_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmul_mask_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwmul.mask.nxv8i64.nxv8i32.i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwmulu.nxv8i16.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vwmulu_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwmulu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vwmulu.nxv8i16.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwmulu.mask.nxv8i16.nxv8i8.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwmulu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwmulu.mask.nxv8i16.nxv8i8.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwmulu.nxv16i16.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vwmulu_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwmulu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vwmulu.nxv16i16.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwmulu.mask.nxv16i16.nxv16i8.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwmulu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwmulu.mask.nxv16i16.nxv16i8.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwmulu.nxv32i16.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vwmulu_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwmulu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vwmulu.nxv32i16.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwmulu.mask.nxv32i16.nxv32i8.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwmulu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwmulu.mask.nxv32i16.nxv32i8.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwmulu.nxv4i32.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vwmulu_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwmulu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vwmulu.nxv4i32.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwmulu.mask.nxv4i32.nxv4i16.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwmulu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwmulu.mask.nxv4i32.nxv4i16.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwmulu.nxv8i32.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vwmulu_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwmulu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vwmulu.nxv8i32.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwmulu.mask.nxv8i32.nxv8i16.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwmulu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwmulu.mask.nxv8i32.nxv8i16.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwmulu.nxv16i32.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vwmulu_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwmulu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vwmulu.nxv16i32.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwmulu.mask.nxv16i32.nxv16i16.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwmulu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwmulu.mask.nxv16i32.nxv16i16.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwmulu.nxv2i64.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vwmulu_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwmulu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vwmulu.nxv2i64.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwmulu.mask.nxv2i64.nxv2i32.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwmulu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwmulu.mask.nxv2i64.nxv2i32.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwmulu.nxv4i64.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vwmulu_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwmulu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vwmulu.nxv4i64.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwmulu.mask.nxv4i64.nxv4i32.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwmulu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwmulu.mask.nxv4i64.nxv4i32.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwmulu.nxv8i64.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vwmulu_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwmulu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vwmulu.nxv8i64.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwmulu.mask.nxv8i64.nxv8i32.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwmulu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwmulu.mask.nxv8i64.nxv8i32.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwmulu.nxv8i16.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vwmulu_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwmulu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vwmulu.nxv8i16.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwmulu.mask.nxv8i16.nxv8i8.i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwmulu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwmulu.mask.nxv8i16.nxv8i8.i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwmulu.nxv16i16.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vwmulu_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwmulu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vwmulu.nxv16i16.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwmulu.mask.nxv16i16.nxv16i8.i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwmulu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwmulu.mask.nxv16i16.nxv16i8.i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwmulu.nxv32i16.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vwmulu_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwmulu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vwmulu.nxv32i16.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwmulu.mask.nxv32i16.nxv32i8.i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwmulu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwmulu.mask.nxv32i16.nxv32i8.i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwmulu.nxv4i32.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vwmulu_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwmulu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vwmulu.nxv4i32.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwmulu.mask.nxv4i32.nxv4i16.i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwmulu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwmulu.mask.nxv4i32.nxv4i16.i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwmulu.nxv8i32.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vwmulu_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwmulu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vwmulu.nxv8i32.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwmulu.mask.nxv8i32.nxv8i16.i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwmulu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwmulu.mask.nxv8i32.nxv8i16.i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwmulu.nxv16i32.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vwmulu_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwmulu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vwmulu.nxv16i32.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwmulu.mask.nxv16i32.nxv16i16.i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwmulu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwmulu.mask.nxv16i32.nxv16i16.i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwmulu.nxv2i64.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vwmulu_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwmulu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vwmulu.nxv2i64.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwmulu.mask.nxv2i64.nxv2i32.i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwmulu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwmulu.mask.nxv2i64.nxv2i32.i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwmulu.nxv4i64.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vwmulu_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwmulu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vwmulu.nxv4i64.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwmulu.mask.nxv4i64.nxv4i32.i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwmulu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwmulu.mask.nxv4i64.nxv4i32.i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwmulu.nxv8i64.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vwmulu_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwmulu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vwmulu.nxv8i64.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwmulu.mask.nxv8i64.nxv8i32.i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulu_mask_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulu_mask_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwmulu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwmulu.mask.nxv8i64.nxv8i32.i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwmulsu.nxv8i16.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vwmulsu_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwmulsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vwmulsu.nxv8i16.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwmulsu.mask.nxv8i16.nxv8i8.nxv8i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vv_nxv8i16_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vv_nxv8i16_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwmulsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwmulsu.mask.nxv8i16.nxv8i8.nxv8i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwmulsu.nxv16i16.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vwmulsu_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwmulsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vwmulsu.nxv16i16.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwmulsu.mask.nxv16i16.nxv16i8.nxv16i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vv_nxv16i16_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vv_nxv16i16_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwmulsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwmulsu.mask.nxv16i16.nxv16i8.nxv16i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwmulsu.nxv32i16.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vwmulsu_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwmulsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vwmulsu.nxv32i16.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwmulsu.mask.nxv32i16.nxv32i8.nxv32i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vv_nxv32i16_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vv_nxv32i16_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwmulsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwmulsu.mask.nxv32i16.nxv32i8.nxv32i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwmulsu.nxv4i32.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vwmulsu_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwmulsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vwmulsu.nxv4i32.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwmulsu.mask.nxv4i32.nxv4i16.nxv4i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vv_nxv4i32_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vv_nxv4i32_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwmulsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwmulsu.mask.nxv4i32.nxv4i16.nxv4i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwmulsu.nxv8i32.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vwmulsu_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwmulsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vwmulsu.nxv8i32.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwmulsu.mask.nxv8i32.nxv8i16.nxv8i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vv_nxv8i32_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vv_nxv8i32_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwmulsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwmulsu.mask.nxv8i32.nxv8i16.nxv8i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwmulsu.nxv16i32.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vwmulsu_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwmulsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vwmulsu.nxv16i32.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwmulsu.mask.nxv16i32.nxv16i16.nxv16i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vv_nxv16i32_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vv_nxv16i32_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwmulsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwmulsu.mask.nxv16i32.nxv16i16.nxv16i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwmulsu.nxv2i64.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vwmulsu_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwmulsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vwmulsu.nxv2i64.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwmulsu.mask.nxv2i64.nxv2i32.nxv2i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vv_nxv2i64_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vv_nxv2i64_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwmulsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwmulsu.mask.nxv2i64.nxv2i32.nxv2i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwmulsu.nxv4i64.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vwmulsu_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwmulsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vwmulsu.nxv4i64.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwmulsu.mask.nxv4i64.nxv4i32.nxv4i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vv_nxv4i64_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vv_nxv4i64_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwmulsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwmulsu.mask.nxv4i64.nxv4i32.nxv4i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwmulsu.nxv8i64.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vwmulsu_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwmulsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vwmulsu.nxv8i64.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwmulsu.mask.nxv8i64.nxv8i32.nxv8i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vv_nxv8i64_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vv_nxv8i64_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwmulsu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwmulsu.mask.nxv8i64.nxv8i32.nxv8i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vwmulsu.nxv8i16.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vwmulsu_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwmulsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vwmulsu.nxv8i16.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vwmulsu.mask.nxv8i16.nxv8i8.i8(
  <vscale x 8 x i16>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vx_nxv8i16_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vx_nxv8i16_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vwmulsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vwmulsu.mask.nxv8i16.nxv8i8.i8(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vwmulsu.nxv16i16.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vwmulsu_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwmulsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vwmulsu.nxv16i16.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vwmulsu.mask.nxv16i16.nxv16i8.i8(
  <vscale x 16 x i16>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vx_nxv16i16_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vx_nxv16i16_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vwmulsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vwmulsu.mask.nxv16i16.nxv16i8.i8(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vwmulsu.nxv32i16.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vwmulsu_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwmulsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vwmulsu.nxv32i16.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vwmulsu.mask.nxv32i16.nxv32i8.i8(
  <vscale x 32 x i16>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vx_nxv32i16_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vx_nxv32i16_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vwmulsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vwmulsu.mask.nxv32i16.nxv32i8.i8(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vwmulsu.nxv4i32.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vwmulsu_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwmulsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vwmulsu.nxv4i32.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vwmulsu.mask.nxv4i32.nxv4i16.i16(
  <vscale x 4 x i32>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vx_nxv4i32_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vx_nxv4i32_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vwmulsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vwmulsu.mask.nxv4i32.nxv4i16.i16(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vwmulsu.nxv8i32.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vwmulsu_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwmulsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vwmulsu.nxv8i32.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vwmulsu.mask.nxv8i32.nxv8i16.i16(
  <vscale x 8 x i32>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vx_nxv8i32_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vx_nxv8i32_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vwmulsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vwmulsu.mask.nxv8i32.nxv8i16.i16(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vwmulsu.nxv16i32.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vwmulsu_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwmulsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vwmulsu.nxv16i32.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vwmulsu.mask.nxv16i32.nxv16i16.i16(
  <vscale x 16 x i32>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vx_nxv16i32_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vx_nxv16i32_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vwmulsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vwmulsu.mask.nxv16i32.nxv16i16.i16(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vwmulsu.nxv2i64.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vwmulsu_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwmulsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vwmulsu.nxv2i64.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vwmulsu.mask.nxv2i64.nxv2i32.i32(
  <vscale x 2 x i64>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vx_nxv2i64_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vx_nxv2i64_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vwmulsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vwmulsu.mask.nxv2i64.nxv2i32.i32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vwmulsu.nxv4i64.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vwmulsu_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwmulsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vwmulsu.nxv4i64.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vwmulsu.mask.nxv4i64.nxv4i32.i32(
  <vscale x 4 x i64>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vx_nxv4i64_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vx_nxv4i64_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vwmulsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vwmulsu.mask.nxv4i64.nxv4i32.i32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vwmulsu.nxv8i64.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vwmulsu_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwmulsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vwmulsu.nxv8i64.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vwmulsu.mask.nxv8i64.nxv8i32.i32(
  <vscale x 8 x i64>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vwmulsu_mask_vx_nxv8i64_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vwmulsu_mask_vx_nxv8i64_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vwmulsu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vwmulsu.mask.nxv8i64.nxv8i32.i32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vdivu.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vdivu_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vdivu.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vdivu.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vdivu.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vdivu.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vdivu_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vdivu.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vdivu.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vdivu.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vdivu.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vdivu_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vdivu.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vdivu.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vdivu.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vdivu.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vdivu_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vdivu.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vdivu.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vdivu.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vdivu.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vdivu_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vdivu.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vdivu.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vdivu.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vdivu.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vdivu_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vdivu.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vdivu.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vdivu.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vdivu.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vdivu_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vdivu.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vdivu.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vdivu.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vdivu.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vdivu_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vdivu.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vdivu.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vdivu.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vdivu.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vdivu_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vdivu.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vdivu.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vdivu.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vdivu.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vdivu_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vdivu.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vdivu.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vdivu.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vdivu.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vdivu_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vdivu.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vdivu.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vdivu.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vdivu.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vdivu_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vdivu.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vdivu.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vdivu.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vdivu.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vdivu_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vdivu.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vdivu.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vdivu.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vdivu.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vdivu_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vdivu.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vdivu.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vdivu.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vdivu.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vdivu_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vdivu.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vdivu.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vdivu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vdivu.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vdivu.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vdivu_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vdivu.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vdivu.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vdivu.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vdivu.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vdivu_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vdivu.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vdivu.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vdivu.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vdivu.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vdivu_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vdivu.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vdivu.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vdivu.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vdivu.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vdivu_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vdivu.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vdivu.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vdivu.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vdivu.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vdivu_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vdivu.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vdivu.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vdivu.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vdivu.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vdivu_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vdivu.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vdivu.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vdivu.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vdivu.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vdivu_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vdivu.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vdivu.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vdivu.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vdivu.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vdivu_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vdivu.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vdivu.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vdivu.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vdivu.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vdivu_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vdivu.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vdivu.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vdivu.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vdivu.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vdivu_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vdivu.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vdivu.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vdivu.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vdivu.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vdivu_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vdivu.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vdivu.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vdivu.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vdivu.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vdivu_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vdivu.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vdivu.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vdivu.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vdivu.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vdivu_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vdivu.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vdivu.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vdivu.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vdivu.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vdivu_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vdivu.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vdivu.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vdivu.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vdivu.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vdivu_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vdivu.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vdivu.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdivu_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdivu_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vdivu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vdivu.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vdiv.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vdiv_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vdiv.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vdiv.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vdiv.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vdiv.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vdiv_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vdiv.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vdiv.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vdiv.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vdiv.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vdiv_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vdiv.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vdiv.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vdiv.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vdiv.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vdiv_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vdiv.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vdiv.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vdiv.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vdiv.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vdiv_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vdiv.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vdiv.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vdiv.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vdiv.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vdiv_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vdiv.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vdiv.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vdiv.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vdiv.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vdiv_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vdiv.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vdiv.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vdiv.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vdiv.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vdiv_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vdiv.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vdiv.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vdiv.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vdiv.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vdiv_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vdiv.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vdiv.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vdiv.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vdiv.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vdiv_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vdiv.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vdiv.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vdiv.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vdiv.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vdiv_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vdiv.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vdiv.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vdiv.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vdiv.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vdiv_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vdiv.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vdiv.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vdiv.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vdiv.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vdiv_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vdiv.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vdiv.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vdiv.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vdiv.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vdiv_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vdiv.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vdiv.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vdiv.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vdiv.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vdiv_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vdiv.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vdiv.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vdiv.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vdiv.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vdiv_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vdiv.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vdiv.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vdiv.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vdiv.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vdiv_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vdiv.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vdiv.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vdiv.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vdiv.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vdiv_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vdiv.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vdiv.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vdiv.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vdiv.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vdiv_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vdiv.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vdiv.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vdiv.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vdiv.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vdiv_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vdiv.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vdiv.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vdiv.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vdiv.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vdiv_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vdiv.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vdiv.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vdiv.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vdiv.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vdiv_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vdiv.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vdiv.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vdiv.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vdiv.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vdiv_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vdiv.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vdiv.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vdiv.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vdiv.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vdiv_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vdiv.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vdiv.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vdiv.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vdiv.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vdiv_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vdiv.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vdiv.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vdiv.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vdiv.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vdiv_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vdiv.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vdiv.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vdiv.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vdiv.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vdiv_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vdiv.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vdiv.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vdiv.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vdiv.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vdiv_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vdiv.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vdiv.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vdiv.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vdiv.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vdiv_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vdiv.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vdiv.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vdiv.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vdiv.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vdiv_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vdiv.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vdiv.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vdiv_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vdiv_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vdiv.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vdiv.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vremu.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vremu_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vremu.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vremu.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vremu.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vremu.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vremu_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vremu.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vremu.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vremu.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vremu.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vremu_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vremu.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vremu.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vremu.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vremu.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vremu_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vremu.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vremu.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vremu.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vremu.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vremu_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vremu.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vremu.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vremu.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vremu.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vremu_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vremu.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vremu.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vremu.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vremu.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vremu_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vremu.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vremu.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vremu.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vremu.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vremu_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vremu.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vremu.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vremu.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vremu.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vremu_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vremu.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vremu.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vremu.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vremu.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vremu_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vremu.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vremu.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vremu.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vremu.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vremu_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vremu.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vremu.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vremu.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vremu.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vremu_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vremu.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vremu.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vremu.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vremu.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vremu_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vremu.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vremu.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vremu.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vremu.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vremu_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vremu.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vremu.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vremu.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vremu.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vremu_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vremu.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vremu.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vremu_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vremu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vremu.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vremu.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vremu_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vremu.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vremu.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vremu.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vremu.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vremu_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vremu.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vremu.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vremu.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vremu.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vremu_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vremu.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vremu.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vremu.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vremu.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vremu_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vremu.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vremu.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vremu.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vremu.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vremu_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vremu.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vremu.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vremu.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vremu.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vremu_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vremu.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vremu.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vremu.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vremu.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vremu_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vremu.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vremu.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vremu.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vremu.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vremu_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vremu.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vremu.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vremu.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vremu.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vremu_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vremu.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vremu.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vremu.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vremu.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vremu_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vremu.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vremu.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vremu.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vremu.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vremu_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vremu.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vremu.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vremu.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vremu.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vremu_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vremu.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vremu.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vremu.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vremu.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vremu_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vremu.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vremu.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vremu.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vremu.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vremu_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vremu.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vremu.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vremu.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vremu.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vremu_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vremu.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vremu.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vremu_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vremu_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vremu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vremu.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vrem.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vrem_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vrem.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vrem.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vrem.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vrem.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vrem_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vrem.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vrem.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vrem.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vrem.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vrem_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vrem.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vrem.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vrem.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vrem.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vrem_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vrem.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vrem.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vrem.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vrem.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vrem_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vrem.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vrem.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vrem.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vrem.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vrem_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vrem.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vrem.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vrem.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vrem.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vrem_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vrem.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vrem.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vrem.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vrem.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vrem_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vrem.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vrem.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vrem.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vrem.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vrem_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vrem.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vrem.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vrem.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vrem.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vrem_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vrem.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vrem.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vrem.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vrem.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vrem_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vrem.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vrem.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vrem.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vrem.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vrem_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vrem.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vrem.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vrem.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vrem.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vrem_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vrem.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vrem.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vrem.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vrem.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vrem_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vrem.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vrem.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vrem.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vrem.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vrem_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vrem.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vrem.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrem_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vrem.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vrem.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vrem.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vrem_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vrem.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vrem.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vrem.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vrem.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vrem_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vrem.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vrem.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vrem.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vrem.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vrem_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vrem.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vrem.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vrem.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vrem.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vrem_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vrem.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vrem.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vrem.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vrem.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vrem_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vrem.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vrem.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vrem.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vrem.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vrem_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vrem.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vrem.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vrem.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vrem.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vrem_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vrem.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vrem.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vrem.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vrem.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vrem_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vrem.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vrem.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vrem.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vrem.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vrem_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vrem.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vrem.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vrem.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vrem.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vrem_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vrem.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vrem.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vrem.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vrem.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vrem_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vrem.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vrem.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vrem.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vrem.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vrem_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vrem.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vrem.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vrem.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vrem.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vrem_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vrem.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vrem.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vrem.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vrem.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vrem_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vrem.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vrem.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vrem.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vrem.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vrem_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vrem.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vrem.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrem_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrem_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vrem.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vrem.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmerge.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmerge_vvm_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vvm_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmerge.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vmerge.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmerge.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmerge_vvm_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vvm_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmerge.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vmerge.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmerge.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmerge_vvm_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vvm_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmerge.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vmerge.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmerge.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmerge_vvm_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vvm_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmerge.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vmerge.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmerge.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmerge_vvm_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vvm_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmerge.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vmerge.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmerge.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmerge_vvm_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vvm_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmerge.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vmerge.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmerge.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmerge_vvm_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vvm_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmerge.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vmerge.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmerge.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmerge_vvm_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vvm_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmerge.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vmerge.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmerge.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmerge_vvm_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vvm_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmerge.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vmerge.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmerge.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmerge_vvm_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vvm_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmerge.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vmerge.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmerge.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmerge_vvm_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vvm_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmerge.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vmerge.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmerge.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmerge_vvm_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vvm_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmerge.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vmerge.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmerge.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmerge_vvm_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vvm_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmerge.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vmerge.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmerge.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmerge_vvm_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vvm_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmerge.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vmerge.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmerge.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmerge_vvm_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vvm_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmerge.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vmerge.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmerge.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmerge_vxm_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vxm_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmerge.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vmerge.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmerge.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmerge_vxm_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vxm_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmerge.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vmerge.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmerge.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmerge_vxm_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vxm_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmerge.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vmerge.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmerge.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmerge_vxm_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vxm_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmerge.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vmerge.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmerge.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmerge_vxm_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vxm_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmerge.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vmerge.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmerge.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmerge_vxm_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vxm_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmerge.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vmerge.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmerge.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmerge_vxm_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vxm_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmerge.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vmerge.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmerge.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmerge_vxm_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vxm_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmerge.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vmerge.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmerge.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmerge_vxm_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vxm_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmerge.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vmerge.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmerge.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmerge_vxm_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vxm_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmerge.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vmerge.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmerge.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmerge_vxm_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vxm_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmerge.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vmerge.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmerge.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmerge_vxm_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vxm_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmerge.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vmerge.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmerge.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmerge_vxm_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vxm_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmerge.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vmerge.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmerge.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmerge_vxm_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vxm_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmerge.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vmerge.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmerge.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmerge_vxm_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vxm_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmerge.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vmerge.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vmerge_vim_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vim_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmerge.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vmerge.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vmerge_vim_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vim_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmerge.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vmerge.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vmerge_vim_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vim_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmerge.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vmerge.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vmerge_vim_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vim_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmerge.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vmerge.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vmerge_vim_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vim_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmerge.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vmerge.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vmerge_vim_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vim_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmerge.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vmerge.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vmerge_vim_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vim_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmerge.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vmerge.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vmerge_vim_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vim_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmerge.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vmerge.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vmerge_vim_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vim_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmerge.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vmerge.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vmerge_vim_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vim_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmerge.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vmerge.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vmerge_vim_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vim_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmerge.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vmerge.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vmerge_vim_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vim_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmerge.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vmerge.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vmerge_vim_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vim_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmerge.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vmerge.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vmerge_vim_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vim_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmerge.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vmerge.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vmerge_vim_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmerge_vim_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmerge.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vmerge.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsaddu.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vsaddu_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vsaddu.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsaddu.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsaddu.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsaddu.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vsaddu_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vsaddu.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsaddu.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsaddu.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsaddu.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vsaddu_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vsaddu.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsaddu.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsaddu.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsaddu.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vsaddu_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vsaddu.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsaddu.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsaddu.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsaddu.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vsaddu_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vsaddu.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsaddu.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsaddu.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsaddu.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vsaddu_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vsaddu.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsaddu.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsaddu.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsaddu.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vsaddu_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vsaddu.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsaddu.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsaddu.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsaddu.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vsaddu_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vsaddu.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsaddu.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsaddu.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsaddu.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vsaddu_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vsaddu.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsaddu.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsaddu.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsaddu.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vsaddu_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vsaddu.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsaddu.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsaddu.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsaddu.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vsaddu_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vsaddu.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsaddu.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsaddu.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsaddu.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vsaddu_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vsaddu.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsaddu.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsaddu.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsaddu.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vsaddu_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vsaddu.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsaddu.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsaddu.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsaddu.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vsaddu_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vsaddu.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsaddu.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsaddu.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsaddu.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vsaddu_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vsaddu.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsaddu.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsaddu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsaddu.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsaddu.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vsaddu_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vsaddu.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsaddu.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsaddu.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsaddu.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vsaddu_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vsaddu.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsaddu.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsaddu.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsaddu.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vsaddu_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vsaddu.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsaddu.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsaddu.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsaddu.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vsaddu_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vsaddu.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsaddu.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsaddu.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsaddu.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vsaddu_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vsaddu.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsaddu.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsaddu.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsaddu.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vsaddu_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vsaddu.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsaddu.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsaddu.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsaddu.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vsaddu_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vsaddu.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsaddu.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsaddu.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsaddu.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vsaddu_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vsaddu.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsaddu.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsaddu.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsaddu.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vsaddu_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vsaddu.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsaddu.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsaddu.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsaddu.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vsaddu_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vsaddu.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsaddu.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsaddu.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsaddu.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vsaddu_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vsaddu.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsaddu.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsaddu.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsaddu.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vsaddu_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vsaddu.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsaddu.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsaddu.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsaddu.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vsaddu_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vsaddu.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsaddu.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsaddu.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsaddu.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vsaddu_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vsaddu.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsaddu.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsaddu.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsaddu.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vsaddu_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vsaddu.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsaddu.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsaddu_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsaddu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsaddu.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vsaddu.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsaddu.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vsaddu.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsaddu.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vsaddu.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsaddu.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vsaddu.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsaddu.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vsaddu.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsaddu.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vsaddu.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsaddu.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vsaddu.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsaddu.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vsaddu.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsaddu.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vsaddu.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsaddu.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vsaddu.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsaddu.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vsaddu.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsaddu.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vsaddu.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsaddu.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vsaddu.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsaddu.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vsaddu.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsaddu.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vsaddu_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vsaddu.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vsaddu_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsaddu_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsaddu.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsaddu.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsadd.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vsadd_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vsadd.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsadd.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsadd.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsadd.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vsadd_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vsadd.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsadd.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsadd.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsadd.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vsadd_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vsadd.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsadd.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsadd.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsadd.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vsadd_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vsadd.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsadd.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsadd.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsadd.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vsadd_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vsadd.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsadd.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsadd.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsadd.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vsadd_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vsadd.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsadd.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsadd.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsadd.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vsadd_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vsadd.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsadd.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsadd.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsadd.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vsadd_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vsadd.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsadd.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsadd.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsadd.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vsadd_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vsadd.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsadd.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsadd.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsadd.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vsadd_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vsadd.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsadd.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsadd.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsadd.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vsadd_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vsadd.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsadd.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsadd.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsadd.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vsadd_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vsadd.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsadd.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsadd.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsadd.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vsadd_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vsadd.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsadd.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsadd.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsadd.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vsadd_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vsadd.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsadd.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsadd.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsadd.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vsadd_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vsadd.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsadd.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsadd.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsadd.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vsadd_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vsadd.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsadd.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsadd.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsadd.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vsadd_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vsadd.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsadd.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsadd.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsadd.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vsadd_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vsadd.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsadd.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsadd.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsadd.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vsadd_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vsadd.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsadd.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsadd.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsadd.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vsadd_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vsadd.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsadd.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsadd.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsadd.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vsadd_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vsadd.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsadd.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsadd.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsadd.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vsadd_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vsadd.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsadd.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsadd.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsadd.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vsadd_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vsadd.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsadd.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsadd.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsadd.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vsadd_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vsadd.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsadd.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsadd.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsadd.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vsadd_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vsadd.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsadd.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsadd.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsadd.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vsadd_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vsadd.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsadd.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsadd.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsadd.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vsadd_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vsadd.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsadd.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsadd.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsadd.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vsadd_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vsadd.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsadd.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsadd.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsadd.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vsadd_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vsadd.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsadd.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsadd.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsadd.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vsadd_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vsadd.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsadd.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsadd_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsadd.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vsadd.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsadd.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vsadd.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsadd.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vsadd.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsadd.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vsadd.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsadd.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vsadd.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsadd.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vsadd.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsadd.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vsadd.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsadd.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vsadd.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsadd.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vsadd.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsadd.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vsadd.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsadd.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vsadd.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsadd.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vsadd.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsadd.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vsadd.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsadd.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vsadd.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsadd.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vsadd_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vsadd.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vsadd_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsadd_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsadd.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsadd.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vssubu.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vssubu_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vssubu.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vssubu.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vssubu.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vssubu.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vssubu_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vssubu.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vssubu.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vssubu.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vssubu.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vssubu_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vssubu.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vssubu.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vssubu.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vssubu.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vssubu_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vssubu.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vssubu.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vssubu.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vssubu.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vssubu_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vssubu.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vssubu.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vssubu.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vssubu.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vssubu_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vssubu.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vssubu.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vssubu.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vssubu.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vssubu_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vssubu.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vssubu.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vssubu.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vssubu.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vssubu_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vssubu.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vssubu.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vssubu.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vssubu.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vssubu_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vssubu.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vssubu.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vssubu.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vssubu.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vssubu_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vssubu.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vssubu.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vssubu.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vssubu.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vssubu_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vssubu.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vssubu.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vssubu.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssubu.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vssubu_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vssubu.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssubu.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vssubu.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vssubu.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vssubu_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vssubu.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vssubu.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vssubu.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vssubu.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vssubu_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vssubu.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vssubu.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vssubu.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vssubu.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vssubu_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vssubu.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vssubu.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vssubu.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vssubu.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vssubu.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vssubu_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vssubu.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vssubu.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vssubu.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vssubu.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vssubu_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vssubu.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vssubu.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vssubu.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vssubu.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vssubu_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vssubu.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vssubu.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vssubu.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vssubu.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vssubu_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vssubu.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vssubu.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vssubu.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vssubu.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vssubu_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vssubu.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vssubu.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vssubu.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vssubu.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vssubu_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vssubu.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vssubu.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vssubu.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vssubu.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vssubu_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vssubu.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vssubu.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vssubu.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vssubu.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vssubu_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vssubu.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vssubu.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vssubu.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vssubu.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vssubu_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vssubu.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vssubu.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vssubu.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vssubu.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vssubu_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vssubu.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vssubu.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vssubu.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vssubu.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vssubu_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vssubu.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vssubu.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vssubu.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssubu.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vssubu_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vssubu.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssubu.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vssubu.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vssubu.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vssubu_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vssubu.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vssubu.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vssubu.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vssubu.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vssubu_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vssubu.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vssubu.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vssubu.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vssubu.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vssubu_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vssubu.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vssubu.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssubu_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssubu_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vssubu.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vssubu.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vssub.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vssub_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vssub.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vssub.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vssub.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vssub.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vssub_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vssub.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vssub.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vssub.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vssub.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vssub_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vssub.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vssub.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vssub.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vssub.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vssub_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vssub.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vssub.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vssub.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vssub.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vssub_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vssub.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vssub.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vssub.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vssub.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vssub_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vssub.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vssub.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vssub.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vssub.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vssub_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vssub.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vssub.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vssub.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vssub.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vssub_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vssub.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vssub.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vssub.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vssub.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vssub_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vssub.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vssub.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vssub.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vssub.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vssub_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vssub.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vssub.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vssub.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vssub.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vssub_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vssub.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vssub.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vssub.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssub.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vssub_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vssub.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssub.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vssub.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vssub.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vssub_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vssub.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vssub.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vssub.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vssub.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vssub_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vssub.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vssub.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vssub.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vssub.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vssub_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vssub.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vssub.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssub_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vssub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vssub.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vssub.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vssub_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vssub.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vssub.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vssub.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vssub.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vssub_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vssub.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vssub.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vssub.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vssub.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vssub_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vssub.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vssub.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vssub.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vssub.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vssub_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vssub.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vssub.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vssub.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vssub.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vssub_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vssub.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vssub.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vssub.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vssub.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vssub_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vssub.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vssub.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vssub.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vssub.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vssub_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vssub.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vssub.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vssub.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vssub.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vssub_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vssub.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vssub.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vssub.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vssub.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vssub_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vssub.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vssub.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vssub.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vssub.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vssub_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vssub.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vssub.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vssub.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vssub.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vssub_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vssub.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vssub.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vssub.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssub.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vssub_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vssub.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssub.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vssub.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vssub.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vssub_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vssub.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vssub.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vssub.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vssub.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vssub_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vssub.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vssub.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vssub.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vssub.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vssub_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vssub.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vssub.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssub_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssub_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vssub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vssub.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vaadd.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vaadd_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vaadd.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vaadd.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vaadd.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vaadd.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vaadd_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vaadd.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vaadd.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vaadd.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vaadd.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vaadd_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vaadd.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vaadd.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vaadd.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vaadd.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vaadd_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vaadd.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vaadd.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vaadd.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vaadd.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vaadd_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vaadd.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vaadd.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vaadd.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vaadd.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vaadd_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vaadd.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vaadd.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vaadd.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vaadd.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vaadd_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vaadd.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vaadd.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vaadd.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vaadd.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vaadd_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vaadd.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vaadd.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vaadd.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vaadd.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vaadd_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vaadd.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vaadd.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vaadd.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vaadd.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vaadd_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vaadd.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vaadd.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vaadd.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vaadd.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vaadd_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vaadd.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vaadd.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vaadd.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vaadd.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vaadd_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vaadd.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vaadd.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vaadd.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vaadd.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vaadd_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vaadd.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vaadd.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vaadd.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vaadd.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vaadd_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vaadd.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vaadd.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vaadd.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vaadd.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vaadd_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vaadd.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vaadd.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vaadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vaadd.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vaadd.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vaadd_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vaadd.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vaadd.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vaadd.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vaadd.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vaadd_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vaadd.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vaadd.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vaadd.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vaadd.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vaadd_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vaadd.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vaadd.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vaadd.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vaadd.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vaadd_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vaadd.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vaadd.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vaadd.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vaadd.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vaadd_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vaadd.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vaadd.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vaadd.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vaadd.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vaadd_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vaadd.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vaadd.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vaadd.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vaadd.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vaadd_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vaadd.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vaadd.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vaadd.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vaadd.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vaadd_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vaadd.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vaadd.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vaadd.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vaadd.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vaadd_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vaadd.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vaadd.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vaadd.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vaadd.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vaadd_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vaadd.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vaadd.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vaadd.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vaadd.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vaadd_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vaadd.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vaadd.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vaadd.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vaadd.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vaadd_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vaadd.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vaadd.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vaadd.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vaadd.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vaadd_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vaadd.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vaadd.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vaadd.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vaadd.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vaadd_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vaadd.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vaadd.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vaadd.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vaadd.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vaadd_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vaadd.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vaadd.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vaadd_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vaadd_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vaadd.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vaadd.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vasub.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vasub_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vasub.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vasub.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vasub.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vasub.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vasub_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vasub.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vasub.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vasub.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vasub.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vasub_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vasub.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vasub.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vasub.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vasub.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vasub_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vasub.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vasub.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vasub.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vasub.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vasub_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vasub.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vasub.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vasub.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vasub.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vasub_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vasub.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vasub.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vasub.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vasub.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vasub_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vasub.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vasub.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vasub.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vasub.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vasub_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vasub.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vasub.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vasub.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vasub.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vasub_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vasub.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vasub.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vasub.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vasub.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vasub_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vasub.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vasub.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vasub.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vasub.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vasub_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vasub.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vasub.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vasub.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vasub.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vasub_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vasub.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vasub.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vasub.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vasub.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vasub_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vasub.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vasub.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vasub.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vasub.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vasub_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vasub.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vasub.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vasub.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vasub.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vasub_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vasub.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vasub.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vasub_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vasub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vasub.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vasub.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vasub_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vasub.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vasub.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vasub.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vasub.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vasub_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vasub.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vasub.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vasub.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vasub.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vasub_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vasub.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vasub.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vasub.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vasub.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vasub_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vasub.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vasub.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vasub.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vasub.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vasub_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vasub.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vasub.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vasub.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vasub.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vasub_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vasub.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vasub.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vasub.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vasub.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vasub_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vasub.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vasub.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vasub.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vasub.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vasub_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vasub.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vasub.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vasub.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vasub.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vasub_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vasub.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vasub.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vasub.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vasub.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vasub_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vasub.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vasub.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vasub.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vasub.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vasub_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vasub.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vasub.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vasub.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vasub.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vasub_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vasub.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vasub.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vasub.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vasub.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vasub_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vasub.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vasub.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vasub.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vasub.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vasub_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vasub.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vasub.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vasub.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vasub.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vasub_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vasub.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vasub.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vasub_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vasub_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vasub.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vasub.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsmul.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vsmul_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vsmul.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsmul.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsmul.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsmul.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vsmul_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vsmul.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsmul.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsmul.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsmul.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vsmul_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vsmul.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsmul.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsmul.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsmul.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vsmul_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vsmul.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsmul.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsmul.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsmul.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vsmul_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vsmul.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsmul.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsmul.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsmul.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vsmul_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vsmul.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsmul.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsmul.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsmul.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vsmul_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vsmul.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsmul.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsmul.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsmul.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vsmul_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vsmul.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsmul.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsmul.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsmul.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vsmul_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vsmul.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsmul.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsmul.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsmul.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vsmul_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vsmul.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsmul.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsmul.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsmul.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vsmul_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vsmul.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsmul.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsmul.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsmul.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vsmul_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vsmul.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsmul.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsmul.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsmul.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vsmul_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vsmul.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsmul.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsmul.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsmul.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vsmul_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vsmul.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsmul.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsmul.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsmul.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vsmul_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vsmul.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsmul.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsmul.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsmul.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vsmul_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vsmul.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vsmul.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vsmul.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsmul.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vsmul_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vsmul.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vsmul.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vsmul.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsmul.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vsmul_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vsmul.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vsmul.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vsmul.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsmul.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vsmul_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vsmul.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vsmul.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vsmul.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsmul.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vsmul_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vsmul.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vsmul.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vsmul.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsmul.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vsmul_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vsmul.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vsmul.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vsmul.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsmul.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vsmul_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vsmul.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vsmul.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vsmul.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsmul.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vsmul_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vsmul.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vsmul.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vsmul.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsmul.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vsmul_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vsmul.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vsmul.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vsmul.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsmul.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vsmul_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vsmul.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vsmul.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vsmul.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsmul.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vsmul_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vsmul.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vsmul.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vsmul.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsmul.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vsmul_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vsmul.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vsmul.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vsmul.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsmul.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vsmul_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vsmul.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vsmul.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vsmul.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsmul.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vsmul_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vsmul.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vsmul.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vsmul.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsmul.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vsmul_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vsmul.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vsmul.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsmul_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsmul_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsmul.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vsmul.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vssrl.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vssrl_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vssrl.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vssrl.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vssrl.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vssrl.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vssrl_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vssrl.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vssrl.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vssrl.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vssrl.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vssrl_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vssrl.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vssrl.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vssrl.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vssrl.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vssrl_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vssrl.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vssrl.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vssrl.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vssrl.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vssrl_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vssrl.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vssrl.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vssrl.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vssrl.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vssrl_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vssrl.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vssrl.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vssrl.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vssrl.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vssrl_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vssrl.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vssrl.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vssrl.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vssrl.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vssrl_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vssrl.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vssrl.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vssrl.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vssrl.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vssrl_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vssrl.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vssrl.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vssrl.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vssrl.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vssrl_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vssrl.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vssrl.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vssrl.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vssrl.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vssrl_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vssrl.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vssrl.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vssrl.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssrl.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vssrl_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vssrl.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssrl.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vssrl.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vssrl.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vssrl_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vssrl.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vssrl.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vssrl.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vssrl.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vssrl_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vssrl.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vssrl.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vssrl.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vssrl.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vssrl_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vssrl.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vssrl.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vssrl.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vssrl.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vssrl.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vssrl_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vssrl.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vssrl.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vssrl.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vssrl.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vssrl_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vssrl.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vssrl.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vssrl.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vssrl.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vssrl_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vssrl.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vssrl.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vssrl.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vssrl.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vssrl_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vssrl.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vssrl.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vssrl.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vssrl.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vssrl_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vssrl.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vssrl.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vssrl.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vssrl.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vssrl_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vssrl.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vssrl.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vssrl.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vssrl.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vssrl_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vssrl.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vssrl.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vssrl.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vssrl.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vssrl_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vssrl.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vssrl.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vssrl.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vssrl.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vssrl_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vssrl.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vssrl.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vssrl.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vssrl.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vssrl_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vssrl.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vssrl.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vssrl.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vssrl.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vssrl_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vssrl.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vssrl.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vssrl.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssrl.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vssrl_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vssrl.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssrl.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vssrl.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vssrl.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vssrl_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vssrl.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vssrl.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vssrl.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vssrl.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vssrl_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vssrl.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vssrl.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vssrl.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vssrl.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vssrl_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vssrl.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vssrl.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssrl_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vssrl.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vssrl.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vssrl.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vssrl.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vssrl.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vssrl.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vssrl.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vssrl.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vssrl.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vssrl.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vssrl.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vssrl.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vssrl.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vssrl.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vssrl.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vssrl.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vssrl.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vssrl.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vssrl.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vssrl.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vssrl.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vssrl.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vssrl.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vssrl.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vssrl.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vssrl.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vssrl.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vssrl.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vssrl.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vssrl.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vssrl_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vssrl.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vssrl_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssrl_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vssrl.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vssrl.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vssra.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vssra_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vssra.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vssra.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vssra.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vssra.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vssra_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vssra.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vssra.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vssra.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vssra.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vssra_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vssra.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vssra.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vssra.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vssra.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vssra_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vssra.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vssra.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vssra.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vssra.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vssra_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vssra.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vssra.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vssra.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vssra.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vssra_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vssra.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vssra.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vssra.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vssra.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vssra_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vssra.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vssra.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vssra.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vssra.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vssra_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vssra.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vssra.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vssra.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vssra.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vssra_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vssra.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vssra.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vssra.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vssra.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vssra_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vssra.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vssra.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vssra.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vssra.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vssra_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vssra.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vssra.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vssra.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssra.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vssra_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vssra.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssra.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vssra.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vssra.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vssra_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vssra.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vssra.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vssra.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vssra.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vssra_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vssra.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vssra.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vssra.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vssra.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vssra_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vssra.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vssra.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssra_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vssra.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vssra.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vssra.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vssra_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vssra.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vssra.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vssra.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vssra.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vssra_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vssra.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vssra.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vssra.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vssra.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vssra_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vssra.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vssra.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vssra.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vssra.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vssra_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vssra.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vssra.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vssra.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vssra.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vssra_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vssra.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vssra.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vssra.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vssra.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vssra_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vssra.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vssra.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vssra.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vssra.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vssra_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vssra.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vssra.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vssra.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vssra.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vssra_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vssra.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vssra.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vssra.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vssra.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vssra_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vssra.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vssra.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vssra.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vssra.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vssra_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vssra.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vssra.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vssra.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vssra.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vssra_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vssra.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vssra.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vssra.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vssra.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vssra_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vssra.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vssra.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vssra.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vssra.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vssra_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vssra.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vssra.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vssra.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vssra.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vssra_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vssra.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vssra.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vssra.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vssra.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vssra_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vssra.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vssra.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vssra_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vssra.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vssra.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vssra.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vssra.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vssra.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vssra.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vssra.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vssra.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vssra.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vssra.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vssra.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vssra.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vssra.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vssra.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vssra.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vssra.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vssra.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vssra.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vssra.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vssra.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vssra.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vssra.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vssra.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vssra.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vssra.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vssra.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vssra.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vssra.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vssra.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vssra.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vssra_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vssra.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vssra_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vssra_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vssra.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vssra.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfadd.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfadd_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x float> @llvm.epi.vfadd.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfadd.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfadd.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfadd.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfadd_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vfadd.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfadd.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfadd.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfadd.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfadd_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vfadd.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfadd.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfadd.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfadd.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfadd_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x float> @llvm.epi.vfadd.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfadd.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfadd.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfadd_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfadd.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfadd.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfadd.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfadd_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfadd.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfadd.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfadd.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfadd.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfadd_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfadd.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfadd.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfadd.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfadd.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfadd_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfadd.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfadd.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfadd.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfadd.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfadd_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfadd.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x float> @llvm.epi.vfadd.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfadd.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfadd.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfadd.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfadd.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfadd_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfadd.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x float> @llvm.epi.vfadd.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfadd.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfadd.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfadd.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfadd.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfadd_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfadd.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x float> @llvm.epi.vfadd.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfadd.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfadd.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfadd.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfadd.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfadd_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfadd.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 16 x float> @llvm.epi.vfadd.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfadd.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfadd.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfadd.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfadd_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfadd.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfadd.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfadd.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfadd.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfadd.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfadd_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfadd.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfadd.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfadd.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfadd.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfadd.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfadd.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfadd_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfadd.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfadd.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfadd.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfadd.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfadd.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfadd.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfadd_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfadd.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfadd.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfadd.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfadd_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfadd_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfadd.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfadd.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfsub.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfsub_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x float> @llvm.epi.vfsub.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfsub.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfsub.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfsub.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfsub_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vfsub.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfsub.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfsub.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfsub.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfsub_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vfsub.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfsub.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfsub.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfsub.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfsub_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x float> @llvm.epi.vfsub.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfsub.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfsub.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfsub_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsub.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfsub.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfsub.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfsub_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfsub.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfsub.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfsub.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfsub.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfsub_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfsub.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfsub.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfsub.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfsub.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfsub_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfsub.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfsub.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfsub.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfsub.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfsub_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfsub.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x float> @llvm.epi.vfsub.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfsub.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfsub.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfsub.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfsub.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfsub_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfsub.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x float> @llvm.epi.vfsub.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfsub.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfsub.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfsub.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfsub.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfsub_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfsub.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x float> @llvm.epi.vfsub.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfsub.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfsub.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfsub.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfsub.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfsub_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfsub.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 16 x float> @llvm.epi.vfsub.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfsub.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfsub.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfsub.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfsub_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfsub.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsub.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfsub.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfsub.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfsub.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfsub_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfsub.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfsub.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfsub.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfsub.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfsub.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfsub.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfsub_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfsub.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfsub.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfsub.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfsub.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfsub.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfsub.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfsub_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfsub.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfsub.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfsub.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsub_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsub_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfsub.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfsub.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfwadd.nxv2f64.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfwadd_vv_nxv2f64_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_vv_nxv2f64_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfwadd.nxv2f64.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfwadd.mask.nxv2f64.nxv2f32.nxv2f32(
  <vscale x 2 x double>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfwadd_mask_vv_nxv2f64_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_mask_vv_nxv2f64_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfwadd.mask.nxv2f64.nxv2f32.nxv2f32(
    <vscale x 2 x double> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfwadd.nxv4f64.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfwadd_vv_nxv4f64_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_vv_nxv4f64_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfwadd.nxv4f64.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfwadd.mask.nxv4f64.nxv4f32.nxv4f32(
  <vscale x 4 x double>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfwadd_mask_vv_nxv4f64_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_mask_vv_nxv4f64_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfwadd.mask.nxv4f64.nxv4f32.nxv4f32(
    <vscale x 4 x double> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfwadd.nxv8f64.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfwadd_vv_nxv8f64_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_vv_nxv8f64_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfwadd.nxv8f64.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfwadd.mask.nxv8f64.nxv8f32.nxv8f32(
  <vscale x 8 x double>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfwadd_mask_vv_nxv8f64_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_mask_vv_nxv8f64_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwadd.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfwadd.mask.nxv8f64.nxv8f32.nxv8f32(
    <vscale x 8 x double> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfwadd.nxv2f64.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfwadd_vf_nxv2f64_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_vf_nxv2f64_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwadd.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfwadd.nxv2f64.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfwadd.mask.nxv2f64.nxv2f32.f32(
  <vscale x 2 x double>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfwadd_mask_vf_nxv2f64_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_mask_vf_nxv2f64_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwadd.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfwadd.mask.nxv2f64.nxv2f32.f32(
    <vscale x 2 x double> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfwadd.nxv4f64.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfwadd_vf_nxv4f64_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_vf_nxv4f64_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwadd.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfwadd.nxv4f64.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfwadd.mask.nxv4f64.nxv4f32.f32(
  <vscale x 4 x double>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfwadd_mask_vf_nxv4f64_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_mask_vf_nxv4f64_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwadd.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfwadd.mask.nxv4f64.nxv4f32.f32(
    <vscale x 4 x double> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfwadd.nxv8f64.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfwadd_vf_nxv8f64_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_vf_nxv8f64_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwadd.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfwadd.nxv8f64.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfwadd.mask.nxv8f64.nxv8f32.f32(
  <vscale x 8 x double>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfwadd_mask_vf_nxv8f64_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd_mask_vf_nxv8f64_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwadd.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfwadd.mask.nxv8f64.nxv8f32.f32(
    <vscale x 8 x double> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfwsub.nxv2f64.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfwsub_vv_nxv2f64_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_vv_nxv2f64_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfwsub.nxv2f64.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfwsub.mask.nxv2f64.nxv2f32.nxv2f32(
  <vscale x 2 x double>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfwsub_mask_vv_nxv2f64_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_mask_vv_nxv2f64_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfwsub.mask.nxv2f64.nxv2f32.nxv2f32(
    <vscale x 2 x double> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfwsub.nxv4f64.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfwsub_vv_nxv4f64_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_vv_nxv4f64_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfwsub.nxv4f64.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfwsub.mask.nxv4f64.nxv4f32.nxv4f32(
  <vscale x 4 x double>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfwsub_mask_vv_nxv4f64_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_mask_vv_nxv4f64_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfwsub.mask.nxv4f64.nxv4f32.nxv4f32(
    <vscale x 4 x double> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfwsub.nxv8f64.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfwsub_vv_nxv8f64_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_vv_nxv8f64_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfwsub.nxv8f64.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfwsub.mask.nxv8f64.nxv8f32.nxv8f32(
  <vscale x 8 x double>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfwsub_mask_vv_nxv8f64_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_mask_vv_nxv8f64_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwsub.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfwsub.mask.nxv8f64.nxv8f32.nxv8f32(
    <vscale x 8 x double> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfwsub.nxv2f64.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfwsub_vf_nxv2f64_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_vf_nxv2f64_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwsub.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfwsub.nxv2f64.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfwsub.mask.nxv2f64.nxv2f32.f32(
  <vscale x 2 x double>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfwsub_mask_vf_nxv2f64_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_mask_vf_nxv2f64_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwsub.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfwsub.mask.nxv2f64.nxv2f32.f32(
    <vscale x 2 x double> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfwsub.nxv4f64.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfwsub_vf_nxv4f64_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_vf_nxv4f64_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwsub.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfwsub.nxv4f64.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfwsub.mask.nxv4f64.nxv4f32.f32(
  <vscale x 4 x double>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfwsub_mask_vf_nxv4f64_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_mask_vf_nxv4f64_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwsub.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfwsub.mask.nxv4f64.nxv4f32.f32(
    <vscale x 4 x double> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfwsub.nxv8f64.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfwsub_vf_nxv8f64_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_vf_nxv8f64_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwsub.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfwsub.nxv8f64.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfwsub.mask.nxv8f64.nxv8f32.f32(
  <vscale x 8 x double>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfwsub_mask_vf_nxv8f64_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub_mask_vf_nxv8f64_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwsub.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfwsub.mask.nxv8f64.nxv8f32.f32(
    <vscale x 8 x double> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfwadd.w.nxv2f64.nxv2f32(
  <vscale x 2 x double>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfwadd.w_wv_nxv2f64_nxv2f64_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd.w_wv_nxv2f64_nxv2f64_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfwadd.w.nxv2f64.nxv2f32(
    <vscale x 2 x double> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfwadd.w.mask.nxv2f64.nxv2f32(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfwadd.w_mask_wv_nxv2f64_nxv2f64_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd.w_mask_wv_nxv2f64_nxv2f64_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfwadd.w.mask.nxv2f64.nxv2f32(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfwadd.w.nxv4f64.nxv4f32(
  <vscale x 4 x double>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfwadd.w_wv_nxv4f64_nxv4f64_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd.w_wv_nxv4f64_nxv4f64_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfwadd.w.nxv4f64.nxv4f32(
    <vscale x 4 x double> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfwadd.w.mask.nxv4f64.nxv4f32(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfwadd.w_mask_wv_nxv4f64_nxv4f64_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd.w_mask_wv_nxv4f64_nxv4f64_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfwadd.w.mask.nxv4f64.nxv4f32(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfwadd.w.nxv8f64.nxv8f32(
  <vscale x 8 x double>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfwadd.w_wv_nxv8f64_nxv8f64_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd.w_wv_nxv8f64_nxv8f64_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfwadd.w.nxv8f64.nxv8f32(
    <vscale x 8 x double> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfwadd.w.mask.nxv8f64.nxv8f32(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfwadd.w_mask_wv_nxv8f64_nxv8f64_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd.w_mask_wv_nxv8f64_nxv8f64_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwadd.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfwadd.w.mask.nxv8f64.nxv8f32(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfwadd.w.nxv2f64.f32(
  <vscale x 2 x double>,
  float,
  i64);

define void @intrinsic_vfwadd.w_wf_nxv2f64_nxv2f64_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd.w_wf_nxv2f64_nxv2f64_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwadd.wf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfwadd.w.nxv2f64.f32(
    <vscale x 2 x double> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfwadd.w.mask.nxv2f64.f32(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfwadd.w_mask_wf_nxv2f64_nxv2f64_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd.w_mask_wf_nxv2f64_nxv2f64_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwadd.wf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfwadd.w.mask.nxv2f64.f32(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfwadd.w.nxv4f64.f32(
  <vscale x 4 x double>,
  float,
  i64);

define void @intrinsic_vfwadd.w_wf_nxv4f64_nxv4f64_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd.w_wf_nxv4f64_nxv4f64_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwadd.wf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfwadd.w.nxv4f64.f32(
    <vscale x 4 x double> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfwadd.w.mask.nxv4f64.f32(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfwadd.w_mask_wf_nxv4f64_nxv4f64_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd.w_mask_wf_nxv4f64_nxv4f64_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwadd.wf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfwadd.w.mask.nxv4f64.f32(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfwadd.w.nxv8f64.f32(
  <vscale x 8 x double>,
  float,
  i64);

define void @intrinsic_vfwadd.w_wf_nxv8f64_nxv8f64_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd.w_wf_nxv8f64_nxv8f64_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwadd.wf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfwadd.w.nxv8f64.f32(
    <vscale x 8 x double> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfwadd.w.mask.nxv8f64.f32(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfwadd.w_mask_wf_nxv8f64_nxv8f64_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwadd.w_mask_wf_nxv8f64_nxv8f64_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwadd.wf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfwadd.w.mask.nxv8f64.f32(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfwsub.w.nxv2f64.nxv2f32(
  <vscale x 2 x double>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfwsub.w_wv_nxv2f64_nxv2f64_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub.w_wv_nxv2f64_nxv2f64_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfwsub.w.nxv2f64.nxv2f32(
    <vscale x 2 x double> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfwsub.w.mask.nxv2f64.nxv2f32(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfwsub.w_mask_wv_nxv2f64_nxv2f64_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub.w_mask_wv_nxv2f64_nxv2f64_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfwsub.w.mask.nxv2f64.nxv2f32(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfwsub.w.nxv4f64.nxv4f32(
  <vscale x 4 x double>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfwsub.w_wv_nxv4f64_nxv4f64_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub.w_wv_nxv4f64_nxv4f64_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfwsub.w.nxv4f64.nxv4f32(
    <vscale x 4 x double> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfwsub.w.mask.nxv4f64.nxv4f32(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfwsub.w_mask_wv_nxv4f64_nxv4f64_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub.w_mask_wv_nxv4f64_nxv4f64_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfwsub.w.mask.nxv4f64.nxv4f32(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfwsub.w.nxv8f64.nxv8f32(
  <vscale x 8 x double>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfwsub.w_wv_nxv8f64_nxv8f64_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub.w_wv_nxv8f64_nxv8f64_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfwsub.w.nxv8f64.nxv8f32(
    <vscale x 8 x double> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfwsub.w.mask.nxv8f64.nxv8f32(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfwsub.w_mask_wv_nxv8f64_nxv8f64_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub.w_mask_wv_nxv8f64_nxv8f64_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwsub.wv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfwsub.w.mask.nxv8f64.nxv8f32(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfwsub.w.nxv2f64.f32(
  <vscale x 2 x double>,
  float,
  i64);

define void @intrinsic_vfwsub.w_wf_nxv2f64_nxv2f64_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub.w_wf_nxv2f64_nxv2f64_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwsub.wf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfwsub.w.nxv2f64.f32(
    <vscale x 2 x double> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfwsub.w.mask.nxv2f64.f32(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfwsub.w_mask_wf_nxv2f64_nxv2f64_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub.w_mask_wf_nxv2f64_nxv2f64_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwsub.wf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfwsub.w.mask.nxv2f64.f32(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfwsub.w.nxv4f64.f32(
  <vscale x 4 x double>,
  float,
  i64);

define void @intrinsic_vfwsub.w_wf_nxv4f64_nxv4f64_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub.w_wf_nxv4f64_nxv4f64_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwsub.wf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfwsub.w.nxv4f64.f32(
    <vscale x 4 x double> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfwsub.w.mask.nxv4f64.f32(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfwsub.w_mask_wf_nxv4f64_nxv4f64_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub.w_mask_wf_nxv4f64_nxv4f64_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwsub.wf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfwsub.w.mask.nxv4f64.f32(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfwsub.w.nxv8f64.f32(
  <vscale x 8 x double>,
  float,
  i64);

define void @intrinsic_vfwsub.w_wf_nxv8f64_nxv8f64_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub.w_wf_nxv8f64_nxv8f64_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwsub.wf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfwsub.w.nxv8f64.f32(
    <vscale x 8 x double> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfwsub.w.mask.nxv8f64.f32(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfwsub.w_mask_wf_nxv8f64_nxv8f64_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwsub.w_mask_wf_nxv8f64_nxv8f64_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwsub.wf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfwsub.w.mask.nxv8f64.f32(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmul.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfmul_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x float> @llvm.epi.vfmul.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfmul.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfmul.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmul.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfmul_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vfmul.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfmul.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfmul.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmul.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfmul_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vfmul.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfmul.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfmul.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmul.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfmul_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x float> @llvm.epi.vfmul.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfmul.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfmul.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfmul_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmul.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfmul.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmul.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfmul_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfmul.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfmul.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfmul.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmul.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfmul_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfmul.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfmul.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfmul.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmul.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfmul_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfmul.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfmul.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfmul.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmul.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfmul_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmul.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x float> @llvm.epi.vfmul.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfmul.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmul.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfmul.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmul.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfmul_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmul.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x float> @llvm.epi.vfmul.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfmul.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmul.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfmul.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmul.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfmul_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmul.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x float> @llvm.epi.vfmul.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfmul.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmul.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfmul.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmul.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfmul_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmul.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 16 x float> @llvm.epi.vfmul.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfmul.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmul.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfmul.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfmul_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmul.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vfmul.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmul.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmul.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfmul.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmul.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfmul_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmul.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfmul.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfmul.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmul.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfmul.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmul.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfmul_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmul.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfmul.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfmul.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmul.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfmul.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmul.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfmul_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmul.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfmul.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfmul.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmul_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmul_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmul.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfmul.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfdiv.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfdiv_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x float> @llvm.epi.vfdiv.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfdiv.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfdiv.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfdiv.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfdiv_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vfdiv.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfdiv.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfdiv.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfdiv.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfdiv_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vfdiv.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfdiv.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfdiv.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfdiv.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfdiv_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x float> @llvm.epi.vfdiv.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfdiv.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfdiv.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfdiv.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfdiv_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x double> @llvm.epi.vfdiv.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfdiv.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfdiv.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfdiv.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfdiv_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfdiv.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfdiv.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfdiv.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfdiv.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfdiv_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfdiv.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfdiv.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfdiv.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfdiv.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfdiv_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfdiv.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfdiv.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfdiv.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfdiv.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfdiv.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfdiv_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x float> @llvm.epi.vfdiv.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfdiv.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfdiv.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfdiv.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfdiv_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x float> @llvm.epi.vfdiv.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfdiv.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfdiv.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfdiv.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfdiv_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x float> @llvm.epi.vfdiv.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfdiv.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfdiv.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfdiv.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfdiv_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 16 x float> @llvm.epi.vfdiv.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfdiv.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfdiv.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfdiv.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfdiv_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vfdiv.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfdiv.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfdiv.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfdiv.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfdiv_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfdiv.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfdiv.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfdiv.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfdiv.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfdiv_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfdiv.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfdiv.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfdiv.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfdiv.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfdiv_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfdiv.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfdiv.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfdiv_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfdiv_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfdiv.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfrdiv.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfrdiv_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfrdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x float> @llvm.epi.vfrdiv.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfrdiv.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfrdiv_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfrdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfrdiv.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfrdiv.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfrdiv_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfrdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x float> @llvm.epi.vfrdiv.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfrdiv.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfrdiv_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfrdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfrdiv.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfrdiv.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfrdiv_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfrdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x float> @llvm.epi.vfrdiv.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfrdiv.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfrdiv_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfrdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfrdiv.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfrdiv.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfrdiv_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfrdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 16 x float> @llvm.epi.vfrdiv.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfrdiv.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfrdiv_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfrdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfrdiv.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfrdiv.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfrdiv_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfrdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vfrdiv.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfrdiv.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfrdiv_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfrdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfrdiv.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfrdiv.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfrdiv_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfrdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfrdiv.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfrdiv.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfrdiv_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfrdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfrdiv.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfrdiv.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfrdiv_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfrdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfrdiv.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfrdiv.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfrdiv_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfrdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfrdiv.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfrdiv.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfrdiv_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfrdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfrdiv.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfrdiv.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfrdiv_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfrdiv_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfrdiv.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfrdiv.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfwmul.nxv2f64.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfwmul_vv_nxv2f64_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_vv_nxv2f64_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfwmul.nxv2f64.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfwmul.mask.nxv2f64.nxv2f32.nxv2f32(
  <vscale x 2 x double>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfwmul_mask_vv_nxv2f64_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_mask_vv_nxv2f64_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfwmul.mask.nxv2f64.nxv2f32.nxv2f32(
    <vscale x 2 x double> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfwmul.nxv4f64.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfwmul_vv_nxv4f64_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_vv_nxv4f64_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfwmul.nxv4f64.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfwmul.mask.nxv4f64.nxv4f32.nxv4f32(
  <vscale x 4 x double>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfwmul_mask_vv_nxv4f64_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_mask_vv_nxv4f64_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfwmul.mask.nxv4f64.nxv4f32.nxv4f32(
    <vscale x 4 x double> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfwmul.nxv8f64.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfwmul_vv_nxv8f64_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_vv_nxv8f64_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfwmul.nxv8f64.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfwmul.mask.nxv8f64.nxv8f32.nxv8f32(
  <vscale x 8 x double>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfwmul_mask_vv_nxv8f64_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_mask_vv_nxv8f64_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwmul.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfwmul.mask.nxv8f64.nxv8f32.nxv8f32(
    <vscale x 8 x double> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfwmul.nxv2f64.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfwmul_vf_nxv2f64_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_vf_nxv2f64_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwmul.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfwmul.nxv2f64.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfwmul.mask.nxv2f64.nxv2f32.f32(
  <vscale x 2 x double>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfwmul_mask_vf_nxv2f64_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_mask_vf_nxv2f64_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwmul.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfwmul.mask.nxv2f64.nxv2f32.f32(
    <vscale x 2 x double> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfwmul.nxv4f64.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfwmul_vf_nxv4f64_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_vf_nxv4f64_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwmul.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfwmul.nxv4f64.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfwmul.mask.nxv4f64.nxv4f32.f32(
  <vscale x 4 x double>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfwmul_mask_vf_nxv4f64_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_mask_vf_nxv4f64_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwmul.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfwmul.mask.nxv4f64.nxv4f32.f32(
    <vscale x 4 x double> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfwmul.nxv8f64.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfwmul_vf_nxv8f64_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_vf_nxv8f64_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwmul.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfwmul.nxv8f64.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfwmul.mask.nxv8f64.nxv8f32.f32(
  <vscale x 8 x double>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfwmul_mask_vf_nxv8f64_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwmul_mask_vf_nxv8f64_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwmul.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfwmul.mask.nxv8f64.nxv8f32.f32(
    <vscale x 8 x double> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmin.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfmin_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x float> @llvm.epi.vfmin.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfmin.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfmin.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmin.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfmin_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vfmin.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfmin.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfmin.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmin.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfmin_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vfmin.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfmin.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfmin.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmin.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfmin_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x float> @llvm.epi.vfmin.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfmin.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfmin.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmin.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfmin_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x double> @llvm.epi.vfmin.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmin.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfmin.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmin.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfmin_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfmin.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfmin.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfmin.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmin.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfmin_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfmin.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfmin.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfmin.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmin.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfmin_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfmin.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfmin.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmin.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfmin.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmin.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfmin_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmin.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x float> @llvm.epi.vfmin.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfmin.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmin.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfmin.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmin.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfmin_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmin.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x float> @llvm.epi.vfmin.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfmin.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmin.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfmin.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmin.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfmin_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmin.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x float> @llvm.epi.vfmin.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfmin.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmin.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfmin.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmin.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfmin_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmin.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 16 x float> @llvm.epi.vfmin.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfmin.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmin.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfmin.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmin.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfmin_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmin.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vfmin.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmin.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmin.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfmin.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmin.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfmin_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmin.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfmin.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfmin.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmin.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfmin.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmin.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfmin_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmin.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfmin.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfmin.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmin.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfmin.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmin.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfmin_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmin.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfmin.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfmin.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmin_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmin_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmin.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfmin.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmax.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfmax_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x float> @llvm.epi.vfmax.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfmax.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfmax.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmax.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfmax_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vfmax.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfmax.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfmax.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmax.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfmax_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vfmax.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfmax.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfmax.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmax.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfmax_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x float> @llvm.epi.vfmax.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfmax.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfmax.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmax.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfmax_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x double> @llvm.epi.vfmax.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmax.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfmax.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmax.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfmax_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfmax.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfmax.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfmax.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmax.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfmax_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfmax.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfmax.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfmax.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmax.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfmax_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfmax.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfmax.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmax.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfmax.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmax.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfmax_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmax.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x float> @llvm.epi.vfmax.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfmax.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmax.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfmax.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmax.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfmax_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmax.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x float> @llvm.epi.vfmax.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfmax.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmax.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfmax.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmax.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfmax_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmax.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x float> @llvm.epi.vfmax.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfmax.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmax.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfmax.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmax.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfmax_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmax.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 16 x float> @llvm.epi.vfmax.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfmax.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmax.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfmax.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmax.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfmax_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmax.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vfmax.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmax.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmax.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfmax.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmax.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfmax_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmax.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfmax.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfmax.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmax.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfmax.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmax.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfmax_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmax.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfmax.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfmax.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmax.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfmax.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmax.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfmax_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmax.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfmax.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfmax.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmax_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmax_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmax.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfmax.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfsgnj.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfsgnj_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfsgnj.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnj.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfsgnj.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfsgnj.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnj.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfsgnj.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfsgnj_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfsgnj.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnj.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfsgnj.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfsgnj.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnj.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfsgnj.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfsgnj_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfsgnj.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnj.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfsgnj.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfsgnj.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnj.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfsgnj.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfsgnj_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfsgnj.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnj.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfsgnj.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfsgnj.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnj.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsgnj.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfsgnj_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfsgnj.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnj.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsgnj.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfsgnj.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnj.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfsgnj.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfsgnj_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfsgnj.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnj.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfsgnj.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfsgnj.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnj.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfsgnj.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfsgnj_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfsgnj.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnj.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfsgnj.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfsgnj.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnj.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfsgnj.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfsgnj_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfsgnj.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnj.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfsgnj.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfsgnj.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnj.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfsgnj.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfsgnj_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfsgnj.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnj.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfsgnj.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfsgnj.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnj.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfsgnj.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfsgnj_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfsgnj.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnj.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfsgnj.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfsgnj.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnj.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfsgnj.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfsgnj_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfsgnj.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnj.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfsgnj.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfsgnj.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnj.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfsgnj.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfsgnj_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfsgnj.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnj.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfsgnj.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfsgnj.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnj.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsgnj.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfsgnj_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfsgnj.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnj.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsgnj.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfsgnj.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnj.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfsgnj.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfsgnj_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfsgnj.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnj.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfsgnj.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfsgnj.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnj.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfsgnj.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfsgnj_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfsgnj.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnj.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfsgnj.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfsgnj.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnj.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfsgnj.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfsgnj_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfsgnj.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnj.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfsgnj.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnj_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnj_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfsgnj.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnj.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfsgnjn.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfsgnjn_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfsgnjn.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnjn.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfsgnjn.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfsgnjn.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnjn.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfsgnjn.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfsgnjn_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfsgnjn.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnjn.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfsgnjn.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfsgnjn.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnjn.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfsgnjn.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfsgnjn_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfsgnjn.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnjn.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfsgnjn.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfsgnjn.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnjn.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfsgnjn.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfsgnjn_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfsgnjn.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnjn.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfsgnjn.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfsgnjn.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnjn.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfsgnjn_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfsgnjn.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfsgnjn.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfsgnjn.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfsgnjn_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfsgnjn.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnjn.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfsgnjn.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfsgnjn.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnjn.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfsgnjn.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfsgnjn_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfsgnjn.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnjn.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfsgnjn.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfsgnjn.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnjn.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfsgnjn.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfsgnjn_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfsgnjn.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnjn.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfsgnjn.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfsgnjn.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnjn.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfsgnjn.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfsgnjn_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfsgnjn.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnjn.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfsgnjn.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfsgnjn.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnjn.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfsgnjn.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfsgnjn_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfsgnjn.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnjn.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfsgnjn.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfsgnjn.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnjn.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfsgnjn.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfsgnjn_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfsgnjn.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnjn.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfsgnjn.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfsgnjn.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnjn.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfsgnjn.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfsgnjn_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfsgnjn.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnjn.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfsgnjn.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfsgnjn.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnjn.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfsgnjn_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfsgnjn.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfsgnjn.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfsgnjn.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfsgnjn_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfsgnjn.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnjn.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfsgnjn.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfsgnjn.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnjn.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfsgnjn.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfsgnjn_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfsgnjn.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnjn.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfsgnjn.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfsgnjn.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnjn.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfsgnjn.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfsgnjn_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfsgnjn.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnjn.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfsgnjn.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnjn_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjn_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfsgnjn.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnjn.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfsgnjx.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfsgnjx_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfsgnjx.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnjx.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfsgnjx.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfsgnjx.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnjx.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfsgnjx.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfsgnjx_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfsgnjx.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnjx.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfsgnjx.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfsgnjx.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnjx.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfsgnjx.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfsgnjx_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfsgnjx.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnjx.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfsgnjx.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfsgnjx.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnjx.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfsgnjx.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfsgnjx_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfsgnjx.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnjx.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfsgnjx.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfsgnjx.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnjx.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsgnjx.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfsgnjx_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfsgnjx.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnjx.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsgnjx.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfsgnjx.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnjx.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfsgnjx.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfsgnjx_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfsgnjx.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnjx.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfsgnjx.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfsgnjx.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnjx.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfsgnjx.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfsgnjx_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfsgnjx.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnjx.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfsgnjx.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfsgnjx.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnjx.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfsgnjx.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfsgnjx_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfsgnjx.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnjx.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfsgnjx.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfsgnjx.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnjx.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfsgnjx.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vfsgnjx_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfsgnjx.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnjx.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfsgnjx.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfsgnjx.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfsgnjx.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfsgnjx.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vfsgnjx_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfsgnjx.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnjx.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfsgnjx.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfsgnjx.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfsgnjx.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfsgnjx.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vfsgnjx_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfsgnjx.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnjx.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfsgnjx.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfsgnjx.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfsgnjx.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfsgnjx.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vfsgnjx_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfsgnjx.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnjx.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfsgnjx.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfsgnjx.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfsgnjx.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfsgnjx.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vfsgnjx_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfsgnjx.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnjx.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfsgnjx.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfsgnjx.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfsgnjx.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfsgnjx.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vfsgnjx_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfsgnjx.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnjx.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfsgnjx.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfsgnjx.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfsgnjx.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfsgnjx.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vfsgnjx_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfsgnjx.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnjx.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfsgnjx.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfsgnjx.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfsgnjx.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfsgnjx.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vfsgnjx_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfsgnjx.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnjx.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfsgnjx.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfsgnjx_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfsgnjx_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfsgnjx.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfsgnjx.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmfeq.nxv2i1.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vmfeq_vv_nxv2i1_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_vv_nxv2i1_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmfeq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmfeq.nxv2i1.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmfeq.mask.nxv2i1.nxv2f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmfeq_mask_vv_nxv2i1_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_mask_vv_nxv2i1_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmfeq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmfeq.mask.nxv2i1.nxv2f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmfeq.nxv4i1.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vmfeq_vv_nxv4i1_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_vv_nxv4i1_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmfeq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmfeq.nxv4i1.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmfeq.mask.nxv4i1.nxv4f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmfeq_mask_vv_nxv4i1_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_mask_vv_nxv4i1_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmfeq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmfeq.mask.nxv4i1.nxv4f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmfeq.nxv8i1.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vmfeq_vv_nxv8i1_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_vv_nxv8i1_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmfeq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmfeq.nxv8i1.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmfeq.mask.nxv8i1.nxv8f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmfeq_mask_vv_nxv8i1_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_mask_vv_nxv8i1_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmfeq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmfeq.mask.nxv8i1.nxv8f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmfeq.nxv16i1.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vmfeq_vv_nxv16i1_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_vv_nxv16i1_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmfeq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmfeq.nxv16i1.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmfeq.mask.nxv16i1.nxv16f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmfeq_mask_vv_nxv16i1_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_mask_vv_nxv16i1_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmfeq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmfeq.mask.nxv16i1.nxv16f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmfeq.nxv1i1.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vmfeq_vv_nxv1i1_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_vv_nxv1i1_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmfeq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmfeq.nxv1i1.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmfeq.mask.nxv1i1.nxv1f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmfeq_mask_vv_nxv1i1_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_mask_vv_nxv1i1_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmfeq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmfeq.mask.nxv1i1.nxv1f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmfeq.nxv2i1.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vmfeq_vv_nxv2i1_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_vv_nxv2i1_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmfeq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmfeq.nxv2i1.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmfeq.mask.nxv2i1.nxv2f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmfeq_mask_vv_nxv2i1_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_mask_vv_nxv2i1_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmfeq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmfeq.mask.nxv2i1.nxv2f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmfeq.nxv4i1.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vmfeq_vv_nxv4i1_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_vv_nxv4i1_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmfeq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmfeq.nxv4i1.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmfeq.mask.nxv4i1.nxv4f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmfeq_mask_vv_nxv4i1_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_mask_vv_nxv4i1_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmfeq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmfeq.mask.nxv4i1.nxv4f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmfeq.nxv8i1.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vmfeq_vv_nxv8i1_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_vv_nxv8i1_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmfeq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmfeq.nxv8i1.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmfeq.mask.nxv8i1.nxv8f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmfeq_mask_vv_nxv8i1_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_mask_vv_nxv8i1_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmfeq.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmfeq.mask.nxv8i1.nxv8f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmfeq.nxv2i1.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vmfeq_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmfeq.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vmfeq.nxv2i1.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmfeq.mask.nxv2i1.f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmfeq_mask_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_mask_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmfeq.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmfeq.mask.nxv2i1.f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmfeq.nxv4i1.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vmfeq_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmfeq.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vmfeq.nxv4i1.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmfeq.mask.nxv4i1.f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmfeq_mask_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_mask_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmfeq.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmfeq.mask.nxv4i1.f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmfeq.nxv8i1.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vmfeq_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmfeq.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vmfeq.nxv8i1.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmfeq.mask.nxv8i1.f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmfeq_mask_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_mask_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmfeq.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmfeq.mask.nxv8i1.f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmfeq.nxv16i1.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vmfeq_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmfeq.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 16 x i1> @llvm.epi.vmfeq.nxv16i1.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmfeq.mask.nxv16i1.f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmfeq_mask_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_mask_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmfeq.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmfeq.mask.nxv16i1.f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmfeq.nxv1i1.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vmfeq_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmfeq.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 1 x i1> @llvm.epi.vmfeq.nxv1i1.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmfeq.mask.nxv1i1.f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmfeq_mask_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_mask_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmfeq.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmfeq.mask.nxv1i1.f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmfeq.nxv2i1.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vmfeq_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmfeq.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vmfeq.nxv2i1.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmfeq.mask.nxv2i1.f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmfeq_mask_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_mask_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmfeq.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmfeq.mask.nxv2i1.f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmfeq.nxv4i1.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vmfeq_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmfeq.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vmfeq.nxv4i1.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmfeq.mask.nxv4i1.f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmfeq_mask_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_mask_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmfeq.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmfeq.mask.nxv4i1.f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmfeq.nxv8i1.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vmfeq_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmfeq.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vmfeq.nxv8i1.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmfeq.mask.nxv8i1.f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmfeq_mask_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfeq_mask_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmfeq.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmfeq.mask.nxv8i1.f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmfne.nxv2i1.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vmfne_vv_nxv2i1_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_vv_nxv2i1_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmfne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmfne.nxv2i1.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmfne.mask.nxv2i1.nxv2f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmfne_mask_vv_nxv2i1_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_mask_vv_nxv2i1_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmfne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmfne.mask.nxv2i1.nxv2f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmfne.nxv4i1.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vmfne_vv_nxv4i1_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_vv_nxv4i1_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmfne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmfne.nxv4i1.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmfne.mask.nxv4i1.nxv4f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmfne_mask_vv_nxv4i1_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_mask_vv_nxv4i1_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmfne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmfne.mask.nxv4i1.nxv4f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmfne.nxv8i1.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vmfne_vv_nxv8i1_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_vv_nxv8i1_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmfne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmfne.nxv8i1.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmfne.mask.nxv8i1.nxv8f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmfne_mask_vv_nxv8i1_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_mask_vv_nxv8i1_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmfne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmfne.mask.nxv8i1.nxv8f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmfne.nxv16i1.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vmfne_vv_nxv16i1_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_vv_nxv16i1_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmfne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmfne.nxv16i1.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmfne.mask.nxv16i1.nxv16f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmfne_mask_vv_nxv16i1_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_mask_vv_nxv16i1_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmfne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmfne.mask.nxv16i1.nxv16f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmfne.nxv1i1.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vmfne_vv_nxv1i1_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_vv_nxv1i1_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmfne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmfne.nxv1i1.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmfne.mask.nxv1i1.nxv1f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmfne_mask_vv_nxv1i1_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_mask_vv_nxv1i1_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmfne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmfne.mask.nxv1i1.nxv1f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmfne.nxv2i1.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vmfne_vv_nxv2i1_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_vv_nxv2i1_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmfne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmfne.nxv2i1.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmfne.mask.nxv2i1.nxv2f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmfne_mask_vv_nxv2i1_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_mask_vv_nxv2i1_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmfne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmfne.mask.nxv2i1.nxv2f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmfne.nxv4i1.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vmfne_vv_nxv4i1_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_vv_nxv4i1_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmfne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmfne.nxv4i1.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmfne.mask.nxv4i1.nxv4f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmfne_mask_vv_nxv4i1_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_mask_vv_nxv4i1_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmfne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmfne.mask.nxv4i1.nxv4f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmfne.nxv8i1.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vmfne_vv_nxv8i1_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_vv_nxv8i1_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmfne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmfne.nxv8i1.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmfne.mask.nxv8i1.nxv8f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmfne_mask_vv_nxv8i1_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_mask_vv_nxv8i1_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmfne.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmfne.mask.nxv8i1.nxv8f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmfne.nxv2i1.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vmfne_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmfne.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vmfne.nxv2i1.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmfne.mask.nxv2i1.f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmfne_mask_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_mask_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmfne.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmfne.mask.nxv2i1.f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmfne.nxv4i1.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vmfne_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmfne.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vmfne.nxv4i1.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmfne.mask.nxv4i1.f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmfne_mask_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_mask_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmfne.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmfne.mask.nxv4i1.f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmfne.nxv8i1.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vmfne_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmfne.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vmfne.nxv8i1.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmfne.mask.nxv8i1.f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmfne_mask_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_mask_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmfne.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmfne.mask.nxv8i1.f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmfne.nxv16i1.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vmfne_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmfne.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 16 x i1> @llvm.epi.vmfne.nxv16i1.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmfne.mask.nxv16i1.f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmfne_mask_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_mask_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmfne.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmfne.mask.nxv16i1.f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmfne.nxv1i1.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vmfne_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmfne.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 1 x i1> @llvm.epi.vmfne.nxv1i1.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmfne.mask.nxv1i1.f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmfne_mask_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_mask_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmfne.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmfne.mask.nxv1i1.f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmfne.nxv2i1.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vmfne_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmfne.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vmfne.nxv2i1.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmfne.mask.nxv2i1.f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmfne_mask_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_mask_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmfne.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmfne.mask.nxv2i1.f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmfne.nxv4i1.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vmfne_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmfne.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vmfne.nxv4i1.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmfne.mask.nxv4i1.f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmfne_mask_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_mask_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmfne.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmfne.mask.nxv4i1.f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmfne.nxv8i1.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vmfne_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmfne.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vmfne.nxv8i1.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmfne.mask.nxv8i1.f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmfne_mask_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfne_mask_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmfne.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmfne.mask.nxv8i1.f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmflt.nxv2i1.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vmflt_vv_nxv2i1_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_vv_nxv2i1_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmflt.nxv2i1.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmflt.mask.nxv2i1.nxv2f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmflt_mask_vv_nxv2i1_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_mask_vv_nxv2i1_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmflt.mask.nxv2i1.nxv2f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmflt.nxv4i1.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vmflt_vv_nxv4i1_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_vv_nxv4i1_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmflt.nxv4i1.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmflt.mask.nxv4i1.nxv4f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmflt_mask_vv_nxv4i1_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_mask_vv_nxv4i1_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmflt.mask.nxv4i1.nxv4f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmflt.nxv8i1.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vmflt_vv_nxv8i1_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_vv_nxv8i1_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmflt.nxv8i1.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmflt.mask.nxv8i1.nxv8f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmflt_mask_vv_nxv8i1_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_mask_vv_nxv8i1_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmflt.mask.nxv8i1.nxv8f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmflt.nxv16i1.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vmflt_vv_nxv16i1_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_vv_nxv16i1_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmflt.nxv16i1.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmflt.mask.nxv16i1.nxv16f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmflt_mask_vv_nxv16i1_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_mask_vv_nxv16i1_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmflt.mask.nxv16i1.nxv16f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmflt.nxv1i1.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vmflt_vv_nxv1i1_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_vv_nxv1i1_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmflt.nxv1i1.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmflt.mask.nxv1i1.nxv1f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmflt_mask_vv_nxv1i1_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_mask_vv_nxv1i1_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmflt.mask.nxv1i1.nxv1f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmflt.nxv2i1.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vmflt_vv_nxv2i1_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_vv_nxv2i1_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmflt.nxv2i1.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmflt.mask.nxv2i1.nxv2f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmflt_mask_vv_nxv2i1_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_mask_vv_nxv2i1_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmflt.mask.nxv2i1.nxv2f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmflt.nxv4i1.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vmflt_vv_nxv4i1_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_vv_nxv4i1_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmflt.nxv4i1.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmflt.mask.nxv4i1.nxv4f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmflt_mask_vv_nxv4i1_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_mask_vv_nxv4i1_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmflt.mask.nxv4i1.nxv4f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmflt.nxv8i1.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vmflt_vv_nxv8i1_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_vv_nxv8i1_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmflt.nxv8i1.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmflt.mask.nxv8i1.nxv8f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmflt_mask_vv_nxv8i1_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_mask_vv_nxv8i1_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmflt.mask.nxv8i1.nxv8f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmflt.nxv2i1.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vmflt_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmflt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vmflt.nxv2i1.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmflt.mask.nxv2i1.f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmflt_mask_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_mask_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmflt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmflt.mask.nxv2i1.f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmflt.nxv4i1.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vmflt_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmflt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vmflt.nxv4i1.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmflt.mask.nxv4i1.f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmflt_mask_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_mask_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmflt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmflt.mask.nxv4i1.f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmflt.nxv8i1.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vmflt_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmflt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vmflt.nxv8i1.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmflt.mask.nxv8i1.f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmflt_mask_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_mask_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmflt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmflt.mask.nxv8i1.f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmflt.nxv16i1.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vmflt_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmflt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 16 x i1> @llvm.epi.vmflt.nxv16i1.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmflt.mask.nxv16i1.f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmflt_mask_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_mask_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmflt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmflt.mask.nxv16i1.f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmflt.nxv1i1.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vmflt_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmflt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 1 x i1> @llvm.epi.vmflt.nxv1i1.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmflt.mask.nxv1i1.f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmflt_mask_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_mask_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmflt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmflt.mask.nxv1i1.f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmflt.nxv2i1.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vmflt_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmflt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vmflt.nxv2i1.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmflt.mask.nxv2i1.f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmflt_mask_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_mask_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmflt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmflt.mask.nxv2i1.f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmflt.nxv4i1.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vmflt_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmflt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vmflt.nxv4i1.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmflt.mask.nxv4i1.f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmflt_mask_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_mask_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmflt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmflt.mask.nxv4i1.f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmflt.nxv8i1.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vmflt_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmflt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vmflt.nxv8i1.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmflt.mask.nxv8i1.f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmflt_mask_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmflt_mask_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmflt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmflt.mask.nxv8i1.f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmfle.nxv2i1.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vmfle_vv_nxv2i1_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_vv_nxv2i1_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmfle.nxv2i1.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmfle.mask.nxv2i1.nxv2f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmfle_mask_vv_nxv2i1_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_mask_vv_nxv2i1_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmfle.mask.nxv2i1.nxv2f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmfle.nxv4i1.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vmfle_vv_nxv4i1_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_vv_nxv4i1_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmfle.nxv4i1.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmfle.mask.nxv4i1.nxv4f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmfle_mask_vv_nxv4i1_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_mask_vv_nxv4i1_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmfle.mask.nxv4i1.nxv4f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmfle.nxv8i1.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vmfle_vv_nxv8i1_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_vv_nxv8i1_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmfle.nxv8i1.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmfle.mask.nxv8i1.nxv8f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmfle_mask_vv_nxv8i1_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_mask_vv_nxv8i1_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmfle.mask.nxv8i1.nxv8f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmfle.nxv16i1.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vmfle_vv_nxv16i1_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_vv_nxv16i1_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmfle.nxv16i1.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmfle.mask.nxv16i1.nxv16f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmfle_mask_vv_nxv16i1_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_mask_vv_nxv16i1_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmfle.mask.nxv16i1.nxv16f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmfle.nxv1i1.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vmfle_vv_nxv1i1_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_vv_nxv1i1_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmfle.nxv1i1.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmfle.mask.nxv1i1.nxv1f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmfle_mask_vv_nxv1i1_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_mask_vv_nxv1i1_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmfle.mask.nxv1i1.nxv1f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmfle.nxv2i1.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vmfle_vv_nxv2i1_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_vv_nxv2i1_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmfle.nxv2i1.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmfle.mask.nxv2i1.nxv2f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmfle_mask_vv_nxv2i1_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_mask_vv_nxv2i1_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmfle.mask.nxv2i1.nxv2f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmfle.nxv4i1.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vmfle_vv_nxv4i1_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_vv_nxv4i1_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmfle.nxv4i1.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmfle.mask.nxv4i1.nxv4f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmfle_mask_vv_nxv4i1_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_mask_vv_nxv4i1_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmfle.mask.nxv4i1.nxv4f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmfle.nxv8i1.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vmfle_vv_nxv8i1_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_vv_nxv8i1_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmfle.nxv8i1.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmfle.mask.nxv8i1.nxv8f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmfle_mask_vv_nxv8i1_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_mask_vv_nxv8i1_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmfle.mask.nxv8i1.nxv8f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmfle.nxv2i1.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vmfle_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmfle.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vmfle.nxv2i1.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmfle.mask.nxv2i1.f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmfle_mask_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_mask_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmfle.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmfle.mask.nxv2i1.f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmfle.nxv4i1.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vmfle_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmfle.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vmfle.nxv4i1.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmfle.mask.nxv4i1.f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmfle_mask_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_mask_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmfle.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmfle.mask.nxv4i1.f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmfle.nxv8i1.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vmfle_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmfle.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vmfle.nxv8i1.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmfle.mask.nxv8i1.f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmfle_mask_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_mask_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmfle.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmfle.mask.nxv8i1.f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmfle.nxv16i1.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vmfle_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmfle.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 16 x i1> @llvm.epi.vmfle.nxv16i1.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmfle.mask.nxv16i1.f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmfle_mask_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_mask_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmfle.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmfle.mask.nxv16i1.f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmfle.nxv1i1.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vmfle_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmfle.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 1 x i1> @llvm.epi.vmfle.nxv1i1.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmfle.mask.nxv1i1.f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmfle_mask_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_mask_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmfle.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmfle.mask.nxv1i1.f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmfle.nxv2i1.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vmfle_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmfle.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vmfle.nxv2i1.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmfle.mask.nxv2i1.f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmfle_mask_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_mask_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmfle.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmfle.mask.nxv2i1.f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmfle.nxv4i1.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vmfle_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmfle.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vmfle.nxv4i1.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmfle.mask.nxv4i1.f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmfle_mask_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_mask_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmfle.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmfle.mask.nxv4i1.f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmfle.nxv8i1.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vmfle_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmfle.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vmfle.nxv8i1.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmfle.mask.nxv8i1.f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmfle_mask_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfle_mask_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmfle.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmfle.mask.nxv8i1.f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmfgt.nxv2i1.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vmfgt_vv_nxv2i1_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_vv_nxv2i1_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmfgt.nxv2i1.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmfgt.mask.nxv2i1.nxv2f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmfgt_mask_vv_nxv2i1_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_mask_vv_nxv2i1_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmfgt.mask.nxv2i1.nxv2f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmfgt.nxv4i1.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vmfgt_vv_nxv4i1_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_vv_nxv4i1_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmfgt.nxv4i1.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmfgt.mask.nxv4i1.nxv4f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmfgt_mask_vv_nxv4i1_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_mask_vv_nxv4i1_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmfgt.mask.nxv4i1.nxv4f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmfgt.nxv8i1.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vmfgt_vv_nxv8i1_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_vv_nxv8i1_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmfgt.nxv8i1.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmfgt.mask.nxv8i1.nxv8f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmfgt_mask_vv_nxv8i1_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_mask_vv_nxv8i1_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmfgt.mask.nxv8i1.nxv8f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmfgt.nxv16i1.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vmfgt_vv_nxv16i1_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_vv_nxv16i1_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmfgt.nxv16i1.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmfgt.mask.nxv16i1.nxv16f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmfgt_mask_vv_nxv16i1_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_mask_vv_nxv16i1_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmfgt.mask.nxv16i1.nxv16f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmfgt.nxv1i1.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vmfgt_vv_nxv1i1_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_vv_nxv1i1_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmfgt.nxv1i1.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmfgt.mask.nxv1i1.nxv1f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmfgt_mask_vv_nxv1i1_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_mask_vv_nxv1i1_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmfgt.mask.nxv1i1.nxv1f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmfgt.nxv2i1.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vmfgt_vv_nxv2i1_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_vv_nxv2i1_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmfgt.nxv2i1.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmfgt.mask.nxv2i1.nxv2f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmfgt_mask_vv_nxv2i1_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_mask_vv_nxv2i1_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmfgt.mask.nxv2i1.nxv2f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmfgt.nxv4i1.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vmfgt_vv_nxv4i1_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_vv_nxv4i1_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmfgt.nxv4i1.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmfgt.mask.nxv4i1.nxv4f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmfgt_mask_vv_nxv4i1_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_mask_vv_nxv4i1_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmfgt.mask.nxv4i1.nxv4f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmfgt.nxv8i1.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vmfgt_vv_nxv8i1_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_vv_nxv8i1_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmfgt.nxv8i1.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmfgt.mask.nxv8i1.nxv8f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmfgt_mask_vv_nxv8i1_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_mask_vv_nxv8i1_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmflt.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmfgt.mask.nxv8i1.nxv8f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmfgt.nxv2i1.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vmfgt_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmfgt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vmfgt.nxv2i1.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmfgt.mask.nxv2i1.f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmfgt_mask_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_mask_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmfgt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmfgt.mask.nxv2i1.f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmfgt.nxv4i1.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vmfgt_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmfgt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vmfgt.nxv4i1.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmfgt.mask.nxv4i1.f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmfgt_mask_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_mask_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmfgt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmfgt.mask.nxv4i1.f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmfgt.nxv8i1.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vmfgt_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmfgt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vmfgt.nxv8i1.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmfgt.mask.nxv8i1.f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmfgt_mask_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_mask_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmfgt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmfgt.mask.nxv8i1.f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmfgt.nxv16i1.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vmfgt_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmfgt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 16 x i1> @llvm.epi.vmfgt.nxv16i1.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmfgt.mask.nxv16i1.f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmfgt_mask_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_mask_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmfgt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmfgt.mask.nxv16i1.f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmfgt.nxv1i1.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vmfgt_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmfgt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 1 x i1> @llvm.epi.vmfgt.nxv1i1.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmfgt.mask.nxv1i1.f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmfgt_mask_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_mask_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmfgt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmfgt.mask.nxv1i1.f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmfgt.nxv2i1.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vmfgt_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmfgt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vmfgt.nxv2i1.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmfgt.mask.nxv2i1.f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmfgt_mask_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_mask_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmfgt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmfgt.mask.nxv2i1.f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmfgt.nxv4i1.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vmfgt_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmfgt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vmfgt.nxv4i1.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmfgt.mask.nxv4i1.f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmfgt_mask_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_mask_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmfgt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmfgt.mask.nxv4i1.f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmfgt.nxv8i1.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vmfgt_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmfgt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vmfgt.nxv8i1.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmfgt.mask.nxv8i1.f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmfgt_mask_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfgt_mask_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmfgt.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmfgt.mask.nxv8i1.f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmfge.nxv2i1.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vmfge_vv_nxv2i1_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_vv_nxv2i1_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmfge.nxv2i1.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmfge.mask.nxv2i1.nxv2f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmfge_mask_vv_nxv2i1_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_mask_vv_nxv2i1_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmfge.mask.nxv2i1.nxv2f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmfge.nxv4i1.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vmfge_vv_nxv4i1_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_vv_nxv4i1_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmfge.nxv4i1.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmfge.mask.nxv4i1.nxv4f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmfge_mask_vv_nxv4i1_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_mask_vv_nxv4i1_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmfge.mask.nxv4i1.nxv4f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmfge.nxv8i1.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vmfge_vv_nxv8i1_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_vv_nxv8i1_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmfge.nxv8i1.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmfge.mask.nxv8i1.nxv8f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmfge_mask_vv_nxv8i1_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_mask_vv_nxv8i1_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmfge.mask.nxv8i1.nxv8f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmfge.nxv16i1.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vmfge_vv_nxv16i1_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_vv_nxv16i1_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmfge.nxv16i1.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmfge.mask.nxv16i1.nxv16f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmfge_mask_vv_nxv16i1_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_mask_vv_nxv16i1_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmfge.mask.nxv16i1.nxv16f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmfge.nxv1i1.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vmfge_vv_nxv1i1_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_vv_nxv1i1_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmfge.nxv1i1.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmfge.mask.nxv1i1.nxv1f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmfge_mask_vv_nxv1i1_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_mask_vv_nxv1i1_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmfge.mask.nxv1i1.nxv1f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmfge.nxv2i1.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vmfge_vv_nxv2i1_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_vv_nxv2i1_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmfge.nxv2i1.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmfge.mask.nxv2i1.nxv2f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmfge_mask_vv_nxv2i1_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_mask_vv_nxv2i1_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmfge.mask.nxv2i1.nxv2f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmfge.nxv4i1.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vmfge_vv_nxv4i1_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_vv_nxv4i1_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmfge.nxv4i1.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmfge.mask.nxv4i1.nxv4f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmfge_mask_vv_nxv4i1_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_mask_vv_nxv4i1_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmfge.mask.nxv4i1.nxv4f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmfge.nxv8i1.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vmfge_vv_nxv8i1_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_vv_nxv8i1_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmfge.nxv8i1.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmfge.mask.nxv8i1.nxv8f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmfge_mask_vv_nxv8i1_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_mask_vv_nxv8i1_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmfle.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmfge.mask.nxv8i1.nxv8f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmfge.nxv2i1.f32(
  <vscale x 2 x float>,
  float,
  i64);

define void @intrinsic_vmfge_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmfge.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vmfge.nxv2i1.f32(
    <vscale x 2 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmfge.mask.nxv2i1.f32(
  <vscale x 2 x i1>,
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmfge_mask_vf_nxv2i1_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_mask_vf_nxv2i1_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmfge.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmfge.mask.nxv2i1.f32(
    <vscale x 2 x i1> undef,
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmfge.nxv4i1.f32(
  <vscale x 4 x float>,
  float,
  i64);

define void @intrinsic_vmfge_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmfge.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vmfge.nxv4i1.f32(
    <vscale x 4 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmfge.mask.nxv4i1.f32(
  <vscale x 4 x i1>,
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmfge_mask_vf_nxv4i1_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_mask_vf_nxv4i1_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmfge.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmfge.mask.nxv4i1.f32(
    <vscale x 4 x i1> undef,
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmfge.nxv8i1.f32(
  <vscale x 8 x float>,
  float,
  i64);

define void @intrinsic_vmfge_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmfge.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vmfge.nxv8i1.f32(
    <vscale x 8 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmfge.mask.nxv8i1.f32(
  <vscale x 8 x i1>,
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmfge_mask_vf_nxv8i1_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_mask_vf_nxv8i1_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmfge.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmfge.mask.nxv8i1.f32(
    <vscale x 8 x i1> undef,
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmfge.nxv16i1.f32(
  <vscale x 16 x float>,
  float,
  i64);

define void @intrinsic_vmfge_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmfge.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 16 x i1> @llvm.epi.vmfge.nxv16i1.f32(
    <vscale x 16 x float> undef,
    float undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}

declare <vscale x 16 x i1> @llvm.epi.vmfge.mask.nxv16i1.f32(
  <vscale x 16 x i1>,
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmfge_mask_vf_nxv16i1_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_mask_vf_nxv16i1_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmfge.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 16 x i1> @llvm.epi.vmfge.mask.nxv16i1.f32(
    <vscale x 16 x i1> undef,
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmfge.nxv1i1.f64(
  <vscale x 1 x double>,
  double,
  i64);

define void @intrinsic_vmfge_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmfge.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 1 x i1> @llvm.epi.vmfge.nxv1i1.f64(
    <vscale x 1 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}

declare <vscale x 1 x i1> @llvm.epi.vmfge.mask.nxv1i1.f64(
  <vscale x 1 x i1>,
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmfge_mask_vf_nxv1i1_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_mask_vf_nxv1i1_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmfge.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 1 x i1> @llvm.epi.vmfge.mask.nxv1i1.f64(
    <vscale x 1 x i1> undef,
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmfge.nxv2i1.f64(
  <vscale x 2 x double>,
  double,
  i64);

define void @intrinsic_vmfge_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmfge.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 2 x i1> @llvm.epi.vmfge.nxv2i1.f64(
    <vscale x 2 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}

declare <vscale x 2 x i1> @llvm.epi.vmfge.mask.nxv2i1.f64(
  <vscale x 2 x i1>,
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmfge_mask_vf_nxv2i1_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_mask_vf_nxv2i1_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmfge.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 2 x i1> @llvm.epi.vmfge.mask.nxv2i1.f64(
    <vscale x 2 x i1> undef,
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmfge.nxv4i1.f64(
  <vscale x 4 x double>,
  double,
  i64);

define void @intrinsic_vmfge_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmfge.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 4 x i1> @llvm.epi.vmfge.nxv4i1.f64(
    <vscale x 4 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}

declare <vscale x 4 x i1> @llvm.epi.vmfge.mask.nxv4i1.f64(
  <vscale x 4 x i1>,
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmfge_mask_vf_nxv4i1_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_mask_vf_nxv4i1_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmfge.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 4 x i1> @llvm.epi.vmfge.mask.nxv4i1.f64(
    <vscale x 4 x i1> undef,
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmfge.nxv8i1.f64(
  <vscale x 8 x double>,
  double,
  i64);

define void @intrinsic_vmfge_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmfge.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0
  %a = call <vscale x 8 x i1> @llvm.epi.vmfge.nxv8i1.f64(
    <vscale x 8 x double> undef,
    double undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}

declare <vscale x 8 x i1> @llvm.epi.vmfge.mask.nxv8i1.f64(
  <vscale x 8 x i1>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmfge_mask_vf_nxv8i1_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmfge_mask_vf_nxv8i1_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmfge.vf {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0.t
  %a = call <vscale x 8 x i1> @llvm.epi.vmfge.mask.nxv8i1.f64(
    <vscale x 8 x i1> undef,
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmerge.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmerge_vfm_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vfm_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmerge.vfm {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0
  %a = call <vscale x 2 x float> @llvm.epi.vfmerge.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmerge.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmerge_vfm_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vfm_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmerge.vfm {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0
  %a = call <vscale x 4 x float> @llvm.epi.vfmerge.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmerge.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmerge_vfm_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vfm_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmerge.vfm {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0
  %a = call <vscale x 8 x float> @llvm.epi.vfmerge.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmerge.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfmerge_vfm_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vfm_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmerge.vfm {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0
  %a = call <vscale x 16 x float> @llvm.epi.vfmerge.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmerge.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfmerge_vfm_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vfm_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmerge.vfm {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0
  %a = call <vscale x 1 x double> @llvm.epi.vfmerge.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmerge.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmerge_vfm_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vfm_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmerge.vfm {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0
  %a = call <vscale x 2 x double> @llvm.epi.vfmerge.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmerge.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmerge_vfm_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vfm_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmerge.vfm {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0
  %a = call <vscale x 4 x double> @llvm.epi.vfmerge.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmerge.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmerge_vfm_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmerge_vfm_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmerge.vfm {{v[0-9]+}}, {{v[0-9]+}}, ft0, v0
  %a = call <vscale x 8 x double> @llvm.epi.vfmerge.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vredsum.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vredsum_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vredsum.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vredsum.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vredsum.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vredsum.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vredsum_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vredsum.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vredsum.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vredsum.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vredsum.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vredsum_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vredsum.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vredsum.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vredsum.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vredsum.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vredsum_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vredsum.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vredsum.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vredsum.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vredsum.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vredsum_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vredsum.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vredsum.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vredsum.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vredsum.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vredsum_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vredsum.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vredsum.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vredsum.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vredsum.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vredsum_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vredsum.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vredsum.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vredsum.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vredsum.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vredsum_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vredsum.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vredsum.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vredsum.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vredsum.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vredsum_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vredsum.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vredsum.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vredsum.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vredsum.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vredsum_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vredsum.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vredsum.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vredsum.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vredsum.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vredsum_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vredsum.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vredsum.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vredsum.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredsum.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vredsum_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vredsum.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredsum.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vredsum.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vredsum.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vredsum_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vredsum.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vredsum.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vredsum.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vredsum.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vredsum_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vredsum.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vredsum.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vredsum.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vredsum.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vredsum_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vredsum.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vredsum.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredsum_mask_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredsum_mask_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vredsum.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vredand.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vredand_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vredand.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vredand.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vredand.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vredand.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vredand_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vredand.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vredand.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vredand.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vredand.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vredand_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vredand.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vredand.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vredand.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vredand.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vredand_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vredand.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vredand.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vredand.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vredand.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vredand_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vredand.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vredand.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vredand.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vredand.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vredand_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vredand.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vredand.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vredand.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vredand.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vredand_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vredand.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vredand.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vredand.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vredand.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vredand_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vredand.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vredand.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vredand.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vredand.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vredand_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vredand.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vredand.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vredand.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vredand.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vredand_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vredand.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vredand.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vredand.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vredand.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vredand_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vredand.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vredand.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vredand.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredand.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vredand_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vredand.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredand.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vredand.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vredand.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vredand_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vredand.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vredand.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vredand.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vredand.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vredand_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vredand.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vredand.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vredand.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vredand.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vredand_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vredand.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vredand.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredand_mask_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredand_mask_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vredand.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vredand.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vredor.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vredor_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vredor.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vredor.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vredor.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vredor.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vredor_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vredor.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vredor.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vredor.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vredor.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vredor_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vredor.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vredor.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vredor.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vredor.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vredor_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vredor.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vredor.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vredor.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vredor.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vredor_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vredor.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vredor.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vredor.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vredor.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vredor_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vredor.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vredor.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vredor.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vredor.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vredor_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vredor.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vredor.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vredor.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vredor.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vredor_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vredor.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vredor.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vredor.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vredor.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vredor_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vredor.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vredor.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vredor.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vredor.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vredor_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vredor.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vredor.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vredor.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vredor.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vredor_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vredor.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vredor.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vredor.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredor.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vredor_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vredor.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredor.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vredor.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vredor.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vredor_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vredor.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vredor.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vredor.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vredor.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vredor_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vredor.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vredor.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vredor.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vredor.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vredor_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vredor.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vredor.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredor_mask_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredor_mask_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vredor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vredor.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vredxor.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vredxor_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vredxor.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vredxor.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vredxor.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vredxor.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vredxor_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vredxor.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vredxor.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vredxor.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vredxor.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vredxor_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vredxor.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vredxor.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vredxor.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vredxor.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vredxor_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vredxor.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vredxor.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vredxor.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vredxor.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vredxor_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vredxor.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vredxor.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vredxor.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vredxor.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vredxor_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vredxor.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vredxor.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vredxor.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vredxor.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vredxor_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vredxor.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vredxor.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vredxor.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vredxor.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vredxor_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vredxor.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vredxor.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vredxor.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vredxor.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vredxor_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vredxor.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vredxor.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vredxor.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vredxor.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vredxor_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vredxor.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vredxor.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vredxor.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vredxor.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vredxor_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vredxor.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vredxor.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vredxor.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredxor.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vredxor_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vredxor.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredxor.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vredxor.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vredxor.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vredxor_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vredxor.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vredxor.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vredxor.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vredxor.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vredxor_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vredxor.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vredxor.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vredxor.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vredxor.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vredxor_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vredxor.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vredxor.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredxor_mask_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredxor_mask_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vredxor.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vredxor.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vredminu.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vredminu_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vredminu.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vredminu.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vredminu.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vredminu.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vredminu_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vredminu.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vredminu.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vredminu.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vredminu.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vredminu_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vredminu.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vredminu.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vredminu.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vredminu.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vredminu_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vredminu.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vredminu.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vredminu.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vredminu.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vredminu_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vredminu.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vredminu.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vredminu.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vredminu.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vredminu_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vredminu.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vredminu.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vredminu.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vredminu.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vredminu_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vredminu.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vredminu.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vredminu.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vredminu.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vredminu_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vredminu.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vredminu.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vredminu.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vredminu.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vredminu_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vredminu.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vredminu.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vredminu.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vredminu.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vredminu_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vredminu.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vredminu.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vredminu.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vredminu.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vredminu_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vredminu.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vredminu.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vredminu.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredminu.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vredminu_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vredminu.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredminu.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vredminu.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vredminu.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vredminu_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vredminu.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vredminu.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vredminu.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vredminu.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vredminu_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vredminu.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vredminu.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vredminu.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vredminu.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vredminu_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vredminu.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vredminu.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredminu_mask_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredminu_mask_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vredminu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vredminu.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vredmin.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vredmin_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vredmin.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vredmin.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vredmin.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vredmin.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vredmin_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vredmin.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vredmin.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vredmin.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vredmin.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vredmin_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vredmin.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vredmin.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vredmin.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vredmin.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vredmin_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vredmin.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vredmin.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vredmin.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vredmin.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vredmin_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vredmin.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vredmin.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vredmin.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vredmin.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vredmin_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vredmin.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vredmin.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vredmin.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vredmin.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vredmin_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vredmin.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vredmin.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vredmin.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vredmin.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vredmin_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vredmin.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vredmin.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vredmin.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vredmin.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vredmin_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vredmin.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vredmin.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vredmin.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vredmin.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vredmin_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vredmin.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vredmin.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vredmin.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vredmin.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vredmin_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vredmin.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vredmin.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vredmin.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredmin.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vredmin_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vredmin.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredmin.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vredmin.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vredmin.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vredmin_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vredmin.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vredmin.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vredmin.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vredmin.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vredmin_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vredmin.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vredmin.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vredmin.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vredmin.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vredmin_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vredmin.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vredmin.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmin_mask_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmin_mask_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vredmin.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vredmaxu.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vredmaxu.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vredmaxu.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vredmaxu.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vredmaxu.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vredmaxu.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vredmaxu.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vredmaxu.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vredmaxu.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vredmaxu.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vredmaxu.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vredmaxu.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vredmaxu.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vredmaxu.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vredmaxu.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vredmaxu.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vredmaxu.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vredmaxu.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vredmaxu.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vredmaxu.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vredmaxu.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vredmaxu.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vredmaxu.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vredmaxu.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vredmaxu.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vredmaxu.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vredmaxu.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vredmaxu.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vredmaxu.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vredmaxu.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vredmaxu.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vredmaxu.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vredmaxu.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vredmaxu.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vredmaxu.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vredmaxu.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vredmaxu.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vredmaxu.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vredmaxu.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vredmaxu.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vredmaxu.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vredmaxu.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vredmaxu.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vredmaxu.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredmaxu.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vredmaxu.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredmaxu.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vredmaxu.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vredmaxu.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vredmaxu.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vredmaxu.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vredmaxu.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vredmaxu.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vredmaxu.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vredmaxu.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vredmaxu.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vredmaxu.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vredmaxu_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vredmaxu.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vredmaxu.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmaxu_mask_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmaxu_mask_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vredmaxu.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vredmaxu.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vredmax.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vredmax_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vredmax.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vredmax.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vredmax.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vredmax.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vredmax_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vredmax.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vredmax.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vredmax.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vredmax.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vredmax_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vredmax.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vredmax.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vredmax.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vredmax.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vredmax_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vredmax.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vredmax.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vredmax.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vredmax.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vredmax_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vredmax.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vredmax.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vredmax.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vredmax.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vredmax_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vredmax.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vredmax.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vredmax.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vredmax.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vredmax_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vredmax.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vredmax.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vredmax.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vredmax.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vredmax_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vredmax.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vredmax.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vredmax.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vredmax.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vredmax_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vredmax.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vredmax.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vredmax.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vredmax.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vredmax_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vredmax.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vredmax.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vredmax.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vredmax.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vredmax_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vredmax.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vredmax.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vredmax.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vredmax.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vredmax_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vredmax.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vredmax.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vredmax.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vredmax.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vredmax_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vredmax.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vredmax.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vredmax.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vredmax.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vredmax_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vredmax.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vredmax.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vredmax.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vredmax.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vredmax_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vredmax.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vredmax.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vredmax_mask_vs_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vredmax_mask_vs_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vredmax.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfredsum.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfredsum_vs_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_vs_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x float> @llvm.epi.vfredsum.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfredsum.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfredsum_mask_vs_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_mask_vs_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfredsum.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfredsum.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfredsum_vs_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_vs_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vfredsum.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfredsum.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfredsum_mask_vs_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_mask_vs_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfredsum.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfredsum.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfredsum_vs_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_vs_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vfredsum.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfredsum.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfredsum_mask_vs_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_mask_vs_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfredsum.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfredsum.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfredsum_vs_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_vs_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x float> @llvm.epi.vfredsum.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfredsum.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfredsum_mask_vs_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_mask_vs_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfredsum.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfredsum.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfredsum_vs_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_vs_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x double> @llvm.epi.vfredsum.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfredsum.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfredsum_mask_vs_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_mask_vs_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfredsum.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfredsum.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfredsum_vs_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_vs_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfredsum.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfredsum.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfredsum_mask_vs_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_mask_vs_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfredsum.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfredsum.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfredsum_vs_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_vs_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfredsum.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfredsum.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfredsum_mask_vs_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_mask_vs_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfredsum.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfredsum.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfredsum_vs_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_vs_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfredsum.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfredsum.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfredsum_mask_vs_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredsum_mask_vs_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfredsum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfredsum.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfredosum.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfredosum_vs_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_vs_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfredosum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x float> @llvm.epi.vfredosum.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfredosum.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfredosum_mask_vs_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_mask_vs_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfredosum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfredosum.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfredosum.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfredosum_vs_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_vs_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfredosum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vfredosum.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfredosum.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfredosum_mask_vs_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_mask_vs_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfredosum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfredosum.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfredosum.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfredosum_vs_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_vs_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfredosum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vfredosum.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfredosum.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfredosum_mask_vs_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_mask_vs_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfredosum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfredosum.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfredosum.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfredosum_vs_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_vs_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfredosum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x float> @llvm.epi.vfredosum.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfredosum.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfredosum_mask_vs_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_mask_vs_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfredosum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfredosum.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfredosum.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfredosum_vs_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_vs_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfredosum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x double> @llvm.epi.vfredosum.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfredosum.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfredosum_mask_vs_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_mask_vs_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfredosum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfredosum.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfredosum.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfredosum_vs_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_vs_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfredosum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfredosum.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfredosum.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfredosum_mask_vs_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_mask_vs_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfredosum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfredosum.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfredosum.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfredosum_vs_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_vs_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfredosum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfredosum.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfredosum.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfredosum_mask_vs_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_mask_vs_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfredosum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfredosum.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfredosum.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfredosum_vs_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_vs_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfredosum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfredosum.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfredosum.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfredosum_mask_vs_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredosum_mask_vs_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfredosum.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfredosum.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfredmin.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfredmin_vs_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_vs_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x float> @llvm.epi.vfredmin.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfredmin.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfredmin_mask_vs_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_mask_vs_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfredmin.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfredmin.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfredmin_vs_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_vs_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vfredmin.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfredmin.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfredmin_mask_vs_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_mask_vs_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfredmin.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfredmin.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfredmin_vs_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_vs_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vfredmin.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfredmin.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfredmin_mask_vs_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_mask_vs_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfredmin.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfredmin.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfredmin_vs_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_vs_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x float> @llvm.epi.vfredmin.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfredmin.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfredmin_mask_vs_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_mask_vs_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfredmin.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfredmin.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfredmin_vs_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_vs_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x double> @llvm.epi.vfredmin.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfredmin.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfredmin_mask_vs_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_mask_vs_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfredmin.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfredmin.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfredmin_vs_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_vs_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfredmin.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfredmin.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfredmin_mask_vs_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_mask_vs_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfredmin.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfredmin.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfredmin_vs_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_vs_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfredmin.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfredmin.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfredmin_mask_vs_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_mask_vs_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfredmin.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfredmin.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfredmin_vs_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_vs_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfredmin.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfredmin.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfredmin_mask_vs_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmin_mask_vs_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfredmin.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfredmin.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfredmax.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfredmax_vs_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_vs_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x float> @llvm.epi.vfredmax.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfredmax.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfredmax_mask_vs_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_mask_vs_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfredmax.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfredmax.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfredmax_vs_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_vs_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vfredmax.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfredmax.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfredmax_mask_vs_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_mask_vs_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfredmax.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfredmax.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfredmax_vs_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_vs_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vfredmax.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfredmax.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfredmax_mask_vs_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_mask_vs_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfredmax.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfredmax.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfredmax_vs_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_vs_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x float> @llvm.epi.vfredmax.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfredmax.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfredmax_mask_vs_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_mask_vs_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfredmax.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfredmax.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfredmax_vs_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_vs_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x double> @llvm.epi.vfredmax.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfredmax.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfredmax_mask_vs_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_mask_vs_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfredmax.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfredmax.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfredmax_vs_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_vs_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfredmax.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfredmax.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfredmax_mask_vs_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_mask_vs_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfredmax.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfredmax.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfredmax_vs_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_vs_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfredmax.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfredmax.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfredmax_mask_vs_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_mask_vs_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfredmax.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfredmax.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfredmax_vs_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_vs_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfredmax.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfredmax.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfredmax_mask_vs_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfredmax_mask_vs_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfredmax.vs {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfredmax.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmandnot.nxv1i1.nxv1i1(
  <vscale x 1 x i1>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmandnot_mm_nxv1i1_nxv1i1_nxv1i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmandnot_mm_nxv1i1_nxv1i1_nxv1i1
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmandnot.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmandnot.nxv1i1.nxv1i1(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmandnot.nxv2i1.nxv2i1(
  <vscale x 2 x i1>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmandnot_mm_nxv2i1_nxv2i1_nxv2i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmandnot_mm_nxv2i1_nxv2i1_nxv2i1
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmandnot.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmandnot.nxv2i1.nxv2i1(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmandnot.nxv4i1.nxv4i1(
  <vscale x 4 x i1>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmandnot_mm_nxv4i1_nxv4i1_nxv4i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmandnot_mm_nxv4i1_nxv4i1_nxv4i1
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmandnot.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmandnot.nxv4i1.nxv4i1(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmandnot.nxv8i1.nxv8i1(
  <vscale x 8 x i1>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmandnot_mm_nxv8i1_nxv8i1_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmandnot_mm_nxv8i1_nxv8i1_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmandnot.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmandnot.nxv8i1.nxv8i1(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmand.nxv1i1.nxv1i1(
  <vscale x 1 x i1>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmand_mm_nxv1i1_nxv1i1_nxv1i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmand_mm_nxv1i1_nxv1i1_nxv1i1
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmand.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmand.nxv1i1.nxv1i1(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmand.nxv2i1.nxv2i1(
  <vscale x 2 x i1>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmand_mm_nxv2i1_nxv2i1_nxv2i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmand_mm_nxv2i1_nxv2i1_nxv2i1
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmand.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmand.nxv2i1.nxv2i1(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmand.nxv4i1.nxv4i1(
  <vscale x 4 x i1>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmand_mm_nxv4i1_nxv4i1_nxv4i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmand_mm_nxv4i1_nxv4i1_nxv4i1
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmand.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmand.nxv4i1.nxv4i1(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmand.nxv8i1.nxv8i1(
  <vscale x 8 x i1>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmand_mm_nxv8i1_nxv8i1_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmand_mm_nxv8i1_nxv8i1_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmand.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmand.nxv8i1.nxv8i1(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmor.nxv1i1.nxv1i1(
  <vscale x 1 x i1>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmor_mm_nxv1i1_nxv1i1_nxv1i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmor_mm_nxv1i1_nxv1i1_nxv1i1
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmor.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmor.nxv1i1.nxv1i1(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmor.nxv2i1.nxv2i1(
  <vscale x 2 x i1>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmor_mm_nxv2i1_nxv2i1_nxv2i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmor_mm_nxv2i1_nxv2i1_nxv2i1
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmor.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmor.nxv2i1.nxv2i1(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmor.nxv4i1.nxv4i1(
  <vscale x 4 x i1>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmor_mm_nxv4i1_nxv4i1_nxv4i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmor_mm_nxv4i1_nxv4i1_nxv4i1
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmor.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmor.nxv4i1.nxv4i1(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmor.nxv8i1.nxv8i1(
  <vscale x 8 x i1>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmor_mm_nxv8i1_nxv8i1_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmor_mm_nxv8i1_nxv8i1_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmor.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmor.nxv8i1.nxv8i1(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmxor.nxv1i1.nxv1i1(
  <vscale x 1 x i1>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmxor_mm_nxv1i1_nxv1i1_nxv1i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmxor_mm_nxv1i1_nxv1i1_nxv1i1
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmxor.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmxor.nxv1i1.nxv1i1(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmxor.nxv2i1.nxv2i1(
  <vscale x 2 x i1>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmxor_mm_nxv2i1_nxv2i1_nxv2i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmxor_mm_nxv2i1_nxv2i1_nxv2i1
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmxor.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmxor.nxv2i1.nxv2i1(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmxor.nxv4i1.nxv4i1(
  <vscale x 4 x i1>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmxor_mm_nxv4i1_nxv4i1_nxv4i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmxor_mm_nxv4i1_nxv4i1_nxv4i1
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmxor.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmxor.nxv4i1.nxv4i1(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmxor.nxv8i1.nxv8i1(
  <vscale x 8 x i1>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmxor_mm_nxv8i1_nxv8i1_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmxor_mm_nxv8i1_nxv8i1_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmxor.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmxor.nxv8i1.nxv8i1(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmornot.nxv1i1.nxv1i1(
  <vscale x 1 x i1>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmornot_mm_nxv1i1_nxv1i1_nxv1i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmornot_mm_nxv1i1_nxv1i1_nxv1i1
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmornot.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmornot.nxv1i1.nxv1i1(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmornot.nxv2i1.nxv2i1(
  <vscale x 2 x i1>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmornot_mm_nxv2i1_nxv2i1_nxv2i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmornot_mm_nxv2i1_nxv2i1_nxv2i1
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmornot.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmornot.nxv2i1.nxv2i1(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmornot.nxv4i1.nxv4i1(
  <vscale x 4 x i1>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmornot_mm_nxv4i1_nxv4i1_nxv4i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmornot_mm_nxv4i1_nxv4i1_nxv4i1
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmornot.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmornot.nxv4i1.nxv4i1(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmornot.nxv8i1.nxv8i1(
  <vscale x 8 x i1>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmornot_mm_nxv8i1_nxv8i1_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmornot_mm_nxv8i1_nxv8i1_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmornot.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmornot.nxv8i1.nxv8i1(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmnand.nxv1i1.nxv1i1(
  <vscale x 1 x i1>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmnand_mm_nxv1i1_nxv1i1_nxv1i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmnand_mm_nxv1i1_nxv1i1_nxv1i1
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmnand.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmnand.nxv1i1.nxv1i1(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmnand.nxv2i1.nxv2i1(
  <vscale x 2 x i1>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmnand_mm_nxv2i1_nxv2i1_nxv2i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmnand_mm_nxv2i1_nxv2i1_nxv2i1
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmnand.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmnand.nxv2i1.nxv2i1(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmnand.nxv4i1.nxv4i1(
  <vscale x 4 x i1>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmnand_mm_nxv4i1_nxv4i1_nxv4i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmnand_mm_nxv4i1_nxv4i1_nxv4i1
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmnand.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmnand.nxv4i1.nxv4i1(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmnand.nxv8i1.nxv8i1(
  <vscale x 8 x i1>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmnand_mm_nxv8i1_nxv8i1_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmnand_mm_nxv8i1_nxv8i1_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmnand.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmnand.nxv8i1.nxv8i1(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmnor.nxv1i1.nxv1i1(
  <vscale x 1 x i1>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmnor_mm_nxv1i1_nxv1i1_nxv1i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmnor_mm_nxv1i1_nxv1i1_nxv1i1
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmnor.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmnor.nxv1i1.nxv1i1(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmnor.nxv2i1.nxv2i1(
  <vscale x 2 x i1>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmnor_mm_nxv2i1_nxv2i1_nxv2i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmnor_mm_nxv2i1_nxv2i1_nxv2i1
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmnor.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmnor.nxv2i1.nxv2i1(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmnor.nxv4i1.nxv4i1(
  <vscale x 4 x i1>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmnor_mm_nxv4i1_nxv4i1_nxv4i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmnor_mm_nxv4i1_nxv4i1_nxv4i1
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmnor.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmnor.nxv4i1.nxv4i1(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmnor.nxv8i1.nxv8i1(
  <vscale x 8 x i1>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmnor_mm_nxv8i1_nxv8i1_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmnor_mm_nxv8i1_nxv8i1_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmnor.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmnor.nxv8i1.nxv8i1(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmxnor.nxv1i1.nxv1i1(
  <vscale x 1 x i1>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmxnor_mm_nxv1i1_nxv1i1_nxv1i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmxnor_mm_nxv1i1_nxv1i1_nxv1i1
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmxnor.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmxnor.nxv1i1.nxv1i1(
    <vscale x 1 x i1> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmxnor.nxv2i1.nxv2i1(
  <vscale x 2 x i1>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmxnor_mm_nxv2i1_nxv2i1_nxv2i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmxnor_mm_nxv2i1_nxv2i1_nxv2i1
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmxnor.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmxnor.nxv2i1.nxv2i1(
    <vscale x 2 x i1> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmxnor.nxv4i1.nxv4i1(
  <vscale x 4 x i1>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmxnor_mm_nxv4i1_nxv4i1_nxv4i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmxnor_mm_nxv4i1_nxv4i1_nxv4i1
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmxnor.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmxnor.nxv4i1.nxv4i1(
    <vscale x 4 x i1> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmxnor.nxv8i1.nxv8i1(
  <vscale x 8 x i1>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmxnor_mm_nxv8i1_nxv8i1_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmxnor_mm_nxv8i1_nxv8i1_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmxnor.mm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmxnor.nxv8i1.nxv8i1(
    <vscale x 8 x i1> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vcompress.nxv8i8.nxv8i1(
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vcompress_vm_nxv8i8_nxv8i8_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_nxv8i8_nxv8i8_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vcompress.vm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vcompress.nxv8i8.nxv8i1(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vcompress.nxv16i8.nxv16i1(
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vcompress_vm_nxv16i8_nxv16i8_nxv16i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_nxv16i8_nxv16i8_nxv16i1
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vcompress.vm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vcompress.nxv16i8.nxv16i1(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vcompress.nxv32i8.nxv32i1(
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vcompress_vm_nxv32i8_nxv32i8_nxv32i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_nxv32i8_nxv32i8_nxv32i1
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vcompress.vm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vcompress.nxv32i8.nxv32i1(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vcompress.nxv4i16.nxv4i1(
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vcompress_vm_nxv4i16_nxv4i16_nxv4i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_nxv4i16_nxv4i16_nxv4i1
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vcompress.vm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vcompress.nxv4i16.nxv4i1(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vcompress.nxv8i16.nxv8i1(
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vcompress_vm_nxv8i16_nxv8i16_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_nxv8i16_nxv8i16_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vcompress.vm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vcompress.nxv8i16.nxv8i1(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vcompress.nxv16i16.nxv16i1(
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vcompress_vm_nxv16i16_nxv16i16_nxv16i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_nxv16i16_nxv16i16_nxv16i1
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vcompress.vm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vcompress.nxv16i16.nxv16i1(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vcompress.nxv32i16.nxv32i1(
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vcompress_vm_nxv32i16_nxv32i16_nxv32i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_nxv32i16_nxv32i16_nxv32i1
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vcompress.vm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vcompress.nxv32i16.nxv32i1(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vcompress.nxv2i32.nxv2i1(
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vcompress_vm_nxv2i32_nxv2i32_nxv2i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_nxv2i32_nxv2i32_nxv2i1
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vcompress.vm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vcompress.nxv2i32.nxv2i1(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vcompress.nxv4i32.nxv4i1(
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vcompress_vm_nxv4i32_nxv4i32_nxv4i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_nxv4i32_nxv4i32_nxv4i1
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vcompress.vm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vcompress.nxv4i32.nxv4i1(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vcompress.nxv8i32.nxv8i1(
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vcompress_vm_nxv8i32_nxv8i32_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_nxv8i32_nxv8i32_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vcompress.vm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vcompress.nxv8i32.nxv8i1(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vcompress.nxv16i32.nxv16i1(
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vcompress_vm_nxv16i32_nxv16i32_nxv16i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_nxv16i32_nxv16i32_nxv16i1
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vcompress.vm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vcompress.nxv16i32.nxv16i1(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vcompress.nxv1i64.nxv1i1(
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vcompress_vm_nxv1i64_nxv1i64_nxv1i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_nxv1i64_nxv1i64_nxv1i1
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vcompress.vm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vcompress.nxv1i64.nxv1i1(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vcompress.nxv2i64.nxv2i1(
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vcompress_vm_nxv2i64_nxv2i64_nxv2i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_nxv2i64_nxv2i64_nxv2i1
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vcompress.vm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vcompress.nxv2i64.nxv2i1(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vcompress.nxv4i64.nxv4i1(
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vcompress_vm_nxv4i64_nxv4i64_nxv4i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_nxv4i64_nxv4i64_nxv4i1
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vcompress.vm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vcompress.nxv4i64.nxv4i1(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vcompress.nxv8i64.nxv8i1(
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vcompress_vm_nxv8i64_nxv8i64_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_nxv8i64_nxv8i64_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vcompress.vm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vcompress.nxv8i64.nxv8i1(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vcompress.nxv2f32.nxv2i1(
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vcompress_vm_nxv2f32_nxv2f32_nxv2i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_nxv2f32_nxv2f32_nxv2i1
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vcompress.vm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x float> @llvm.epi.vcompress.nxv2f32.nxv2i1(
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vcompress.nxv4f32.nxv4i1(
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vcompress_vm_nxv4f32_nxv4f32_nxv4i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_nxv4f32_nxv4f32_nxv4i1
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vcompress.vm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vcompress.nxv4f32.nxv4i1(
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vcompress.nxv8f32.nxv8i1(
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vcompress_vm_nxv8f32_nxv8f32_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_nxv8f32_nxv8f32_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vcompress.vm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vcompress.nxv8f32.nxv8i1(
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vcompress.nxv16f32.nxv16i1(
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vcompress_vm_nxv16f32_nxv16f32_nxv16i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_nxv16f32_nxv16f32_nxv16i1
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vcompress.vm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x float> @llvm.epi.vcompress.nxv16f32.nxv16i1(
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vcompress.nxv1f64.nxv1i1(
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vcompress_vm_nxv1f64_nxv1f64_nxv1i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_nxv1f64_nxv1f64_nxv1i1
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vcompress.vm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x double> @llvm.epi.vcompress.nxv1f64.nxv1i1(
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vcompress.nxv2f64.nxv2i1(
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vcompress_vm_nxv2f64_nxv2f64_nxv2i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_nxv2f64_nxv2f64_nxv2i1
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vcompress.vm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vcompress.nxv2f64.nxv2i1(
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vcompress.nxv4f64.nxv4i1(
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vcompress_vm_nxv4f64_nxv4f64_nxv4i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_nxv4f64_nxv4f64_nxv4i1
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vcompress.vm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vcompress.nxv4f64.nxv4i1(
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vcompress.nxv8f64.nxv8i1(
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vcompress_vm_nxv8f64_nxv8f64_nxv8i1() nounwind {
entry:
; CHECK-LABEL: intrinsic_vcompress_vm_nxv8f64_nxv8f64_nxv8i1
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vcompress.vm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vcompress.nxv8f64.nxv8i1(
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vrgather.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vrgather_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i8> @llvm.epi.vrgather.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vrgather.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vrgather.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vrgather.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vrgather_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i8> @llvm.epi.vrgather.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vrgather.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vrgather.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vrgather.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vrgather_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i8> @llvm.epi.vrgather.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vrgather.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vrgather.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vrgather.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vrgather_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vrgather.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vrgather.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vrgather.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vrgather.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vrgather_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vrgather.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vrgather.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vrgather.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vrgather.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vrgather_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vrgather.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vrgather.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vrgather.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vrgather.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vrgather_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i16> @llvm.epi.vrgather.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vrgather.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vrgather.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vrgather.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vrgather_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vrgather.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vrgather.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vrgather.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vrgather.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vrgather_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vrgather.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vrgather.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vrgather.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vrgather.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vrgather_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vrgather.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vrgather.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vrgather.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vrgather.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vrgather_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vrgather.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vrgather.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vrgather.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vrgather_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vrgather.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vrgather.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vrgather.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vrgather_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vrgather.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vrgather.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vrgather.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vrgather.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vrgather_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vrgather.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vrgather.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vrgather.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vrgather.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vrgather_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vrgather.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vrgather.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vrgather.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vrgather.nxv2f32.nxv2i32(
  <vscale x 2 x float>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vrgather_vv_nxv2f32_nxv2f32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv2f32_nxv2f32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x float> @llvm.epi.vrgather.nxv2f32.nxv2i32(
    <vscale x 2 x float> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vrgather.mask.nxv2f32.nxv2i32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv2f32_nxv2f32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv2f32_nxv2f32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vrgather.mask.nxv2f32.nxv2i32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vrgather.nxv4f32.nxv4i32(
  <vscale x 4 x float>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vrgather_vv_nxv4f32_nxv4f32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv4f32_nxv4f32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vrgather.nxv4f32.nxv4i32(
    <vscale x 4 x float> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vrgather.mask.nxv4f32.nxv4i32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv4f32_nxv4f32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv4f32_nxv4f32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vrgather.mask.nxv4f32.nxv4i32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vrgather.nxv8f32.nxv8i32(
  <vscale x 8 x float>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vrgather_vv_nxv8f32_nxv8f32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv8f32_nxv8f32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vrgather.nxv8f32.nxv8i32(
    <vscale x 8 x float> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vrgather.mask.nxv8f32.nxv8i32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv8f32_nxv8f32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv8f32_nxv8f32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vrgather.mask.nxv8f32.nxv8i32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vrgather.nxv16f32.nxv16i32(
  <vscale x 16 x float>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vrgather_vv_nxv16f32_nxv16f32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv16f32_nxv16f32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x float> @llvm.epi.vrgather.nxv16f32.nxv16i32(
    <vscale x 16 x float> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vrgather.mask.nxv16f32.nxv16i32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv16f32_nxv16f32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv16f32_nxv16f32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vrgather.mask.nxv16f32.nxv16i32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(
  <vscale x 1 x double>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vrgather_vv_nxv1f64_nxv1f64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv1f64_nxv1f64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(
    <vscale x 1 x double> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vrgather.mask.nxv1f64.nxv1i64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv1f64_nxv1f64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv1f64_nxv1f64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vrgather.mask.nxv1f64.nxv1i64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vrgather.nxv2f64.nxv2i64(
  <vscale x 2 x double>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vrgather_vv_nxv2f64_nxv2f64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv2f64_nxv2f64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vrgather.nxv2f64.nxv2i64(
    <vscale x 2 x double> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vrgather.mask.nxv2f64.nxv2i64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv2f64_nxv2f64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv2f64_nxv2f64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vrgather.mask.nxv2f64.nxv2i64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vrgather.nxv4f64.nxv4i64(
  <vscale x 4 x double>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vrgather_vv_nxv4f64_nxv4f64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv4f64_nxv4f64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vrgather.nxv4f64.nxv4i64(
    <vscale x 4 x double> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vrgather.mask.nxv4f64.nxv4i64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv4f64_nxv4f64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv4f64_nxv4f64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vrgather.mask.nxv4f64.nxv4i64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vrgather.nxv8f64.nxv8i64(
  <vscale x 8 x double>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vrgather_vv_nxv8f64_nxv8f64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vv_nxv8f64_nxv8f64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vrgather.nxv8f64.nxv8i64(
    <vscale x 8 x double> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vrgather.mask.nxv8f64.nxv8i64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vv_nxv8f64_nxv8f64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vv_nxv8f64_nxv8f64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vrgather.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vrgather.mask.nxv8f64.nxv8i64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vrgather.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vrgather_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vrgather.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vrgather.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vrgather.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vrgather.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vrgather_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vrgather.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vrgather.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vrgather.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vrgather.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vrgather_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vrgather.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vrgather.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vrgather.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vrgather.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vrgather_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vrgather.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vrgather.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vrgather.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vrgather.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vrgather_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vrgather.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vrgather.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vrgather.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vrgather.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vrgather_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vrgather.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vrgather.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vrgather.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vrgather.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vrgather_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vrgather.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vrgather.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vrgather.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vrgather.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vrgather_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vrgather.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vrgather.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vrgather.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vrgather.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vrgather_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vrgather.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vrgather.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vrgather.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vrgather.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vrgather_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vrgather.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vrgather.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vrgather.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vrgather.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vrgather_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vrgather.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vrgather.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vrgather.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vrgather_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vrgather.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vrgather.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vrgather.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vrgather_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vrgather.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vrgather.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vrgather.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vrgather.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vrgather_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vrgather.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vrgather.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vrgather.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vrgather.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vrgather_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vrgather.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vrgather.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vrgather.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vrgather.nxv2f32.i32(
  <vscale x 2 x float>,
  i32,
  i64);

define void @intrinsic_vrgather_vx_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x float> @llvm.epi.vrgather.nxv2f32.i32(
    <vscale x 2 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vrgather.mask.nxv2f32.i32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vrgather.mask.nxv2f32.i32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vrgather.nxv4f32.i32(
  <vscale x 4 x float>,
  i32,
  i64);

define void @intrinsic_vrgather_vx_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x float> @llvm.epi.vrgather.nxv4f32.i32(
    <vscale x 4 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vrgather.mask.nxv4f32.i32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vrgather.mask.nxv4f32.i32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vrgather.nxv8f32.i32(
  <vscale x 8 x float>,
  i32,
  i64);

define void @intrinsic_vrgather_vx_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x float> @llvm.epi.vrgather.nxv8f32.i32(
    <vscale x 8 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vrgather.mask.nxv8f32.i32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vrgather.mask.nxv8f32.i32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vrgather.nxv16f32.i32(
  <vscale x 16 x float>,
  i32,
  i64);

define void @intrinsic_vrgather_vx_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x float> @llvm.epi.vrgather.nxv16f32.i32(
    <vscale x 16 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vrgather.mask.nxv16f32.i32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vrgather.mask.nxv16f32.i32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.i64(
  <vscale x 1 x double>,
  i64,
  i64);

define void @intrinsic_vrgather_vx_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.i64(
    <vscale x 1 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vrgather.mask.nxv1f64.i64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vrgather.mask.nxv1f64.i64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vrgather.nxv2f64.i64(
  <vscale x 2 x double>,
  i64,
  i64);

define void @intrinsic_vrgather_vx_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x double> @llvm.epi.vrgather.nxv2f64.i64(
    <vscale x 2 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vrgather.mask.nxv2f64.i64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vrgather.mask.nxv2f64.i64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vrgather.nxv4f64.i64(
  <vscale x 4 x double>,
  i64,
  i64);

define void @intrinsic_vrgather_vx_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x double> @llvm.epi.vrgather.nxv4f64.i64(
    <vscale x 4 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vrgather.mask.nxv4f64.i64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vrgather.mask.nxv4f64.i64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vrgather.nxv8f64.i64(
  <vscale x 8 x double>,
  i64,
  i64);

define void @intrinsic_vrgather_vx_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vx_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x double> @llvm.epi.vrgather.nxv8f64.i64(
    <vscale x 8 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vrgather.mask.nxv8f64.i64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vrgather_mask_vx_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vx_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vrgather.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vrgather.mask.nxv8f64.i64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vrgather.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vrgather.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vrgather.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vrgather.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vrgather.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vrgather.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vrgather.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vrgather.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vrgather.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vrgather.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vrgather.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vrgather.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vrgather.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vrgather.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vrgather.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vrgather.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vrgather.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vrgather.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vrgather.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vrgather.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vrgather.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vrgather.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vrgather.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vrgather.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vrgather.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vrgather.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vrgather.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vrgather.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vrgather.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vrgather.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x float> @llvm.epi.vrgather.nxv2f32.i32(
    <vscale x 2 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vrgather.mask.nxv2f32.i32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x float> @llvm.epi.vrgather.nxv4f32.i32(
    <vscale x 4 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vrgather.mask.nxv4f32.i32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x float> @llvm.epi.vrgather.nxv8f32.i32(
    <vscale x 8 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vrgather.mask.nxv8f32.i32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x float> @llvm.epi.vrgather.nxv16f32.i32(
    <vscale x 16 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vrgather.mask.nxv16f32.i32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.i64(
    <vscale x 1 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vrgather.mask.nxv1f64.i64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x double> @llvm.epi.vrgather.nxv2f64.i64(
    <vscale x 2 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vrgather.mask.nxv2f64.i64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x double> @llvm.epi.vrgather.nxv4f64.i64(
    <vscale x 4 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vrgather.mask.nxv4f64.i64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


define void @intrinsic_vrgather_vi_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_vi_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x double> @llvm.epi.vrgather.nxv8f64.i64(
    <vscale x 8 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

define void @intrinsic_vrgather_mask_vi_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vrgather_mask_vi_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vrgather.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vrgather.mask.nxv8f64.i64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vslideup.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vslideup_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vslideup.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vslideup.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vslideup.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vslideup.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vslideup_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vslideup.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vslideup.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vslideup.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vslideup.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vslideup_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vslideup.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vslideup.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vslideup.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vslideup.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vslideup_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vslideup.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vslideup.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vslideup.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vslideup.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vslideup_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vslideup.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vslideup.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vslideup.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vslideup.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vslideup_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vslideup.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vslideup.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vslideup.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vslideup.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vslideup_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vslideup.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vslideup.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vslideup.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vslideup.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vslideup_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vslideup.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vslideup.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vslideup.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vslideup.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vslideup_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vslideup.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vslideup.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vslideup.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vslideup.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vslideup_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vslideup.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vslideup.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vslideup.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vslideup.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vslideup_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vslideup.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vslideup.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vslideup.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslideup.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vslideup_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vslideup.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslideup.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vslideup.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vslideup.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vslideup_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vslideup.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vslideup.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vslideup.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vslideup.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vslideup_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vslideup.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vslideup.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vslideup.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vslideup.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vslideup_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vslideup.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vslideup.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vslideup.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vslideup.nxv2f32.i32(
  <vscale x 2 x float>,
  i32,
  i64);

define void @intrinsic_vslideup_vx_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x float> @llvm.epi.vslideup.nxv2f32.i32(
    <vscale x 2 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vslideup.mask.nxv2f32.i32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vslideup.mask.nxv2f32.i32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vslideup.nxv4f32.i32(
  <vscale x 4 x float>,
  i32,
  i64);

define void @intrinsic_vslideup_vx_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x float> @llvm.epi.vslideup.nxv4f32.i32(
    <vscale x 4 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vslideup.mask.nxv4f32.i32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vslideup.mask.nxv4f32.i32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vslideup.nxv8f32.i32(
  <vscale x 8 x float>,
  i32,
  i64);

define void @intrinsic_vslideup_vx_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x float> @llvm.epi.vslideup.nxv8f32.i32(
    <vscale x 8 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vslideup.mask.nxv8f32.i32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vslideup.mask.nxv8f32.i32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vslideup.nxv16f32.i32(
  <vscale x 16 x float>,
  i32,
  i64);

define void @intrinsic_vslideup_vx_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x float> @llvm.epi.vslideup.nxv16f32.i32(
    <vscale x 16 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vslideup.mask.nxv16f32.i32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vslideup.mask.nxv16f32.i32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslideup.nxv1f64.i64(
  <vscale x 1 x double>,
  i64,
  i64);

define void @intrinsic_vslideup_vx_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x double> @llvm.epi.vslideup.nxv1f64.i64(
    <vscale x 1 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslideup.mask.nxv1f64.i64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vslideup.mask.nxv1f64.i64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vslideup.nxv2f64.i64(
  <vscale x 2 x double>,
  i64,
  i64);

define void @intrinsic_vslideup_vx_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x double> @llvm.epi.vslideup.nxv2f64.i64(
    <vscale x 2 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vslideup.mask.nxv2f64.i64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vslideup.mask.nxv2f64.i64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vslideup.nxv4f64.i64(
  <vscale x 4 x double>,
  i64,
  i64);

define void @intrinsic_vslideup_vx_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x double> @llvm.epi.vslideup.nxv4f64.i64(
    <vscale x 4 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vslideup.mask.nxv4f64.i64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vslideup.mask.nxv4f64.i64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vslideup.nxv8f64.i64(
  <vscale x 8 x double>,
  i64,
  i64);

define void @intrinsic_vslideup_vx_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vx_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x double> @llvm.epi.vslideup.nxv8f64.i64(
    <vscale x 8 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vslideup.mask.nxv8f64.i64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslideup_mask_vx_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vx_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslideup.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vslideup.mask.nxv8f64.i64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vslideup.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vslideup.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vslideup.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vslideup.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vslideup.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vslideup.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vslideup.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vslideup.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vslideup.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vslideup.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vslideup.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vslideup.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vslideup.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vslideup.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vslideup.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vslideup.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vslideup.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vslideup.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vslideup.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vslideup.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vslideup.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vslideup.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vslideup.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vslideup.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vslideup.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vslideup.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vslideup.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vslideup.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vslideup.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vslideup.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x float> @llvm.epi.vslideup.nxv2f32.i32(
    <vscale x 2 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vslideup.mask.nxv2f32.i32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x float> @llvm.epi.vslideup.nxv4f32.i32(
    <vscale x 4 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vslideup.mask.nxv4f32.i32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x float> @llvm.epi.vslideup.nxv8f32.i32(
    <vscale x 8 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vslideup.mask.nxv8f32.i32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x float> @llvm.epi.vslideup.nxv16f32.i32(
    <vscale x 16 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vslideup.mask.nxv16f32.i32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x double> @llvm.epi.vslideup.nxv1f64.i64(
    <vscale x 1 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vslideup.mask.nxv1f64.i64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x double> @llvm.epi.vslideup.nxv2f64.i64(
    <vscale x 2 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vslideup.mask.nxv2f64.i64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x double> @llvm.epi.vslideup.nxv4f64.i64(
    <vscale x 4 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vslideup.mask.nxv4f64.i64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


define void @intrinsic_vslideup_vi_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_vi_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x double> @llvm.epi.vslideup.nxv8f64.i64(
    <vscale x 8 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

define void @intrinsic_vslideup_mask_vi_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslideup_mask_vi_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslideup.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vslideup.mask.nxv8f64.i64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vslidedown.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vslidedown_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vslidedown.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vslidedown.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vslidedown.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vslidedown.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vslidedown_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vslidedown.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vslidedown.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vslidedown.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vslidedown.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vslidedown_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vslidedown.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vslidedown.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vslidedown.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vslidedown.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vslidedown_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vslidedown.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vslidedown.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vslidedown.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vslidedown.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vslidedown_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vslidedown.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vslidedown.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vslidedown.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vslidedown.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vslidedown_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vslidedown.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vslidedown.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vslidedown.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vslidedown.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vslidedown_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vslidedown.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vslidedown.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vslidedown.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vslidedown.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vslidedown_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vslidedown.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vslidedown.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vslidedown.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vslidedown.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vslidedown_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vslidedown.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vslidedown.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vslidedown.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vslidedown.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vslidedown_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vslidedown.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vslidedown.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vslidedown.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vslidedown.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vslidedown_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vslidedown.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vslidedown.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vslidedown.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslidedown.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vslidedown_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vslidedown.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslidedown.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vslidedown.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vslidedown.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vslidedown_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vslidedown.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vslidedown.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vslidedown.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vslidedown.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vslidedown_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vslidedown.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vslidedown.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vslidedown.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vslidedown.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vslidedown_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vslidedown.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vslidedown.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vslidedown.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vslidedown.nxv2f32.i32(
  <vscale x 2 x float>,
  i32,
  i64);

define void @intrinsic_vslidedown_vx_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x float> @llvm.epi.vslidedown.nxv2f32.i32(
    <vscale x 2 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vslidedown.mask.nxv2f32.i32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vslidedown.mask.nxv2f32.i32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vslidedown.nxv4f32.i32(
  <vscale x 4 x float>,
  i32,
  i64);

define void @intrinsic_vslidedown_vx_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x float> @llvm.epi.vslidedown.nxv4f32.i32(
    <vscale x 4 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vslidedown.mask.nxv4f32.i32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vslidedown.mask.nxv4f32.i32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vslidedown.nxv8f32.i32(
  <vscale x 8 x float>,
  i32,
  i64);

define void @intrinsic_vslidedown_vx_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x float> @llvm.epi.vslidedown.nxv8f32.i32(
    <vscale x 8 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vslidedown.mask.nxv8f32.i32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vslidedown.mask.nxv8f32.i32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vslidedown.nxv16f32.i32(
  <vscale x 16 x float>,
  i32,
  i64);

define void @intrinsic_vslidedown_vx_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x float> @llvm.epi.vslidedown.nxv16f32.i32(
    <vscale x 16 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vslidedown.mask.nxv16f32.i32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vslidedown.mask.nxv16f32.i32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslidedown.nxv1f64.i64(
  <vscale x 1 x double>,
  i64,
  i64);

define void @intrinsic_vslidedown_vx_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x double> @llvm.epi.vslidedown.nxv1f64.i64(
    <vscale x 1 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslidedown.mask.nxv1f64.i64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vslidedown.mask.nxv1f64.i64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vslidedown.nxv2f64.i64(
  <vscale x 2 x double>,
  i64,
  i64);

define void @intrinsic_vslidedown_vx_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x double> @llvm.epi.vslidedown.nxv2f64.i64(
    <vscale x 2 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vslidedown.mask.nxv2f64.i64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vslidedown.mask.nxv2f64.i64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vslidedown.nxv4f64.i64(
  <vscale x 4 x double>,
  i64,
  i64);

define void @intrinsic_vslidedown_vx_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x double> @llvm.epi.vslidedown.nxv4f64.i64(
    <vscale x 4 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vslidedown.mask.nxv4f64.i64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vslidedown.mask.nxv4f64.i64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vslidedown.nxv8f64.i64(
  <vscale x 8 x double>,
  i64,
  i64);

define void @intrinsic_vslidedown_vx_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vx_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x double> @llvm.epi.vslidedown.nxv8f64.i64(
    <vscale x 8 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vslidedown.mask.nxv8f64.i64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslidedown_mask_vx_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vx_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslidedown.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vslidedown.mask.nxv8f64.i64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i8> @llvm.epi.vslidedown.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vslidedown.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i8> @llvm.epi.vslidedown.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vslidedown.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i8> @llvm.epi.vslidedown.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vslidedown.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i16> @llvm.epi.vslidedown.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vslidedown.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i16> @llvm.epi.vslidedown.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vslidedown.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i16> @llvm.epi.vslidedown.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vslidedown.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i16> @llvm.epi.vslidedown.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vslidedown.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i32> @llvm.epi.vslidedown.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vslidedown.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i32> @llvm.epi.vslidedown.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vslidedown.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i32> @llvm.epi.vslidedown.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vslidedown.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i32> @llvm.epi.vslidedown.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vslidedown.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x i64> @llvm.epi.vslidedown.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vslidedown.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i64> @llvm.epi.vslidedown.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vslidedown.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i64> @llvm.epi.vslidedown.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vslidedown.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i64> @llvm.epi.vslidedown.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vslidedown.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x float> @llvm.epi.vslidedown.nxv2f32.i32(
    <vscale x 2 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vslidedown.mask.nxv2f32.i32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x float> @llvm.epi.vslidedown.nxv4f32.i32(
    <vscale x 4 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vslidedown.mask.nxv4f32.i32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x float> @llvm.epi.vslidedown.nxv8f32.i32(
    <vscale x 8 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vslidedown.mask.nxv8f32.i32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x float> @llvm.epi.vslidedown.nxv16f32.i32(
    <vscale x 16 x float> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vslidedown.mask.nxv16f32.i32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x double> @llvm.epi.vslidedown.nxv1f64.i64(
    <vscale x 1 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vslidedown.mask.nxv1f64.i64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x double> @llvm.epi.vslidedown.nxv2f64.i64(
    <vscale x 2 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vslidedown.mask.nxv2f64.i64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x double> @llvm.epi.vslidedown.nxv4f64.i64(
    <vscale x 4 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vslidedown.mask.nxv4f64.i64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


define void @intrinsic_vslidedown_vi_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_vi_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x double> @llvm.epi.vslidedown.nxv8f64.i64(
    <vscale x 8 x double> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

define void @intrinsic_vslidedown_mask_vi_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslidedown_mask_vi_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslidedown.vi {{v[0-9]+}}, {{v[0-9]+}}, 9, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vslidedown.mask.nxv8f64.i64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vslide1up.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vslide1up_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vslide1up.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vslide1up.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vslide1up.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vslide1up.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vslide1up_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vslide1up.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vslide1up.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vslide1up.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vslide1up.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vslide1up_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vslide1up.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vslide1up.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vslide1up.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vslide1up.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vslide1up_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vslide1up.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vslide1up.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vslide1up.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vslide1up.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vslide1up_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vslide1up.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vslide1up.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vslide1up.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vslide1up.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vslide1up_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vslide1up.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vslide1up.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vslide1up.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vslide1up.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vslide1up_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vslide1up.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vslide1up.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vslide1up.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vslide1up.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vslide1up_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vslide1up.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vslide1up.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vslide1up.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vslide1up.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vslide1up_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vslide1up.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vslide1up.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vslide1up.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vslide1up.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vslide1up_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vslide1up.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vslide1up.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vslide1up.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vslide1up.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vslide1up_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vslide1up.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vslide1up.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vslide1up.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslide1up.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vslide1up_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vslide1up.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslide1up.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vslide1up.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vslide1up.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vslide1up_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vslide1up.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vslide1up.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vslide1up.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vslide1up.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vslide1up_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vslide1up.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vslide1up.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vslide1up.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vslide1up.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vslide1up_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vslide1up.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vslide1up.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vslide1up.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vslide1up.nxv2f32.i32(
  <vscale x 2 x float>,
  i32,
  i64);

define void @intrinsic_vslide1up_vx_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x float> @llvm.epi.vslide1up.nxv2f32.i32(
    <vscale x 2 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vslide1up.mask.nxv2f32.i32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vslide1up.mask.nxv2f32.i32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vslide1up.nxv4f32.i32(
  <vscale x 4 x float>,
  i32,
  i64);

define void @intrinsic_vslide1up_vx_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x float> @llvm.epi.vslide1up.nxv4f32.i32(
    <vscale x 4 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vslide1up.mask.nxv4f32.i32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vslide1up.mask.nxv4f32.i32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vslide1up.nxv8f32.i32(
  <vscale x 8 x float>,
  i32,
  i64);

define void @intrinsic_vslide1up_vx_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x float> @llvm.epi.vslide1up.nxv8f32.i32(
    <vscale x 8 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vslide1up.mask.nxv8f32.i32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vslide1up.mask.nxv8f32.i32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vslide1up.nxv16f32.i32(
  <vscale x 16 x float>,
  i32,
  i64);

define void @intrinsic_vslide1up_vx_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x float> @llvm.epi.vslide1up.nxv16f32.i32(
    <vscale x 16 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vslide1up.mask.nxv16f32.i32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vslide1up.mask.nxv16f32.i32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslide1up.nxv1f64.i64(
  <vscale x 1 x double>,
  i64,
  i64);

define void @intrinsic_vslide1up_vx_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x double> @llvm.epi.vslide1up.nxv1f64.i64(
    <vscale x 1 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslide1up.mask.nxv1f64.i64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vslide1up.mask.nxv1f64.i64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vslide1up.nxv2f64.i64(
  <vscale x 2 x double>,
  i64,
  i64);

define void @intrinsic_vslide1up_vx_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x double> @llvm.epi.vslide1up.nxv2f64.i64(
    <vscale x 2 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vslide1up.mask.nxv2f64.i64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vslide1up.mask.nxv2f64.i64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vslide1up.nxv4f64.i64(
  <vscale x 4 x double>,
  i64,
  i64);

define void @intrinsic_vslide1up_vx_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x double> @llvm.epi.vslide1up.nxv4f64.i64(
    <vscale x 4 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vslide1up.mask.nxv4f64.i64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vslide1up.mask.nxv4f64.i64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vslide1up.nxv8f64.i64(
  <vscale x 8 x double>,
  i64,
  i64);

define void @intrinsic_vslide1up_vx_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_vx_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x double> @llvm.epi.vslide1up.nxv8f64.i64(
    <vscale x 8 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vslide1up.mask.nxv8f64.i64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1up_mask_vx_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1up_mask_vx_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslide1up.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vslide1up.mask.nxv8f64.i64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vslide1down.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vslide1down_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i8> @llvm.epi.vslide1down.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vslide1down.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vslide1down.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vslide1down.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vslide1down_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i8> @llvm.epi.vslide1down.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vslide1down.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vslide1down.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vslide1down.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vslide1down_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i8> @llvm.epi.vslide1down.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vslide1down.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vslide1down.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vslide1down.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vslide1down_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i16> @llvm.epi.vslide1down.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vslide1down.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vslide1down.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vslide1down.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vslide1down_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i16> @llvm.epi.vslide1down.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vslide1down.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vslide1down.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vslide1down.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vslide1down_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i16> @llvm.epi.vslide1down.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vslide1down.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vslide1down.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vslide1down.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vslide1down_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i16> @llvm.epi.vslide1down.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vslide1down.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vslide1down.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vslide1down.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vslide1down_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i32> @llvm.epi.vslide1down.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vslide1down.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vslide1down.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vslide1down.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vslide1down_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i32> @llvm.epi.vslide1down.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vslide1down.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vslide1down.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vslide1down.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vslide1down_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i32> @llvm.epi.vslide1down.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vslide1down.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vslide1down.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vslide1down.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vslide1down_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i32> @llvm.epi.vslide1down.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vslide1down.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vslide1down.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vslide1down.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vslide1down_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i64> @llvm.epi.vslide1down.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vslide1down.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vslide1down.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vslide1down.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vslide1down_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i64> @llvm.epi.vslide1down.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vslide1down.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vslide1down.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vslide1down.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vslide1down_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i64> @llvm.epi.vslide1down.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vslide1down.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vslide1down.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vslide1down.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vslide1down_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i64> @llvm.epi.vslide1down.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vslide1down.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vslide1down.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vslide1down.nxv2f32.i32(
  <vscale x 2 x float>,
  i32,
  i64);

define void @intrinsic_vslide1down_vx_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x float> @llvm.epi.vslide1down.nxv2f32.i32(
    <vscale x 2 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vslide1down.mask.nxv2f32.i32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv2f32_nxv2f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv2f32_nxv2f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vslide1down.mask.nxv2f32.i32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vslide1down.nxv4f32.i32(
  <vscale x 4 x float>,
  i32,
  i64);

define void @intrinsic_vslide1down_vx_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x float> @llvm.epi.vslide1down.nxv4f32.i32(
    <vscale x 4 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vslide1down.mask.nxv4f32.i32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv4f32_nxv4f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv4f32_nxv4f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vslide1down.mask.nxv4f32.i32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vslide1down.nxv8f32.i32(
  <vscale x 8 x float>,
  i32,
  i64);

define void @intrinsic_vslide1down_vx_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x float> @llvm.epi.vslide1down.nxv8f32.i32(
    <vscale x 8 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vslide1down.mask.nxv8f32.i32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv8f32_nxv8f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv8f32_nxv8f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vslide1down.mask.nxv8f32.i32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vslide1down.nxv16f32.i32(
  <vscale x 16 x float>,
  i32,
  i64);

define void @intrinsic_vslide1down_vx_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x float> @llvm.epi.vslide1down.nxv16f32.i32(
    <vscale x 16 x float> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vslide1down.mask.nxv16f32.i32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv16f32_nxv16f32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv16f32_nxv16f32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vslide1down.mask.nxv16f32.i32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vslide1down.nxv1f64.i64(
  <vscale x 1 x double>,
  i64,
  i64);

define void @intrinsic_vslide1down_vx_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x double> @llvm.epi.vslide1down.nxv1f64.i64(
    <vscale x 1 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vslide1down.mask.nxv1f64.i64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv1f64_nxv1f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv1f64_nxv1f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vslide1down.mask.nxv1f64.i64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vslide1down.nxv2f64.i64(
  <vscale x 2 x double>,
  i64,
  i64);

define void @intrinsic_vslide1down_vx_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x double> @llvm.epi.vslide1down.nxv2f64.i64(
    <vscale x 2 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vslide1down.mask.nxv2f64.i64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv2f64_nxv2f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv2f64_nxv2f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vslide1down.mask.nxv2f64.i64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vslide1down.nxv4f64.i64(
  <vscale x 4 x double>,
  i64,
  i64);

define void @intrinsic_vslide1down_vx_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x double> @llvm.epi.vslide1down.nxv4f64.i64(
    <vscale x 4 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vslide1down.mask.nxv4f64.i64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv4f64_nxv4f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv4f64_nxv4f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vslide1down.mask.nxv4f64.i64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vslide1down.nxv8f64.i64(
  <vscale x 8 x double>,
  i64,
  i64);

define void @intrinsic_vslide1down_vx_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_vx_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x double> @llvm.epi.vslide1down.nxv8f64.i64(
    <vscale x 8 x double> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vslide1down.mask.nxv8f64.i64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vslide1down_mask_vx_nxv8f64_nxv8f64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vslide1down_mask_vx_nxv8f64_nxv8f64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vslide1down.vx {{v[0-9]+}}, {{v[0-9]+}}, a0, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vslide1down.mask.nxv8f64.i64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vadc.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadc_vvm_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vvm_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vadc.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vadc.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vadc_vvm_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vvm_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vadc.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vadc.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vadc_vvm_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vvm_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vadc.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vadc.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vadc_vvm_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vvm_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vadc.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vadc.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadc_vvm_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vvm_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vadc.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vadc.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vadc_vvm_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vvm_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vadc.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vadc.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vadc_vvm_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vvm_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vadc.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vadc.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vadc_vvm_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vvm_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vadc.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vadc.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vadc_vvm_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vvm_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vadc.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vadc.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadc_vvm_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vvm_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vadc.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vadc.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vadc_vvm_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vvm_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vadc.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vadc.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vadc_vvm_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vvm_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vadc.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vadc.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vadc_vvm_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vvm_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vadc.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vadc.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vadc_vvm_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vvm_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vadc.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vadc.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadc_vvm_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vvm_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vadc.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vadc.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadc_vxm_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vxm_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vadc.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vadc.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vadc_vxm_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vxm_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vadc.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vadc.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vadc_vxm_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vxm_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vadc.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vadc.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vadc_vxm_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vxm_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vadc.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vadc.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadc_vxm_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vxm_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vadc.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vadc.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vadc_vxm_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vxm_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vadc.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vadc.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vadc_vxm_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vxm_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vadc.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vadc.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vadc_vxm_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vxm_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vadc.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vadc.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vadc_vxm_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vxm_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vadc.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vadc.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadc_vxm_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vxm_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vadc.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vadc.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vadc_vxm_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vxm_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vadc.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vadc.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vadc_vxm_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vxm_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vadc.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vadc.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vadc_vxm_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vxm_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vadc.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vadc.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vadc_vxm_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vxm_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vadc.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vadc.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vadc_vxm_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vxm_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vadc.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


define void @intrinsic_vadc_vim_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vim_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vadc.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


define void @intrinsic_vadc_vim_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vim_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vadc.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


define void @intrinsic_vadc_vim_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vim_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vadc.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


define void @intrinsic_vadc_vim_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vim_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vadc.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


define void @intrinsic_vadc_vim_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vim_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vadc.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


define void @intrinsic_vadc_vim_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vim_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vadc.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


define void @intrinsic_vadc_vim_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vim_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vadc.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


define void @intrinsic_vadc_vim_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vim_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vadc.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


define void @intrinsic_vadc_vim_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vim_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vadc.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


define void @intrinsic_vadc_vim_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vim_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vadc.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


define void @intrinsic_vadc_vim_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vim_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vadc.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


define void @intrinsic_vadc_vim_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vim_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vadc.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


define void @intrinsic_vadc_vim_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vim_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vadc.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


define void @intrinsic_vadc_vim_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vim_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vadc.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


define void @intrinsic_vadc_vim_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vadc_vim_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vadc.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmadc.carry.in.nxv8i1.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vvm_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vvm_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.carry.in.nxv8i1.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmadc.carry.in.nxv16i1.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vvm_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vvm_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vmadc.carry.in.nxv16i1.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmadc.carry.in.nxv32i1.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vvm_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vvm_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 32 x i1> @llvm.epi.vmadc.carry.in.nxv32i1.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmadc.carry.in.nxv4i1.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vvm_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vvm_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vmadc.carry.in.nxv4i1.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmadc.carry.in.nxv8i1.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vvm_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vvm_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.carry.in.nxv8i1.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmadc.carry.in.nxv16i1.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vvm_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vvm_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vmadc.carry.in.nxv16i1.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmadc.carry.in.nxv32i1.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vvm_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vvm_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 32 x i1> @llvm.epi.vmadc.carry.in.nxv32i1.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmadc.carry.in.nxv2i1.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vvm_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vvm_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vmadc.carry.in.nxv2i1.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmadc.carry.in.nxv4i1.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vvm_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vvm_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vmadc.carry.in.nxv4i1.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmadc.carry.in.nxv8i1.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vvm_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vvm_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.carry.in.nxv8i1.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmadc.carry.in.nxv16i1.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vvm_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vvm_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vmadc.carry.in.nxv16i1.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmadc.carry.in.nxv1i1.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vvm_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vvm_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 1 x i1> @llvm.epi.vmadc.carry.in.nxv1i1.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmadc.carry.in.nxv2i1.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vvm_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vvm_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vmadc.carry.in.nxv2i1.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmadc.carry.in.nxv4i1.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vvm_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vvm_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vmadc.carry.in.nxv4i1.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmadc.carry.in.nxv8i1.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vvm_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vvm_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmadc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.carry.in.nxv8i1.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmadc.carry.in.nxv8i1.i8(
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vxm_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vxm_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.carry.in.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmadc.carry.in.nxv16i1.i8(
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vxm_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vxm_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vmadc.carry.in.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmadc.carry.in.nxv32i1.i8(
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vxm_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vxm_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 32 x i1> @llvm.epi.vmadc.carry.in.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmadc.carry.in.nxv4i1.i16(
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vxm_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vxm_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vmadc.carry.in.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmadc.carry.in.nxv8i1.i16(
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vxm_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vxm_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.carry.in.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmadc.carry.in.nxv16i1.i16(
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vxm_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vxm_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vmadc.carry.in.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmadc.carry.in.nxv32i1.i16(
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vxm_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vxm_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 32 x i1> @llvm.epi.vmadc.carry.in.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmadc.carry.in.nxv2i1.i32(
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vxm_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vxm_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vmadc.carry.in.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmadc.carry.in.nxv4i1.i32(
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vxm_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vxm_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vmadc.carry.in.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmadc.carry.in.nxv8i1.i32(
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vxm_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vxm_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.carry.in.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmadc.carry.in.nxv16i1.i32(
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vxm_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vxm_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vmadc.carry.in.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmadc.carry.in.nxv1i1.i64(
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vxm_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vxm_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 1 x i1> @llvm.epi.vmadc.carry.in.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmadc.carry.in.nxv2i1.i64(
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vxm_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vxm_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vmadc.carry.in.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmadc.carry.in.nxv4i1.i64(
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vxm_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vxm_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vmadc.carry.in.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmadc.carry.in.nxv8i1.i64(
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmadc.carry.in_vxm_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vxm_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmadc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.carry.in.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmadc.carry.in_vim_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vim_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.carry.in.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmadc.carry.in_vim_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vim_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vmadc.carry.in.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmadc.carry.in_vim_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vim_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 32 x i1> @llvm.epi.vmadc.carry.in.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vmadc.carry.in_vim_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vim_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vmadc.carry.in.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmadc.carry.in_vim_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vim_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.carry.in.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmadc.carry.in_vim_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vim_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vmadc.carry.in.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmadc.carry.in_vim_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vim_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 32 x i1> @llvm.epi.vmadc.carry.in.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vmadc.carry.in_vim_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vim_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vmadc.carry.in.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vmadc.carry.in_vim_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vim_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vmadc.carry.in.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmadc.carry.in_vim_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vim_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.carry.in.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmadc.carry.in_vim_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vim_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vmadc.carry.in.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmadc.carry.in_vim_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vim_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 1 x i1> @llvm.epi.vmadc.carry.in.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


define void @intrinsic_vmadc.carry.in_vim_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vim_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vmadc.carry.in.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vmadc.carry.in_vim_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vim_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vmadc.carry.in.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmadc.carry.in_vim_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc.carry.in_vim_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmadc.vim {{v[0-9]+}}, {{v[0-9]+}}, 9, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.carry.in.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmadc.nxv8i1.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmadc_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmadc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.nxv8i1.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmadc.nxv16i1.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmadc_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmadc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmadc.nxv16i1.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmadc.nxv32i1.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmadc_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmadc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i1> @llvm.epi.vmadc.nxv32i1.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmadc.nxv4i1.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmadc_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmadc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmadc.nxv4i1.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmadc.nxv8i1.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmadc_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmadc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.nxv8i1.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmadc.nxv16i1.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmadc_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmadc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmadc.nxv16i1.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmadc.nxv32i1.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmadc_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmadc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i1> @llvm.epi.vmadc.nxv32i1.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmadc.nxv2i1.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmadc_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmadc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmadc.nxv2i1.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmadc.nxv4i1.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmadc_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmadc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmadc.nxv4i1.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmadc.nxv8i1.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmadc_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmadc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.nxv8i1.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmadc.nxv16i1.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmadc_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmadc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmadc.nxv16i1.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmadc.nxv1i1.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmadc_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmadc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmadc.nxv1i1.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmadc.nxv2i1.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmadc_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmadc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmadc.nxv2i1.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmadc.nxv4i1.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmadc_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmadc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmadc.nxv4i1.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmadc.nxv8i1.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmadc_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmadc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.nxv8i1.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmadc.nxv8i1.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmadc_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmadc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmadc.nxv16i1.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmadc_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmadc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmadc.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmadc.nxv32i1.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmadc_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmadc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vmadc.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmadc.nxv4i1.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmadc_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmadc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmadc.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmadc.nxv8i1.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmadc_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmadc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmadc.nxv16i1.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmadc_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmadc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmadc.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmadc.nxv32i1.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmadc_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmadc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vmadc.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmadc.nxv2i1.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmadc_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmadc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vmadc.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmadc.nxv4i1.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmadc_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmadc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmadc.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmadc.nxv8i1.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmadc_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmadc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmadc.nxv16i1.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmadc_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmadc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmadc.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmadc.nxv1i1.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmadc_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmadc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i1> @llvm.epi.vmadc.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmadc.nxv2i1.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmadc_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmadc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vmadc.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmadc.nxv4i1.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmadc_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmadc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmadc.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmadc.nxv8i1.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmadc_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmadc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmadc_vi_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vi_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmadc.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmadc_vi_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vi_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmadc.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vmadc.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmadc_vi_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vi_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmadc.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vmadc.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vmadc_vi_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vi_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmadc.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vmadc.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmadc_vi_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vi_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmadc.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmadc_vi_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vi_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmadc.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vmadc.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmadc_vi_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vi_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmadc.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 32 x i1> @llvm.epi.vmadc.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


define void @intrinsic_vmadc_vi_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vi_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmadc.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vmadc.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vmadc_vi_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vi_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmadc.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vmadc.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmadc_vi_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vi_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmadc.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


define void @intrinsic_vmadc_vi_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vi_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmadc.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 16 x i1> @llvm.epi.vmadc.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


define void @intrinsic_vmadc_vi_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vi_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmadc.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 1 x i1> @llvm.epi.vmadc.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


define void @intrinsic_vmadc_vi_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vi_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmadc.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 2 x i1> @llvm.epi.vmadc.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


define void @intrinsic_vmadc_vi_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vi_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmadc.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 4 x i1> @llvm.epi.vmadc.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


define void @intrinsic_vmadc_vi_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadc_vi_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmadc.vi {{v[0-9]+}}, {{v[0-9]+}}, 9
  %a = call <vscale x 8 x i1> @llvm.epi.vmadc.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 9,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsbc.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsbc_vvm_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vvm_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vsbc.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsbc.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsbc_vvm_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vvm_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vsbc.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsbc.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsbc_vvm_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vvm_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vsbc.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsbc.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsbc_vvm_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vvm_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vsbc.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsbc.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsbc_vvm_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vvm_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vsbc.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsbc.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsbc_vvm_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vvm_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vsbc.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsbc.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsbc_vvm_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vvm_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vsbc.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsbc.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsbc_vvm_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vvm_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vsbc.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsbc.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsbc_vvm_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vvm_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vsbc.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsbc.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsbc_vvm_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vvm_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vsbc.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsbc.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsbc_vvm_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vvm_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vsbc.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsbc.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsbc_vvm_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vvm_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vsbc.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsbc.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsbc_vvm_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vvm_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vsbc.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsbc.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsbc_vvm_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vvm_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vsbc.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsbc.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsbc_vvm_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vvm_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vsbc.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vsbc.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsbc_vxm_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vxm_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 8 x i8> @llvm.epi.vsbc.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vsbc.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsbc_vxm_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vxm_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 16 x i8> @llvm.epi.vsbc.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vsbc.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsbc_vxm_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vxm_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 32 x i8> @llvm.epi.vsbc.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vsbc.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsbc_vxm_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vxm_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 4 x i16> @llvm.epi.vsbc.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vsbc.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsbc_vxm_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vxm_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 8 x i16> @llvm.epi.vsbc.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vsbc.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsbc_vxm_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vxm_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 16 x i16> @llvm.epi.vsbc.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vsbc.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vsbc_vxm_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vxm_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 32 x i16> @llvm.epi.vsbc.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vsbc.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsbc_vxm_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vxm_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 2 x i32> @llvm.epi.vsbc.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vsbc.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsbc_vxm_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vxm_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 4 x i32> @llvm.epi.vsbc.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vsbc.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsbc_vxm_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vxm_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 8 x i32> @llvm.epi.vsbc.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vsbc.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vsbc_vxm_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vxm_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 16 x i32> @llvm.epi.vsbc.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vsbc.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vsbc_vxm_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vxm_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 1 x i64> @llvm.epi.vsbc.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vsbc.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vsbc_vxm_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vxm_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 2 x i64> @llvm.epi.vsbc.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vsbc.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vsbc_vxm_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vxm_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 4 x i64> @llvm.epi.vsbc.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vsbc.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vsbc_vxm_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vsbc_vxm_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 8 x i64> @llvm.epi.vsbc.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsbc.borrow.in.nxv8i1.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vvm_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vvm_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsbc.borrow.in.nxv8i1.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsbc.borrow.in.nxv16i1.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vvm_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vvm_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsbc.borrow.in.nxv16i1.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsbc.borrow.in.nxv32i1.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vvm_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vvm_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 32 x i1> @llvm.epi.vmsbc.borrow.in.nxv32i1.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsbc.borrow.in.nxv4i1.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vvm_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vvm_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsbc.borrow.in.nxv4i1.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsbc.borrow.in.nxv8i1.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vvm_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vvm_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsbc.borrow.in.nxv8i1.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsbc.borrow.in.nxv16i1.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vvm_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vvm_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsbc.borrow.in.nxv16i1.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsbc.borrow.in.nxv32i1.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vvm_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vvm_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 32 x i1> @llvm.epi.vmsbc.borrow.in.nxv32i1.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsbc.borrow.in.nxv2i1.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vvm_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vvm_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vmsbc.borrow.in.nxv2i1.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsbc.borrow.in.nxv4i1.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vvm_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vvm_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsbc.borrow.in.nxv4i1.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsbc.borrow.in.nxv8i1.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vvm_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vvm_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsbc.borrow.in.nxv8i1.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsbc.borrow.in.nxv16i1.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vvm_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vvm_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsbc.borrow.in.nxv16i1.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmsbc.borrow.in.nxv1i1.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vvm_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vvm_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 1 x i1> @llvm.epi.vmsbc.borrow.in.nxv1i1.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsbc.borrow.in.nxv2i1.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vvm_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vvm_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vmsbc.borrow.in.nxv2i1.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsbc.borrow.in.nxv4i1.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vvm_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vvm_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsbc.borrow.in.nxv4i1.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsbc.borrow.in.nxv8i1.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vvm_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vvm_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsbc.vvm {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsbc.borrow.in.nxv8i1.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsbc.borrow.in.nxv8i1.i8(
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vxm_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vxm_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsbc.borrow.in.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsbc.borrow.in.nxv16i1.i8(
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vxm_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vxm_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsbc.borrow.in.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsbc.borrow.in.nxv32i1.i8(
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vxm_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vxm_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 32 x i1> @llvm.epi.vmsbc.borrow.in.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsbc.borrow.in.nxv4i1.i16(
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vxm_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vxm_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsbc.borrow.in.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsbc.borrow.in.nxv8i1.i16(
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vxm_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vxm_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsbc.borrow.in.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsbc.borrow.in.nxv16i1.i16(
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vxm_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vxm_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsbc.borrow.in.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsbc.borrow.in.nxv32i1.i16(
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vxm_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vxm_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 32 x i1> @llvm.epi.vmsbc.borrow.in.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsbc.borrow.in.nxv2i1.i32(
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vxm_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vxm_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vmsbc.borrow.in.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsbc.borrow.in.nxv4i1.i32(
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vxm_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vxm_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsbc.borrow.in.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsbc.borrow.in.nxv8i1.i32(
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vxm_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vxm_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsbc.borrow.in.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsbc.borrow.in.nxv16i1.i32(
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vxm_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vxm_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsbc.borrow.in.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmsbc.borrow.in.nxv1i1.i64(
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vxm_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vxm_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 1 x i1> @llvm.epi.vmsbc.borrow.in.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsbc.borrow.in.nxv2i1.i64(
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vxm_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vxm_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 2 x i1> @llvm.epi.vmsbc.borrow.in.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsbc.borrow.in.nxv4i1.i64(
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vxm_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vxm_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsbc.borrow.in.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsbc.borrow.in.nxv8i1.i64(
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmsbc.borrow.in_vxm_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc.borrow.in_vxm_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsbc.vxm {{v[0-9]+}}, {{v[0-9]+}}, a0, v0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsbc.borrow.in.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsbc.nxv8i1.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmsbc_vv_nxv8i1_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vv_nxv8i1_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsbc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsbc.nxv8i1.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsbc.nxv16i1.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmsbc_vv_nxv16i1_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vv_nxv16i1_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsbc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmsbc.nxv16i1.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsbc.nxv32i1.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmsbc_vv_nxv32i1_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vv_nxv32i1_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsbc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i1> @llvm.epi.vmsbc.nxv32i1.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsbc.nxv4i1.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmsbc_vv_nxv4i1_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vv_nxv4i1_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsbc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmsbc.nxv4i1.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsbc.nxv8i1.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmsbc_vv_nxv8i1_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vv_nxv8i1_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsbc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsbc.nxv8i1.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsbc.nxv16i1.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmsbc_vv_nxv16i1_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vv_nxv16i1_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsbc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmsbc.nxv16i1.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsbc.nxv32i1.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmsbc_vv_nxv32i1_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vv_nxv32i1_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsbc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 32 x i1> @llvm.epi.vmsbc.nxv32i1.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsbc.nxv2i1.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmsbc_vv_nxv2i1_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vv_nxv2i1_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsbc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmsbc.nxv2i1.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsbc.nxv4i1.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmsbc_vv_nxv4i1_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vv_nxv4i1_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsbc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmsbc.nxv4i1.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsbc.nxv8i1.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmsbc_vv_nxv8i1_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vv_nxv8i1_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsbc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsbc.nxv8i1.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsbc.nxv16i1.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmsbc_vv_nxv16i1_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vv_nxv16i1_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsbc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i1> @llvm.epi.vmsbc.nxv16i1.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmsbc.nxv1i1.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmsbc_vv_nxv1i1_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vv_nxv1i1_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsbc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i1> @llvm.epi.vmsbc.nxv1i1.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsbc.nxv2i1.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmsbc_vv_nxv2i1_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vv_nxv2i1_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsbc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i1> @llvm.epi.vmsbc.nxv2i1.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsbc.nxv4i1.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmsbc_vv_nxv4i1_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vv_nxv4i1_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsbc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i1> @llvm.epi.vmsbc.nxv4i1.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsbc.nxv8i1.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmsbc_vv_nxv8i1_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vv_nxv8i1_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsbc.vv {{v[0-9]+}}, {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i1> @llvm.epi.vmsbc.nxv8i1.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsbc.nxv8i1.i8(
  <vscale x 8 x i8>,
  i8,
  i64);

define void @intrinsic_vmsbc_vx_nxv8i1_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vx_nxv8i1_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmsbc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsbc.nxv8i1.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsbc.nxv16i1.i8(
  <vscale x 16 x i8>,
  i8,
  i64);

define void @intrinsic_vmsbc_vx_nxv16i1_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vx_nxv16i1_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmsbc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsbc.nxv16i1.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsbc.nxv32i1.i8(
  <vscale x 32 x i8>,
  i8,
  i64);

define void @intrinsic_vmsbc_vx_nxv32i1_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vx_nxv32i1_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmsbc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vmsbc.nxv32i1.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsbc.nxv4i1.i16(
  <vscale x 4 x i16>,
  i16,
  i64);

define void @intrinsic_vmsbc_vx_nxv4i1_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vx_nxv4i1_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmsbc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsbc.nxv4i1.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsbc.nxv8i1.i16(
  <vscale x 8 x i16>,
  i16,
  i64);

define void @intrinsic_vmsbc_vx_nxv8i1_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vx_nxv8i1_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmsbc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsbc.nxv8i1.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsbc.nxv16i1.i16(
  <vscale x 16 x i16>,
  i16,
  i64);

define void @intrinsic_vmsbc_vx_nxv16i1_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vx_nxv16i1_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmsbc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsbc.nxv16i1.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 32 x i1> @llvm.epi.vmsbc.nxv32i1.i16(
  <vscale x 32 x i16>,
  i16,
  i64);

define void @intrinsic_vmsbc_vx_nxv32i1_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vx_nxv32i1_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmsbc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 32 x i1> @llvm.epi.vmsbc.nxv32i1.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i1>*
  store <vscale x 32 x i1> %a, <vscale x 32 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsbc.nxv2i1.i32(
  <vscale x 2 x i32>,
  i32,
  i64);

define void @intrinsic_vmsbc_vx_nxv2i1_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vx_nxv2i1_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmsbc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vmsbc.nxv2i1.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsbc.nxv4i1.i32(
  <vscale x 4 x i32>,
  i32,
  i64);

define void @intrinsic_vmsbc_vx_nxv4i1_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vx_nxv4i1_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmsbc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsbc.nxv4i1.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsbc.nxv8i1.i32(
  <vscale x 8 x i32>,
  i32,
  i64);

define void @intrinsic_vmsbc_vx_nxv8i1_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vx_nxv8i1_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmsbc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsbc.nxv8i1.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 16 x i1> @llvm.epi.vmsbc.nxv16i1.i32(
  <vscale x 16 x i32>,
  i32,
  i64);

define void @intrinsic_vmsbc_vx_nxv16i1_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vx_nxv16i1_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmsbc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 16 x i1> @llvm.epi.vmsbc.nxv16i1.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i1>*
  store <vscale x 16 x i1> %a, <vscale x 16 x i1>* %p

  ret void
}


declare <vscale x 1 x i1> @llvm.epi.vmsbc.nxv1i1.i64(
  <vscale x 1 x i64>,
  i64,
  i64);

define void @intrinsic_vmsbc_vx_nxv1i1_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vx_nxv1i1_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmsbc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 1 x i1> @llvm.epi.vmsbc.nxv1i1.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i1>*
  store <vscale x 1 x i1> %a, <vscale x 1 x i1>* %p

  ret void
}


declare <vscale x 2 x i1> @llvm.epi.vmsbc.nxv2i1.i64(
  <vscale x 2 x i64>,
  i64,
  i64);

define void @intrinsic_vmsbc_vx_nxv2i1_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vx_nxv2i1_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmsbc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 2 x i1> @llvm.epi.vmsbc.nxv2i1.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i1>*
  store <vscale x 2 x i1> %a, <vscale x 2 x i1>* %p

  ret void
}


declare <vscale x 4 x i1> @llvm.epi.vmsbc.nxv4i1.i64(
  <vscale x 4 x i64>,
  i64,
  i64);

define void @intrinsic_vmsbc_vx_nxv4i1_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vx_nxv4i1_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmsbc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 4 x i1> @llvm.epi.vmsbc.nxv4i1.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i1>*
  store <vscale x 4 x i1> %a, <vscale x 4 x i1>* %p

  ret void
}


declare <vscale x 8 x i1> @llvm.epi.vmsbc.nxv8i1.i64(
  <vscale x 8 x i64>,
  i64,
  i64);

define void @intrinsic_vmsbc_vx_nxv8i1_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmsbc_vx_nxv8i1_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmsbc.vx {{v[0-9]+}}, {{v[0-9]+}}, a0
  %a = call <vscale x 8 x i1> @llvm.epi.vmsbc.nxv8i1.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i1>*
  store <vscale x 8 x i1> %a, <vscale x 8 x i1>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmacc.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmacc_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x i8> @llvm.epi.vmacc.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmacc.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmacc.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmacc.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmacc_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 16 x i8> @llvm.epi.vmacc.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmacc.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmacc.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmacc.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmacc_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 32 x i8> @llvm.epi.vmacc.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmacc.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmacc.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmacc.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmacc_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x i16> @llvm.epi.vmacc.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmacc.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmacc.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmacc.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmacc_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x i16> @llvm.epi.vmacc.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmacc.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmacc.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmacc.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmacc_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 16 x i16> @llvm.epi.vmacc.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmacc.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmacc.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmacc.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmacc_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 32 x i16> @llvm.epi.vmacc.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmacc.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmacc.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmacc.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmacc_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x i32> @llvm.epi.vmacc.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmacc.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmacc.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmacc.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmacc_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x i32> @llvm.epi.vmacc.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmacc.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmacc.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmacc.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmacc_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x i32> @llvm.epi.vmacc.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmacc.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmacc.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmacc.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmacc_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 16 x i32> @llvm.epi.vmacc.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmacc.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmacc.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmacc.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmacc_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 1 x i64> @llvm.epi.vmacc.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmacc.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmacc.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmacc.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmacc_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x i64> @llvm.epi.vmacc.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmacc.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmacc.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmacc.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmacc_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x i64> @llvm.epi.vmacc.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmacc.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmacc.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmacc.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmacc_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x i64> @llvm.epi.vmacc.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmacc.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmacc.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmacc.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmacc_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 8 x i8> @llvm.epi.vmacc.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmacc.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmacc.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmacc.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmacc_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 16 x i8> @llvm.epi.vmacc.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmacc.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmacc.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmacc.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmacc_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 32 x i8> @llvm.epi.vmacc.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmacc.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmacc.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmacc.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmacc_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 4 x i16> @llvm.epi.vmacc.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmacc.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmacc.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmacc.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmacc_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 8 x i16> @llvm.epi.vmacc.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmacc.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmacc.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmacc.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmacc_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 16 x i16> @llvm.epi.vmacc.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmacc.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmacc.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmacc.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmacc_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 32 x i16> @llvm.epi.vmacc.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmacc.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmacc.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmacc.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmacc_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 2 x i32> @llvm.epi.vmacc.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmacc.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmacc.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmacc.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmacc_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 4 x i32> @llvm.epi.vmacc.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmacc.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmacc.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmacc.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmacc_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 8 x i32> @llvm.epi.vmacc.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmacc.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmacc.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmacc.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmacc_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 16 x i32> @llvm.epi.vmacc.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmacc.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmacc.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmacc.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmacc_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 1 x i64> @llvm.epi.vmacc.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmacc.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmacc.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmacc.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmacc_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 2 x i64> @llvm.epi.vmacc.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmacc.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmacc.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmacc.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmacc_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 4 x i64> @llvm.epi.vmacc.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmacc.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmacc.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmacc.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmacc_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 8 x i64> @llvm.epi.vmacc.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmacc.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmacc_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmacc_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmacc.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmacc.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vnmsac.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vnmsac_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x i8> @llvm.epi.vnmsac.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vnmsac.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vnmsac.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vnmsac.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vnmsac_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 16 x i8> @llvm.epi.vnmsac.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vnmsac.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vnmsac.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vnmsac.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vnmsac_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 32 x i8> @llvm.epi.vnmsac.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vnmsac.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vnmsac.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vnmsac.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vnmsac_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x i16> @llvm.epi.vnmsac.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vnmsac.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vnmsac.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vnmsac.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vnmsac_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x i16> @llvm.epi.vnmsac.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vnmsac.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vnmsac.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vnmsac.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vnmsac_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 16 x i16> @llvm.epi.vnmsac.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vnmsac.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vnmsac.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vnmsac.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vnmsac_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 32 x i16> @llvm.epi.vnmsac.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vnmsac.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vnmsac.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vnmsac.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vnmsac_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x i32> @llvm.epi.vnmsac.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vnmsac.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vnmsac.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vnmsac.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vnmsac_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x i32> @llvm.epi.vnmsac.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vnmsac.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vnmsac.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vnmsac.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vnmsac_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x i32> @llvm.epi.vnmsac.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vnmsac.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vnmsac.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vnmsac.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vnmsac_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 16 x i32> @llvm.epi.vnmsac.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vnmsac.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vnmsac.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vnmsac.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vnmsac_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 1 x i64> @llvm.epi.vnmsac.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vnmsac.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vnmsac.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vnmsac.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vnmsac_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x i64> @llvm.epi.vnmsac.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vnmsac.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vnmsac.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vnmsac.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vnmsac_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x i64> @llvm.epi.vnmsac.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vnmsac.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vnmsac.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vnmsac.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vnmsac_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x i64> @llvm.epi.vnmsac.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vnmsac.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vnmsac.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vnmsac.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vnmsac_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 8 x i8> @llvm.epi.vnmsac.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vnmsac.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vnmsac.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vnmsac.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vnmsac_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 16 x i8> @llvm.epi.vnmsac.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vnmsac.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vnmsac.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vnmsac.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vnmsac_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 32 x i8> @llvm.epi.vnmsac.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vnmsac.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vnmsac.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vnmsac.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vnmsac_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 4 x i16> @llvm.epi.vnmsac.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vnmsac.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vnmsac.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vnmsac.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vnmsac_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 8 x i16> @llvm.epi.vnmsac.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vnmsac.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vnmsac.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vnmsac.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vnmsac_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 16 x i16> @llvm.epi.vnmsac.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vnmsac.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vnmsac.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vnmsac.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vnmsac_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 32 x i16> @llvm.epi.vnmsac.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vnmsac.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vnmsac.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vnmsac.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vnmsac_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 2 x i32> @llvm.epi.vnmsac.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vnmsac.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vnmsac.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vnmsac.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vnmsac_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 4 x i32> @llvm.epi.vnmsac.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vnmsac.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vnmsac.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vnmsac.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vnmsac_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 8 x i32> @llvm.epi.vnmsac.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vnmsac.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vnmsac.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vnmsac.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vnmsac_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 16 x i32> @llvm.epi.vnmsac.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vnmsac.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vnmsac.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vnmsac.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vnmsac_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 1 x i64> @llvm.epi.vnmsac.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vnmsac.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vnmsac.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vnmsac.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vnmsac_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 2 x i64> @llvm.epi.vnmsac.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vnmsac.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vnmsac.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vnmsac.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vnmsac_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 4 x i64> @llvm.epi.vnmsac.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vnmsac.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vnmsac.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vnmsac.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vnmsac_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 8 x i64> @llvm.epi.vnmsac.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vnmsac.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnmsac_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsac_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vnmsac.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vnmsac.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmadd.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmadd_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x i8> @llvm.epi.vmadd.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmadd.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmadd.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmadd.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmadd_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 16 x i8> @llvm.epi.vmadd.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmadd.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmadd.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmadd.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmadd_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 32 x i8> @llvm.epi.vmadd.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmadd.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmadd.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmadd.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmadd_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x i16> @llvm.epi.vmadd.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmadd.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmadd.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmadd.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmadd_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x i16> @llvm.epi.vmadd.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmadd.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmadd.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmadd.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmadd_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 16 x i16> @llvm.epi.vmadd.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmadd.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmadd.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmadd.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmadd_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 32 x i16> @llvm.epi.vmadd.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmadd.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmadd.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmadd.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmadd_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x i32> @llvm.epi.vmadd.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmadd.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmadd.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmadd.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmadd_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x i32> @llvm.epi.vmadd.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmadd.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmadd.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmadd.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmadd_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x i32> @llvm.epi.vmadd.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmadd.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmadd.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmadd.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmadd_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 16 x i32> @llvm.epi.vmadd.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmadd.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmadd.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmadd.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmadd_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 1 x i64> @llvm.epi.vmadd.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmadd.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmadd.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmadd.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmadd_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x i64> @llvm.epi.vmadd.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmadd.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmadd.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmadd.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmadd_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x i64> @llvm.epi.vmadd.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmadd.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmadd.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmadd.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmadd_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x i64> @llvm.epi.vmadd.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmadd.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmadd.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vmadd.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vmadd_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 8 x i8> @llvm.epi.vmadd.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vmadd.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vmadd.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vmadd.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vmadd_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 16 x i8> @llvm.epi.vmadd.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vmadd.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vmadd.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vmadd.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vmadd_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 32 x i8> @llvm.epi.vmadd.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vmadd.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vmadd.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vmadd.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vmadd_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 4 x i16> @llvm.epi.vmadd.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vmadd.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vmadd.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vmadd.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vmadd_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 8 x i16> @llvm.epi.vmadd.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vmadd.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vmadd.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vmadd.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vmadd_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 16 x i16> @llvm.epi.vmadd.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vmadd.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vmadd.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vmadd.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vmadd_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 32 x i16> @llvm.epi.vmadd.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vmadd.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vmadd.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vmadd.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vmadd_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 2 x i32> @llvm.epi.vmadd.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vmadd.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vmadd.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vmadd.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vmadd_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 4 x i32> @llvm.epi.vmadd.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vmadd.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vmadd.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vmadd.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vmadd_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 8 x i32> @llvm.epi.vmadd.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vmadd.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vmadd.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vmadd.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vmadd_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 16 x i32> @llvm.epi.vmadd.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vmadd.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vmadd.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vmadd.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vmadd_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 1 x i64> @llvm.epi.vmadd.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vmadd.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vmadd.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vmadd.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vmadd_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 2 x i64> @llvm.epi.vmadd.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vmadd.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vmadd.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vmadd.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vmadd_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 4 x i64> @llvm.epi.vmadd.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vmadd.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vmadd.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vmadd.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vmadd_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 8 x i64> @llvm.epi.vmadd.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vmadd.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vmadd_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vmadd_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vmadd.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vmadd.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vnmsub.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vnmsub_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x i8> @llvm.epi.vnmsub.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vnmsub.mask.nxv8i8.nxv8i8(
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vv_nxv8i8_nxv8i8_nxv8i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vv_nxv8i8_nxv8i8_nxv8i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vnmsub.mask.nxv8i8.nxv8i8(
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vnmsub.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vnmsub_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 16 x i8> @llvm.epi.vnmsub.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vnmsub.mask.nxv16i8.nxv16i8(
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vv_nxv16i8_nxv16i8_nxv16i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vv_nxv16i8_nxv16i8_nxv16i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vnmsub.mask.nxv16i8.nxv16i8(
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vnmsub.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vnmsub_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 32 x i8> @llvm.epi.vnmsub.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vnmsub.mask.nxv32i8.nxv32i8(
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vv_nxv32i8_nxv32i8_nxv32i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vv_nxv32i8_nxv32i8_nxv32i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vnmsub.mask.nxv32i8.nxv32i8(
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vnmsub.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vnmsub_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x i16> @llvm.epi.vnmsub.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vnmsub.mask.nxv4i16.nxv4i16(
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vv_nxv4i16_nxv4i16_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vv_nxv4i16_nxv4i16_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vnmsub.mask.nxv4i16.nxv4i16(
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vnmsub.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vnmsub_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x i16> @llvm.epi.vnmsub.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vnmsub.mask.nxv8i16.nxv8i16(
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vv_nxv8i16_nxv8i16_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vv_nxv8i16_nxv8i16_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vnmsub.mask.nxv8i16.nxv8i16(
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vnmsub.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vnmsub_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 16 x i16> @llvm.epi.vnmsub.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vnmsub.mask.nxv16i16.nxv16i16(
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vv_nxv16i16_nxv16i16_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vv_nxv16i16_nxv16i16_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vnmsub.mask.nxv16i16.nxv16i16(
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vnmsub.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vnmsub_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 32 x i16> @llvm.epi.vnmsub.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vnmsub.mask.nxv32i16.nxv32i16(
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vv_nxv32i16_nxv32i16_nxv32i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vv_nxv32i16_nxv32i16_nxv32i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vnmsub.mask.nxv32i16.nxv32i16(
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vnmsub.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vnmsub_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x i32> @llvm.epi.vnmsub.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vnmsub.mask.nxv2i32.nxv2i32(
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vv_nxv2i32_nxv2i32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vv_nxv2i32_nxv2i32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vnmsub.mask.nxv2i32.nxv2i32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vnmsub.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vnmsub_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x i32> @llvm.epi.vnmsub.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vnmsub.mask.nxv4i32.nxv4i32(
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vv_nxv4i32_nxv4i32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vv_nxv4i32_nxv4i32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vnmsub.mask.nxv4i32.nxv4i32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vnmsub.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vnmsub_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x i32> @llvm.epi.vnmsub.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vnmsub.mask.nxv8i32.nxv8i32(
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vv_nxv8i32_nxv8i32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vv_nxv8i32_nxv8i32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vnmsub.mask.nxv8i32.nxv8i32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vnmsub.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vnmsub_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 16 x i32> @llvm.epi.vnmsub.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vnmsub.mask.nxv16i32.nxv16i32(
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vv_nxv16i32_nxv16i32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vv_nxv16i32_nxv16i32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vnmsub.mask.nxv16i32.nxv16i32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vnmsub.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vnmsub_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 1 x i64> @llvm.epi.vnmsub.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vnmsub.mask.nxv1i64.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vv_nxv1i64_nxv1i64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vv_nxv1i64_nxv1i64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vnmsub.mask.nxv1i64.nxv1i64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vnmsub.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vnmsub_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x i64> @llvm.epi.vnmsub.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vnmsub.mask.nxv2i64.nxv2i64(
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vv_nxv2i64_nxv2i64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vv_nxv2i64_nxv2i64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vnmsub.mask.nxv2i64.nxv2i64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vnmsub.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vnmsub_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x i64> @llvm.epi.vnmsub.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vnmsub.mask.nxv4i64.nxv4i64(
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vv_nxv4i64_nxv4i64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vv_nxv4i64_nxv4i64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vnmsub.mask.nxv4i64.nxv4i64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vnmsub.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vnmsub_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x i64> @llvm.epi.vnmsub.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vnmsub.mask.nxv8i64.nxv8i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vv_nxv8i64_nxv8i64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vv_nxv8i64_nxv8i64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vnmsub.mask.nxv8i64.nxv8i64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 8 x i8> @llvm.epi.vnmsub.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i8>,
  i64);

define void @intrinsic_vnmsub_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 8 x i8> @llvm.epi.vnmsub.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}

declare <vscale x 8 x i8> @llvm.epi.vnmsub.mask.nxv8i8.i8(
  <vscale x 8 x i8>,
  i8,
  <vscale x 8 x i8>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vx_nxv8i8_nxv8i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vx_nxv8i8_nxv8i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m1, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 8 x i8> @llvm.epi.vnmsub.mask.nxv8i8.i8(
    <vscale x 8 x i8> undef,
    i8 undef,
    <vscale x 8 x i8> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i8>*
  store <vscale x 8 x i8> %a, <vscale x 8 x i8>* %p

  ret void
}


declare <vscale x 16 x i8> @llvm.epi.vnmsub.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i8>,
  i64);

define void @intrinsic_vnmsub_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 16 x i8> @llvm.epi.vnmsub.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}

declare <vscale x 16 x i8> @llvm.epi.vnmsub.mask.nxv16i8.i8(
  <vscale x 16 x i8>,
  i8,
  <vscale x 16 x i8>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vx_nxv16i8_nxv16i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vx_nxv16i8_nxv16i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m2, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 16 x i8> @llvm.epi.vnmsub.mask.nxv16i8.i8(
    <vscale x 16 x i8> undef,
    i8 undef,
    <vscale x 16 x i8> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i8>*
  store <vscale x 16 x i8> %a, <vscale x 16 x i8>* %p

  ret void
}


declare <vscale x 32 x i8> @llvm.epi.vnmsub.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i8>,
  i64);

define void @intrinsic_vnmsub_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 32 x i8> @llvm.epi.vnmsub.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i8> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}

declare <vscale x 32 x i8> @llvm.epi.vnmsub.mask.nxv32i8.i8(
  <vscale x 32 x i8>,
  i8,
  <vscale x 32 x i8>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vx_nxv32i8_nxv32i8_i8() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vx_nxv32i8_nxv32i8_i8
; CHECK:       vsetvli {{.*}}, a0, e8, m4, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 32 x i8> @llvm.epi.vnmsub.mask.nxv32i8.i8(
    <vscale x 32 x i8> undef,
    i8 undef,
    <vscale x 32 x i8> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i8>*
  store <vscale x 32 x i8> %a, <vscale x 32 x i8>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vnmsub.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vnmsub_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 4 x i16> @llvm.epi.vnmsub.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vnmsub.mask.nxv4i16.i16(
  <vscale x 4 x i16>,
  i16,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vx_nxv4i16_nxv4i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vx_nxv4i16_nxv4i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vnmsub.mask.nxv4i16.i16(
    <vscale x 4 x i16> undef,
    i16 undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vnmsub.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vnmsub_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 8 x i16> @llvm.epi.vnmsub.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vnmsub.mask.nxv8i16.i16(
  <vscale x 8 x i16>,
  i16,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vx_nxv8i16_nxv8i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vx_nxv8i16_nxv8i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vnmsub.mask.nxv8i16.i16(
    <vscale x 8 x i16> undef,
    i16 undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vnmsub.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vnmsub_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 16 x i16> @llvm.epi.vnmsub.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vnmsub.mask.nxv16i16.i16(
  <vscale x 16 x i16>,
  i16,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vx_nxv16i16_nxv16i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vx_nxv16i16_nxv16i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vnmsub.mask.nxv16i16.i16(
    <vscale x 16 x i16> undef,
    i16 undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 32 x i16> @llvm.epi.vnmsub.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i16>,
  i64);

define void @intrinsic_vnmsub_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 32 x i16> @llvm.epi.vnmsub.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}

declare <vscale x 32 x i16> @llvm.epi.vnmsub.mask.nxv32i16.i16(
  <vscale x 32 x i16>,
  i16,
  <vscale x 32 x i16>,
  <vscale x 32 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vx_nxv32i16_nxv32i16_i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vx_nxv32i16_nxv32i16_i16
; CHECK:       vsetvli {{.*}}, a0, e16, m8, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 32 x i16> @llvm.epi.vnmsub.mask.nxv32i16.i16(
    <vscale x 32 x i16> undef,
    i16 undef,
    <vscale x 32 x i16> undef,
    <vscale x 32 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 32 x i16>*
  store <vscale x 32 x i16> %a, <vscale x 32 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vnmsub.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vnmsub_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 2 x i32> @llvm.epi.vnmsub.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vnmsub.mask.nxv2i32.i32(
  <vscale x 2 x i32>,
  i32,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vx_nxv2i32_nxv2i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vx_nxv2i32_nxv2i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vnmsub.mask.nxv2i32.i32(
    <vscale x 2 x i32> undef,
    i32 undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vnmsub.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vnmsub_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 4 x i32> @llvm.epi.vnmsub.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vnmsub.mask.nxv4i32.i32(
  <vscale x 4 x i32>,
  i32,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vx_nxv4i32_nxv4i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vx_nxv4i32_nxv4i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vnmsub.mask.nxv4i32.i32(
    <vscale x 4 x i32> undef,
    i32 undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vnmsub.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vnmsub_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 8 x i32> @llvm.epi.vnmsub.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vnmsub.mask.nxv8i32.i32(
  <vscale x 8 x i32>,
  i32,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vx_nxv8i32_nxv8i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vx_nxv8i32_nxv8i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vnmsub.mask.nxv8i32.i32(
    <vscale x 8 x i32> undef,
    i32 undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vnmsub.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vnmsub_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 16 x i32> @llvm.epi.vnmsub.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vnmsub.mask.nxv16i32.i32(
  <vscale x 16 x i32>,
  i32,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vx_nxv16i32_nxv16i32_i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vx_nxv16i32_nxv16i32_i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vnmsub.mask.nxv16i32.i32(
    <vscale x 16 x i32> undef,
    i32 undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vnmsub.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vnmsub_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 1 x i64> @llvm.epi.vnmsub.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vnmsub.mask.nxv1i64.i64(
  <vscale x 1 x i64>,
  i64,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vx_nxv1i64_nxv1i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vx_nxv1i64_nxv1i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vnmsub.mask.nxv1i64.i64(
    <vscale x 1 x i64> undef,
    i64 undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vnmsub.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vnmsub_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 2 x i64> @llvm.epi.vnmsub.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vnmsub.mask.nxv2i64.i64(
  <vscale x 2 x i64>,
  i64,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vx_nxv2i64_nxv2i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vx_nxv2i64_nxv2i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vnmsub.mask.nxv2i64.i64(
    <vscale x 2 x i64> undef,
    i64 undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vnmsub.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vnmsub_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 4 x i64> @llvm.epi.vnmsub.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vnmsub.mask.nxv4i64.i64(
  <vscale x 4 x i64>,
  i64,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vx_nxv4i64_nxv4i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vx_nxv4i64_nxv4i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vnmsub.mask.nxv4i64.i64(
    <vscale x 4 x i64> undef,
    i64 undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vnmsub.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vnmsub_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]]
  %a = call <vscale x 8 x i64> @llvm.epi.vnmsub.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vnmsub.mask.nxv8i64.i64(
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vnmsub_mask_vx_nxv8i64_nxv8i64_i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vnmsub_mask_vx_nxv8i64_nxv8i64_i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vnmsub.vx [[VR:v[0-9]+]], a0, [[VR]], v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vnmsub.mask.nxv8i64.i64(
    <vscale x 8 x i64> undef,
    i64 undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmadd.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfmadd_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x float> @llvm.epi.vfmadd.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfmadd.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmadd_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfmadd.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmadd.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfmadd_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x float> @llvm.epi.vfmadd.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfmadd.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmadd_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfmadd.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmadd.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfmadd_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x float> @llvm.epi.vfmadd.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfmadd.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmadd_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfmadd.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmadd.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfmadd_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 16 x float> @llvm.epi.vfmadd.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfmadd.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfmadd_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfmadd.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmadd.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfmadd_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 1 x double> @llvm.epi.vfmadd.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmadd.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfmadd_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfmadd.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmadd.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfmadd_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x double> @llvm.epi.vfmadd.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfmadd.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmadd_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfmadd.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmadd.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfmadd_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x double> @llvm.epi.vfmadd.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfmadd.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmadd_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfmadd.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmadd.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfmadd_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x double> @llvm.epi.vfmadd.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfmadd.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmadd_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfmadd.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmadd.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfmadd_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmadd.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 2 x float> @llvm.epi.vfmadd.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfmadd.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmadd_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmadd.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfmadd.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmadd.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfmadd_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmadd.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 4 x float> @llvm.epi.vfmadd.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfmadd.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmadd_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmadd.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfmadd.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmadd.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfmadd_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmadd.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 8 x float> @llvm.epi.vfmadd.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfmadd.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmadd_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmadd.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfmadd.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmadd.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfmadd_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmadd.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 16 x float> @llvm.epi.vfmadd.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfmadd.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfmadd_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmadd.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfmadd.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmadd.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfmadd_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmadd.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 1 x double> @llvm.epi.vfmadd.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmadd.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfmadd_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmadd.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfmadd.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmadd.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfmadd_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmadd.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 2 x double> @llvm.epi.vfmadd.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfmadd.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmadd_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmadd.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfmadd.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmadd.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfmadd_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmadd.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 4 x double> @llvm.epi.vfmadd.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfmadd.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmadd_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmadd.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfmadd.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmadd.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfmadd_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmadd.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 8 x double> @llvm.epi.vfmadd.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfmadd.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmadd_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmadd_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmadd.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfmadd.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfnmadd.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfnmadd_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfnmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x float> @llvm.epi.vfnmadd.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfnmadd.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfnmadd_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfnmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfnmadd.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfnmadd.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfnmadd_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfnmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x float> @llvm.epi.vfnmadd.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfnmadd.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfnmadd_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfnmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfnmadd.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfnmadd.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfnmadd_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfnmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x float> @llvm.epi.vfnmadd.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfnmadd.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfnmadd_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfnmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfnmadd.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfnmadd.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfnmadd_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfnmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 16 x float> @llvm.epi.vfnmadd.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfnmadd.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfnmadd_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfnmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfnmadd.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfnmadd.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfnmadd_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfnmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 1 x double> @llvm.epi.vfnmadd.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfnmadd.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfnmadd_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfnmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfnmadd.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfnmadd.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfnmadd_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfnmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x double> @llvm.epi.vfnmadd.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfnmadd.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfnmadd_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfnmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfnmadd.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfnmadd.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfnmadd_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfnmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x double> @llvm.epi.vfnmadd.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfnmadd.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfnmadd_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfnmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfnmadd.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfnmadd.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfnmadd_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfnmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x double> @llvm.epi.vfnmadd.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfnmadd.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfnmadd_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfnmadd.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfnmadd.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfnmadd.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfnmadd_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfnmadd.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 2 x float> @llvm.epi.vfnmadd.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfnmadd.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfnmadd_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfnmadd.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfnmadd.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfnmadd.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfnmadd_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfnmadd.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 4 x float> @llvm.epi.vfnmadd.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfnmadd.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfnmadd_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfnmadd.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfnmadd.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfnmadd.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfnmadd_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfnmadd.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 8 x float> @llvm.epi.vfnmadd.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfnmadd.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfnmadd_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfnmadd.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfnmadd.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfnmadd.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfnmadd_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfnmadd.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 16 x float> @llvm.epi.vfnmadd.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfnmadd.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfnmadd_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfnmadd.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfnmadd.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfnmadd.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfnmadd_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfnmadd.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 1 x double> @llvm.epi.vfnmadd.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfnmadd.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfnmadd_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfnmadd.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfnmadd.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfnmadd.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfnmadd_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfnmadd.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 2 x double> @llvm.epi.vfnmadd.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfnmadd.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfnmadd_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfnmadd.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfnmadd.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfnmadd.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfnmadd_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfnmadd.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 4 x double> @llvm.epi.vfnmadd.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfnmadd.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfnmadd_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfnmadd.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfnmadd.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfnmadd.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfnmadd_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfnmadd.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 8 x double> @llvm.epi.vfnmadd.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfnmadd.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfnmadd_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmadd_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfnmadd.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfnmadd.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmsub.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfmsub_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x float> @llvm.epi.vfmsub.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfmsub.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmsub_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfmsub.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmsub.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfmsub_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x float> @llvm.epi.vfmsub.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfmsub.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmsub_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfmsub.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmsub.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfmsub_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x float> @llvm.epi.vfmsub.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfmsub.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmsub_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfmsub.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmsub.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfmsub_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 16 x float> @llvm.epi.vfmsub.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfmsub.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfmsub_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfmsub.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmsub.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfmsub_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 1 x double> @llvm.epi.vfmsub.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmsub.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfmsub_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfmsub.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmsub.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfmsub_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x double> @llvm.epi.vfmsub.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfmsub.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmsub_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfmsub.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmsub.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfmsub_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x double> @llvm.epi.vfmsub.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfmsub.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmsub_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfmsub.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmsub.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfmsub_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x double> @llvm.epi.vfmsub.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfmsub.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmsub_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfmsub.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmsub.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfmsub_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmsub.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 2 x float> @llvm.epi.vfmsub.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfmsub.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmsub_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmsub.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfmsub.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmsub.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfmsub_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmsub.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 4 x float> @llvm.epi.vfmsub.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfmsub.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmsub_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmsub.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfmsub.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmsub.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfmsub_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmsub.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 8 x float> @llvm.epi.vfmsub.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfmsub.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmsub_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmsub.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfmsub.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmsub.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfmsub_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmsub.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 16 x float> @llvm.epi.vfmsub.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfmsub.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfmsub_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmsub.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfmsub.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmsub.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfmsub_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmsub.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 1 x double> @llvm.epi.vfmsub.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmsub.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfmsub_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmsub.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfmsub.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmsub.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfmsub_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmsub.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 2 x double> @llvm.epi.vfmsub.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfmsub.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmsub_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmsub.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfmsub.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmsub.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfmsub_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmsub.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 4 x double> @llvm.epi.vfmsub.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfmsub.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmsub_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmsub.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfmsub.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmsub.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfmsub_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmsub.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 8 x double> @llvm.epi.vfmsub.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfmsub.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmsub_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsub_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmsub.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfmsub.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfnmsub.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfnmsub_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x float> @llvm.epi.vfnmsub.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfnmsub.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfnmsub_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfnmsub.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfnmsub.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfnmsub_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x float> @llvm.epi.vfnmsub.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfnmsub.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfnmsub_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfnmsub.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfnmsub.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfnmsub_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x float> @llvm.epi.vfnmsub.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfnmsub.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfnmsub_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfnmsub.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfnmsub.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfnmsub_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 16 x float> @llvm.epi.vfnmsub.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfnmsub.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfnmsub_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfnmsub.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfnmsub.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfnmsub_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 1 x double> @llvm.epi.vfnmsub.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfnmsub.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfnmsub_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfnmsub.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfnmsub.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfnmsub_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x double> @llvm.epi.vfnmsub.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfnmsub.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfnmsub_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfnmsub.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfnmsub.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfnmsub_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x double> @llvm.epi.vfnmsub.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfnmsub.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfnmsub_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfnmsub.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfnmsub.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfnmsub_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x double> @llvm.epi.vfnmsub.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfnmsub.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfnmsub_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfnmsub.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfnmsub.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfnmsub.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfnmsub_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfnmsub.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 2 x float> @llvm.epi.vfnmsub.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfnmsub.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfnmsub_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfnmsub.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfnmsub.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfnmsub.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfnmsub_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfnmsub.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 4 x float> @llvm.epi.vfnmsub.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfnmsub.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfnmsub_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfnmsub.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfnmsub.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfnmsub.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfnmsub_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfnmsub.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 8 x float> @llvm.epi.vfnmsub.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfnmsub.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfnmsub_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfnmsub.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfnmsub.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfnmsub.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfnmsub_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfnmsub.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 16 x float> @llvm.epi.vfnmsub.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfnmsub.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfnmsub_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfnmsub.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfnmsub.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfnmsub.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfnmsub_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfnmsub.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 1 x double> @llvm.epi.vfnmsub.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfnmsub.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfnmsub_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfnmsub.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfnmsub.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfnmsub.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfnmsub_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfnmsub.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 2 x double> @llvm.epi.vfnmsub.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfnmsub.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfnmsub_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfnmsub.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfnmsub.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfnmsub.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfnmsub_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfnmsub.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 4 x double> @llvm.epi.vfnmsub.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfnmsub.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfnmsub_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfnmsub.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfnmsub.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfnmsub.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfnmsub_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfnmsub.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 8 x double> @llvm.epi.vfnmsub.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfnmsub.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfnmsub_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsub_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfnmsub.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfnmsub.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmacc.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfmacc_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x float> @llvm.epi.vfmacc.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfmacc.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmacc_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfmacc.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmacc.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfmacc_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x float> @llvm.epi.vfmacc.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfmacc.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmacc_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfmacc.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmacc.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfmacc_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x float> @llvm.epi.vfmacc.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfmacc.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmacc_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfmacc.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmacc.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfmacc_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 16 x float> @llvm.epi.vfmacc.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfmacc.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfmacc_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfmacc.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfmacc_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmacc.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfmacc_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfmacc.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmacc.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfmacc_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x double> @llvm.epi.vfmacc.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfmacc.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmacc_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfmacc.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmacc.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfmacc_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x double> @llvm.epi.vfmacc.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfmacc.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmacc_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfmacc.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmacc.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfmacc_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x double> @llvm.epi.vfmacc.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfmacc.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmacc_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfmacc.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmacc.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfmacc_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmacc.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 2 x float> @llvm.epi.vfmacc.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfmacc.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmacc_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmacc.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfmacc.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmacc.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfmacc_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmacc.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 4 x float> @llvm.epi.vfmacc.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfmacc.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmacc_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmacc.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfmacc.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmacc.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfmacc_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmacc.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 8 x float> @llvm.epi.vfmacc.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfmacc.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmacc_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmacc.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfmacc.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmacc.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfmacc_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmacc.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 16 x float> @llvm.epi.vfmacc.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfmacc.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfmacc_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmacc.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfmacc.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfmacc_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmacc.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmacc.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfmacc_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmacc.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfmacc.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmacc.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfmacc_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmacc.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 2 x double> @llvm.epi.vfmacc.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfmacc.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmacc_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmacc.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfmacc.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmacc.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfmacc_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmacc.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 4 x double> @llvm.epi.vfmacc.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfmacc.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmacc_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmacc.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfmacc.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmacc.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfmacc_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmacc.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 8 x double> @llvm.epi.vfmacc.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfmacc.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmacc_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmacc_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmacc.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfmacc.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfnmacc.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfnmacc_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfnmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x float> @llvm.epi.vfnmacc.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfnmacc.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfnmacc_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfnmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfnmacc.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfnmacc.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfnmacc_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfnmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x float> @llvm.epi.vfnmacc.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfnmacc.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfnmacc_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfnmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfnmacc.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfnmacc.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfnmacc_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfnmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x float> @llvm.epi.vfnmacc.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfnmacc.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfnmacc_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfnmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfnmacc.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfnmacc.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfnmacc_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfnmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 16 x float> @llvm.epi.vfnmacc.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfnmacc.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfnmacc_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfnmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfnmacc.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfnmacc.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfnmacc_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfnmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 1 x double> @llvm.epi.vfnmacc.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfnmacc.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfnmacc_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfnmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfnmacc.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfnmacc.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfnmacc_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfnmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x double> @llvm.epi.vfnmacc.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfnmacc.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfnmacc_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfnmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfnmacc.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfnmacc.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfnmacc_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfnmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x double> @llvm.epi.vfnmacc.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfnmacc.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfnmacc_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfnmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfnmacc.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfnmacc.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfnmacc_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfnmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x double> @llvm.epi.vfnmacc.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfnmacc.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfnmacc_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfnmacc.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfnmacc.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfnmacc.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfnmacc_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfnmacc.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 2 x float> @llvm.epi.vfnmacc.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfnmacc.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfnmacc_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfnmacc.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfnmacc.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfnmacc.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfnmacc_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfnmacc.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 4 x float> @llvm.epi.vfnmacc.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfnmacc.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfnmacc_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfnmacc.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfnmacc.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfnmacc.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfnmacc_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfnmacc.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 8 x float> @llvm.epi.vfnmacc.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfnmacc.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfnmacc_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfnmacc.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfnmacc.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfnmacc.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfnmacc_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfnmacc.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 16 x float> @llvm.epi.vfnmacc.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfnmacc.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfnmacc_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfnmacc.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfnmacc.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfnmacc.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfnmacc_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfnmacc.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 1 x double> @llvm.epi.vfnmacc.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfnmacc.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfnmacc_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfnmacc.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfnmacc.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfnmacc.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfnmacc_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfnmacc.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 2 x double> @llvm.epi.vfnmacc.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfnmacc.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfnmacc_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfnmacc.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfnmacc.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfnmacc.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfnmacc_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfnmacc.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 4 x double> @llvm.epi.vfnmacc.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfnmacc.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfnmacc_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfnmacc.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfnmacc.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfnmacc.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfnmacc_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfnmacc.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 8 x double> @llvm.epi.vfnmacc.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfnmacc.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfnmacc_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmacc_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfnmacc.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfnmacc.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmsac.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfmsac_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x float> @llvm.epi.vfmsac.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfmsac.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmsac_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfmsac.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmsac.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfmsac_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x float> @llvm.epi.vfmsac.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfmsac.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmsac_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfmsac.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmsac.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfmsac_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x float> @llvm.epi.vfmsac.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfmsac.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmsac_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfmsac.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmsac.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfmsac_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 16 x float> @llvm.epi.vfmsac.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfmsac.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfmsac_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfmsac.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfmsac_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmsac.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfmsac_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfmsac.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmsac.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfmsac_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x double> @llvm.epi.vfmsac.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfmsac.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmsac_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfmsac.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmsac.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfmsac_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x double> @llvm.epi.vfmsac.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfmsac.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmsac_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfmsac.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmsac.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfmsac_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x double> @llvm.epi.vfmsac.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfmsac.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmsac_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfmsac.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfmsac.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfmsac_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmsac.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 2 x float> @llvm.epi.vfmsac.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfmsac.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmsac_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfmsac.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfmsac.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfmsac.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfmsac_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmsac.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 4 x float> @llvm.epi.vfmsac.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfmsac.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmsac_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfmsac.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfmsac.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfmsac.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfmsac_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmsac.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 8 x float> @llvm.epi.vfmsac.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfmsac.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmsac_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfmsac.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfmsac.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfmsac.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfmsac_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmsac.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 16 x float> @llvm.epi.vfmsac.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfmsac.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfmsac_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfmsac.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfmsac.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfmsac_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmsac.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfmsac.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfmsac_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfmsac.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfmsac.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfmsac.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfmsac_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmsac.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 2 x double> @llvm.epi.vfmsac.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfmsac.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfmsac_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfmsac.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfmsac.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfmsac.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfmsac_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmsac.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 4 x double> @llvm.epi.vfmsac.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfmsac.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfmsac_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfmsac.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfmsac.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfmsac.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfmsac_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmsac.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 8 x double> @llvm.epi.vfmsac.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfmsac.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfmsac_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfmsac_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfmsac.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfmsac.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfnmsac.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfnmsac_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x float> @llvm.epi.vfnmsac.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfnmsac.mask.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfnmsac_mask_vv_nxv2f32_nxv2f32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_mask_vv_nxv2f32_nxv2f32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfnmsac.mask.nxv2f32.nxv2f32(
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfnmsac.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfnmsac_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x float> @llvm.epi.vfnmsac.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfnmsac.mask.nxv4f32.nxv4f32(
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfnmsac_mask_vv_nxv4f32_nxv4f32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_mask_vv_nxv4f32_nxv4f32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfnmsac.mask.nxv4f32.nxv4f32(
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfnmsac.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfnmsac_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x float> @llvm.epi.vfnmsac.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfnmsac.mask.nxv8f32.nxv8f32(
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfnmsac_mask_vv_nxv8f32_nxv8f32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_mask_vv_nxv8f32_nxv8f32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfnmsac.mask.nxv8f32.nxv8f32(
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfnmsac.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfnmsac_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 16 x float> @llvm.epi.vfnmsac.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfnmsac.mask.nxv16f32.nxv16f32(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfnmsac_mask_vv_nxv16f32_nxv16f32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_mask_vv_nxv16f32_nxv16f32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfnmsac.mask.nxv16f32.nxv16f32(
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfnmsac_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfnmsac.mask.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfnmsac_mask_vv_nxv1f64_nxv1f64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_mask_vv_nxv1f64_nxv1f64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfnmsac.mask.nxv1f64.nxv1f64(
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfnmsac.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfnmsac_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 2 x double> @llvm.epi.vfnmsac.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfnmsac.mask.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfnmsac_mask_vv_nxv2f64_nxv2f64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_mask_vv_nxv2f64_nxv2f64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfnmsac.mask.nxv2f64.nxv2f64(
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfnmsac.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfnmsac_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 4 x double> @llvm.epi.vfnmsac.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfnmsac.mask.nxv4f64.nxv4f64(
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfnmsac_mask_vv_nxv4f64_nxv4f64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_mask_vv_nxv4f64_nxv4f64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfnmsac.mask.nxv4f64.nxv4f64(
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfnmsac.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfnmsac_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]]
  %a = call <vscale x 8 x double> @llvm.epi.vfnmsac.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfnmsac.mask.nxv8f64.nxv8f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfnmsac_mask_vv_nxv8f64_nxv8f64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_mask_vv_nxv8f64_nxv8f64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfnmsac.vv [[VR:v[0-9]+]], [[VR]], [[VR]], v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfnmsac.mask.nxv8f64.nxv8f64(
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfnmsac.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfnmsac_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfnmsac.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 2 x float> @llvm.epi.vfnmsac.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfnmsac.mask.nxv2f32.f32(
  <vscale x 2 x float>,
  float,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfnmsac_mask_vf_nxv2f32_nxv2f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_mask_vf_nxv2f32_nxv2f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfnmsac.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfnmsac.mask.nxv2f32.f32(
    <vscale x 2 x float> undef,
    float undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfnmsac.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfnmsac_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfnmsac.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 4 x float> @llvm.epi.vfnmsac.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfnmsac.mask.nxv4f32.f32(
  <vscale x 4 x float>,
  float,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfnmsac_mask_vf_nxv4f32_nxv4f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_mask_vf_nxv4f32_nxv4f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfnmsac.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfnmsac.mask.nxv4f32.f32(
    <vscale x 4 x float> undef,
    float undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfnmsac.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfnmsac_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfnmsac.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 8 x float> @llvm.epi.vfnmsac.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfnmsac.mask.nxv8f32.f32(
  <vscale x 8 x float>,
  float,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfnmsac_mask_vf_nxv8f32_nxv8f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_mask_vf_nxv8f32_nxv8f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfnmsac.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfnmsac.mask.nxv8f32.f32(
    <vscale x 8 x float> undef,
    float undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfnmsac.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfnmsac_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfnmsac.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 16 x float> @llvm.epi.vfnmsac.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfnmsac.mask.nxv16f32.f32(
  <vscale x 16 x float>,
  float,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfnmsac_mask_vf_nxv16f32_nxv16f32_f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_mask_vf_nxv16f32_nxv16f32_f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfnmsac.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfnmsac.mask.nxv16f32.f32(
    <vscale x 16 x float> undef,
    float undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfnmsac_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfnmsac.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 1 x double> @llvm.epi.vfnmsac.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfnmsac.mask.nxv1f64.f64(
  <vscale x 1 x double>,
  double,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfnmsac_mask_vf_nxv1f64_nxv1f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_mask_vf_nxv1f64_nxv1f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfnmsac.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfnmsac.mask.nxv1f64.f64(
    <vscale x 1 x double> undef,
    double undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfnmsac.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfnmsac_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfnmsac.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 2 x double> @llvm.epi.vfnmsac.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfnmsac.mask.nxv2f64.f64(
  <vscale x 2 x double>,
  double,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfnmsac_mask_vf_nxv2f64_nxv2f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_mask_vf_nxv2f64_nxv2f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfnmsac.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfnmsac.mask.nxv2f64.f64(
    <vscale x 2 x double> undef,
    double undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfnmsac.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfnmsac_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfnmsac.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 4 x double> @llvm.epi.vfnmsac.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfnmsac.mask.nxv4f64.f64(
  <vscale x 4 x double>,
  double,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfnmsac_mask_vf_nxv4f64_nxv4f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_mask_vf_nxv4f64_nxv4f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfnmsac.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfnmsac.mask.nxv4f64.f64(
    <vscale x 4 x double> undef,
    double undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfnmsac.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfnmsac_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfnmsac.vf [[VR:v[0-9]+]], ft0, [[VR]]
  %a = call <vscale x 8 x double> @llvm.epi.vfnmsac.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfnmsac.mask.nxv8f64.f64(
  <vscale x 8 x double>,
  double,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfnmsac_mask_vf_nxv8f64_nxv8f64_f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfnmsac_mask_vf_nxv8f64_nxv8f64_f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfnmsac.vf [[VR:v[0-9]+]], ft0, [[VR]], v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfnmsac.mask.nxv8f64.f64(
    <vscale x 8 x double> undef,
    double undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vfcvt.xu.f.nxv2i32.nxv2f32(
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfcvt_xu.f_nxv2i32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_xu.f_nxv2i32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfcvt.xu.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vfcvt.xu.f.nxv2i32.nxv2f32(
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vfcvt.xu.f.mask.nxv2i32.nxv2f32(
  <vscale x 2 x i32>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_xu.f_nxv2i32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_xu.f_nxv2i32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfcvt.xu.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vfcvt.xu.f.mask.nxv2i32.nxv2f32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vfcvt.xu.f.nxv4i32.nxv4f32(
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfcvt_xu.f_nxv4i32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_xu.f_nxv4i32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfcvt.xu.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vfcvt.xu.f.nxv4i32.nxv4f32(
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vfcvt.xu.f.mask.nxv4i32.nxv4f32(
  <vscale x 4 x i32>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_xu.f_nxv4i32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_xu.f_nxv4i32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfcvt.xu.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vfcvt.xu.f.mask.nxv4i32.nxv4f32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vfcvt.xu.f.nxv8i32.nxv8f32(
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfcvt_xu.f_nxv8i32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_xu.f_nxv8i32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfcvt.xu.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vfcvt.xu.f.nxv8i32.nxv8f32(
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vfcvt.xu.f.mask.nxv8i32.nxv8f32(
  <vscale x 8 x i32>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_xu.f_nxv8i32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_xu.f_nxv8i32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfcvt.xu.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vfcvt.xu.f.mask.nxv8i32.nxv8f32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vfcvt.xu.f.nxv16i32.nxv16f32(
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfcvt_xu.f_nxv16i32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_xu.f_nxv16i32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfcvt.xu.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vfcvt.xu.f.nxv16i32.nxv16f32(
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vfcvt.xu.f.mask.nxv16i32.nxv16f32(
  <vscale x 16 x i32>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_xu.f_nxv16i32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_xu.f_nxv16i32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfcvt.xu.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vfcvt.xu.f.mask.nxv16i32.nxv16f32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vfcvt.xu.f.nxv1i64.nxv1f64(
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfcvt_xu.f_nxv1i64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_xu.f_nxv1i64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfcvt.xu.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vfcvt.xu.f.nxv1i64.nxv1f64(
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vfcvt.xu.f.mask.nxv1i64.nxv1f64(
  <vscale x 1 x i64>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_xu.f_nxv1i64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_xu.f_nxv1i64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfcvt.xu.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vfcvt.xu.f.mask.nxv1i64.nxv1f64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vfcvt.xu.f.nxv2i64.nxv2f64(
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfcvt_xu.f_nxv2i64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_xu.f_nxv2i64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfcvt.xu.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vfcvt.xu.f.nxv2i64.nxv2f64(
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vfcvt.xu.f.mask.nxv2i64.nxv2f64(
  <vscale x 2 x i64>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_xu.f_nxv2i64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_xu.f_nxv2i64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfcvt.xu.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vfcvt.xu.f.mask.nxv2i64.nxv2f64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vfcvt.xu.f.nxv4i64.nxv4f64(
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfcvt_xu.f_nxv4i64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_xu.f_nxv4i64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfcvt.xu.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vfcvt.xu.f.nxv4i64.nxv4f64(
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vfcvt.xu.f.mask.nxv4i64.nxv4f64(
  <vscale x 4 x i64>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_xu.f_nxv4i64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_xu.f_nxv4i64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfcvt.xu.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vfcvt.xu.f.mask.nxv4i64.nxv4f64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vfcvt.xu.f.nxv8i64.nxv8f64(
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfcvt_xu.f_nxv8i64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_xu.f_nxv8i64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfcvt.xu.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vfcvt.xu.f.nxv8i64.nxv8f64(
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vfcvt.xu.f.mask.nxv8i64.nxv8f64(
  <vscale x 8 x i64>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_xu.f_nxv8i64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_xu.f_nxv8i64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfcvt.xu.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vfcvt.xu.f.mask.nxv8i64.nxv8f64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vfcvt.x.f.nxv2i32.nxv2f32(
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfcvt_x.f_nxv2i32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_x.f_nxv2i32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfcvt.x.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vfcvt.x.f.nxv2i32.nxv2f32(
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vfcvt.x.f.mask.nxv2i32.nxv2f32(
  <vscale x 2 x i32>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_x.f_nxv2i32_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_x.f_nxv2i32_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfcvt.x.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vfcvt.x.f.mask.nxv2i32.nxv2f32(
    <vscale x 2 x i32> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vfcvt.x.f.nxv4i32.nxv4f32(
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfcvt_x.f_nxv4i32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_x.f_nxv4i32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfcvt.x.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vfcvt.x.f.nxv4i32.nxv4f32(
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vfcvt.x.f.mask.nxv4i32.nxv4f32(
  <vscale x 4 x i32>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_x.f_nxv4i32_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_x.f_nxv4i32_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfcvt.x.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vfcvt.x.f.mask.nxv4i32.nxv4f32(
    <vscale x 4 x i32> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vfcvt.x.f.nxv8i32.nxv8f32(
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfcvt_x.f_nxv8i32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_x.f_nxv8i32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfcvt.x.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vfcvt.x.f.nxv8i32.nxv8f32(
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vfcvt.x.f.mask.nxv8i32.nxv8f32(
  <vscale x 8 x i32>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_x.f_nxv8i32_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_x.f_nxv8i32_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfcvt.x.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vfcvt.x.f.mask.nxv8i32.nxv8f32(
    <vscale x 8 x i32> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 16 x i32> @llvm.epi.vfcvt.x.f.nxv16i32.nxv16f32(
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfcvt_x.f_nxv16i32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_x.f_nxv16i32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfcvt.x.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i32> @llvm.epi.vfcvt.x.f.nxv16i32.nxv16f32(
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}

declare <vscale x 16 x i32> @llvm.epi.vfcvt.x.f.mask.nxv16i32.nxv16f32(
  <vscale x 16 x i32>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_x.f_nxv16i32_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_x.f_nxv16i32_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfcvt.x.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i32> @llvm.epi.vfcvt.x.f.mask.nxv16i32.nxv16f32(
    <vscale x 16 x i32> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i32>*
  store <vscale x 16 x i32> %a, <vscale x 16 x i32>* %p

  ret void
}


declare <vscale x 1 x i64> @llvm.epi.vfcvt.x.f.nxv1i64.nxv1f64(
  <vscale x 1 x double>,
  i64);

define void @intrinsic_vfcvt_x.f_nxv1i64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_x.f_nxv1i64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfcvt.x.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x i64> @llvm.epi.vfcvt.x.f.nxv1i64.nxv1f64(
    <vscale x 1 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}

declare <vscale x 1 x i64> @llvm.epi.vfcvt.x.f.mask.nxv1i64.nxv1f64(
  <vscale x 1 x i64>,
  <vscale x 1 x double>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_x.f_nxv1i64_nxv1f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_x.f_nxv1i64_nxv1f64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfcvt.x.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x i64> @llvm.epi.vfcvt.x.f.mask.nxv1i64.nxv1f64(
    <vscale x 1 x i64> undef,
    <vscale x 1 x double> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %a, <vscale x 1 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vfcvt.x.f.nxv2i64.nxv2f64(
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfcvt_x.f_nxv2i64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_x.f_nxv2i64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfcvt.x.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vfcvt.x.f.nxv2i64.nxv2f64(
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vfcvt.x.f.mask.nxv2i64.nxv2f64(
  <vscale x 2 x i64>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_x.f_nxv2i64_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_x.f_nxv2i64_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfcvt.x.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vfcvt.x.f.mask.nxv2i64.nxv2f64(
    <vscale x 2 x i64> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vfcvt.x.f.nxv4i64.nxv4f64(
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfcvt_x.f_nxv4i64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_x.f_nxv4i64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfcvt.x.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vfcvt.x.f.nxv4i64.nxv4f64(
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vfcvt.x.f.mask.nxv4i64.nxv4f64(
  <vscale x 4 x i64>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_x.f_nxv4i64_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_x.f_nxv4i64_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfcvt.x.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vfcvt.x.f.mask.nxv4i64.nxv4f64(
    <vscale x 4 x i64> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vfcvt.x.f.nxv8i64.nxv8f64(
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfcvt_x.f_nxv8i64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_x.f_nxv8i64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfcvt.x.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vfcvt.x.f.nxv8i64.nxv8f64(
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vfcvt.x.f.mask.nxv8i64.nxv8f64(
  <vscale x 8 x i64>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_x.f_nxv8i64_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_x.f_nxv8i64_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfcvt.x.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vfcvt.x.f.mask.nxv8i64.nxv8f64(
    <vscale x 8 x i64> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfcvt.f.xu.nxv2f32.nxv2i32(
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vfcvt_f.xu_nxv2f32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_f.xu_nxv2f32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x float> @llvm.epi.vfcvt.f.xu.nxv2f32.nxv2i32(
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfcvt.f.xu.mask.nxv2f32.nxv2i32(
  <vscale x 2 x float>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_f.xu_nxv2f32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_f.xu_nxv2f32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfcvt.f.xu.mask.nxv2f32.nxv2i32(
    <vscale x 2 x float> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfcvt.f.xu.nxv4f32.nxv4i32(
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vfcvt_f.xu_nxv4f32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_f.xu_nxv4f32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vfcvt.f.xu.nxv4f32.nxv4i32(
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfcvt.f.xu.mask.nxv4f32.nxv4i32(
  <vscale x 4 x float>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_f.xu_nxv4f32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_f.xu_nxv4f32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfcvt.f.xu.mask.nxv4f32.nxv4i32(
    <vscale x 4 x float> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfcvt.f.xu.nxv8f32.nxv8i32(
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vfcvt_f.xu_nxv8f32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_f.xu_nxv8f32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vfcvt.f.xu.nxv8f32.nxv8i32(
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfcvt.f.xu.mask.nxv8f32.nxv8i32(
  <vscale x 8 x float>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_f.xu_nxv8f32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_f.xu_nxv8f32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfcvt.f.xu.mask.nxv8f32.nxv8i32(
    <vscale x 8 x float> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfcvt.f.xu.nxv16f32.nxv16i32(
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vfcvt_f.xu_nxv16f32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_f.xu_nxv16f32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x float> @llvm.epi.vfcvt.f.xu.nxv16f32.nxv16i32(
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfcvt.f.xu.mask.nxv16f32.nxv16i32(
  <vscale x 16 x float>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_f.xu_nxv16f32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_f.xu_nxv16f32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfcvt.f.xu.mask.nxv16f32.nxv16i32(
    <vscale x 16 x float> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfcvt.f.xu.nxv1f64.nxv1i64(
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vfcvt_f.xu_nxv1f64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_f.xu_nxv1f64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x double> @llvm.epi.vfcvt.f.xu.nxv1f64.nxv1i64(
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfcvt.f.xu.mask.nxv1f64.nxv1i64(
  <vscale x 1 x double>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_f.xu_nxv1f64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_f.xu_nxv1f64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfcvt.f.xu.mask.nxv1f64.nxv1i64(
    <vscale x 1 x double> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfcvt.f.xu.nxv2f64.nxv2i64(
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vfcvt_f.xu_nxv2f64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_f.xu_nxv2f64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfcvt.f.xu.nxv2f64.nxv2i64(
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfcvt.f.xu.mask.nxv2f64.nxv2i64(
  <vscale x 2 x double>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_f.xu_nxv2f64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_f.xu_nxv2f64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfcvt.f.xu.mask.nxv2f64.nxv2i64(
    <vscale x 2 x double> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfcvt.f.xu.nxv4f64.nxv4i64(
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vfcvt_f.xu_nxv4f64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_f.xu_nxv4f64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfcvt.f.xu.nxv4f64.nxv4i64(
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfcvt.f.xu.mask.nxv4f64.nxv4i64(
  <vscale x 4 x double>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_f.xu_nxv4f64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_f.xu_nxv4f64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfcvt.f.xu.mask.nxv4f64.nxv4i64(
    <vscale x 4 x double> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfcvt.f.xu.nxv8f64.nxv8i64(
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vfcvt_f.xu_nxv8f64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_f.xu_nxv8f64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfcvt.f.xu.nxv8f64.nxv8i64(
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfcvt.f.xu.mask.nxv8f64.nxv8i64(
  <vscale x 8 x double>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_f.xu_nxv8f64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_f.xu_nxv8f64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfcvt.f.xu.mask.nxv8f64.nxv8i64(
    <vscale x 8 x double> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfcvt.f.x.nxv2f32.nxv2i32(
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vfcvt_f.x_nxv2f32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_f.x_nxv2f32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x float> @llvm.epi.vfcvt.f.x.nxv2f32.nxv2i32(
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfcvt.f.x.mask.nxv2f32.nxv2i32(
  <vscale x 2 x float>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_f.x_nxv2f32_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_f.x_nxv2f32_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfcvt.f.x.mask.nxv2f32.nxv2i32(
    <vscale x 2 x float> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfcvt.f.x.nxv4f32.nxv4i32(
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vfcvt_f.x_nxv4f32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_f.x_nxv4f32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vfcvt.f.x.nxv4f32.nxv4i32(
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfcvt.f.x.mask.nxv4f32.nxv4i32(
  <vscale x 4 x float>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_f.x_nxv4f32_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_f.x_nxv4f32_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfcvt.f.x.mask.nxv4f32.nxv4i32(
    <vscale x 4 x float> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfcvt.f.x.nxv8f32.nxv8i32(
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vfcvt_f.x_nxv8f32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_f.x_nxv8f32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vfcvt.f.x.nxv8f32.nxv8i32(
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfcvt.f.x.mask.nxv8f32.nxv8i32(
  <vscale x 8 x float>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_f.x_nxv8f32_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_f.x_nxv8f32_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfcvt.f.x.mask.nxv8f32.nxv8i32(
    <vscale x 8 x float> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfcvt.f.x.nxv16f32.nxv16i32(
  <vscale x 16 x i32>,
  i64);

define void @intrinsic_vfcvt_f.x_nxv16f32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_f.x_nxv16f32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x float> @llvm.epi.vfcvt.f.x.nxv16f32.nxv16i32(
    <vscale x 16 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfcvt.f.x.mask.nxv16f32.nxv16i32(
  <vscale x 16 x float>,
  <vscale x 16 x i32>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_f.x_nxv16f32_nxv16i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_f.x_nxv16f32_nxv16i32
; CHECK:       vsetvli {{.*}}, a0, e32, m8, ta, mu
; CHECK:       vfcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfcvt.f.x.mask.nxv16f32.nxv16i32(
    <vscale x 16 x float> undef,
    <vscale x 16 x i32> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 1 x double> @llvm.epi.vfcvt.f.x.nxv1f64.nxv1i64(
  <vscale x 1 x i64>,
  i64);

define void @intrinsic_vfcvt_f.x_nxv1f64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_f.x_nxv1f64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 1 x double> @llvm.epi.vfcvt.f.x.nxv1f64.nxv1i64(
    <vscale x 1 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}

declare <vscale x 1 x double> @llvm.epi.vfcvt.f.x.mask.nxv1f64.nxv1i64(
  <vscale x 1 x double>,
  <vscale x 1 x i64>,
  <vscale x 1 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_f.x_nxv1f64_nxv1i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_f.x_nxv1f64_nxv1i64
; CHECK:       vsetvli {{.*}}, a0, e64, m1, ta, mu
; CHECK:       vfcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 1 x double> @llvm.epi.vfcvt.f.x.mask.nxv1f64.nxv1i64(
    <vscale x 1 x double> undef,
    <vscale x 1 x i64> undef,
    <vscale x 1 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 1 x double>*
  store <vscale x 1 x double> %a, <vscale x 1 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfcvt.f.x.nxv2f64.nxv2i64(
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vfcvt_f.x_nxv2f64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_f.x_nxv2f64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfcvt.f.x.nxv2f64.nxv2i64(
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfcvt.f.x.mask.nxv2f64.nxv2i64(
  <vscale x 2 x double>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_f.x_nxv2f64_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_f.x_nxv2f64_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e64, m2, ta, mu
; CHECK:       vfcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfcvt.f.x.mask.nxv2f64.nxv2i64(
    <vscale x 2 x double> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfcvt.f.x.nxv4f64.nxv4i64(
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vfcvt_f.x_nxv4f64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_f.x_nxv4f64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfcvt.f.x.nxv4f64.nxv4i64(
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfcvt.f.x.mask.nxv4f64.nxv4i64(
  <vscale x 4 x double>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_f.x_nxv4f64_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_f.x_nxv4f64_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e64, m4, ta, mu
; CHECK:       vfcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfcvt.f.x.mask.nxv4f64.nxv4i64(
    <vscale x 4 x double> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfcvt.f.x.nxv8f64.nxv8i64(
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vfcvt_f.x_nxv8f64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_f.x_nxv8f64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfcvt.f.x.nxv8f64.nxv8i64(
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfcvt.f.x.mask.nxv8f64.nxv8i64(
  <vscale x 8 x double>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfcvt_mask_f.x_nxv8f64_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfcvt_mask_f.x_nxv8f64_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e64, m8, ta, mu
; CHECK:       vfcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfcvt.f.x.mask.nxv8f64.nxv8i64(
    <vscale x 8 x double> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vfwcvt.xu.f.nxv2i64.nxv2f32(
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfwcvt_xu.f_nxv2i64_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_xu.f_nxv2i64_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwcvt.xu.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vfwcvt.xu.f.nxv2i64.nxv2f32(
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vfwcvt.xu.f.mask.nxv2i64.nxv2f32(
  <vscale x 2 x i64>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfwcvt_mask_xu.f_nxv2i64_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_mask_xu.f_nxv2i64_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwcvt.xu.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vfwcvt.xu.f.mask.nxv2i64.nxv2f32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vfwcvt.xu.f.nxv4i64.nxv4f32(
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfwcvt_xu.f_nxv4i64_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_xu.f_nxv4i64_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwcvt.xu.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vfwcvt.xu.f.nxv4i64.nxv4f32(
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vfwcvt.xu.f.mask.nxv4i64.nxv4f32(
  <vscale x 4 x i64>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfwcvt_mask_xu.f_nxv4i64_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_mask_xu.f_nxv4i64_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwcvt.xu.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vfwcvt.xu.f.mask.nxv4i64.nxv4f32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vfwcvt.xu.f.nxv8i64.nxv8f32(
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfwcvt_xu.f_nxv8i64_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_xu.f_nxv8i64_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwcvt.xu.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vfwcvt.xu.f.nxv8i64.nxv8f32(
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vfwcvt.xu.f.mask.nxv8i64.nxv8f32(
  <vscale x 8 x i64>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfwcvt_mask_xu.f_nxv8i64_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_mask_xu.f_nxv8i64_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwcvt.xu.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vfwcvt.xu.f.mask.nxv8i64.nxv8f32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 2 x i64> @llvm.epi.vfwcvt.x.f.nxv2i64.nxv2f32(
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfwcvt_x.f_nxv2i64_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_x.f_nxv2i64_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwcvt.x.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i64> @llvm.epi.vfwcvt.x.f.nxv2i64.nxv2f32(
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}

declare <vscale x 2 x i64> @llvm.epi.vfwcvt.x.f.mask.nxv2i64.nxv2f32(
  <vscale x 2 x i64>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfwcvt_mask_x.f_nxv2i64_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_mask_x.f_nxv2i64_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwcvt.x.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i64> @llvm.epi.vfwcvt.x.f.mask.nxv2i64.nxv2f32(
    <vscale x 2 x i64> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i64>*
  store <vscale x 2 x i64> %a, <vscale x 2 x i64>* %p

  ret void
}


declare <vscale x 4 x i64> @llvm.epi.vfwcvt.x.f.nxv4i64.nxv4f32(
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfwcvt_x.f_nxv4i64_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_x.f_nxv4i64_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwcvt.x.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i64> @llvm.epi.vfwcvt.x.f.nxv4i64.nxv4f32(
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}

declare <vscale x 4 x i64> @llvm.epi.vfwcvt.x.f.mask.nxv4i64.nxv4f32(
  <vscale x 4 x i64>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfwcvt_mask_x.f_nxv4i64_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_mask_x.f_nxv4i64_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwcvt.x.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i64> @llvm.epi.vfwcvt.x.f.mask.nxv4i64.nxv4f32(
    <vscale x 4 x i64> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i64>*
  store <vscale x 4 x i64> %a, <vscale x 4 x i64>* %p

  ret void
}


declare <vscale x 8 x i64> @llvm.epi.vfwcvt.x.f.nxv8i64.nxv8f32(
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfwcvt_x.f_nxv8i64_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_x.f_nxv8i64_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwcvt.x.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i64> @llvm.epi.vfwcvt.x.f.nxv8i64.nxv8f32(
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}

declare <vscale x 8 x i64> @llvm.epi.vfwcvt.x.f.mask.nxv8i64.nxv8f32(
  <vscale x 8 x i64>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfwcvt_mask_x.f_nxv8i64_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_mask_x.f_nxv8i64_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwcvt.x.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i64> @llvm.epi.vfwcvt.x.f.mask.nxv8i64.nxv8f32(
    <vscale x 8 x i64> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i64>*
  store <vscale x 8 x i64> %a, <vscale x 8 x i64>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfwcvt.f.xu.nxv4f32.nxv4i16(
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vfwcvt_f.xu_nxv4f32_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_f.xu_nxv4f32_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vfwcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vfwcvt.f.xu.nxv4f32.nxv4i16(
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfwcvt.f.xu.mask.nxv4f32.nxv4i16(
  <vscale x 4 x float>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfwcvt_mask_f.xu_nxv4f32_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_mask_f.xu_nxv4f32_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vfwcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfwcvt.f.xu.mask.nxv4f32.nxv4i16(
    <vscale x 4 x float> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfwcvt.f.xu.nxv8f32.nxv8i16(
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vfwcvt_f.xu_nxv8f32_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_f.xu_nxv8f32_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vfwcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vfwcvt.f.xu.nxv8f32.nxv8i16(
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfwcvt.f.xu.mask.nxv8f32.nxv8i16(
  <vscale x 8 x float>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfwcvt_mask_f.xu_nxv8f32_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_mask_f.xu_nxv8f32_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vfwcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfwcvt.f.xu.mask.nxv8f32.nxv8i16(
    <vscale x 8 x float> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfwcvt.f.xu.nxv16f32.nxv16i16(
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vfwcvt_f.xu_nxv16f32_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_f.xu_nxv16f32_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vfwcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x float> @llvm.epi.vfwcvt.f.xu.nxv16f32.nxv16i16(
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfwcvt.f.xu.mask.nxv16f32.nxv16i16(
  <vscale x 16 x float>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfwcvt_mask_f.xu_nxv16f32_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_mask_f.xu_nxv16f32_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vfwcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfwcvt.f.xu.mask.nxv16f32.nxv16i16(
    <vscale x 16 x float> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfwcvt.f.xu.nxv2f64.nxv2i32(
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vfwcvt_f.xu_nxv2f64_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_f.xu_nxv2f64_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfwcvt.f.xu.nxv2f64.nxv2i32(
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfwcvt.f.xu.mask.nxv2f64.nxv2i32(
  <vscale x 2 x double>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfwcvt_mask_f.xu_nxv2f64_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_mask_f.xu_nxv2f64_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfwcvt.f.xu.mask.nxv2f64.nxv2i32(
    <vscale x 2 x double> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfwcvt.f.xu.nxv4f64.nxv4i32(
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vfwcvt_f.xu_nxv4f64_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_f.xu_nxv4f64_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfwcvt.f.xu.nxv4f64.nxv4i32(
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfwcvt.f.xu.mask.nxv4f64.nxv4i32(
  <vscale x 4 x double>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfwcvt_mask_f.xu_nxv4f64_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_mask_f.xu_nxv4f64_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfwcvt.f.xu.mask.nxv4f64.nxv4i32(
    <vscale x 4 x double> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfwcvt.f.xu.nxv8f64.nxv8i32(
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vfwcvt_f.xu_nxv8f64_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_f.xu_nxv8f64_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfwcvt.f.xu.nxv8f64.nxv8i32(
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfwcvt.f.xu.mask.nxv8f64.nxv8i32(
  <vscale x 8 x double>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfwcvt_mask_f.xu_nxv8f64_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_mask_f.xu_nxv8f64_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwcvt.f.xu.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfwcvt.f.xu.mask.nxv8f64.nxv8i32(
    <vscale x 8 x double> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfwcvt.f.x.nxv4f32.nxv4i16(
  <vscale x 4 x i16>,
  i64);

define void @intrinsic_vfwcvt_f.x_nxv4f32_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_f.x_nxv4f32_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vfwcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vfwcvt.f.x.nxv4f32.nxv4i16(
    <vscale x 4 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfwcvt.f.x.mask.nxv4f32.nxv4i16(
  <vscale x 4 x float>,
  <vscale x 4 x i16>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfwcvt_mask_f.x_nxv4f32_nxv4i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_mask_f.x_nxv4f32_nxv4i16
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vfwcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfwcvt.f.x.mask.nxv4f32.nxv4i16(
    <vscale x 4 x float> undef,
    <vscale x 4 x i16> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfwcvt.f.x.nxv8f32.nxv8i16(
  <vscale x 8 x i16>,
  i64);

define void @intrinsic_vfwcvt_f.x_nxv8f32_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_f.x_nxv8f32_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vfwcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vfwcvt.f.x.nxv8f32.nxv8i16(
    <vscale x 8 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfwcvt.f.x.mask.nxv8f32.nxv8i16(
  <vscale x 8 x float>,
  <vscale x 8 x i16>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfwcvt_mask_f.x_nxv8f32_nxv8i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_mask_f.x_nxv8f32_nxv8i16
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vfwcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfwcvt.f.x.mask.nxv8f32.nxv8i16(
    <vscale x 8 x float> undef,
    <vscale x 8 x i16> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 16 x float> @llvm.epi.vfwcvt.f.x.nxv16f32.nxv16i16(
  <vscale x 16 x i16>,
  i64);

define void @intrinsic_vfwcvt_f.x_nxv16f32_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_f.x_nxv16f32_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vfwcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x float> @llvm.epi.vfwcvt.f.x.nxv16f32.nxv16i16(
    <vscale x 16 x i16> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}

declare <vscale x 16 x float> @llvm.epi.vfwcvt.f.x.mask.nxv16f32.nxv16i16(
  <vscale x 16 x float>,
  <vscale x 16 x i16>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfwcvt_mask_f.x_nxv16f32_nxv16i16() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_mask_f.x_nxv16f32_nxv16i16
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vfwcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x float> @llvm.epi.vfwcvt.f.x.mask.nxv16f32.nxv16i16(
    <vscale x 16 x float> undef,
    <vscale x 16 x i16> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x float>*
  store <vscale x 16 x float> %a, <vscale x 16 x float>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfwcvt.f.x.nxv2f64.nxv2i32(
  <vscale x 2 x i32>,
  i64);

define void @intrinsic_vfwcvt_f.x_nxv2f64_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_f.x_nxv2f64_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfwcvt.f.x.nxv2f64.nxv2i32(
    <vscale x 2 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfwcvt.f.x.mask.nxv2f64.nxv2i32(
  <vscale x 2 x double>,
  <vscale x 2 x i32>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfwcvt_mask_f.x_nxv2f64_nxv2i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_mask_f.x_nxv2f64_nxv2i32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfwcvt.f.x.mask.nxv2f64.nxv2i32(
    <vscale x 2 x double> undef,
    <vscale x 2 x i32> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfwcvt.f.x.nxv4f64.nxv4i32(
  <vscale x 4 x i32>,
  i64);

define void @intrinsic_vfwcvt_f.x_nxv4f64_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_f.x_nxv4f64_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfwcvt.f.x.nxv4f64.nxv4i32(
    <vscale x 4 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfwcvt.f.x.mask.nxv4f64.nxv4i32(
  <vscale x 4 x double>,
  <vscale x 4 x i32>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfwcvt_mask_f.x_nxv4f64_nxv4i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_mask_f.x_nxv4f64_nxv4i32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfwcvt.f.x.mask.nxv4f64.nxv4i32(
    <vscale x 4 x double> undef,
    <vscale x 4 x i32> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfwcvt.f.x.nxv8f64.nxv8i32(
  <vscale x 8 x i32>,
  i64);

define void @intrinsic_vfwcvt_f.x_nxv8f64_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_f.x_nxv8f64_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfwcvt.f.x.nxv8f64.nxv8i32(
    <vscale x 8 x i32> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfwcvt.f.x.mask.nxv8f64.nxv8i32(
  <vscale x 8 x double>,
  <vscale x 8 x i32>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfwcvt_mask_f.x_nxv8f64_nxv8i32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_mask_f.x_nxv8f64_nxv8i32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwcvt.f.x.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfwcvt.f.x.mask.nxv8f64.nxv8i32(
    <vscale x 8 x double> undef,
    <vscale x 8 x i32> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x double> @llvm.epi.vfwcvt.f.f.nxv2f64.nxv2f32(
  <vscale x 2 x float>,
  i64);

define void @intrinsic_vfwcvt_f.f_nxv2f64_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_f.f_nxv2f64_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwcvt.f.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x double> @llvm.epi.vfwcvt.f.f.nxv2f64.nxv2f32(
    <vscale x 2 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}

declare <vscale x 2 x double> @llvm.epi.vfwcvt.f.f.mask.nxv2f64.nxv2f32(
  <vscale x 2 x double>,
  <vscale x 2 x float>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfwcvt_mask_f.f_nxv2f64_nxv2f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_mask_f.f_nxv2f64_nxv2f32
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfwcvt.f.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x double> @llvm.epi.vfwcvt.f.f.mask.nxv2f64.nxv2f32(
    <vscale x 2 x double> undef,
    <vscale x 2 x float> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x double>*
  store <vscale x 2 x double> %a, <vscale x 2 x double>* %p

  ret void
}


declare <vscale x 4 x double> @llvm.epi.vfwcvt.f.f.nxv4f64.nxv4f32(
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfwcvt_f.f_nxv4f64_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_f.f_nxv4f64_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwcvt.f.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x double> @llvm.epi.vfwcvt.f.f.nxv4f64.nxv4f32(
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}

declare <vscale x 4 x double> @llvm.epi.vfwcvt.f.f.mask.nxv4f64.nxv4f32(
  <vscale x 4 x double>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfwcvt_mask_f.f_nxv4f64_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_mask_f.f_nxv4f64_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfwcvt.f.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x double> @llvm.epi.vfwcvt.f.f.mask.nxv4f64.nxv4f32(
    <vscale x 4 x double> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x double>*
  store <vscale x 4 x double> %a, <vscale x 4 x double>* %p

  ret void
}


declare <vscale x 8 x double> @llvm.epi.vfwcvt.f.f.nxv8f64.nxv8f32(
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfwcvt_f.f_nxv8f64_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_f.f_nxv8f64_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwcvt.f.f.v {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x double> @llvm.epi.vfwcvt.f.f.nxv8f64.nxv8f32(
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}

declare <vscale x 8 x double> @llvm.epi.vfwcvt.f.f.mask.nxv8f64.nxv8f32(
  <vscale x 8 x double>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfwcvt_mask_f.f_nxv8f64_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfwcvt_mask_f.f_nxv8f64_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfwcvt.f.f.v {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x double> @llvm.epi.vfwcvt.f.f.mask.nxv8f64.nxv8f32(
    <vscale x 8 x double> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x double>*
  store <vscale x 8 x double> %a, <vscale x 8 x double>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vfncvt.xu.f.nxv2i32.nxv2f64(
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfncvt_xu.f_nxv2i32_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_xu.f_nxv2i32_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfncvt.xu.f.w {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vfncvt.xu.f.nxv2i32.nxv2f64(
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vfncvt.xu.f.mask.nxv2i32.nxv2f64(
  <vscale x 2 x i32>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfncvt_mask_xu.f_nxv2i32_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_mask_xu.f_nxv2i32_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfncvt.xu.f.w {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vfncvt.xu.f.mask.nxv2i32.nxv2f64(
    <vscale x 2 x i32> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vfncvt.xu.f.nxv4i32.nxv4f64(
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfncvt_xu.f_nxv4i32_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_xu.f_nxv4i32_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfncvt.xu.f.w {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vfncvt.xu.f.nxv4i32.nxv4f64(
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vfncvt.xu.f.mask.nxv4i32.nxv4f64(
  <vscale x 4 x i32>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfncvt_mask_xu.f_nxv4i32_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_mask_xu.f_nxv4i32_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfncvt.xu.f.w {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vfncvt.xu.f.mask.nxv4i32.nxv4f64(
    <vscale x 4 x i32> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vfncvt.xu.f.nxv8i32.nxv8f64(
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfncvt_xu.f_nxv8i32_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_xu.f_nxv8i32_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfncvt.xu.f.w {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vfncvt.xu.f.nxv8i32.nxv8f64(
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vfncvt.xu.f.mask.nxv8i32.nxv8f64(
  <vscale x 8 x i32>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfncvt_mask_xu.f_nxv8i32_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_mask_xu.f_nxv8i32_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfncvt.xu.f.w {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vfncvt.xu.f.mask.nxv8i32.nxv8f64(
    <vscale x 8 x i32> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vfncvt.xu.f.nxv4i16.nxv4f32(
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfncvt_xu.f_nxv4i16_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_xu.f_nxv4i16_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vfncvt.xu.f.w {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vfncvt.xu.f.nxv4i16.nxv4f32(
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vfncvt.xu.f.mask.nxv4i16.nxv4f32(
  <vscale x 4 x i16>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfncvt_mask_xu.f_nxv4i16_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_mask_xu.f_nxv4i16_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vfncvt.xu.f.w {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vfncvt.xu.f.mask.nxv4i16.nxv4f32(
    <vscale x 4 x i16> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vfncvt.xu.f.nxv8i16.nxv8f32(
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfncvt_xu.f_nxv8i16_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_xu.f_nxv8i16_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vfncvt.xu.f.w {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vfncvt.xu.f.nxv8i16.nxv8f32(
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vfncvt.xu.f.mask.nxv8i16.nxv8f32(
  <vscale x 8 x i16>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfncvt_mask_xu.f_nxv8i16_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_mask_xu.f_nxv8i16_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vfncvt.xu.f.w {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vfncvt.xu.f.mask.nxv8i16.nxv8f32(
    <vscale x 8 x i16> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vfncvt.xu.f.nxv16i16.nxv16f32(
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfncvt_xu.f_nxv16i16_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_xu.f_nxv16i16_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vfncvt.xu.f.w {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vfncvt.xu.f.nxv16i16.nxv16f32(
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vfncvt.xu.f.mask.nxv16i16.nxv16f32(
  <vscale x 16 x i16>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfncvt_mask_xu.f_nxv16i16_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_mask_xu.f_nxv16i16_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vfncvt.xu.f.w {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vfncvt.xu.f.mask.nxv16i16.nxv16f32(
    <vscale x 16 x i16> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 2 x i32> @llvm.epi.vfncvt.x.f.nxv2i32.nxv2f64(
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfncvt_x.f_nxv2i32_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_x.f_nxv2i32_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfncvt.x.f.w {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x i32> @llvm.epi.vfncvt.x.f.nxv2i32.nxv2f64(
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}

declare <vscale x 2 x i32> @llvm.epi.vfncvt.x.f.mask.nxv2i32.nxv2f64(
  <vscale x 2 x i32>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfncvt_mask_x.f_nxv2i32_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_mask_x.f_nxv2i32_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfncvt.x.f.w {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x i32> @llvm.epi.vfncvt.x.f.mask.nxv2i32.nxv2f64(
    <vscale x 2 x i32> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %a, <vscale x 2 x i32>* %p

  ret void
}


declare <vscale x 4 x i32> @llvm.epi.vfncvt.x.f.nxv4i32.nxv4f64(
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfncvt_x.f_nxv4i32_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_x.f_nxv4i32_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfncvt.x.f.w {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i32> @llvm.epi.vfncvt.x.f.nxv4i32.nxv4f64(
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}

declare <vscale x 4 x i32> @llvm.epi.vfncvt.x.f.mask.nxv4i32.nxv4f64(
  <vscale x 4 x i32>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfncvt_mask_x.f_nxv4i32_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_mask_x.f_nxv4i32_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfncvt.x.f.w {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i32> @llvm.epi.vfncvt.x.f.mask.nxv4i32.nxv4f64(
    <vscale x 4 x i32> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i32>*
  store <vscale x 4 x i32> %a, <vscale x 4 x i32>* %p

  ret void
}


declare <vscale x 8 x i32> @llvm.epi.vfncvt.x.f.nxv8i32.nxv8f64(
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfncvt_x.f_nxv8i32_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_x.f_nxv8i32_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfncvt.x.f.w {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i32> @llvm.epi.vfncvt.x.f.nxv8i32.nxv8f64(
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}

declare <vscale x 8 x i32> @llvm.epi.vfncvt.x.f.mask.nxv8i32.nxv8f64(
  <vscale x 8 x i32>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfncvt_mask_x.f_nxv8i32_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_mask_x.f_nxv8i32_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfncvt.x.f.w {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i32> @llvm.epi.vfncvt.x.f.mask.nxv8i32.nxv8f64(
    <vscale x 8 x i32> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i32>*
  store <vscale x 8 x i32> %a, <vscale x 8 x i32>* %p

  ret void
}


declare <vscale x 4 x i16> @llvm.epi.vfncvt.x.f.nxv4i16.nxv4f32(
  <vscale x 4 x float>,
  i64);

define void @intrinsic_vfncvt_x.f_nxv4i16_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_x.f_nxv4i16_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vfncvt.x.f.w {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x i16> @llvm.epi.vfncvt.x.f.nxv4i16.nxv4f32(
    <vscale x 4 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}

declare <vscale x 4 x i16> @llvm.epi.vfncvt.x.f.mask.nxv4i16.nxv4f32(
  <vscale x 4 x i16>,
  <vscale x 4 x float>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfncvt_mask_x.f_nxv4i16_nxv4f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_mask_x.f_nxv4i16_nxv4f32
; CHECK:       vsetvli {{.*}}, a0, e16, m1, ta, mu
; CHECK:       vfncvt.x.f.w {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x i16> @llvm.epi.vfncvt.x.f.mask.nxv4i16.nxv4f32(
    <vscale x 4 x i16> undef,
    <vscale x 4 x float> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %a, <vscale x 4 x i16>* %p

  ret void
}


declare <vscale x 8 x i16> @llvm.epi.vfncvt.x.f.nxv8i16.nxv8f32(
  <vscale x 8 x float>,
  i64);

define void @intrinsic_vfncvt_x.f_nxv8i16_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_x.f_nxv8i16_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vfncvt.x.f.w {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x i16> @llvm.epi.vfncvt.x.f.nxv8i16.nxv8f32(
    <vscale x 8 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}

declare <vscale x 8 x i16> @llvm.epi.vfncvt.x.f.mask.nxv8i16.nxv8f32(
  <vscale x 8 x i16>,
  <vscale x 8 x float>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfncvt_mask_x.f_nxv8i16_nxv8f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_mask_x.f_nxv8i16_nxv8f32
; CHECK:       vsetvli {{.*}}, a0, e16, m2, ta, mu
; CHECK:       vfncvt.x.f.w {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x i16> @llvm.epi.vfncvt.x.f.mask.nxv8i16.nxv8f32(
    <vscale x 8 x i16> undef,
    <vscale x 8 x float> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x i16>*
  store <vscale x 8 x i16> %a, <vscale x 8 x i16>* %p

  ret void
}


declare <vscale x 16 x i16> @llvm.epi.vfncvt.x.f.nxv16i16.nxv16f32(
  <vscale x 16 x float>,
  i64);

define void @intrinsic_vfncvt_x.f_nxv16i16_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_x.f_nxv16i16_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vfncvt.x.f.w {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 16 x i16> @llvm.epi.vfncvt.x.f.nxv16i16.nxv16f32(
    <vscale x 16 x float> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}

declare <vscale x 16 x i16> @llvm.epi.vfncvt.x.f.mask.nxv16i16.nxv16f32(
  <vscale x 16 x i16>,
  <vscale x 16 x float>,
  <vscale x 16 x i1>,
  i64);

define void @intrinsic_vfncvt_mask_x.f_nxv16i16_nxv16f32() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_mask_x.f_nxv16i16_nxv16f32
; CHECK:       vsetvli {{.*}}, a0, e16, m4, ta, mu
; CHECK:       vfncvt.x.f.w {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 16 x i16> @llvm.epi.vfncvt.x.f.mask.nxv16i16.nxv16f32(
    <vscale x 16 x i16> undef,
    <vscale x 16 x float> undef,
    <vscale x 16 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 16 x i16>*
  store <vscale x 16 x i16> %a, <vscale x 16 x i16>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfncvt.f.xu.nxv2f32.nxv2i64(
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vfncvt_f.xu_nxv2f32_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_f.xu_nxv2f32_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfncvt.f.xu.w {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x float> @llvm.epi.vfncvt.f.xu.nxv2f32.nxv2i64(
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfncvt.f.xu.mask.nxv2f32.nxv2i64(
  <vscale x 2 x float>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfncvt_mask_f.xu_nxv2f32_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_mask_f.xu_nxv2f32_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfncvt.f.xu.w {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfncvt.f.xu.mask.nxv2f32.nxv2i64(
    <vscale x 2 x float> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfncvt.f.xu.nxv4f32.nxv4i64(
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vfncvt_f.xu_nxv4f32_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_f.xu_nxv4f32_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfncvt.f.xu.w {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vfncvt.f.xu.nxv4f32.nxv4i64(
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfncvt.f.xu.mask.nxv4f32.nxv4i64(
  <vscale x 4 x float>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfncvt_mask_f.xu_nxv4f32_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_mask_f.xu_nxv4f32_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfncvt.f.xu.w {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfncvt.f.xu.mask.nxv4f32.nxv4i64(
    <vscale x 4 x float> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfncvt.f.xu.nxv8f32.nxv8i64(
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vfncvt_f.xu_nxv8f32_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_f.xu_nxv8f32_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfncvt.f.xu.w {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vfncvt.f.xu.nxv8f32.nxv8i64(
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfncvt.f.xu.mask.nxv8f32.nxv8i64(
  <vscale x 8 x float>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfncvt_mask_f.xu_nxv8f32_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_mask_f.xu_nxv8f32_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfncvt.f.xu.w {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfncvt.f.xu.mask.nxv8f32.nxv8i64(
    <vscale x 8 x float> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfncvt.f.x.nxv2f32.nxv2i64(
  <vscale x 2 x i64>,
  i64);

define void @intrinsic_vfncvt_f.x_nxv2f32_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_f.x_nxv2f32_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfncvt.f.x.w {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x float> @llvm.epi.vfncvt.f.x.nxv2f32.nxv2i64(
    <vscale x 2 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfncvt.f.x.mask.nxv2f32.nxv2i64(
  <vscale x 2 x float>,
  <vscale x 2 x i64>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfncvt_mask_f.x_nxv2f32_nxv2i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_mask_f.x_nxv2f32_nxv2i64
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfncvt.f.x.w {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfncvt.f.x.mask.nxv2f32.nxv2i64(
    <vscale x 2 x float> undef,
    <vscale x 2 x i64> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfncvt.f.x.nxv4f32.nxv4i64(
  <vscale x 4 x i64>,
  i64);

define void @intrinsic_vfncvt_f.x_nxv4f32_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_f.x_nxv4f32_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfncvt.f.x.w {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vfncvt.f.x.nxv4f32.nxv4i64(
    <vscale x 4 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfncvt.f.x.mask.nxv4f32.nxv4i64(
  <vscale x 4 x float>,
  <vscale x 4 x i64>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfncvt_mask_f.x_nxv4f32_nxv4i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_mask_f.x_nxv4f32_nxv4i64
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfncvt.f.x.w {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfncvt.f.x.mask.nxv4f32.nxv4i64(
    <vscale x 4 x float> undef,
    <vscale x 4 x i64> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfncvt.f.x.nxv8f32.nxv8i64(
  <vscale x 8 x i64>,
  i64);

define void @intrinsic_vfncvt_f.x_nxv8f32_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_f.x_nxv8f32_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfncvt.f.x.w {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vfncvt.f.x.nxv8f32.nxv8i64(
    <vscale x 8 x i64> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfncvt.f.x.mask.nxv8f32.nxv8i64(
  <vscale x 8 x float>,
  <vscale x 8 x i64>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfncvt_mask_f.x_nxv8f32_nxv8i64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_mask_f.x_nxv8f32_nxv8i64
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfncvt.f.x.w {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfncvt.f.x.mask.nxv8f32.nxv8i64(
    <vscale x 8 x float> undef,
    <vscale x 8 x i64> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}


declare <vscale x 2 x float> @llvm.epi.vfncvt.f.f.nxv2f32.nxv2f64(
  <vscale x 2 x double>,
  i64);

define void @intrinsic_vfncvt_f.f_nxv2f32_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_f.f_nxv2f32_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfncvt.f.f.w {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 2 x float> @llvm.epi.vfncvt.f.f.nxv2f32.nxv2f64(
    <vscale x 2 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}

declare <vscale x 2 x float> @llvm.epi.vfncvt.f.f.mask.nxv2f32.nxv2f64(
  <vscale x 2 x float>,
  <vscale x 2 x double>,
  <vscale x 2 x i1>,
  i64);

define void @intrinsic_vfncvt_mask_f.f_nxv2f32_nxv2f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_mask_f.f_nxv2f32_nxv2f64
; CHECK:       vsetvli {{.*}}, a0, e32, m1, ta, mu
; CHECK:       vfncvt.f.f.w {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 2 x float> @llvm.epi.vfncvt.f.f.mask.nxv2f32.nxv2f64(
    <vscale x 2 x float> undef,
    <vscale x 2 x double> undef,
    <vscale x 2 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 2 x float>*
  store <vscale x 2 x float> %a, <vscale x 2 x float>* %p

  ret void
}


declare <vscale x 4 x float> @llvm.epi.vfncvt.f.f.nxv4f32.nxv4f64(
  <vscale x 4 x double>,
  i64);

define void @intrinsic_vfncvt_f.f_nxv4f32_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_f.f_nxv4f32_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfncvt.f.f.w {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 4 x float> @llvm.epi.vfncvt.f.f.nxv4f32.nxv4f64(
    <vscale x 4 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}

declare <vscale x 4 x float> @llvm.epi.vfncvt.f.f.mask.nxv4f32.nxv4f64(
  <vscale x 4 x float>,
  <vscale x 4 x double>,
  <vscale x 4 x i1>,
  i64);

define void @intrinsic_vfncvt_mask_f.f_nxv4f32_nxv4f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_mask_f.f_nxv4f32_nxv4f64
; CHECK:       vsetvli {{.*}}, a0, e32, m2, ta, mu
; CHECK:       vfncvt.f.f.w {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 4 x float> @llvm.epi.vfncvt.f.f.mask.nxv4f32.nxv4f64(
    <vscale x 4 x float> undef,
    <vscale x 4 x double> undef,
    <vscale x 4 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 4 x float>*
  store <vscale x 4 x float> %a, <vscale x 4 x float>* %p

  ret void
}


declare <vscale x 8 x float> @llvm.epi.vfncvt.f.f.nxv8f32.nxv8f64(
  <vscale x 8 x double>,
  i64);

define void @intrinsic_vfncvt_f.f_nxv8f32_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_f.f_nxv8f32_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfncvt.f.f.w {{v[0-9]+}}, {{v[0-9]+}}
  %a = call <vscale x 8 x float> @llvm.epi.vfncvt.f.f.nxv8f32.nxv8f64(
    <vscale x 8 x double> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

declare <vscale x 8 x float> @llvm.epi.vfncvt.f.f.mask.nxv8f32.nxv8f64(
  <vscale x 8 x float>,
  <vscale x 8 x double>,
  <vscale x 8 x i1>,
  i64);

define void @intrinsic_vfncvt_mask_f.f_nxv8f32_nxv8f64() nounwind {
entry:
; CHECK-LABEL: intrinsic_vfncvt_mask_f.f_nxv8f32_nxv8f64
; CHECK:       vsetvli {{.*}}, a0, e32, m4, ta, mu
; CHECK:       vfncvt.f.f.w {{v[0-9]+}}, {{v[0-9]+}}, v0.t
  %a = call <vscale x 8 x float> @llvm.epi.vfncvt.f.f.mask.nxv8f32.nxv8f64(
    <vscale x 8 x float> undef,
    <vscale x 8 x double> undef,
    <vscale x 8 x i1> undef,
    i64 undef)

  %p = bitcast i8* @scratch to <vscale x 8 x float>*
  store <vscale x 8 x float> %a, <vscale x 8 x float>* %p

  ret void
}

