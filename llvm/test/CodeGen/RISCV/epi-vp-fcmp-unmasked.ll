; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+m,+f,+d,+v -verify-machineinstrs -O0 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=CHECK-O0 %s
; RUN: llc -mtriple=riscv64 -mattr=+m,+f,+d,+v -verify-machineinstrs -O2 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=CHECK-O2 %s

@scratch = global i8 0, align 16

define void @test_vp_fcmp(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_fcmp:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    csrr a1, vlenb
; CHECK-O0-NEXT:    sub sp, sp, a1
; CHECK-O0-NEXT:    vmv1r.v v10, v9
; CHECK-O0-NEXT:    vmv1r.v v9, v8
; CHECK-O0-NEXT:    slli a1, a0, 32
; CHECK-O0-NEXT:    srli a1, a1, 32
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O0-NEXT:    vmset.m v8
; CHECK-O0-NEXT:    vmclr.m v11
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vmfeq.vv v11, v9, v10
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vmflt.vv v11, v10, v9
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vmfle.vv v11, v10, v9
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vmflt.vv v11, v9, v10
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vmfle.vv v11, v9, v10
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vmfeq.vv v12, v10, v10
; CHECK-O0-NEXT:    vmfeq.vv v11, v9, v9
; CHECK-O0-NEXT:    vmand.mm v0, v11, v12
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vs1r.v v0, (a2) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    # implicit-def: $v11
; CHECK-O0-NEXT:    vmfne.vv v11, v9, v10, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmand.mm v11, v11, v0
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    vsm.v v0, (a0)
; CHECK-O0-NEXT:    vmnand.mm v12, v0, v0
; CHECK-O0-NEXT:    vsm.v v12, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v11
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vmfeq.vv v11, v9, v10, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmor.mm v11, v11, v12
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v11
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vmflt.vv v11, v10, v9, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmor.mm v11, v11, v12
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v11
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vmfle.vv v11, v10, v9, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmor.mm v11, v11, v12
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v11
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vmflt.vv v11, v9, v10, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmor.mm v11, v11, v12
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v11
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vmfle.vv v11, v9, v10, v0.t
; CHECK-O0-NEXT:    vmor.mm v11, v11, v12
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vmfne.vv v9, v9, v10
; CHECK-O0-NEXT:    vsetvli a1, zero, e8, mf8, ta, mu
; CHECK-O0-NEXT:    vsm.v v9, (a0)
; CHECK-O0-NEXT:    vsm.v v8, (a0)
; CHECK-O0-NEXT:    csrr a0, vlenb
; CHECK-O0-NEXT:    add sp, sp, a0
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_fcmp:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    slli a0, a0, 32
; CHECK-O2-NEXT:    srli a1, a0, 32
; CHECK-O2-NEXT:    lui a0, %hi(scratch)
; CHECK-O2-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O2-NEXT:    vmset.m v10
; CHECK-O2-NEXT:    vmclr.m v11
; CHECK-O2-NEXT:    vsm.v v11, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vmfeq.vv v11, v8, v9
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O2-NEXT:    vsm.v v11, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vmflt.vv v11, v9, v8
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O2-NEXT:    vsm.v v11, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vmfle.vv v11, v9, v8
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O2-NEXT:    vsm.v v11, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vmflt.vv v11, v8, v9
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O2-NEXT:    vsm.v v11, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vmfle.vv v11, v8, v9
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O2-NEXT:    vsm.v v11, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vmfeq.vv v11, v9, v9
; CHECK-O2-NEXT:    vmfeq.vv v12, v8, v8
; CHECK-O2-NEXT:    vmand.mm v0, v12, v11
; CHECK-O2-NEXT:    vmfne.vv v11, v8, v9, v0.t
; CHECK-O2-NEXT:    vmand.mm v11, v11, v0
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O2-NEXT:    vsm.v v11, (a0)
; CHECK-O2-NEXT:    vsm.v v0, (a0)
; CHECK-O2-NEXT:    vmnand.mm v11, v0, v0
; CHECK-O2-NEXT:    vsm.v v11, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vmfeq.vv v12, v8, v9, v0.t
; CHECK-O2-NEXT:    vmor.mm v12, v12, v11
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O2-NEXT:    vsm.v v12, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vmflt.vv v12, v9, v8, v0.t
; CHECK-O2-NEXT:    vmor.mm v12, v12, v11
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O2-NEXT:    vsm.v v12, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vmfle.vv v12, v9, v8, v0.t
; CHECK-O2-NEXT:    vmor.mm v12, v12, v11
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O2-NEXT:    vsm.v v12, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vmflt.vv v12, v8, v9, v0.t
; CHECK-O2-NEXT:    vmor.mm v12, v12, v11
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O2-NEXT:    vsm.v v12, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vmfle.vv v12, v8, v9, v0.t
; CHECK-O2-NEXT:    vmor.mm v11, v12, v11
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf8, ta, mu
; CHECK-O2-NEXT:    vsm.v v11, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vmfne.vv v8, v8, v9
; CHECK-O2-NEXT:    vsetvli a1, zero, e8, mf8, ta, mu
; CHECK-O2-NEXT:    vsm.v v8, (a0)
; CHECK-O2-NEXT:    vsm.v v10, (a0)
; CHECK-O2-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 1 x i1>*

  %head = insertelement <vscale x 1 x i1> undef, i1 1, i32 0
  %allones = shufflevector <vscale x 1 x i1> %head, <vscale x 1 x i1> undef, <vscale x 1 x i32> zeroinitializer

  %false = call <vscale x 1 x i1> @llvm.vp.fcmp.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i8 0, <vscale x 1 x i1> %allones, i32 %n)
  store <vscale x 1 x i1> %false, <vscale x 1 x i1>* %store_addr

  %oeq = call <vscale x 1 x i1> @llvm.vp.fcmp.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i8 1, <vscale x 1 x i1> %allones, i32 %n)
  store <vscale x 1 x i1> %oeq, <vscale x 1 x i1>* %store_addr

  %ogt = call <vscale x 1 x i1> @llvm.vp.fcmp.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i8 2, <vscale x 1 x i1> %allones, i32 %n)
  store <vscale x 1 x i1> %ogt, <vscale x 1 x i1>* %store_addr

  %oge = call <vscale x 1 x i1> @llvm.vp.fcmp.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i8 3, <vscale x 1 x i1> %allones, i32 %n)
  store <vscale x 1 x i1> %oge, <vscale x 1 x i1>* %store_addr

  %olt = call <vscale x 1 x i1> @llvm.vp.fcmp.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i8 4, <vscale x 1 x i1> %allones, i32 %n)
  store <vscale x 1 x i1> %olt, <vscale x 1 x i1>* %store_addr

  %ole = call <vscale x 1 x i1> @llvm.vp.fcmp.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i8 5, <vscale x 1 x i1> %allones, i32 %n)
  store <vscale x 1 x i1> %ole, <vscale x 1 x i1>* %store_addr

  %one = call <vscale x 1 x i1> @llvm.vp.fcmp.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i8 6, <vscale x 1 x i1> %allones, i32 %n)
  store <vscale x 1 x i1> %one, <vscale x 1 x i1>* %store_addr

  %ord = call <vscale x 1 x i1> @llvm.vp.fcmp.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i8 7, <vscale x 1 x i1> %allones, i32 %n)
  store <vscale x 1 x i1> %ord, <vscale x 1 x i1>* %store_addr

  %uno = call <vscale x 1 x i1> @llvm.vp.fcmp.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i8 8, <vscale x 1 x i1> %allones, i32 %n)
  store <vscale x 1 x i1> %uno, <vscale x 1 x i1>* %store_addr

  %ueq = call <vscale x 1 x i1> @llvm.vp.fcmp.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i8 9, <vscale x 1 x i1> %allones, i32 %n)
  store <vscale x 1 x i1> %ueq, <vscale x 1 x i1>* %store_addr

  %ugt = call <vscale x 1 x i1> @llvm.vp.fcmp.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i8 10, <vscale x 1 x i1> %allones, i32 %n)
  store <vscale x 1 x i1> %ugt, <vscale x 1 x i1>* %store_addr

  %uge = call <vscale x 1 x i1> @llvm.vp.fcmp.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i8 11, <vscale x 1 x i1> %allones, i32 %n)
  store <vscale x 1 x i1> %uge, <vscale x 1 x i1>* %store_addr

  %ult = call <vscale x 1 x i1> @llvm.vp.fcmp.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i8 12, <vscale x 1 x i1> %allones, i32 %n)
  store <vscale x 1 x i1> %ult, <vscale x 1 x i1>* %store_addr

  %ule = call <vscale x 1 x i1> @llvm.vp.fcmp.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i8 13, <vscale x 1 x i1> %allones, i32 %n)
  store <vscale x 1 x i1> %ule, <vscale x 1 x i1>* %store_addr

  %une = call <vscale x 1 x i1> @llvm.vp.fcmp.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i8 14, <vscale x 1 x i1> %allones, i32 %n)
  store <vscale x 1 x i1> %une, <vscale x 1 x i1>* %store_addr

  %true = call <vscale x 1 x i1> @llvm.vp.fcmp.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> %b, i8 15, <vscale x 1 x i1> %allones, i32 %n)
  store <vscale x 1 x i1> %true, <vscale x 1 x i1>* %store_addr

  ret void
}

define void @test_vp_fcmp_2(<vscale x 2 x float> %a, <vscale x 2 x float> %b, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_fcmp_2:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    csrr a1, vlenb
; CHECK-O0-NEXT:    sub sp, sp, a1
; CHECK-O0-NEXT:    vmv1r.v v10, v9
; CHECK-O0-NEXT:    vmv1r.v v9, v8
; CHECK-O0-NEXT:    slli a1, a0, 32
; CHECK-O0-NEXT:    srli a1, a1, 32
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vmset.m v8
; CHECK-O0-NEXT:    vmclr.m v11
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O0-NEXT:    vmfeq.vv v11, v9, v10
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O0-NEXT:    vmflt.vv v11, v10, v9
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O0-NEXT:    vmfle.vv v11, v10, v9
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O0-NEXT:    vmflt.vv v11, v9, v10
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O0-NEXT:    vmfle.vv v11, v9, v10
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O0-NEXT:    vmfeq.vv v12, v10, v10
; CHECK-O0-NEXT:    vmfeq.vv v11, v9, v9
; CHECK-O0-NEXT:    vmand.mm v0, v11, v12
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vs1r.v v0, (a2) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    # implicit-def: $v11
; CHECK-O0-NEXT:    vmfne.vv v11, v9, v10, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmand.mm v11, v11, v0
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    vsm.v v0, (a0)
; CHECK-O0-NEXT:    vmnand.mm v12, v0, v0
; CHECK-O0-NEXT:    vsm.v v12, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v11
; CHECK-O0-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O0-NEXT:    vmfeq.vv v11, v9, v10, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmor.mm v11, v11, v12
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v11
; CHECK-O0-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O0-NEXT:    vmflt.vv v11, v10, v9, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmor.mm v11, v11, v12
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v11
; CHECK-O0-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O0-NEXT:    vmfle.vv v11, v10, v9, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmor.mm v11, v11, v12
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v11
; CHECK-O0-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O0-NEXT:    vmflt.vv v11, v9, v10, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmor.mm v11, v11, v12
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v11
; CHECK-O0-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O0-NEXT:    vmfle.vv v11, v9, v10, v0.t
; CHECK-O0-NEXT:    vmor.mm v11, v11, v12
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v11, (a0)
; CHECK-O0-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O0-NEXT:    vmfne.vv v9, v9, v10
; CHECK-O0-NEXT:    vsetvli a1, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v9, (a0)
; CHECK-O0-NEXT:    vsm.v v8, (a0)
; CHECK-O0-NEXT:    csrr a0, vlenb
; CHECK-O0-NEXT:    add sp, sp, a0
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_fcmp_2:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    slli a0, a0, 32
; CHECK-O2-NEXT:    srli a1, a0, 32
; CHECK-O2-NEXT:    lui a0, %hi(scratch)
; CHECK-O2-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vmset.m v10
; CHECK-O2-NEXT:    vmclr.m v11
; CHECK-O2-NEXT:    vsm.v v11, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O2-NEXT:    vmfeq.vv v11, v8, v9
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v11, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O2-NEXT:    vmflt.vv v11, v9, v8
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v11, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O2-NEXT:    vmfle.vv v11, v9, v8
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v11, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O2-NEXT:    vmflt.vv v11, v8, v9
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v11, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O2-NEXT:    vmfle.vv v11, v8, v9
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v11, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O2-NEXT:    vmfeq.vv v11, v9, v9
; CHECK-O2-NEXT:    vmfeq.vv v12, v8, v8
; CHECK-O2-NEXT:    vmand.mm v0, v12, v11
; CHECK-O2-NEXT:    vmfne.vv v11, v8, v9, v0.t
; CHECK-O2-NEXT:    vmand.mm v11, v11, v0
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v11, (a0)
; CHECK-O2-NEXT:    vsm.v v0, (a0)
; CHECK-O2-NEXT:    vmnand.mm v11, v0, v0
; CHECK-O2-NEXT:    vsm.v v11, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O2-NEXT:    vmfeq.vv v12, v8, v9, v0.t
; CHECK-O2-NEXT:    vmor.mm v12, v12, v11
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v12, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O2-NEXT:    vmflt.vv v12, v9, v8, v0.t
; CHECK-O2-NEXT:    vmor.mm v12, v12, v11
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v12, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O2-NEXT:    vmfle.vv v12, v9, v8, v0.t
; CHECK-O2-NEXT:    vmor.mm v12, v12, v11
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v12, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O2-NEXT:    vmflt.vv v12, v8, v9, v0.t
; CHECK-O2-NEXT:    vmor.mm v12, v12, v11
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v12, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O2-NEXT:    vmfle.vv v12, v8, v9, v0.t
; CHECK-O2-NEXT:    vmor.mm v11, v12, v11
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v11, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O2-NEXT:    vmfne.vv v8, v8, v9
; CHECK-O2-NEXT:    vsetvli a1, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v8, (a0)
; CHECK-O2-NEXT:    vsm.v v10, (a0)
; CHECK-O2-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 2 x i1>*

  %head = insertelement <vscale x 2 x i1> undef, i1 1, i32 0
  %allones = shufflevector <vscale x 2 x i1> %head, <vscale x 2 x i1> undef, <vscale x 2 x i32> zeroinitializer

  %false = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f32(<vscale x 2 x float> %a, <vscale x 2 x float> %b, i8 0, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %false, <vscale x 2 x i1>* %store_addr

  %oeq = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f32(<vscale x 2 x float> %a, <vscale x 2 x float> %b, i8 1, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %oeq, <vscale x 2 x i1>* %store_addr

  %ogt = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f32(<vscale x 2 x float> %a, <vscale x 2 x float> %b, i8 2, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %ogt, <vscale x 2 x i1>* %store_addr

  %oge = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f32(<vscale x 2 x float> %a, <vscale x 2 x float> %b, i8 3, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %oge, <vscale x 2 x i1>* %store_addr

  %olt = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f32(<vscale x 2 x float> %a, <vscale x 2 x float> %b, i8 4, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %olt, <vscale x 2 x i1>* %store_addr

  %ole = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f32(<vscale x 2 x float> %a, <vscale x 2 x float> %b, i8 5, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %ole, <vscale x 2 x i1>* %store_addr

  %one = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f32(<vscale x 2 x float> %a, <vscale x 2 x float> %b, i8 6, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %one, <vscale x 2 x i1>* %store_addr

  %ord = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f32(<vscale x 2 x float> %a, <vscale x 2 x float> %b, i8 7, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %ord, <vscale x 2 x i1>* %store_addr

  %uno = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f32(<vscale x 2 x float> %a, <vscale x 2 x float> %b, i8 8, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %uno, <vscale x 2 x i1>* %store_addr

  %ueq = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f32(<vscale x 2 x float> %a, <vscale x 2 x float> %b, i8 9, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %ueq, <vscale x 2 x i1>* %store_addr

  %ugt = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f32(<vscale x 2 x float> %a, <vscale x 2 x float> %b, i8 10, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %ugt, <vscale x 2 x i1>* %store_addr

  %uge = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f32(<vscale x 2 x float> %a, <vscale x 2 x float> %b, i8 11, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %uge, <vscale x 2 x i1>* %store_addr

  %ult = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f32(<vscale x 2 x float> %a, <vscale x 2 x float> %b, i8 12, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %ult, <vscale x 2 x i1>* %store_addr

  %ule = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f32(<vscale x 2 x float> %a, <vscale x 2 x float> %b, i8 13, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %ule, <vscale x 2 x i1>* %store_addr

  %une = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f32(<vscale x 2 x float> %a, <vscale x 2 x float> %b, i8 14, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %une, <vscale x 2 x i1>* %store_addr

  %true = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f32(<vscale x 2 x float> %a, <vscale x 2 x float> %b, i8 15, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %true, <vscale x 2 x i1>* %store_addr

  ret void
}

define void @test_vp_fcmp_3(<vscale x 2 x double> %a, <vscale x 2 x double> %b, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_fcmp_3:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    csrr a1, vlenb
; CHECK-O0-NEXT:    sub sp, sp, a1
; CHECK-O0-NEXT:    vmv2r.v v12, v10
; CHECK-O0-NEXT:    vmv2r.v v10, v8
; CHECK-O0-NEXT:    slli a1, a0, 32
; CHECK-O0-NEXT:    srli a1, a1, 32
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vmset.m v8
; CHECK-O0-NEXT:    vmclr.m v9
; CHECK-O0-NEXT:    vsm.v v9, (a0)
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O0-NEXT:    vmfeq.vv v9, v10, v12
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v9, (a0)
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O0-NEXT:    vmflt.vv v9, v12, v10
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v9, (a0)
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O0-NEXT:    vmfle.vv v9, v12, v10
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v9, (a0)
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O0-NEXT:    vmflt.vv v9, v10, v12
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v9, (a0)
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O0-NEXT:    vmfle.vv v9, v10, v12
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v9, (a0)
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O0-NEXT:    vmfeq.vv v14, v12, v12
; CHECK-O0-NEXT:    vmfeq.vv v9, v10, v10
; CHECK-O0-NEXT:    vmand.mm v0, v9, v14
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vs1r.v v0, (a2) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    # implicit-def: $v9
; CHECK-O0-NEXT:    vmfne.vv v9, v10, v12, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmand.mm v9, v9, v0
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v9, (a0)
; CHECK-O0-NEXT:    vsm.v v0, (a0)
; CHECK-O0-NEXT:    vmnand.mm v14, v0, v0
; CHECK-O0-NEXT:    vsm.v v14, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v9
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O0-NEXT:    vmfeq.vv v9, v10, v12, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmor.mm v9, v9, v14
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v9, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v9
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O0-NEXT:    vmflt.vv v9, v12, v10, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmor.mm v9, v9, v14
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v9, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v9
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O0-NEXT:    vmfle.vv v9, v12, v10, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmor.mm v9, v9, v14
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v9, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v9
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O0-NEXT:    vmflt.vv v9, v10, v12, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmor.mm v9, v9, v14
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v9, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v9
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O0-NEXT:    vmfle.vv v9, v10, v12, v0.t
; CHECK-O0-NEXT:    vmor.mm v9, v9, v14
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v9, (a0)
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O0-NEXT:    vmfne.vv v9, v10, v12
; CHECK-O0-NEXT:    vsetvli a1, zero, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vsm.v v9, (a0)
; CHECK-O0-NEXT:    vsm.v v8, (a0)
; CHECK-O0-NEXT:    csrr a0, vlenb
; CHECK-O0-NEXT:    add sp, sp, a0
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_fcmp_3:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    slli a0, a0, 32
; CHECK-O2-NEXT:    srli a1, a0, 32
; CHECK-O2-NEXT:    lui a0, %hi(scratch)
; CHECK-O2-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vmset.m v12
; CHECK-O2-NEXT:    vmclr.m v13
; CHECK-O2-NEXT:    vsm.v v13, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O2-NEXT:    vmfeq.vv v13, v8, v10
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v13, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O2-NEXT:    vmflt.vv v13, v10, v8
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v13, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O2-NEXT:    vmfle.vv v13, v10, v8
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v13, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O2-NEXT:    vmflt.vv v13, v8, v10
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v13, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O2-NEXT:    vmfle.vv v13, v8, v10
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v13, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O2-NEXT:    vmfeq.vv v13, v10, v10
; CHECK-O2-NEXT:    vmfeq.vv v14, v8, v8
; CHECK-O2-NEXT:    vmand.mm v0, v14, v13
; CHECK-O2-NEXT:    vmfne.vv v13, v8, v10, v0.t
; CHECK-O2-NEXT:    vmand.mm v13, v13, v0
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v13, (a0)
; CHECK-O2-NEXT:    vsm.v v0, (a0)
; CHECK-O2-NEXT:    vmnand.mm v13, v0, v0
; CHECK-O2-NEXT:    vsm.v v13, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O2-NEXT:    vmfeq.vv v14, v8, v10, v0.t
; CHECK-O2-NEXT:    vmor.mm v14, v14, v13
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v14, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O2-NEXT:    vmflt.vv v14, v10, v8, v0.t
; CHECK-O2-NEXT:    vmor.mm v14, v14, v13
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v14, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O2-NEXT:    vmfle.vv v14, v10, v8, v0.t
; CHECK-O2-NEXT:    vmor.mm v14, v14, v13
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v14, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O2-NEXT:    vmflt.vv v14, v8, v10, v0.t
; CHECK-O2-NEXT:    vmor.mm v14, v14, v13
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v14, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O2-NEXT:    vmfle.vv v14, v8, v10, v0.t
; CHECK-O2-NEXT:    vmor.mm v13, v14, v13
; CHECK-O2-NEXT:    vsetvli a2, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v13, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m2, ta, mu
; CHECK-O2-NEXT:    vmfne.vv v13, v8, v10
; CHECK-O2-NEXT:    vsetvli a1, zero, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vsm.v v13, (a0)
; CHECK-O2-NEXT:    vsm.v v12, (a0)
; CHECK-O2-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 2 x i1>*

  %head = insertelement <vscale x 2 x i1> undef, i1 1, i32 0
  %allones = shufflevector <vscale x 2 x i1> %head, <vscale x 2 x i1> undef, <vscale x 2 x i32> zeroinitializer

  %false = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f64(<vscale x 2 x double> %a, <vscale x 2 x double> %b, i8 0, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %false, <vscale x 2 x i1>* %store_addr

  %oeq = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f64(<vscale x 2 x double> %a, <vscale x 2 x double> %b, i8 1, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %oeq, <vscale x 2 x i1>* %store_addr

  %ogt = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f64(<vscale x 2 x double> %a, <vscale x 2 x double> %b, i8 2, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %ogt, <vscale x 2 x i1>* %store_addr

  %oge = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f64(<vscale x 2 x double> %a, <vscale x 2 x double> %b, i8 3, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %oge, <vscale x 2 x i1>* %store_addr

  %olt = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f64(<vscale x 2 x double> %a, <vscale x 2 x double> %b, i8 4, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %olt, <vscale x 2 x i1>* %store_addr

  %ole = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f64(<vscale x 2 x double> %a, <vscale x 2 x double> %b, i8 5, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %ole, <vscale x 2 x i1>* %store_addr

  %one = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f64(<vscale x 2 x double> %a, <vscale x 2 x double> %b, i8 6, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %one, <vscale x 2 x i1>* %store_addr

  %ord = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f64(<vscale x 2 x double> %a, <vscale x 2 x double> %b, i8 7, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %ord, <vscale x 2 x i1>* %store_addr

  %uno = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f64(<vscale x 2 x double> %a, <vscale x 2 x double> %b, i8 8, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %uno, <vscale x 2 x i1>* %store_addr

  %ueq = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f64(<vscale x 2 x double> %a, <vscale x 2 x double> %b, i8 9, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %ueq, <vscale x 2 x i1>* %store_addr

  %ugt = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f64(<vscale x 2 x double> %a, <vscale x 2 x double> %b, i8 10, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %ugt, <vscale x 2 x i1>* %store_addr

  %uge = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f64(<vscale x 2 x double> %a, <vscale x 2 x double> %b, i8 11, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %uge, <vscale x 2 x i1>* %store_addr

  %ult = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f64(<vscale x 2 x double> %a, <vscale x 2 x double> %b, i8 12, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %ult, <vscale x 2 x i1>* %store_addr

  %ule = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f64(<vscale x 2 x double> %a, <vscale x 2 x double> %b, i8 13, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %ule, <vscale x 2 x i1>* %store_addr

  %une = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f64(<vscale x 2 x double> %a, <vscale x 2 x double> %b, i8 14, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %une, <vscale x 2 x i1>* %store_addr

  %true = call <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f64(<vscale x 2 x double> %a, <vscale x 2 x double> %b, i8 15, <vscale x 2 x i1> %allones, i32 %n)
  store <vscale x 2 x i1> %true, <vscale x 2 x i1>* %store_addr

  ret void
}

; store
declare void @llvm.vp.store.nxv1i1(<vscale x 1 x i1>, <vscale x 1 x i1>*, i1, <vscale x 1 x i1>, i32)
declare void @llvm.vp.store.nxv2i1(<vscale x 2 x i1>, <vscale x 2 x i1>*, i1, <vscale x 2 x i1>, i32)

; fcmp
declare <vscale x 1 x i1> @llvm.vp.fcmp.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i8 immarg, <vscale x 1 x i1>, i32)
declare <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f32(<vscale x 2 x float>, <vscale x 2 x float>, i8 immarg, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i1> @llvm.vp.fcmp.nxv2f64(<vscale x 2 x double>, <vscale x 2 x double>, i8 immarg, <vscale x 2 x i1>, i32)
