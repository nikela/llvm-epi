; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+d,+experimental-v -verify-machineinstrs -O0 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=CHECK-O0 %s
; RUN: llc -mtriple=riscv64 -mattr=+d,+experimental-v -verify-machineinstrs -O2 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=CHECK-O2 %s

define void @test_llvm_IR_infinite_loop(i64 %n, double* %a, double* %b, double* %c) {
; CHECK-O0-LABEL: test_llvm_IR_infinite_loop:
; CHECK-O0:       # %bb.0: # %entry
; CHECK-O0-NEXT:    addi sp, sp, -80
; CHECK-O0-NEXT:    .cfi_def_cfa_offset 80
; CHECK-O0-NEXT:    sd a3, 48(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a2, 56(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a1, 64(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    mv a1, a0
; CHECK-O0-NEXT:    sd a1, 72(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    li a0, 0
; CHECK-O0-NEXT:    bge a0, a1, .LBB0_4
; CHECK-O0-NEXT:    j .LBB0_1
; CHECK-O0-NEXT:  .LBB0_1: # %for.body.lr.ph
; CHECK-O0-NEXT:    ld a1, 72(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    li a0, 25
; CHECK-O0-NEXT:    slt a0, a0, a1
; CHECK-O0-NEXT:    slli a0, a0, 9
; CHECK-O0-NEXT:    sd a0, 32(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    li a2, 1024
; CHECK-O0-NEXT:    li a0, 400
; CHECK-O0-NEXT:    sd a2, 40(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    blt a0, a1, .LBB0_3
; CHECK-O0-NEXT:  # %bb.2: # %for.body.lr.ph
; CHECK-O0-NEXT:    ld a0, 32(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    sd a0, 40(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:  .LBB0_3: # %for.body.lr.ph
; CHECK-O0-NEXT:    ld a0, 48(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a1, 56(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a2, 64(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a3, 72(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a5, 40(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ori a4, a5, 88
; CHECK-O0-NEXT:    vsetvl a3, a3, a4
; CHECK-O0-NEXT:    sd a3, 0(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a2, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a1, 16(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a0, 24(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    j .LBB0_5
; CHECK-O0-NEXT:  .LBB0_4: # %for.cond.cleanup
; CHECK-O0-NEXT:    addi sp, sp, 80
; CHECK-O0-NEXT:    ret
; CHECK-O0-NEXT:  .LBB0_5: # %for.body
; CHECK-O0-NEXT:    # =>This Inner Loop Header: Depth=1
; CHECK-O0-NEXT:    ld a0, 24(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a1, 16(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a2, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a3, 0(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v9
; CHECK-O0-NEXT:    vle64.v v9, (a2)
; CHECK-O0-NEXT:    # implicit-def: $v10
; CHECK-O0-NEXT:    vle64.v v10, (a1)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vfadd.vv v8, v9, v10
; CHECK-O0-NEXT:    vse64.v v8, (a0)
; CHECK-O0-NEXT:    slli a3, a3, 3
; CHECK-O0-NEXT:    add a2, a2, a3
; CHECK-O0-NEXT:    add a1, a1, a3
; CHECK-O0-NEXT:    add a0, a0, a3
; CHECK-O0-NEXT:    sd a2, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a1, 16(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a0, 24(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    j .LBB0_5
;
; CHECK-O2-LABEL: test_llvm_IR_infinite_loop:
; CHECK-O2:       # %bb.0: # %entry
; CHECK-O2-NEXT:    blez a0, .LBB0_5
; CHECK-O2-NEXT:  # %bb.1: # %for.body.lr.ph
; CHECK-O2-NEXT:    li a4, 400
; CHECK-O2-NEXT:    li a6, 1024
; CHECK-O2-NEXT:    blt a4, a0, .LBB0_3
; CHECK-O2-NEXT:  # %bb.2: # %for.body.lr.ph
; CHECK-O2-NEXT:    li a4, 25
; CHECK-O2-NEXT:    slt a4, a4, a0
; CHECK-O2-NEXT:    slli a6, a4, 9
; CHECK-O2-NEXT:  .LBB0_3: # %for.body.lr.ph
; CHECK-O2-NEXT:    li a4, 0
; CHECK-O2-NEXT:    ori a5, a6, 88
; CHECK-O2-NEXT:    vsetvl a0, a0, a5
; CHECK-O2-NEXT:    slli a0, a0, 3
; CHECK-O2-NEXT:  .LBB0_4: # %for.body
; CHECK-O2-NEXT:    # =>This Inner Loop Header: Depth=1
; CHECK-O2-NEXT:    add a5, a1, a4
; CHECK-O2-NEXT:    vle64.v v8, (a5)
; CHECK-O2-NEXT:    add a5, a2, a4
; CHECK-O2-NEXT:    vle64.v v9, (a5)
; CHECK-O2-NEXT:    vfadd.vv v8, v8, v9
; CHECK-O2-NEXT:    add a5, a3, a4
; CHECK-O2-NEXT:    vse64.v v8, (a5)
; CHECK-O2-NEXT:    add a4, a4, a0
; CHECK-O2-NEXT:    j .LBB0_4
; CHECK-O2-NEXT:  .LBB0_5: # %for.cond.cleanup
; CHECK-O2-NEXT:    ret
entry:
  %cmp = icmp sgt i64 %n, 0
  br i1 %cmp, label %for.body.lr.ph, label %for.cond.cleanup

for.body.lr.ph:                                   ; preds = %entry
  %cmp2 = icmp sgt i64 %n, 400
  %cmp4 = icmp sgt i64 %n, 25
  %spec.select = select i1 %cmp4, i64 512, i64 0
  %extra.0 = select i1 %cmp2, i64 1024, i64 %spec.select
  %0 = tail call i64 @llvm.epi.vsetvl.ext(i64 %n, i64 3, i64 0, i64 %extra.0)
  br label %for.body

for.cond.cleanup:                                 ; preds = %entry
  ret void

for.body:                                         ; preds = %for.body, %for.body.lr.ph
  %a.addr.032 = phi double* [ %a, %for.body.lr.ph ], [ %add.ptr, %for.body ]
  %b.addr.031 = phi double* [ %b, %for.body.lr.ph ], [ %add.ptr10, %for.body ]
  %c.addr.030 = phi double* [ %c, %for.body.lr.ph ], [ %add.ptr11, %for.body ]
  %1 = bitcast double* %a.addr.032 to <vscale x 1 x double>*
  %2 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %1, i64 %0)
  %3 = bitcast double* %b.addr.031 to <vscale x 1 x double>*
  %4 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %3, i64 %0)
  %5 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %2, <vscale x 1 x double> %4, i64 %0)
  %6 = bitcast double* %c.addr.030 to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %5, <vscale x 1 x double>* %6, i64 %0)
  %add.ptr = getelementptr inbounds double, double* %a.addr.032, i64 %0
  %add.ptr10 = getelementptr inbounds double, double* %b.addr.031, i64 %0
  %add.ptr11 = getelementptr inbounds double, double* %c.addr.030, i64 %0
  br label %for.body
}

declare i64 @llvm.epi.vsetvl.ext(i64, i64, i64, i64)

declare <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* nocapture, i64)
declare <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64)
declare void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>* nocapture, i64)
