; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+m,+experimental-v -verify-machineinstrs -O0 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=CHECK-O0 %s
; RUN: llc -mtriple=riscv64 -mattr=+m,+experimental-v -verify-machineinstrs -O2 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=CHECK-O2 %s

@scratch = global i8 0, align 16

define void @test_vp_logical(<vscale x 1 x i64>* %a0, <vscale x 1 x i64>* %a1, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_logical:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -32
; CHECK-O0-NEXT:    csrr a3, vlenb
; CHECK-O0-NEXT:    slli a3, a3, 1
; CHECK-O0-NEXT:    sub sp, sp, a3
; CHECK-O0-NEXT:    sd a2, 24(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    mv a2, a1
; CHECK-O0-NEXT:    ld a1, 24(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    mv a3, a0
; CHECK-O0-NEXT:    # kill: def $x10 killed $x11
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    slli a1, a1, 32
; CHECK-O0-NEXT:    srli a1, a1, 32
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vle64.v v8, (a3)
; CHECK-O0-NEXT:    vle64.v v9, (a2)
; CHECK-O0-NEXT:    vsetvli a2, zero, e64, m1, ta, mu
; CHECK-O0-NEXT:    vand.vi v8, v8, 1
; CHECK-O0-NEXT:    vmsne.vi v8, v8, 0
; CHECK-O0-NEXT:    vand.vi v9, v9, 1
; CHECK-O0-NEXT:    vmsne.vi v9, v9, 0
; CHECK-O0-NEXT:    vsetvli zero, a1, e8, mf8, ta, mu
; CHECK-O0-NEXT:    vmand.mm v0, v8, v9
; CHECK-O0-NEXT:    vmor.mm v10, v8, v9
; CHECK-O0-NEXT:    addi a2, sp, 32
; CHECK-O0-NEXT:    vs1r.v v10, (a2) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    vmxor.mm v8, v8, v9
; CHECK-O0-NEXT:    csrr a2, vlenb
; CHECK-O0-NEXT:    add a2, sp, a2
; CHECK-O0-NEXT:    addi a2, a2, 32
; CHECK-O0-NEXT:    vs1r.v v8, (a2) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    vsetvli a2, zero, e64, m1, ta, mu
; CHECK-O0-NEXT:    vmv.v.i v8, 0
; CHECK-O0-NEXT:    vmerge.vim v10, v8, 1, v0
; CHECK-O0-NEXT:    addi a2, sp, 32
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmerge.vim v9, v8, 1, v0
; CHECK-O0-NEXT:    csrr a2, vlenb
; CHECK-O0-NEXT:    add a2, sp, a2
; CHECK-O0-NEXT:    addi a2, a2, 32
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmerge.vim v8, v8, 1, v0
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    vse64.v v10, (a0)
; CHECK-O0-NEXT:    vse64.v v9, (a0)
; CHECK-O0-NEXT:    vse64.v v8, (a0)
; CHECK-O0-NEXT:    csrr a0, vlenb
; CHECK-O0-NEXT:    slli a0, a0, 1
; CHECK-O0-NEXT:    add sp, sp, a0
; CHECK-O0-NEXT:    addi sp, sp, 32
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_logical:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    lui a3, %hi(scratch)
; CHECK-O2-NEXT:    slli a2, a2, 32
; CHECK-O2-NEXT:    srli a2, a2, 32
; CHECK-O2-NEXT:    vsetvli zero, a2, e64, m1, ta, mu
; CHECK-O2-NEXT:    vle64.v v8, (a0)
; CHECK-O2-NEXT:    addi a0, a3, %lo(scratch)
; CHECK-O2-NEXT:    vle64.v v9, (a1)
; CHECK-O2-NEXT:    vsetvli a1, zero, e64, m1, ta, mu
; CHECK-O2-NEXT:    vand.vi v8, v8, 1
; CHECK-O2-NEXT:    vmsne.vi v10, v8, 0
; CHECK-O2-NEXT:    vand.vi v8, v9, 1
; CHECK-O2-NEXT:    vmsne.vi v9, v8, 0
; CHECK-O2-NEXT:    vsetvli zero, a2, e8, mf8, ta, mu
; CHECK-O2-NEXT:    vmand.mm v0, v10, v9
; CHECK-O2-NEXT:    vmor.mm v8, v10, v9
; CHECK-O2-NEXT:    vmxor.mm v9, v10, v9
; CHECK-O2-NEXT:    vsetvli a1, zero, e64, m1, ta, mu
; CHECK-O2-NEXT:    vmv.v.i v10, 0
; CHECK-O2-NEXT:    vmerge.vim v11, v10, 1, v0
; CHECK-O2-NEXT:    vmv1r.v v0, v8
; CHECK-O2-NEXT:    vmerge.vim v8, v10, 1, v0
; CHECK-O2-NEXT:    vmv1r.v v0, v9
; CHECK-O2-NEXT:    vmerge.vim v9, v10, 1, v0
; CHECK-O2-NEXT:    vsetvli zero, a2, e64, m1, ta, mu
; CHECK-O2-NEXT:    vse64.v v11, (a0)
; CHECK-O2-NEXT:    vse64.v v8, (a0)
; CHECK-O2-NEXT:    vse64.v v9, (a0)
; CHECK-O2-NEXT:    ret
  %head = insertelement <vscale x 1 x i1> undef, i1 1, i32 0
  %allones = shufflevector <vscale x 1 x i1> %head, <vscale x 1 x i1> undef, <vscale x 1 x i32> zeroinitializer

  %store_addr = bitcast i8* @scratch to <vscale x 1 x i64>*

  %i0 = call <vscale x 1 x i64> @llvm.vp.load.nxv1i64(<vscale x 1 x i64>* %a0, <vscale x 1 x i1> %allones, i32 %n)
  %i1 = call <vscale x 1 x i64> @llvm.vp.load.nxv1i64(<vscale x 1 x i64>* %a1, <vscale x 1 x i1> %allones, i32 %n)

  %trunc_i0 = trunc <vscale x 1 x i64> %i0 to <vscale x 1 x i1>
  %trunc_i1 = trunc <vscale x 1 x i64> %i1 to <vscale x 1 x i1>

  %r0 = call <vscale x 1 x i1> @llvm.vp.and.nxv1i1(<vscale x 1 x i1> %trunc_i0, <vscale x 1 x i1> %trunc_i1, <vscale x 1 x i1> %allones, i32 %n)
  %r1 = call <vscale x 1 x i1> @llvm.vp.or.nxv1i1(<vscale x 1 x i1> %trunc_i0, <vscale x 1 x i1> %trunc_i1, <vscale x 1 x i1> %allones, i32 %n)
  %r2 = call <vscale x 1 x i1> @llvm.vp.xor.nxv1i1(<vscale x 1 x i1> %trunc_i0, <vscale x 1 x i1> %trunc_i1, <vscale x 1 x i1> %allones, i32 %n)

  %zext_r0 = zext <vscale x 1 x i1> %r0 to <vscale x 1 x i64>
  %zext_r1 = zext <vscale x 1 x i1> %r1 to <vscale x 1 x i64>
  %zext_r2 = zext <vscale x 1 x i1> %r2 to <vscale x 1 x i64>

  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %zext_r0, <vscale x 1 x i64>* %store_addr, <vscale x 1 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %zext_r1, <vscale x 1 x i64>* %store_addr, <vscale x 1 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %zext_r2, <vscale x 1 x i64>* %store_addr, <vscale x 1 x i1> %allones, i32 %n)

  ret void
}

define void @test_vp_logical_2(<vscale x 2 x i32>* %a0, <vscale x 2 x i32>* %a1, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_logical_2:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -32
; CHECK-O0-NEXT:    csrr a3, vlenb
; CHECK-O0-NEXT:    slli a3, a3, 1
; CHECK-O0-NEXT:    sub sp, sp, a3
; CHECK-O0-NEXT:    sd a2, 24(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    mv a2, a1
; CHECK-O0-NEXT:    ld a1, 24(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    mv a3, a0
; CHECK-O0-NEXT:    # kill: def $x10 killed $x11
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    slli a1, a1, 32
; CHECK-O0-NEXT:    srli a1, a1, 32
; CHECK-O0-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O0-NEXT:    vle32.v v8, (a3)
; CHECK-O0-NEXT:    vle32.v v9, (a2)
; CHECK-O0-NEXT:    vsetvli a2, zero, e32, m1, ta, mu
; CHECK-O0-NEXT:    vand.vi v8, v8, 1
; CHECK-O0-NEXT:    vmsne.vi v8, v8, 0
; CHECK-O0-NEXT:    vand.vi v9, v9, 1
; CHECK-O0-NEXT:    vmsne.vi v9, v9, 0
; CHECK-O0-NEXT:    vsetvli zero, a1, e8, mf4, ta, mu
; CHECK-O0-NEXT:    vmand.mm v0, v8, v9
; CHECK-O0-NEXT:    vmor.mm v10, v8, v9
; CHECK-O0-NEXT:    addi a2, sp, 32
; CHECK-O0-NEXT:    vs1r.v v10, (a2) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    vmxor.mm v8, v8, v9
; CHECK-O0-NEXT:    csrr a2, vlenb
; CHECK-O0-NEXT:    add a2, sp, a2
; CHECK-O0-NEXT:    addi a2, a2, 32
; CHECK-O0-NEXT:    vs1r.v v8, (a2) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    vsetvli a2, zero, e32, m1, ta, mu
; CHECK-O0-NEXT:    vmv.v.i v8, 0
; CHECK-O0-NEXT:    vmerge.vim v10, v8, 1, v0
; CHECK-O0-NEXT:    addi a2, sp, 32
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmerge.vim v9, v8, 1, v0
; CHECK-O0-NEXT:    csrr a2, vlenb
; CHECK-O0-NEXT:    add a2, sp, a2
; CHECK-O0-NEXT:    addi a2, a2, 32
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmerge.vim v8, v8, 1, v0
; CHECK-O0-NEXT:    vsetvli zero, a1, e32, m1, ta, mu
; CHECK-O0-NEXT:    vse32.v v10, (a0)
; CHECK-O0-NEXT:    vse32.v v9, (a0)
; CHECK-O0-NEXT:    vse32.v v8, (a0)
; CHECK-O0-NEXT:    csrr a0, vlenb
; CHECK-O0-NEXT:    slli a0, a0, 1
; CHECK-O0-NEXT:    add sp, sp, a0
; CHECK-O0-NEXT:    addi sp, sp, 32
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_logical_2:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    lui a3, %hi(scratch)
; CHECK-O2-NEXT:    slli a2, a2, 32
; CHECK-O2-NEXT:    srli a2, a2, 32
; CHECK-O2-NEXT:    vsetvli zero, a2, e32, m1, ta, mu
; CHECK-O2-NEXT:    vle32.v v8, (a0)
; CHECK-O2-NEXT:    addi a0, a3, %lo(scratch)
; CHECK-O2-NEXT:    vle32.v v9, (a1)
; CHECK-O2-NEXT:    vsetvli a1, zero, e32, m1, ta, mu
; CHECK-O2-NEXT:    vand.vi v8, v8, 1
; CHECK-O2-NEXT:    vmsne.vi v10, v8, 0
; CHECK-O2-NEXT:    vand.vi v8, v9, 1
; CHECK-O2-NEXT:    vmsne.vi v9, v8, 0
; CHECK-O2-NEXT:    vsetvli zero, a2, e8, mf4, ta, mu
; CHECK-O2-NEXT:    vmand.mm v0, v10, v9
; CHECK-O2-NEXT:    vmor.mm v8, v10, v9
; CHECK-O2-NEXT:    vmxor.mm v9, v10, v9
; CHECK-O2-NEXT:    vsetvli a1, zero, e32, m1, ta, mu
; CHECK-O2-NEXT:    vmv.v.i v10, 0
; CHECK-O2-NEXT:    vmerge.vim v11, v10, 1, v0
; CHECK-O2-NEXT:    vmv1r.v v0, v8
; CHECK-O2-NEXT:    vmerge.vim v8, v10, 1, v0
; CHECK-O2-NEXT:    vmv1r.v v0, v9
; CHECK-O2-NEXT:    vmerge.vim v9, v10, 1, v0
; CHECK-O2-NEXT:    vsetvli zero, a2, e32, m1, ta, mu
; CHECK-O2-NEXT:    vse32.v v11, (a0)
; CHECK-O2-NEXT:    vse32.v v8, (a0)
; CHECK-O2-NEXT:    vse32.v v9, (a0)
; CHECK-O2-NEXT:    ret
  %head = insertelement <vscale x 2 x i1> undef, i1 1, i32 0
  %allones = shufflevector <vscale x 2 x i1> %head, <vscale x 2 x i1> undef, <vscale x 2 x i32> zeroinitializer

  %store_addr = bitcast i8* @scratch to <vscale x 2 x i32>*

  %i0 = call <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>* %a0, <vscale x 2 x i1> %allones, i32 %n)
  %i1 = call <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>* %a1, <vscale x 2 x i1> %allones, i32 %n)

  %trunc_i0 = trunc <vscale x 2 x i32> %i0 to <vscale x 2 x i1>
  %trunc_i1 = trunc <vscale x 2 x i32> %i1 to <vscale x 2 x i1>

  %r0 = call <vscale x 2 x i1> @llvm.vp.and.nxv2i1(<vscale x 2 x i1> %trunc_i0, <vscale x 2 x i1> %trunc_i1, <vscale x 2 x i1> %allones, i32 %n)
  %r1 = call <vscale x 2 x i1> @llvm.vp.or.nxv2i1(<vscale x 2 x i1> %trunc_i0, <vscale x 2 x i1> %trunc_i1, <vscale x 2 x i1> %allones, i32 %n)
  %r2 = call <vscale x 2 x i1> @llvm.vp.xor.nxv2i1(<vscale x 2 x i1> %trunc_i0, <vscale x 2 x i1> %trunc_i1, <vscale x 2 x i1> %allones, i32 %n)

  %zext_r0 = zext <vscale x 2 x i1> %r0 to <vscale x 2 x i32>
  %zext_r1 = zext <vscale x 2 x i1> %r1 to <vscale x 2 x i32>
  %zext_r2 = zext <vscale x 2 x i1> %r2 to <vscale x 2 x i32>

  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %zext_r0, <vscale x 2 x i32>* %store_addr, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %zext_r1, <vscale x 2 x i32>* %store_addr, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %zext_r2, <vscale x 2 x i32>* %store_addr, <vscale x 2 x i1> %allones, i32 %n)

  ret void
}

define void @test_vp_logical_3(<vscale x 4 x i16>* %a0, <vscale x 4 x i16>* %a1, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_logical_3:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -32
; CHECK-O0-NEXT:    csrr a3, vlenb
; CHECK-O0-NEXT:    slli a3, a3, 1
; CHECK-O0-NEXT:    sub sp, sp, a3
; CHECK-O0-NEXT:    sd a2, 24(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    mv a2, a1
; CHECK-O0-NEXT:    ld a1, 24(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    mv a3, a0
; CHECK-O0-NEXT:    # kill: def $x10 killed $x11
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    slli a1, a1, 32
; CHECK-O0-NEXT:    srli a1, a1, 32
; CHECK-O0-NEXT:    vsetvli zero, a1, e16, m1, ta, mu
; CHECK-O0-NEXT:    vle16.v v8, (a3)
; CHECK-O0-NEXT:    vle16.v v9, (a2)
; CHECK-O0-NEXT:    vsetvli a2, zero, e16, m1, ta, mu
; CHECK-O0-NEXT:    vand.vi v8, v8, 1
; CHECK-O0-NEXT:    vmsne.vi v8, v8, 0
; CHECK-O0-NEXT:    vand.vi v9, v9, 1
; CHECK-O0-NEXT:    vmsne.vi v9, v9, 0
; CHECK-O0-NEXT:    vsetvli zero, a1, e8, mf2, ta, mu
; CHECK-O0-NEXT:    vmand.mm v0, v8, v9
; CHECK-O0-NEXT:    vmor.mm v10, v8, v9
; CHECK-O0-NEXT:    addi a2, sp, 32
; CHECK-O0-NEXT:    vs1r.v v10, (a2) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    vmxor.mm v8, v8, v9
; CHECK-O0-NEXT:    csrr a2, vlenb
; CHECK-O0-NEXT:    add a2, sp, a2
; CHECK-O0-NEXT:    addi a2, a2, 32
; CHECK-O0-NEXT:    vs1r.v v8, (a2) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    vsetvli a2, zero, e16, m1, ta, mu
; CHECK-O0-NEXT:    vmv.v.i v8, 0
; CHECK-O0-NEXT:    vmerge.vim v10, v8, 1, v0
; CHECK-O0-NEXT:    addi a2, sp, 32
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmerge.vim v9, v8, 1, v0
; CHECK-O0-NEXT:    csrr a2, vlenb
; CHECK-O0-NEXT:    add a2, sp, a2
; CHECK-O0-NEXT:    addi a2, a2, 32
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmerge.vim v8, v8, 1, v0
; CHECK-O0-NEXT:    vsetvli zero, a1, e16, m1, ta, mu
; CHECK-O0-NEXT:    vse16.v v10, (a0)
; CHECK-O0-NEXT:    vse16.v v9, (a0)
; CHECK-O0-NEXT:    vse16.v v8, (a0)
; CHECK-O0-NEXT:    csrr a0, vlenb
; CHECK-O0-NEXT:    slli a0, a0, 1
; CHECK-O0-NEXT:    add sp, sp, a0
; CHECK-O0-NEXT:    addi sp, sp, 32
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_logical_3:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    lui a3, %hi(scratch)
; CHECK-O2-NEXT:    slli a2, a2, 32
; CHECK-O2-NEXT:    srli a2, a2, 32
; CHECK-O2-NEXT:    vsetvli zero, a2, e16, m1, ta, mu
; CHECK-O2-NEXT:    vle16.v v8, (a0)
; CHECK-O2-NEXT:    addi a0, a3, %lo(scratch)
; CHECK-O2-NEXT:    vle16.v v9, (a1)
; CHECK-O2-NEXT:    vsetvli a1, zero, e16, m1, ta, mu
; CHECK-O2-NEXT:    vand.vi v8, v8, 1
; CHECK-O2-NEXT:    vmsne.vi v10, v8, 0
; CHECK-O2-NEXT:    vand.vi v8, v9, 1
; CHECK-O2-NEXT:    vmsne.vi v9, v8, 0
; CHECK-O2-NEXT:    vsetvli zero, a2, e8, mf2, ta, mu
; CHECK-O2-NEXT:    vmand.mm v0, v10, v9
; CHECK-O2-NEXT:    vmor.mm v8, v10, v9
; CHECK-O2-NEXT:    vmxor.mm v9, v10, v9
; CHECK-O2-NEXT:    vsetvli a1, zero, e16, m1, ta, mu
; CHECK-O2-NEXT:    vmv.v.i v10, 0
; CHECK-O2-NEXT:    vmerge.vim v11, v10, 1, v0
; CHECK-O2-NEXT:    vmv1r.v v0, v8
; CHECK-O2-NEXT:    vmerge.vim v8, v10, 1, v0
; CHECK-O2-NEXT:    vmv1r.v v0, v9
; CHECK-O2-NEXT:    vmerge.vim v9, v10, 1, v0
; CHECK-O2-NEXT:    vsetvli zero, a2, e16, m1, ta, mu
; CHECK-O2-NEXT:    vse16.v v11, (a0)
; CHECK-O2-NEXT:    vse16.v v8, (a0)
; CHECK-O2-NEXT:    vse16.v v9, (a0)
; CHECK-O2-NEXT:    ret
  %head = insertelement <vscale x 4 x i1> undef, i1 1, i32 0
  %allones = shufflevector <vscale x 4 x i1> %head, <vscale x 4 x i1> undef, <vscale x 4 x i32> zeroinitializer

  %store_addr = bitcast i8* @scratch to <vscale x 4 x i16>*

  %i0 = call <vscale x 4 x i16> @llvm.vp.load.nxv4i16(<vscale x 4 x i16>* %a0, <vscale x 4 x i1> %allones, i32 %n)
  %i1 = call <vscale x 4 x i16> @llvm.vp.load.nxv4i16(<vscale x 4 x i16>* %a1, <vscale x 4 x i1> %allones, i32 %n)

  %trunc_i0 = trunc <vscale x 4 x i16> %i0 to <vscale x 4 x i1>
  %trunc_i1 = trunc <vscale x 4 x i16> %i1 to <vscale x 4 x i1>

  %r0 = call <vscale x 4 x i1> @llvm.vp.and.nxv4i1(<vscale x 4 x i1> %trunc_i0, <vscale x 4 x i1> %trunc_i1, <vscale x 4 x i1> %allones, i32 %n)
  %r1 = call <vscale x 4 x i1> @llvm.vp.or.nxv4i1(<vscale x 4 x i1> %trunc_i0, <vscale x 4 x i1> %trunc_i1, <vscale x 4 x i1> %allones, i32 %n)
  %r2 = call <vscale x 4 x i1> @llvm.vp.xor.nxv4i1(<vscale x 4 x i1> %trunc_i0, <vscale x 4 x i1> %trunc_i1, <vscale x 4 x i1> %allones, i32 %n)

  %zext_r0 = zext <vscale x 4 x i1> %r0 to <vscale x 4 x i16>
  %zext_r1 = zext <vscale x 4 x i1> %r1 to <vscale x 4 x i16>
  %zext_r2 = zext <vscale x 4 x i1> %r2 to <vscale x 4 x i16>

  call void @llvm.vp.store.nxv4i16(<vscale x 4 x i16> %zext_r0, <vscale x 4 x i16>* %store_addr, <vscale x 4 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv4i16(<vscale x 4 x i16> %zext_r1, <vscale x 4 x i16>* %store_addr, <vscale x 4 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv4i16(<vscale x 4 x i16> %zext_r2, <vscale x 4 x i16>* %store_addr, <vscale x 4 x i1> %allones, i32 %n)

  ret void
}

define void @test_vp_logical_4(<vscale x 8 x i8>* %a0, <vscale x 8 x i8>* %a1, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_logical_4:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -32
; CHECK-O0-NEXT:    csrr a3, vlenb
; CHECK-O0-NEXT:    slli a3, a3, 1
; CHECK-O0-NEXT:    sub sp, sp, a3
; CHECK-O0-NEXT:    sd a2, 24(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    mv a2, a1
; CHECK-O0-NEXT:    ld a1, 24(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    mv a3, a0
; CHECK-O0-NEXT:    # kill: def $x10 killed $x11
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    slli a1, a1, 32
; CHECK-O0-NEXT:    srli a1, a1, 32
; CHECK-O0-NEXT:    vsetvli zero, a1, e8, m1, ta, mu
; CHECK-O0-NEXT:    vle8.v v8, (a3)
; CHECK-O0-NEXT:    vle8.v v9, (a2)
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, m1, ta, mu
; CHECK-O0-NEXT:    vand.vi v8, v8, 1
; CHECK-O0-NEXT:    vmsne.vi v8, v8, 0
; CHECK-O0-NEXT:    vand.vi v9, v9, 1
; CHECK-O0-NEXT:    vmsne.vi v9, v9, 0
; CHECK-O0-NEXT:    vsetvli zero, a1, e8, m1, ta, mu
; CHECK-O0-NEXT:    vmand.mm v0, v8, v9
; CHECK-O0-NEXT:    vmor.mm v10, v8, v9
; CHECK-O0-NEXT:    addi a2, sp, 32
; CHECK-O0-NEXT:    vs1r.v v10, (a2) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    vmxor.mm v8, v8, v9
; CHECK-O0-NEXT:    csrr a2, vlenb
; CHECK-O0-NEXT:    add a2, sp, a2
; CHECK-O0-NEXT:    addi a2, a2, 32
; CHECK-O0-NEXT:    vs1r.v v8, (a2) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, m1, ta, mu
; CHECK-O0-NEXT:    vmv.v.i v8, 0
; CHECK-O0-NEXT:    vmerge.vim v10, v8, 1, v0
; CHECK-O0-NEXT:    addi a2, sp, 32
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmerge.vim v9, v8, 1, v0
; CHECK-O0-NEXT:    csrr a2, vlenb
; CHECK-O0-NEXT:    add a2, sp, a2
; CHECK-O0-NEXT:    addi a2, a2, 32
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmerge.vim v8, v8, 1, v0
; CHECK-O0-NEXT:    vsetvli zero, a1, e8, m1, ta, mu
; CHECK-O0-NEXT:    vse8.v v10, (a0)
; CHECK-O0-NEXT:    vse8.v v9, (a0)
; CHECK-O0-NEXT:    vse8.v v8, (a0)
; CHECK-O0-NEXT:    csrr a0, vlenb
; CHECK-O0-NEXT:    slli a0, a0, 1
; CHECK-O0-NEXT:    add sp, sp, a0
; CHECK-O0-NEXT:    addi sp, sp, 32
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_logical_4:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    lui a3, %hi(scratch)
; CHECK-O2-NEXT:    slli a2, a2, 32
; CHECK-O2-NEXT:    srli a2, a2, 32
; CHECK-O2-NEXT:    vsetvli zero, a2, e8, m1, ta, mu
; CHECK-O2-NEXT:    vle8.v v8, (a0)
; CHECK-O2-NEXT:    addi a0, a3, %lo(scratch)
; CHECK-O2-NEXT:    vle8.v v9, (a1)
; CHECK-O2-NEXT:    vsetvli a1, zero, e8, m1, ta, mu
; CHECK-O2-NEXT:    vand.vi v8, v8, 1
; CHECK-O2-NEXT:    vmsne.vi v10, v8, 0
; CHECK-O2-NEXT:    vand.vi v8, v9, 1
; CHECK-O2-NEXT:    vmsne.vi v9, v8, 0
; CHECK-O2-NEXT:    vsetvli zero, a2, e8, m1, ta, mu
; CHECK-O2-NEXT:    vmand.mm v0, v10, v9
; CHECK-O2-NEXT:    vmor.mm v8, v10, v9
; CHECK-O2-NEXT:    vmxor.mm v9, v10, v9
; CHECK-O2-NEXT:    vsetvli a1, zero, e8, m1, ta, mu
; CHECK-O2-NEXT:    vmv.v.i v10, 0
; CHECK-O2-NEXT:    vmerge.vim v11, v10, 1, v0
; CHECK-O2-NEXT:    vmv1r.v v0, v8
; CHECK-O2-NEXT:    vmerge.vim v8, v10, 1, v0
; CHECK-O2-NEXT:    vmv1r.v v0, v9
; CHECK-O2-NEXT:    vmerge.vim v9, v10, 1, v0
; CHECK-O2-NEXT:    vsetvli zero, a2, e8, m1, ta, mu
; CHECK-O2-NEXT:    vse8.v v11, (a0)
; CHECK-O2-NEXT:    vse8.v v8, (a0)
; CHECK-O2-NEXT:    vse8.v v9, (a0)
; CHECK-O2-NEXT:    ret
  %head = insertelement <vscale x 8 x i1> undef, i1 1, i32 0
  %allones = shufflevector <vscale x 8 x i1> %head, <vscale x 8 x i1> undef, <vscale x 8 x i32> zeroinitializer

  %store_addr = bitcast i8* @scratch to <vscale x 8 x i8>*

  %i0 = call <vscale x 8 x i8> @llvm.vp.load.nxv8i8(<vscale x 8 x i8>* %a0, <vscale x 8 x i1> %allones, i32 %n)
  %i1 = call <vscale x 8 x i8> @llvm.vp.load.nxv8i8(<vscale x 8 x i8>* %a1, <vscale x 8 x i1> %allones, i32 %n)

  %trunc_i0 = trunc <vscale x 8 x i8> %i0 to <vscale x 8 x i1>
  %trunc_i1 = trunc <vscale x 8 x i8> %i1 to <vscale x 8 x i1>

  %r0 = call <vscale x 8 x i1> @llvm.vp.and.nxv8i1(<vscale x 8 x i1> %trunc_i0, <vscale x 8 x i1> %trunc_i1, <vscale x 8 x i1> %allones, i32 %n)
  %r1 = call <vscale x 8 x i1> @llvm.vp.or.nxv8i1(<vscale x 8 x i1> %trunc_i0, <vscale x 8 x i1> %trunc_i1, <vscale x 8 x i1> %allones, i32 %n)
  %r2 = call <vscale x 8 x i1> @llvm.vp.xor.nxv8i1(<vscale x 8 x i1> %trunc_i0, <vscale x 8 x i1> %trunc_i1, <vscale x 8 x i1> %allones, i32 %n)

  %zext_r0 = zext <vscale x 8 x i1> %r0 to <vscale x 8 x i8>
  %zext_r1 = zext <vscale x 8 x i1> %r1 to <vscale x 8 x i8>
  %zext_r2 = zext <vscale x 8 x i1> %r2 to <vscale x 8 x i8>

  call void @llvm.vp.store.nxv8i8(<vscale x 8 x i8> %zext_r0, <vscale x 8 x i8>* %store_addr, <vscale x 8 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv8i8(<vscale x 8 x i8> %zext_r1, <vscale x 8 x i8>* %store_addr, <vscale x 8 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv8i8(<vscale x 8 x i8> %zext_r2, <vscale x 8 x i8>* %store_addr, <vscale x 8 x i1> %allones, i32 %n)

  ret void
}

define void @test_vp_logical_5(<vscale x 16 x i8>* %a0, <vscale x 16 x i8>* %a1, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_logical_5:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -32
; CHECK-O0-NEXT:    csrr a3, vlenb
; CHECK-O0-NEXT:    slli a3, a3, 1
; CHECK-O0-NEXT:    sub sp, sp, a3
; CHECK-O0-NEXT:    sd a2, 24(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    mv a2, a1
; CHECK-O0-NEXT:    ld a1, 24(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    mv a3, a0
; CHECK-O0-NEXT:    # kill: def $x10 killed $x11
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    slli a1, a1, 32
; CHECK-O0-NEXT:    srli a1, a1, 32
; CHECK-O0-NEXT:    vsetvli zero, a1, e8, m2, ta, mu
; CHECK-O0-NEXT:    vle8.v v8, (a3)
; CHECK-O0-NEXT:    vle8.v v10, (a2)
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, m2, ta, mu
; CHECK-O0-NEXT:    vand.vi v12, v8, 1
; CHECK-O0-NEXT:    vmsne.vi v8, v12, 0
; CHECK-O0-NEXT:    vand.vi v10, v10, 1
; CHECK-O0-NEXT:    vmsne.vi v9, v10, 0
; CHECK-O0-NEXT:    vsetvli zero, a1, e8, m2, ta, mu
; CHECK-O0-NEXT:    vmand.mm v0, v8, v9
; CHECK-O0-NEXT:    vmor.mm v10, v8, v9
; CHECK-O0-NEXT:    addi a2, sp, 32
; CHECK-O0-NEXT:    vs1r.v v10, (a2) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    vmxor.mm v8, v8, v9
; CHECK-O0-NEXT:    csrr a2, vlenb
; CHECK-O0-NEXT:    add a2, sp, a2
; CHECK-O0-NEXT:    addi a2, a2, 32
; CHECK-O0-NEXT:    vs1r.v v8, (a2) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, m2, ta, mu
; CHECK-O0-NEXT:    vmv.v.i v8, 0
; CHECK-O0-NEXT:    vmerge.vim v12, v8, 1, v0
; CHECK-O0-NEXT:    addi a2, sp, 32
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmerge.vim v10, v8, 1, v0
; CHECK-O0-NEXT:    csrr a2, vlenb
; CHECK-O0-NEXT:    add a2, sp, a2
; CHECK-O0-NEXT:    addi a2, a2, 32
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmerge.vim v8, v8, 1, v0
; CHECK-O0-NEXT:    vsetvli zero, a1, e8, m2, ta, mu
; CHECK-O0-NEXT:    vse8.v v12, (a0)
; CHECK-O0-NEXT:    vse8.v v10, (a0)
; CHECK-O0-NEXT:    vse8.v v8, (a0)
; CHECK-O0-NEXT:    csrr a0, vlenb
; CHECK-O0-NEXT:    slli a0, a0, 1
; CHECK-O0-NEXT:    add sp, sp, a0
; CHECK-O0-NEXT:    addi sp, sp, 32
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_logical_5:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    lui a3, %hi(scratch)
; CHECK-O2-NEXT:    slli a2, a2, 32
; CHECK-O2-NEXT:    srli a2, a2, 32
; CHECK-O2-NEXT:    vsetvli zero, a2, e8, m2, ta, mu
; CHECK-O2-NEXT:    vle8.v v8, (a0)
; CHECK-O2-NEXT:    addi a0, a3, %lo(scratch)
; CHECK-O2-NEXT:    vle8.v v10, (a1)
; CHECK-O2-NEXT:    vsetvli a1, zero, e8, m2, ta, mu
; CHECK-O2-NEXT:    vand.vi v8, v8, 1
; CHECK-O2-NEXT:    vmsne.vi v12, v8, 0
; CHECK-O2-NEXT:    vand.vi v8, v10, 1
; CHECK-O2-NEXT:    vmsne.vi v10, v8, 0
; CHECK-O2-NEXT:    vsetvli zero, a2, e8, m2, ta, mu
; CHECK-O2-NEXT:    vmand.mm v0, v12, v10
; CHECK-O2-NEXT:    vmor.mm v8, v12, v10
; CHECK-O2-NEXT:    vmxor.mm v9, v12, v10
; CHECK-O2-NEXT:    vsetvli a1, zero, e8, m2, ta, mu
; CHECK-O2-NEXT:    vmv.v.i v10, 0
; CHECK-O2-NEXT:    vmerge.vim v12, v10, 1, v0
; CHECK-O2-NEXT:    vmv1r.v v0, v8
; CHECK-O2-NEXT:    vmerge.vim v14, v10, 1, v0
; CHECK-O2-NEXT:    vmv1r.v v0, v9
; CHECK-O2-NEXT:    vmerge.vim v8, v10, 1, v0
; CHECK-O2-NEXT:    vsetvli zero, a2, e8, m2, ta, mu
; CHECK-O2-NEXT:    vse8.v v12, (a0)
; CHECK-O2-NEXT:    vse8.v v14, (a0)
; CHECK-O2-NEXT:    vse8.v v8, (a0)
; CHECK-O2-NEXT:    ret
  %head = insertelement <vscale x 16 x i1> undef, i1 1, i32 0
  %allones = shufflevector <vscale x 16 x i1> %head, <vscale x 16 x i1> undef, <vscale x 16 x i32> zeroinitializer

  %store_addr = bitcast i8* @scratch to <vscale x 16 x i8>*

  %i0 = call <vscale x 16 x i8> @llvm.vp.load.nxv16i8(<vscale x 16 x i8>* %a0, <vscale x 16 x i1> %allones, i32 %n)
  %i1 = call <vscale x 16 x i8> @llvm.vp.load.nxv16i8(<vscale x 16 x i8>* %a1, <vscale x 16 x i1> %allones, i32 %n)

  %trunc_i0 = trunc <vscale x 16 x i8> %i0 to <vscale x 16 x i1>
  %trunc_i1 = trunc <vscale x 16 x i8> %i1 to <vscale x 16 x i1>

  %r0 = call <vscale x 16 x i1> @llvm.vp.and.nxv16i1(<vscale x 16 x i1> %trunc_i0, <vscale x 16 x i1> %trunc_i1, <vscale x 16 x i1> %allones, i32 %n)
  %r1 = call <vscale x 16 x i1> @llvm.vp.or.nxv16i1(<vscale x 16 x i1> %trunc_i0, <vscale x 16 x i1> %trunc_i1, <vscale x 16 x i1> %allones, i32 %n)
  %r2 = call <vscale x 16 x i1> @llvm.vp.xor.nxv16i1(<vscale x 16 x i1> %trunc_i0, <vscale x 16 x i1> %trunc_i1, <vscale x 16 x i1> %allones, i32 %n)

  %zext_r0 = zext <vscale x 16 x i1> %r0 to <vscale x 16 x i8>
  %zext_r1 = zext <vscale x 16 x i1> %r1 to <vscale x 16 x i8>
  %zext_r2 = zext <vscale x 16 x i1> %r2 to <vscale x 16 x i8>

  call void @llvm.vp.store.nxv16i8(<vscale x 16 x i8> %zext_r0, <vscale x 16 x i8>* %store_addr, <vscale x 16 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv16i8(<vscale x 16 x i8> %zext_r1, <vscale x 16 x i8>* %store_addr, <vscale x 16 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv16i8(<vscale x 16 x i8> %zext_r2, <vscale x 16 x i8>* %store_addr, <vscale x 16 x i1> %allones, i32 %n)

  ret void
}

define void @test_vp_logical_6(<vscale x 32 x i8>* %a0, <vscale x 32 x i8>* %a1, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_logical_6:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -32
; CHECK-O0-NEXT:    csrr a3, vlenb
; CHECK-O0-NEXT:    slli a3, a3, 1
; CHECK-O0-NEXT:    sub sp, sp, a3
; CHECK-O0-NEXT:    sd a2, 24(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    mv a2, a1
; CHECK-O0-NEXT:    ld a1, 24(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    mv a3, a0
; CHECK-O0-NEXT:    # kill: def $x10 killed $x11
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    slli a1, a1, 32
; CHECK-O0-NEXT:    srli a1, a1, 32
; CHECK-O0-NEXT:    vsetvli zero, a1, e8, m4, ta, mu
; CHECK-O0-NEXT:    vle8.v v8, (a3)
; CHECK-O0-NEXT:    vle8.v v12, (a2)
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, m4, ta, mu
; CHECK-O0-NEXT:    vand.vi v16, v8, 1
; CHECK-O0-NEXT:    vmsne.vi v8, v16, 0
; CHECK-O0-NEXT:    vand.vi v12, v12, 1
; CHECK-O0-NEXT:    vmsne.vi v9, v12, 0
; CHECK-O0-NEXT:    vsetvli zero, a1, e8, m4, ta, mu
; CHECK-O0-NEXT:    vmand.mm v0, v8, v9
; CHECK-O0-NEXT:    vmor.mm v10, v8, v9
; CHECK-O0-NEXT:    addi a2, sp, 32
; CHECK-O0-NEXT:    vs1r.v v10, (a2) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    vmxor.mm v8, v8, v9
; CHECK-O0-NEXT:    csrr a2, vlenb
; CHECK-O0-NEXT:    add a2, sp, a2
; CHECK-O0-NEXT:    addi a2, a2, 32
; CHECK-O0-NEXT:    vs1r.v v8, (a2) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, m4, ta, mu
; CHECK-O0-NEXT:    vmv.v.i v8, 0
; CHECK-O0-NEXT:    vmerge.vim v16, v8, 1, v0
; CHECK-O0-NEXT:    addi a2, sp, 32
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmerge.vim v12, v8, 1, v0
; CHECK-O0-NEXT:    csrr a2, vlenb
; CHECK-O0-NEXT:    add a2, sp, a2
; CHECK-O0-NEXT:    addi a2, a2, 32
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vmerge.vim v8, v8, 1, v0
; CHECK-O0-NEXT:    vsetvli zero, a1, e8, m4, ta, mu
; CHECK-O0-NEXT:    vse8.v v16, (a0)
; CHECK-O0-NEXT:    vse8.v v12, (a0)
; CHECK-O0-NEXT:    vse8.v v8, (a0)
; CHECK-O0-NEXT:    csrr a0, vlenb
; CHECK-O0-NEXT:    slli a0, a0, 1
; CHECK-O0-NEXT:    add sp, sp, a0
; CHECK-O0-NEXT:    addi sp, sp, 32
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_logical_6:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    lui a3, %hi(scratch)
; CHECK-O2-NEXT:    slli a2, a2, 32
; CHECK-O2-NEXT:    srli a2, a2, 32
; CHECK-O2-NEXT:    vsetvli zero, a2, e8, m4, ta, mu
; CHECK-O2-NEXT:    vle8.v v8, (a0)
; CHECK-O2-NEXT:    addi a0, a3, %lo(scratch)
; CHECK-O2-NEXT:    vle8.v v12, (a1)
; CHECK-O2-NEXT:    vsetvli a1, zero, e8, m4, ta, mu
; CHECK-O2-NEXT:    vand.vi v8, v8, 1
; CHECK-O2-NEXT:    vmsne.vi v16, v8, 0
; CHECK-O2-NEXT:    vand.vi v8, v12, 1
; CHECK-O2-NEXT:    vmsne.vi v12, v8, 0
; CHECK-O2-NEXT:    vsetvli zero, a2, e8, m4, ta, mu
; CHECK-O2-NEXT:    vmand.mm v0, v16, v12
; CHECK-O2-NEXT:    vmor.mm v8, v16, v12
; CHECK-O2-NEXT:    vmxor.mm v9, v16, v12
; CHECK-O2-NEXT:    vsetvli a1, zero, e8, m4, ta, mu
; CHECK-O2-NEXT:    vmv.v.i v12, 0
; CHECK-O2-NEXT:    vmerge.vim v16, v12, 1, v0
; CHECK-O2-NEXT:    vmv1r.v v0, v8
; CHECK-O2-NEXT:    vmerge.vim v20, v12, 1, v0
; CHECK-O2-NEXT:    vmv1r.v v0, v9
; CHECK-O2-NEXT:    vmerge.vim v8, v12, 1, v0
; CHECK-O2-NEXT:    vsetvli zero, a2, e8, m4, ta, mu
; CHECK-O2-NEXT:    vse8.v v16, (a0)
; CHECK-O2-NEXT:    vse8.v v20, (a0)
; CHECK-O2-NEXT:    vse8.v v8, (a0)
; CHECK-O2-NEXT:    ret
  %head = insertelement <vscale x 32 x i1> undef, i1 1, i32 0
  %allones = shufflevector <vscale x 32 x i1> %head, <vscale x 32 x i1> undef, <vscale x 32 x i32> zeroinitializer

  %store_addr = bitcast i8* @scratch to <vscale x 32 x i8>*

  %i0 = call <vscale x 32 x i8> @llvm.vp.load.nxv32i8(<vscale x 32 x i8>* %a0, <vscale x 32 x i1> %allones, i32 %n)
  %i1 = call <vscale x 32 x i8> @llvm.vp.load.nxv32i8(<vscale x 32 x i8>* %a1, <vscale x 32 x i1> %allones, i32 %n)

  %trunc_i0 = trunc <vscale x 32 x i8> %i0 to <vscale x 32 x i1>
  %trunc_i1 = trunc <vscale x 32 x i8> %i1 to <vscale x 32 x i1>

  %r0 = call <vscale x 32 x i1> @llvm.vp.and.nxv32i1(<vscale x 32 x i1> %trunc_i0, <vscale x 32 x i1> %trunc_i1, <vscale x 32 x i1> %allones, i32 %n)
  %r1 = call <vscale x 32 x i1> @llvm.vp.or.nxv32i1(<vscale x 32 x i1> %trunc_i0, <vscale x 32 x i1> %trunc_i1, <vscale x 32 x i1> %allones, i32 %n)
  %r2 = call <vscale x 32 x i1> @llvm.vp.xor.nxv32i1(<vscale x 32 x i1> %trunc_i0, <vscale x 32 x i1> %trunc_i1, <vscale x 32 x i1> %allones, i32 %n)

  %zext_r0 = zext <vscale x 32 x i1> %r0 to <vscale x 32 x i8>
  %zext_r1 = zext <vscale x 32 x i1> %r1 to <vscale x 32 x i8>
  %zext_r2 = zext <vscale x 32 x i1> %r2 to <vscale x 32 x i8>

  call void @llvm.vp.store.nxv32i8(<vscale x 32 x i8> %zext_r0, <vscale x 32 x i8>* %store_addr, <vscale x 32 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv32i8(<vscale x 32 x i8> %zext_r1, <vscale x 32 x i8>* %store_addr, <vscale x 32 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv32i8(<vscale x 32 x i8> %zext_r2, <vscale x 32 x i8>* %store_addr, <vscale x 32 x i1> %allones, i32 %n)

  ret void
}

; FIXME: Enable when nxv64i8 is supported.
;define void @test_vp_logical_7(<vscale x 64 x i8>* %a0, <vscale x 64 x i8>* %a1, i32 %n) nounwind {
;  %head = insertelement <vscale x 64 x i1> undef, i1 1, i32 0
;  %allones = shufflevector <vscale x 64 x i1> %head, <vscale x 64 x i1> undef, <vscale x 64 x i32> zeroinitializer
;
;  %store_addr = bitcast i8* @scratch to <vscale x 64 x i8>*
;
;  %i0 = call <vscale x 64 x i8> @llvm.vp.load.nxv64i8(<vscale x 64 x i8>* %a0, <vscale x 64 x i1> %allones, i32 %n)
;  %i1 = call <vscale x 64 x i8> @llvm.vp.load.nxv64i8(<vscale x 64 x i8>* %a1, <vscale x 64 x i1> %allones, i32 %n)
;
;  %trunc_i0 = trunc <vscale x 64 x i8> %i0 to <vscale x 64 x i1>
;  %trunc_i1 = trunc <vscale x 64 x i8> %i1 to <vscale x 64 x i1>
;
;  %r0 = call <vscale x 64 x i1> @llvm.vp.and.nxv64i1(<vscale x 64 x i1> %trunc_i0, <vscale x 64 x i1> %trunc_i1, <vscale x 64 x i1> %allones, i32 %n)
;  %r1 = call <vscale x 64 x i1> @llvm.vp.or.nxv64i1(<vscale x 64 x i1> %trunc_i0, <vscale x 64 x i1> %trunc_i1, <vscale x 64 x i1> %allones, i32 %n)
;  %r2 = call <vscale x 64 x i1> @llvm.vp.xor.nxv64i1(<vscale x 64 x i1> %trunc_i0, <vscale x 64 x i1> %trunc_i1, <vscale x 64 x i1> %allones, i32 %n)
;
;  %zext_r0 = zext <vscale x 64 x i1> %r0 to <vscale x 64 x i8>
;  %zext_r1 = zext <vscale x 64 x i1> %r1 to <vscale x 64 x i8>
;  %zext_r2 = zext <vscale x 64 x i1> %r2 to <vscale x 64 x i8>
;
;  call void @llvm.vp.store.nxv64i8(<vscale x 64 x i8> %zext_r0, <vscale x 64 x i8>* %store_addr, <vscale x 64 x i1> %allones, i32 %n)
;  call void @llvm.vp.store.nxv64i8(<vscale x 64 x i8> %zext_r1, <vscale x 64 x i8>* %store_addr, <vscale x 64 x i1> %allones, i32 %n)
;  call void @llvm.vp.store.nxv64i8(<vscale x 64 x i8> %zext_r2, <vscale x 64 x i8>* %store_addr, <vscale x 64 x i1> %allones, i32 %n)
;
;  ret void
;}

declare <vscale x 1 x i64> @llvm.vp.load.nxv1i64(<vscale x 1 x i64>*, <vscale x 1 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>*, <vscale x 2 x i1>, i32)
declare <vscale x 4 x i16> @llvm.vp.load.nxv4i16(<vscale x 4 x i16>*, <vscale x 4 x i1>, i32)
declare <vscale x 8 x i8> @llvm.vp.load.nxv8i8(<vscale x 8 x i8>*, <vscale x 8 x i1>, i32)
declare <vscale x 16 x i8> @llvm.vp.load.nxv16i8(<vscale x 16 x i8>*, <vscale x 16 x i1>, i32)
declare <vscale x 32 x i8> @llvm.vp.load.nxv32i8(<vscale x 32 x i8>*, <vscale x 32 x i1>, i32)
declare <vscale x 64 x i8> @llvm.vp.load.nxv64i8(<vscale x 64 x i8>*, <vscale x 64 x i1>, i32)

declare void @llvm.vp.store.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>*, <vscale x 1 x i1>, i32)
declare void @llvm.vp.store.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>*, <vscale x 2 x i1>, i32)
declare void @llvm.vp.store.nxv4i16(<vscale x 4 x i16>, <vscale x 4 x i16>*, <vscale x 4 x i1>, i32)
declare void @llvm.vp.store.nxv8i8(<vscale x 8 x i8>, <vscale x 8 x i8>*, <vscale x 8 x i1>, i32)
declare void @llvm.vp.store.nxv16i8(<vscale x 16 x i8>, <vscale x 16 x i8>*, <vscale x 16 x i1>, i32)
declare void @llvm.vp.store.nxv32i8(<vscale x 32 x i8>, <vscale x 32 x i8>*, <vscale x 32 x i1>, i32)
declare void @llvm.vp.store.nxv64i8(<vscale x 64 x i8>, <vscale x 64 x i8>*, <vscale x 64 x i1>, i32)

declare <vscale x 1 x i1> @llvm.vp.and.nxv1i1(<vscale x 1 x i1>, <vscale x 1 x i1>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i1> @llvm.vp.or.nxv1i1(<vscale x 1 x i1>, <vscale x 1 x i1>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i1> @llvm.vp.xor.nxv1i1(<vscale x 1 x i1>, <vscale x 1 x i1>, <vscale x 1 x i1>, i32)

declare <vscale x 2 x i1> @llvm.vp.and.nxv2i1(<vscale x 2 x i1>, <vscale x 2 x i1>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i1> @llvm.vp.or.nxv2i1(<vscale x 2 x i1>, <vscale x 2 x i1>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i1> @llvm.vp.xor.nxv2i1(<vscale x 2 x i1>, <vscale x 2 x i1>, <vscale x 2 x i1>, i32)

declare <vscale x 4 x i1> @llvm.vp.and.nxv4i1(<vscale x 4 x i1>, <vscale x 4 x i1>, <vscale x 4 x i1>, i32)
declare <vscale x 4 x i1> @llvm.vp.or.nxv4i1(<vscale x 4 x i1>, <vscale x 4 x i1>, <vscale x 4 x i1>, i32)
declare <vscale x 4 x i1> @llvm.vp.xor.nxv4i1(<vscale x 4 x i1>, <vscale x 4 x i1>, <vscale x 4 x i1>, i32)

declare <vscale x 8 x i1> @llvm.vp.and.nxv8i1(<vscale x 8 x i1>, <vscale x 8 x i1>, <vscale x 8 x i1>, i32)
declare <vscale x 8 x i1> @llvm.vp.or.nxv8i1(<vscale x 8 x i1>, <vscale x 8 x i1>, <vscale x 8 x i1>, i32)
declare <vscale x 8 x i1> @llvm.vp.xor.nxv8i1(<vscale x 8 x i1>, <vscale x 8 x i1>, <vscale x 8 x i1>, i32)

declare <vscale x 16 x i1> @llvm.vp.and.nxv16i1(<vscale x 16 x i1>, <vscale x 16 x i1>, <vscale x 16 x i1>, i32)
declare <vscale x 16 x i1> @llvm.vp.or.nxv16i1(<vscale x 16 x i1>, <vscale x 16 x i1>, <vscale x 16 x i1>, i32)
declare <vscale x 16 x i1> @llvm.vp.xor.nxv16i1(<vscale x 16 x i1>, <vscale x 16 x i1>, <vscale x 16 x i1>, i32)

declare <vscale x 32 x i1> @llvm.vp.and.nxv32i1(<vscale x 32 x i1>, <vscale x 32 x i1>, <vscale x 32 x i1>, i32)
declare <vscale x 32 x i1> @llvm.vp.or.nxv32i1(<vscale x 32 x i1>, <vscale x 32 x i1>, <vscale x 32 x i1>, i32)
declare <vscale x 32 x i1> @llvm.vp.xor.nxv32i1(<vscale x 32 x i1>, <vscale x 32 x i1>, <vscale x 32 x i1>, i32)

declare <vscale x 64 x i1> @llvm.vp.and.nxv64i1(<vscale x 64 x i1>, <vscale x 64 x i1>, <vscale x 64 x i1>, i32)
declare <vscale x 64 x i1> @llvm.vp.or.nxv64i1(<vscale x 64 x i1>, <vscale x 64 x i1>, <vscale x 64 x i1>, i32)
declare <vscale x 64 x i1> @llvm.vp.xor.nxv64i1(<vscale x 64 x i1>, <vscale x 64 x i1>, <vscale x 64 x i1>, i32)
