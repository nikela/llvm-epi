; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+f,+d,+experimental-v -verify-machineinstrs -O0 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=SPILL-O0 %s
; RUN: llc -mtriple=riscv64 -mattr=+f,+d,+experimental-v -verify-machineinstrs -O2 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=SPILL-O2 %s

define dso_local void @bar(double* nocapture %pa, double* nocapture readonly %pb, i64 %gvl) nounwind {
; SPILL-O0-LABEL: bar:
; SPILL-O0:       # %bb.0: # %entry
; SPILL-O0-NEXT:    addi sp, sp, -64
; SPILL-O0-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; SPILL-O0-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; SPILL-O0-NEXT:    addi s0, sp, 64
; SPILL-O0-NEXT:    rdvlenb a3
; SPILL-O0-NEXT:    sub sp, sp, a3
; SPILL-O0-NEXT:    sd sp, -56(s0)
; SPILL-O0-NEXT:    sub sp, sp, a3
; SPILL-O0-NEXT:    sd sp, -64(s0)
; SPILL-O0-NEXT:    sd a2, -48(s0) # 8-byte Folded Spill
; SPILL-O0-NEXT:    sd a0, -40(s0) # 8-byte Folded Spill
; SPILL-O0-NEXT:    # implicit-def: $v25
; SPILL-O0-NEXT:    vsetvli a2, a2, e64,m1,ta,mu
; SPILL-O0-NEXT:    vle64.v v25, (a0)
; SPILL-O0-NEXT:    # implicit-def: $v26
; SPILL-O0-NEXT:    vle64.v v26, (a1)
; SPILL-O0-NEXT:    # implicit-def: $v1_v2
; SPILL-O0-NEXT:    vzip2.vv v1, v25, v26
; SPILL-O0-NEXT:    vmv1r.v v25, v2
; SPILL-O0-NEXT:    ld a0, -56(s0)
; SPILL-O0-NEXT:    vs1r.v v25, (a0)
; SPILL-O0-NEXT:    vmv1r.v v25, v1
; SPILL-O0-NEXT:    ld a0, -64(s0)
; SPILL-O0-NEXT:    vs1r.v v25, (a0)
; SPILL-O0-NEXT:    call foo
; SPILL-O0-NEXT:    ld a2, -48(s0) # 8-byte Folded Reload
; SPILL-O0-NEXT:    ld a0, -64(s0)
; SPILL-O0-NEXT:    vl1r.v v26, (a0)
; SPILL-O0-NEXT:    ld a0, -40(s0) # 8-byte Folded Reload
; SPILL-O0-NEXT:    ld a1, -56(s0)
; SPILL-O0-NEXT:    vl1r.v v25, (a1)
; SPILL-O0-NEXT:    vsetvli a1, a2, e64,m1,ta,mu
; SPILL-O0-NEXT:    vse64.v v26, (a0)
; SPILL-O0-NEXT:    vse64.v v25, (a0)
; SPILL-O0-NEXT:    addi sp, s0, -64
; SPILL-O0-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; SPILL-O0-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; SPILL-O0-NEXT:    addi sp, sp, 64
; SPILL-O0-NEXT:    ret
;
; SPILL-O2-LABEL: bar:
; SPILL-O2:       # %bb.0: # %entry
; SPILL-O2-NEXT:    addi sp, sp, -64
; SPILL-O2-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; SPILL-O2-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; SPILL-O2-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; SPILL-O2-NEXT:    sd s2, 32(sp) # 8-byte Folded Spill
; SPILL-O2-NEXT:    addi s0, sp, 64
; SPILL-O2-NEXT:    rdvlenb a3
; SPILL-O2-NEXT:    slli a3, a3, 1
; SPILL-O2-NEXT:    sub sp, sp, a3
; SPILL-O2-NEXT:    sd sp, -56(s0)
; SPILL-O2-NEXT:    mv s2, a2
; SPILL-O2-NEXT:    mv s1, a0
; SPILL-O2-NEXT:    vsetvli a0, a2, e64,m1,ta,mu
; SPILL-O2-NEXT:    vle64.v v25, (s1)
; SPILL-O2-NEXT:    vle64.v v26, (a1)
; SPILL-O2-NEXT:    vzip2.vv v1, v25, v26
; SPILL-O2-NEXT:    ld a1, -56(s0)
; SPILL-O2-NEXT:    rdvlenb a0
; SPILL-O2-NEXT:    add a0, a1, a0
; SPILL-O2-NEXT:    vs1r.v v1, (a1)
; SPILL-O2-NEXT:    vs1r.v v2, (a0)
; SPILL-O2-NEXT:    call foo
; SPILL-O2-NEXT:    vsetvli a0, s2, e64,m1,ta,mu
; SPILL-O2-NEXT:    ld a1, -56(s0)
; SPILL-O2-NEXT:    rdvlenb a0
; SPILL-O2-NEXT:    add a0, a1, a0
; SPILL-O2-NEXT:    vl1r.v v1, (a1)
; SPILL-O2-NEXT:    vl1r.v v2, (a0)
; SPILL-O2-NEXT:    vse64.v v1, (s1)
; SPILL-O2-NEXT:    vse64.v v2, (s1)
; SPILL-O2-NEXT:    addi sp, s0, -64
; SPILL-O2-NEXT:    ld s2, 32(sp) # 8-byte Folded Reload
; SPILL-O2-NEXT:    ld s1, 40(sp) # 8-byte Folded Reload
; SPILL-O2-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; SPILL-O2-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; SPILL-O2-NEXT:    addi sp, sp, 64
; SPILL-O2-NEXT:    ret
entry:
  %0 = bitcast double* %pa to <vscale x 1 x double>*
  %1 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %0, i64 %gvl)
  %2 = bitcast double* %pb to <vscale x 1 x double>*
  %3 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %2, i64 %gvl)
  %4 = tail call { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vzip2.nxv1f64(<vscale x 1 x double> %1, <vscale x 1 x double> %3, i64 %gvl)
  %5 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %4, 0
  %6 = extractvalue { <vscale x 1 x double>, <vscale x 1 x double> } %4, 1
  tail call void @foo() #5
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %5, <vscale x 1 x double>* %0, i64 %gvl)
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %6, <vscale x 1 x double>* %0, i64 %gvl)
  ret void
}

declare <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* nocapture, i64)

declare { <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vzip2.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64)

declare dso_local void @foo()

declare void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>* nocapture, i64)

