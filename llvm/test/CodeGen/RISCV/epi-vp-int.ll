; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+m,+v -verify-machineinstrs -O0 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=CHECK-O0 %s
; RUN: llc -mtriple=riscv64 -mattr=+m,+v -verify-machineinstrs -O2 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=CHECK-O2 %s

@scratch = global i8 0, align 16

define void @test_vp_int(<vscale x 1 x i64>* %a0, <vscale x 1 x i64>* %a1, <vscale x 1 x i1> %m, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_int:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    csrr a3, vlenb
; CHECK-O0-NEXT:    slli a3, a3, 1
; CHECK-O0-NEXT:    sub sp, sp, a3
; CHECK-O0-NEXT:    mv a3, a2
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vs1r.v v0, (a2) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    mv a2, a0
; CHECK-O0-NEXT:    # kill: def $x10 killed $x13
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    slli a3, a3, 32
; CHECK-O0-NEXT:    srli a3, a3, 32
; CHECK-O0-NEXT:    # implicit-def: $v21
; CHECK-O0-NEXT:    vsetvli zero, a3, e64, m1, ta, mu
; CHECK-O0-NEXT:    vle64.v v21, (a2), v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v22
; CHECK-O0-NEXT:    vle64.v v22, (a1), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v20
; CHECK-O0-NEXT:    vadd.vv v20, v21, v22, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v19
; CHECK-O0-NEXT:    vsub.vv v19, v21, v22, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v18
; CHECK-O0-NEXT:    vmul.vv v18, v21, v22, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v17
; CHECK-O0-NEXT:    vdiv.vv v17, v21, v22, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v16
; CHECK-O0-NEXT:    vrem.vv v16, v21, v22, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v15
; CHECK-O0-NEXT:    vdivu.vv v15, v21, v22, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v14
; CHECK-O0-NEXT:    vremu.vv v14, v21, v22, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v13
; CHECK-O0-NEXT:    vand.vv v13, v21, v22, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v12
; CHECK-O0-NEXT:    vor.vv v12, v21, v22, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v11
; CHECK-O0-NEXT:    vxor.vv v11, v21, v22, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v10
; CHECK-O0-NEXT:    vsra.vv v10, v21, v22, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v9
; CHECK-O0-NEXT:    vsrl.vv v9, v21, v22, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vsll.vv v8, v21, v22, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v20, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v19, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v18, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v17, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v16, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v15, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v14, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v13, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v12, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v11, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v10, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v9, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v8, (a0), v0.t
; CHECK-O0-NEXT:    csrr a0, vlenb
; CHECK-O0-NEXT:    slli a0, a0, 1
; CHECK-O0-NEXT:    add sp, sp, a0
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_int:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    slli a2, a2, 32
; CHECK-O2-NEXT:    srli a2, a2, 32
; CHECK-O2-NEXT:    vsetvli zero, a2, e64, m1, ta, mu
; CHECK-O2-NEXT:    vle64.v v8, (a0), v0.t
; CHECK-O2-NEXT:    vle64.v v9, (a1), v0.t
; CHECK-O2-NEXT:    lui a0, %hi(scratch)
; CHECK-O2-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O2-NEXT:    vadd.vv v10, v8, v9, v0.t
; CHECK-O2-NEXT:    vsub.vv v11, v8, v9, v0.t
; CHECK-O2-NEXT:    vmul.vv v12, v8, v9, v0.t
; CHECK-O2-NEXT:    vdiv.vv v13, v8, v9, v0.t
; CHECK-O2-NEXT:    vrem.vv v14, v8, v9, v0.t
; CHECK-O2-NEXT:    vdivu.vv v15, v8, v9, v0.t
; CHECK-O2-NEXT:    vremu.vv v16, v8, v9, v0.t
; CHECK-O2-NEXT:    vand.vv v17, v8, v9, v0.t
; CHECK-O2-NEXT:    vor.vv v18, v8, v9, v0.t
; CHECK-O2-NEXT:    vxor.vv v19, v8, v9, v0.t
; CHECK-O2-NEXT:    vsra.vv v20, v8, v9, v0.t
; CHECK-O2-NEXT:    vsrl.vv v21, v8, v9, v0.t
; CHECK-O2-NEXT:    vsll.vv v8, v8, v9, v0.t
; CHECK-O2-NEXT:    vse64.v v10, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v11, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v12, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v13, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v14, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v15, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v16, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v17, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v18, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v19, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v20, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v21, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v8, (a0), v0.t
; CHECK-O2-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 1 x i64>*

  %i0 = call <vscale x 1 x i64> @llvm.vp.load.nxv1i64(<vscale x 1 x i64>* %a0, <vscale x 1 x i1> %m, i32 %n)
  %i1 = call <vscale x 1 x i64> @llvm.vp.load.nxv1i64(<vscale x 1 x i64>* %a1, <vscale x 1 x i1> %m, i32 %n)

  %r0 = call <vscale x 1 x i64> @llvm.vp.add.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r1 = call <vscale x 1 x i64> @llvm.vp.sub.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r2 = call <vscale x 1 x i64> @llvm.vp.mul.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r3 = call <vscale x 1 x i64> @llvm.vp.sdiv.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r4 = call <vscale x 1 x i64> @llvm.vp.srem.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r5 = call <vscale x 1 x i64> @llvm.vp.udiv.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r6 = call <vscale x 1 x i64> @llvm.vp.urem.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r7 = call <vscale x 1 x i64> @llvm.vp.and.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r8 = call <vscale x 1 x i64> @llvm.vp.or.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r9 = call <vscale x 1 x i64> @llvm.vp.xor.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r10 = call <vscale x 1 x i64> @llvm.vp.ashr.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r11 = call <vscale x 1 x i64> @llvm.vp.lshr.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r12 = call <vscale x 1 x i64> @llvm.vp.shl.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)

  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r0, <vscale x 1 x i64>* %store_addr, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r1, <vscale x 1 x i64>* %store_addr, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r2, <vscale x 1 x i64>* %store_addr, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r3, <vscale x 1 x i64>* %store_addr, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r4, <vscale x 1 x i64>* %store_addr, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r5, <vscale x 1 x i64>* %store_addr, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r6, <vscale x 1 x i64>* %store_addr, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r7, <vscale x 1 x i64>* %store_addr, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r8, <vscale x 1 x i64>* %store_addr, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r9, <vscale x 1 x i64>* %store_addr, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r10, <vscale x 1 x i64>* %store_addr, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r11, <vscale x 1 x i64>* %store_addr, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r12, <vscale x 1 x i64>* %store_addr, <vscale x 1 x i1> %m, i32 %n)

  ret void
}

define void @test_vp_int_2(<vscale x 2 x i32>* %a0, <vscale x 2 x i32>* %a1, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_int_2:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    mv a3, a2
; CHECK-O0-NEXT:    mv a2, a0
; CHECK-O0-NEXT:    # kill: def $x10 killed $x13
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    slli a3, a3, 32
; CHECK-O0-NEXT:    srli a3, a3, 32
; CHECK-O0-NEXT:    vsetvli zero, a3, e32, m1, ta, ma
; CHECK-O0-NEXT:    vle32.v v8, (a2)
; CHECK-O0-NEXT:    vle32.v v21, (a1)
; CHECK-O0-NEXT:    vadd.vv v20, v8, v21
; CHECK-O0-NEXT:    vsub.vv v19, v8, v21
; CHECK-O0-NEXT:    vmul.vv v18, v8, v21
; CHECK-O0-NEXT:    vdiv.vv v17, v8, v21
; CHECK-O0-NEXT:    vrem.vv v16, v8, v21
; CHECK-O0-NEXT:    vdivu.vv v15, v8, v21
; CHECK-O0-NEXT:    vremu.vv v14, v8, v21
; CHECK-O0-NEXT:    vand.vv v13, v8, v21
; CHECK-O0-NEXT:    vor.vv v12, v8, v21
; CHECK-O0-NEXT:    vxor.vv v11, v8, v21
; CHECK-O0-NEXT:    vsra.vv v10, v8, v21
; CHECK-O0-NEXT:    vsrl.vv v9, v8, v21
; CHECK-O0-NEXT:    vsll.vv v8, v8, v21
; CHECK-O0-NEXT:    vse32.v v20, (a0)
; CHECK-O0-NEXT:    vse32.v v19, (a0)
; CHECK-O0-NEXT:    vse32.v v18, (a0)
; CHECK-O0-NEXT:    vse32.v v17, (a0)
; CHECK-O0-NEXT:    vse32.v v16, (a0)
; CHECK-O0-NEXT:    vse32.v v15, (a0)
; CHECK-O0-NEXT:    vse32.v v14, (a0)
; CHECK-O0-NEXT:    vse32.v v13, (a0)
; CHECK-O0-NEXT:    vse32.v v12, (a0)
; CHECK-O0-NEXT:    vse32.v v11, (a0)
; CHECK-O0-NEXT:    vse32.v v10, (a0)
; CHECK-O0-NEXT:    vse32.v v9, (a0)
; CHECK-O0-NEXT:    vse32.v v8, (a0)
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_int_2:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    slli a2, a2, 32
; CHECK-O2-NEXT:    srli a2, a2, 32
; CHECK-O2-NEXT:    vsetvli zero, a2, e32, m1, ta, ma
; CHECK-O2-NEXT:    vle32.v v8, (a0)
; CHECK-O2-NEXT:    vle32.v v9, (a1)
; CHECK-O2-NEXT:    lui a0, %hi(scratch)
; CHECK-O2-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O2-NEXT:    vadd.vv v10, v8, v9
; CHECK-O2-NEXT:    vsub.vv v11, v8, v9
; CHECK-O2-NEXT:    vmul.vv v12, v8, v9
; CHECK-O2-NEXT:    vdiv.vv v13, v8, v9
; CHECK-O2-NEXT:    vrem.vv v14, v8, v9
; CHECK-O2-NEXT:    vdivu.vv v15, v8, v9
; CHECK-O2-NEXT:    vremu.vv v16, v8, v9
; CHECK-O2-NEXT:    vand.vv v17, v8, v9
; CHECK-O2-NEXT:    vor.vv v18, v8, v9
; CHECK-O2-NEXT:    vxor.vv v19, v8, v9
; CHECK-O2-NEXT:    vsra.vv v20, v8, v9
; CHECK-O2-NEXT:    vsrl.vv v21, v8, v9
; CHECK-O2-NEXT:    vsll.vv v8, v8, v9
; CHECK-O2-NEXT:    vse32.v v10, (a0)
; CHECK-O2-NEXT:    vse32.v v11, (a0)
; CHECK-O2-NEXT:    vse32.v v12, (a0)
; CHECK-O2-NEXT:    vse32.v v13, (a0)
; CHECK-O2-NEXT:    vse32.v v14, (a0)
; CHECK-O2-NEXT:    vse32.v v15, (a0)
; CHECK-O2-NEXT:    vse32.v v16, (a0)
; CHECK-O2-NEXT:    vse32.v v17, (a0)
; CHECK-O2-NEXT:    vse32.v v18, (a0)
; CHECK-O2-NEXT:    vse32.v v19, (a0)
; CHECK-O2-NEXT:    vse32.v v20, (a0)
; CHECK-O2-NEXT:    vse32.v v21, (a0)
; CHECK-O2-NEXT:    vse32.v v8, (a0)
; CHECK-O2-NEXT:    ret
  %head = insertelement <vscale x 2 x i1> undef, i1 1, i32 0
  %allones = shufflevector <vscale x 2 x i1> %head, <vscale x 2 x i1> undef, <vscale x 2 x i32> zeroinitializer

  %store_addr = bitcast i8* @scratch to <vscale x 2 x i32>*

  %i0 = call <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>* %a0, <vscale x 2 x i1> %allones, i32 %n)
  %i1 = call <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>* %a1, <vscale x 2 x i1> %allones, i32 %n)

  %r0 = call <vscale x 2 x i32> @llvm.vp.add.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r1 = call <vscale x 2 x i32> @llvm.vp.sub.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r2 = call <vscale x 2 x i32> @llvm.vp.mul.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r3 = call <vscale x 2 x i32> @llvm.vp.sdiv.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r4 = call <vscale x 2 x i32> @llvm.vp.srem.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r5 = call <vscale x 2 x i32> @llvm.vp.udiv.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r6 = call <vscale x 2 x i32> @llvm.vp.urem.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r7 = call <vscale x 2 x i32> @llvm.vp.and.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r8 = call <vscale x 2 x i32> @llvm.vp.or.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r9 = call <vscale x 2 x i32> @llvm.vp.xor.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r10 = call <vscale x 2 x i32> @llvm.vp.ashr.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r11 = call <vscale x 2 x i32> @llvm.vp.lshr.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r12 = call <vscale x 2 x i32> @llvm.vp.shl.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)

  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r0, <vscale x 2 x i32>* %store_addr, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r1, <vscale x 2 x i32>* %store_addr, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r2, <vscale x 2 x i32>* %store_addr, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r3, <vscale x 2 x i32>* %store_addr, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r4, <vscale x 2 x i32>* %store_addr, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r5, <vscale x 2 x i32>* %store_addr, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r6, <vscale x 2 x i32>* %store_addr, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r7, <vscale x 2 x i32>* %store_addr, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r8, <vscale x 2 x i32>* %store_addr, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r9, <vscale x 2 x i32>* %store_addr, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r10, <vscale x 2 x i32>* %store_addr, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r11, <vscale x 2 x i32>* %store_addr, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r12, <vscale x 2 x i32>* %store_addr, <vscale x 2 x i1> %allones, i32 %n)

  ret void
}

define void @test_vp_int_3(<vscale x 2 x i64>* %a0, <vscale x 2 x i64>* %a1, <vscale x 2 x i1> %m, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_int_3:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    csrr a3, vlenb
; CHECK-O0-NEXT:    slli a3, a3, 1
; CHECK-O0-NEXT:    sub sp, sp, a3
; CHECK-O0-NEXT:    mv a3, a2
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vs1r.v v0, (a2) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    mv a2, a0
; CHECK-O0-NEXT:    # kill: def $x10 killed $x13
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    slli a3, a3, 32
; CHECK-O0-NEXT:    srli a3, a3, 32
; CHECK-O0-NEXT:    # implicit-def: $v4m2
; CHECK-O0-NEXT:    vsetvli zero, a3, e64, m2, ta, mu
; CHECK-O0-NEXT:    vle64.v v4, (a2), v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v6m2
; CHECK-O0-NEXT:    vle64.v v6, (a1), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v2m2
; CHECK-O0-NEXT:    vadd.vv v2, v4, v6, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v30m2
; CHECK-O0-NEXT:    vsub.vv v30, v4, v6, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v28m2
; CHECK-O0-NEXT:    vmul.vv v28, v4, v6, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v26m2
; CHECK-O0-NEXT:    vdiv.vv v26, v4, v6, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v24m2
; CHECK-O0-NEXT:    vrem.vv v24, v4, v6, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v22m2
; CHECK-O0-NEXT:    vdivu.vv v22, v4, v6, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v20m2
; CHECK-O0-NEXT:    vremu.vv v20, v4, v6, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v18m2
; CHECK-O0-NEXT:    vand.vv v18, v4, v6, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v16m2
; CHECK-O0-NEXT:    vor.vv v16, v4, v6, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v14m2
; CHECK-O0-NEXT:    vxor.vv v14, v4, v6, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v12m2
; CHECK-O0-NEXT:    vsra.vv v12, v4, v6, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v10m2
; CHECK-O0-NEXT:    vsrl.vv v10, v4, v6, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    # implicit-def: $v8m2
; CHECK-O0-NEXT:    vsll.vv v8, v4, v6, v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v2, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v30, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v28, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v26, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v24, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v22, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v20, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v18, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v16, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v14, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v12, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v10, (a0), v0.t
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a1) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vse64.v v8, (a0), v0.t
; CHECK-O0-NEXT:    csrr a0, vlenb
; CHECK-O0-NEXT:    slli a0, a0, 1
; CHECK-O0-NEXT:    add sp, sp, a0
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_int_3:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    slli a2, a2, 32
; CHECK-O2-NEXT:    srli a2, a2, 32
; CHECK-O2-NEXT:    vsetvli zero, a2, e64, m2, ta, mu
; CHECK-O2-NEXT:    vle64.v v8, (a0), v0.t
; CHECK-O2-NEXT:    vle64.v v10, (a1), v0.t
; CHECK-O2-NEXT:    lui a0, %hi(scratch)
; CHECK-O2-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O2-NEXT:    vadd.vv v12, v8, v10, v0.t
; CHECK-O2-NEXT:    vsub.vv v14, v8, v10, v0.t
; CHECK-O2-NEXT:    vmul.vv v16, v8, v10, v0.t
; CHECK-O2-NEXT:    vdiv.vv v18, v8, v10, v0.t
; CHECK-O2-NEXT:    vrem.vv v20, v8, v10, v0.t
; CHECK-O2-NEXT:    vdivu.vv v22, v8, v10, v0.t
; CHECK-O2-NEXT:    vremu.vv v24, v8, v10, v0.t
; CHECK-O2-NEXT:    vand.vv v26, v8, v10, v0.t
; CHECK-O2-NEXT:    vor.vv v28, v8, v10, v0.t
; CHECK-O2-NEXT:    vxor.vv v30, v8, v10, v0.t
; CHECK-O2-NEXT:    vsra.vv v2, v8, v10, v0.t
; CHECK-O2-NEXT:    vsrl.vv v4, v8, v10, v0.t
; CHECK-O2-NEXT:    vsll.vv v8, v8, v10, v0.t
; CHECK-O2-NEXT:    vse64.v v12, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v14, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v16, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v18, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v20, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v22, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v24, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v26, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v28, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v30, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v2, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v4, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v8, (a0), v0.t
; CHECK-O2-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 2 x i64>*

  %i0 = call <vscale x 2 x i64> @llvm.vp.load.nxv2i64(<vscale x 2 x i64>* %a0, <vscale x 2 x i1> %m, i32 %n)
  %i1 = call <vscale x 2 x i64> @llvm.vp.load.nxv2i64(<vscale x 2 x i64>* %a1, <vscale x 2 x i1> %m, i32 %n)

  %r0 = call <vscale x 2 x i64> @llvm.vp.add.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)
  %r1 = call <vscale x 2 x i64> @llvm.vp.sub.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)
  %r2 = call <vscale x 2 x i64> @llvm.vp.mul.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)
  %r3 = call <vscale x 2 x i64> @llvm.vp.sdiv.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)
  %r4 = call <vscale x 2 x i64> @llvm.vp.srem.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)
  %r5 = call <vscale x 2 x i64> @llvm.vp.udiv.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)
  %r6 = call <vscale x 2 x i64> @llvm.vp.urem.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)
  %r7 = call <vscale x 2 x i64> @llvm.vp.and.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)

  %r8 = call <vscale x 2 x i64> @llvm.vp.or.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)
  %r9 = call <vscale x 2 x i64> @llvm.vp.xor.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)
  %r10 = call <vscale x 2 x i64> @llvm.vp.ashr.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)
  %r11 = call <vscale x 2 x i64> @llvm.vp.lshr.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)
  %r12 = call <vscale x 2 x i64> @llvm.vp.shl.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)

  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r0, <vscale x 2 x i64>* %store_addr, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r1, <vscale x 2 x i64>* %store_addr, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r2, <vscale x 2 x i64>* %store_addr, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r3, <vscale x 2 x i64>* %store_addr, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r4, <vscale x 2 x i64>* %store_addr, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r5, <vscale x 2 x i64>* %store_addr, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r6, <vscale x 2 x i64>* %store_addr, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r7, <vscale x 2 x i64>* %store_addr, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r8, <vscale x 2 x i64>* %store_addr, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r9, <vscale x 2 x i64>* %store_addr, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r10, <vscale x 2 x i64>* %store_addr, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r11, <vscale x 2 x i64>* %store_addr, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r12, <vscale x 2 x i64>* %store_addr, <vscale x 2 x i1> %m, i32 %n)

  ret void
}

; load/store (i64, m1)
declare <vscale x 1 x i64> @llvm.vp.load.nxv1i64(<vscale x 1 x i64>*, <vscale x 1 x i1>, i32)
declare void @llvm.vp.store.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>*, <vscale x 1 x i1>, i32)
; integer arith (i64, m1)
declare <vscale x 1 x i64> @llvm.vp.add.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i64> @llvm.vp.sub.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i64> @llvm.vp.mul.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i64> @llvm.vp.sdiv.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i64> @llvm.vp.srem.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i64> @llvm.vp.udiv.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i64> @llvm.vp.urem.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
; bit arith (i64, m1)
declare <vscale x 1 x i64> @llvm.vp.and.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i64> @llvm.vp.or.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i64> @llvm.vp.xor.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i64> @llvm.vp.ashr.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i64> @llvm.vp.lshr.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i64> @llvm.vp.shl.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)

; load/store (i32, m1)
declare <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>*, <vscale x 2 x i1>, i32)
declare void @llvm.vp.store.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>*, <vscale x 2 x i1>, i32)
; integer arith (i32, m1)
declare <vscale x 2 x i32> @llvm.vp.add.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.sub.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.mul.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.sdiv.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.srem.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.udiv.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.urem.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
; bit arith (i32, m1)
declare <vscale x 2 x i32> @llvm.vp.and.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.or.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.xor.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.ashr.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.lshr.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.shl.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)

; load/store (i64, m2)
declare <vscale x 2 x i64> @llvm.vp.load.nxv2i64(<vscale x 2 x i64>*, <vscale x 2 x i1>, i32)
declare void @llvm.vp.store.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>*, <vscale x 2 x i1>, i32)
; integer arith (i64, m2)
declare <vscale x 2 x i64> @llvm.vp.add.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i64> @llvm.vp.sub.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i64> @llvm.vp.mul.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i64> @llvm.vp.sdiv.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i64> @llvm.vp.srem.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i64> @llvm.vp.udiv.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i64> @llvm.vp.urem.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
; bit arith (i64, m2)
declare <vscale x 2 x i64> @llvm.vp.and.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i64> @llvm.vp.or.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i64> @llvm.vp.xor.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i64> @llvm.vp.ashr.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i64> @llvm.vp.lshr.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i64> @llvm.vp.shl.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
