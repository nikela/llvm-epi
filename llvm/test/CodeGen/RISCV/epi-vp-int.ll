; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+experimental-v -verify-machineinstrs -O0 \
; RUN:    < %s | FileCheck --check-prefix=CHECK-O0 %s
; RUN: llc -mtriple=riscv64 -mattr=+experimental-v -verify-machineinstrs -O2 \
; RUN:    < %s | FileCheck --check-prefix=CHECK-O2 %s

@scratch = global i8 0, align 16

define void @test_vp_int(<vscale x 1 x i64>* %a0, <vscale x 1 x i64>* %a1, <vscale x 1 x i1> %m, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_int:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -48
; CHECK-O0-NEXT:    sd ra, 40(sp)
; CHECK-O0-NEXT:    sd s0, 32(sp)
; CHECK-O0-NEXT:    addi s0, sp, 48
; CHECK-O0-NEXT:    rdvlenb a3
; CHECK-O0-NEXT:    sub sp, sp, a3
; CHECK-O0-NEXT:    sd sp, -40(s0)
; CHECK-O0-NEXT:    # kill: def $x13 killed $x12
; CHECK-O0-NEXT:    lui a3, %hi(scratch)
; CHECK-O0-NEXT:    addi a3, a3, %lo(scratch)
; CHECK-O0-NEXT:    ld a4, -40(s0)
; CHECK-O0-NEXT:    vs1r.v v0, (a4)
; CHECK-O0-NEXT:    # implicit-def: $v1
; CHECK-O0-NEXT:    vsetvli a2, a2, e64,m1,tu,mu
; CHECK-O0-NEXT:    vle64.v v1, (a0), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v2
; CHECK-O0-NEXT:    vle64.v v2, (a1), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v3
; CHECK-O0-NEXT:    vadd.vv v3, v1, v2, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v4
; CHECK-O0-NEXT:    vsub.vv v4, v1, v2, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v5
; CHECK-O0-NEXT:    vmul.vv v5, v1, v2, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v6
; CHECK-O0-NEXT:    vdiv.vv v6, v1, v2, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v7
; CHECK-O0-NEXT:    vrem.vv v7, v1, v2, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v16
; CHECK-O0-NEXT:    vdivu.vv v16, v1, v2, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v17
; CHECK-O0-NEXT:    vremu.vv v17, v1, v2, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v18
; CHECK-O0-NEXT:    vand.vv v18, v1, v2, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v19
; CHECK-O0-NEXT:    vor.vv v19, v1, v2, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v20
; CHECK-O0-NEXT:    vxor.vv v20, v1, v2, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v21
; CHECK-O0-NEXT:    vsra.vv v21, v1, v2, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v22
; CHECK-O0-NEXT:    vsrl.vv v22, v1, v2, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v23
; CHECK-O0-NEXT:    vsll.vv v23, v1, v2, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v3, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v4, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v5, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v6, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v7, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v16, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v17, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v18, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v19, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v20, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v21, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v22, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v23, (a3), v0.t
; CHECK-O0-NEXT:    addi sp, s0, -48
; CHECK-O0-NEXT:    ld s0, 32(sp)
; CHECK-O0-NEXT:    ld ra, 40(sp)
; CHECK-O0-NEXT:    addi sp, sp, 48
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_int:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    vsetvli a2, a2, e64,m1,tu,mu
; CHECK-O2-NEXT:    vle64.v v1, (a0), v0.t
; CHECK-O2-NEXT:    vle64.v v2, (a1), v0.t
; CHECK-O2-NEXT:    lui a0, %hi(scratch)
; CHECK-O2-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O2-NEXT:    vadd.vv v3, v1, v2, v0.t
; CHECK-O2-NEXT:    vsub.vv v4, v1, v2, v0.t
; CHECK-O2-NEXT:    vmul.vv v5, v1, v2, v0.t
; CHECK-O2-NEXT:    vdiv.vv v6, v1, v2, v0.t
; CHECK-O2-NEXT:    vrem.vv v7, v1, v2, v0.t
; CHECK-O2-NEXT:    vdivu.vv v16, v1, v2, v0.t
; CHECK-O2-NEXT:    vremu.vv v17, v1, v2, v0.t
; CHECK-O2-NEXT:    vand.vv v18, v1, v2, v0.t
; CHECK-O2-NEXT:    vor.vv v19, v1, v2, v0.t
; CHECK-O2-NEXT:    vxor.vv v20, v1, v2, v0.t
; CHECK-O2-NEXT:    vsra.vv v21, v1, v2, v0.t
; CHECK-O2-NEXT:    vsrl.vv v22, v1, v2, v0.t
; CHECK-O2-NEXT:    vsll.vv v1, v1, v2, v0.t
; CHECK-O2-NEXT:    vse64.v v3, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v4, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v5, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v6, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v7, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v16, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v17, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v18, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v19, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v20, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v21, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v22, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v1, (a0), v0.t
; CHECK-O2-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 1 x i64>*

  %i0 = call <vscale x 1 x i64> @llvm.vp.load.nxv1i64(<vscale x 1 x i64>* %a0, i32 8, <vscale x 1 x i1> %m, i32 %n)
  %i1 = call <vscale x 1 x i64> @llvm.vp.load.nxv1i64(<vscale x 1 x i64>* %a1, i32 8, <vscale x 1 x i1> %m, i32 %n)

  %r0 = call <vscale x 1 x i64> @llvm.vp.add.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r1 = call <vscale x 1 x i64> @llvm.vp.sub.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r2 = call <vscale x 1 x i64> @llvm.vp.mul.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r3 = call <vscale x 1 x i64> @llvm.vp.sdiv.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r4 = call <vscale x 1 x i64> @llvm.vp.srem.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r5 = call <vscale x 1 x i64> @llvm.vp.udiv.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r6 = call <vscale x 1 x i64> @llvm.vp.urem.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r7 = call <vscale x 1 x i64> @llvm.vp.and.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r8 = call <vscale x 1 x i64> @llvm.vp.or.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r9 = call <vscale x 1 x i64> @llvm.vp.xor.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r10 = call <vscale x 1 x i64> @llvm.vp.ashr.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r11 = call <vscale x 1 x i64> @llvm.vp.lshr.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)
  %r12 = call <vscale x 1 x i64> @llvm.vp.shl.nxv1i64(<vscale x 1 x i64> %i0, <vscale x 1 x i64> %i1, <vscale x 1 x i1> %m, i32 %n)

  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r0, <vscale x 1 x i64>* %store_addr, i32 8, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r1, <vscale x 1 x i64>* %store_addr, i32 8, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r2, <vscale x 1 x i64>* %store_addr, i32 8, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r3, <vscale x 1 x i64>* %store_addr, i32 8, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r4, <vscale x 1 x i64>* %store_addr, i32 8, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r5, <vscale x 1 x i64>* %store_addr, i32 8, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r6, <vscale x 1 x i64>* %store_addr, i32 8, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r7, <vscale x 1 x i64>* %store_addr, i32 8, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r8, <vscale x 1 x i64>* %store_addr, i32 8, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r9, <vscale x 1 x i64>* %store_addr, i32 8, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r10, <vscale x 1 x i64>* %store_addr, i32 8, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r11, <vscale x 1 x i64>* %store_addr, i32 8, <vscale x 1 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv1i64(<vscale x 1 x i64> %r12, <vscale x 1 x i64>* %store_addr, i32 8, <vscale x 1 x i1> %m, i32 %n)

  ret void
}

define void @test_vp_int_2(<vscale x 2 x i32>* %a0, <vscale x 2 x i32>* %a1, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_int_2:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    # kill: def $x13 killed $x12
; CHECK-O0-NEXT:    lui a3, %hi(scratch)
; CHECK-O0-NEXT:    addi a3, a3, %lo(scratch)
; CHECK-O0-NEXT:    # implicit-def: $v1
; CHECK-O0-NEXT:    vsetvli a2, a2, e32,m1,tu,mu
; CHECK-O0-NEXT:    vle32.v v1, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v2
; CHECK-O0-NEXT:    vle32.v v2, (a1)
; CHECK-O0-NEXT:    # implicit-def: $v3
; CHECK-O0-NEXT:    vadd.vv v3, v1, v2
; CHECK-O0-NEXT:    # implicit-def: $v4
; CHECK-O0-NEXT:    vsub.vv v4, v1, v2
; CHECK-O0-NEXT:    # implicit-def: $v5
; CHECK-O0-NEXT:    vmul.vv v5, v1, v2
; CHECK-O0-NEXT:    # implicit-def: $v6
; CHECK-O0-NEXT:    vdiv.vv v6, v1, v2
; CHECK-O0-NEXT:    # implicit-def: $v7
; CHECK-O0-NEXT:    vrem.vv v7, v1, v2
; CHECK-O0-NEXT:    # implicit-def: $v16
; CHECK-O0-NEXT:    vdivu.vv v16, v1, v2
; CHECK-O0-NEXT:    # implicit-def: $v17
; CHECK-O0-NEXT:    vremu.vv v17, v1, v2
; CHECK-O0-NEXT:    # implicit-def: $v18
; CHECK-O0-NEXT:    vand.vv v18, v1, v2
; CHECK-O0-NEXT:    # implicit-def: $v19
; CHECK-O0-NEXT:    vor.vv v19, v1, v2
; CHECK-O0-NEXT:    # implicit-def: $v20
; CHECK-O0-NEXT:    vxor.vv v20, v1, v2
; CHECK-O0-NEXT:    # implicit-def: $v21
; CHECK-O0-NEXT:    vsra.vv v21, v1, v2
; CHECK-O0-NEXT:    # implicit-def: $v22
; CHECK-O0-NEXT:    vsrl.vv v22, v1, v2
; CHECK-O0-NEXT:    # implicit-def: $v23
; CHECK-O0-NEXT:    vsll.vv v23, v1, v2
; CHECK-O0-NEXT:    vse32.v v3, (a3)
; CHECK-O0-NEXT:    vse32.v v4, (a3)
; CHECK-O0-NEXT:    vse32.v v5, (a3)
; CHECK-O0-NEXT:    vse32.v v6, (a3)
; CHECK-O0-NEXT:    vse32.v v7, (a3)
; CHECK-O0-NEXT:    vse32.v v16, (a3)
; CHECK-O0-NEXT:    vse32.v v17, (a3)
; CHECK-O0-NEXT:    vse32.v v18, (a3)
; CHECK-O0-NEXT:    vse32.v v19, (a3)
; CHECK-O0-NEXT:    vse32.v v20, (a3)
; CHECK-O0-NEXT:    vse32.v v21, (a3)
; CHECK-O0-NEXT:    vse32.v v22, (a3)
; CHECK-O0-NEXT:    vse32.v v23, (a3)
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_int_2:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    vsetvli a2, a2, e32,m1,tu,mu
; CHECK-O2-NEXT:    vle32.v v1, (a0)
; CHECK-O2-NEXT:    vle32.v v2, (a1)
; CHECK-O2-NEXT:    lui a0, %hi(scratch)
; CHECK-O2-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O2-NEXT:    vadd.vv v3, v1, v2
; CHECK-O2-NEXT:    vsub.vv v4, v1, v2
; CHECK-O2-NEXT:    vmul.vv v5, v1, v2
; CHECK-O2-NEXT:    vdiv.vv v6, v1, v2
; CHECK-O2-NEXT:    vrem.vv v7, v1, v2
; CHECK-O2-NEXT:    vdivu.vv v16, v1, v2
; CHECK-O2-NEXT:    vremu.vv v17, v1, v2
; CHECK-O2-NEXT:    vand.vv v18, v1, v2
; CHECK-O2-NEXT:    vor.vv v19, v1, v2
; CHECK-O2-NEXT:    vxor.vv v20, v1, v2
; CHECK-O2-NEXT:    vsra.vv v21, v1, v2
; CHECK-O2-NEXT:    vsrl.vv v22, v1, v2
; CHECK-O2-NEXT:    vsll.vv v1, v1, v2
; CHECK-O2-NEXT:    vse32.v v3, (a0)
; CHECK-O2-NEXT:    vse32.v v4, (a0)
; CHECK-O2-NEXT:    vse32.v v5, (a0)
; CHECK-O2-NEXT:    vse32.v v6, (a0)
; CHECK-O2-NEXT:    vse32.v v7, (a0)
; CHECK-O2-NEXT:    vse32.v v16, (a0)
; CHECK-O2-NEXT:    vse32.v v17, (a0)
; CHECK-O2-NEXT:    vse32.v v18, (a0)
; CHECK-O2-NEXT:    vse32.v v19, (a0)
; CHECK-O2-NEXT:    vse32.v v20, (a0)
; CHECK-O2-NEXT:    vse32.v v21, (a0)
; CHECK-O2-NEXT:    vse32.v v22, (a0)
; CHECK-O2-NEXT:    vse32.v v1, (a0)
; CHECK-O2-NEXT:    ret
  %head = insertelement <vscale x 2 x i1> undef, i1 1, i32 0
  %allones = shufflevector <vscale x 2 x i1> %head, <vscale x 2 x i1> undef, <vscale x 2 x i32> zeroinitializer

  %store_addr = bitcast i8* @scratch to <vscale x 2 x i32>*

  %i0 = call <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>* %a0, i32 4, <vscale x 2 x i1> %allones, i32 %n)
  %i1 = call <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>* %a1, i32 4, <vscale x 2 x i1> %allones, i32 %n)

  %r0 = call <vscale x 2 x i32> @llvm.vp.add.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r1 = call <vscale x 2 x i32> @llvm.vp.sub.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r2 = call <vscale x 2 x i32> @llvm.vp.mul.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r3 = call <vscale x 2 x i32> @llvm.vp.sdiv.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r4 = call <vscale x 2 x i32> @llvm.vp.srem.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r5 = call <vscale x 2 x i32> @llvm.vp.udiv.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r6 = call <vscale x 2 x i32> @llvm.vp.urem.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r7 = call <vscale x 2 x i32> @llvm.vp.and.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r8 = call <vscale x 2 x i32> @llvm.vp.or.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r9 = call <vscale x 2 x i32> @llvm.vp.xor.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r10 = call <vscale x 2 x i32> @llvm.vp.ashr.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r11 = call <vscale x 2 x i32> @llvm.vp.lshr.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)
  %r12 = call <vscale x 2 x i32> @llvm.vp.shl.nxv2i32(<vscale x 2 x i32> %i0, <vscale x 2 x i32> %i1, <vscale x 2 x i1> %allones, i32 %n)

  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r0, <vscale x 2 x i32>* %store_addr, i32 4, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r1, <vscale x 2 x i32>* %store_addr, i32 4, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r2, <vscale x 2 x i32>* %store_addr, i32 4, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r3, <vscale x 2 x i32>* %store_addr, i32 4, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r4, <vscale x 2 x i32>* %store_addr, i32 4, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r5, <vscale x 2 x i32>* %store_addr, i32 4, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r6, <vscale x 2 x i32>* %store_addr, i32 4, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r7, <vscale x 2 x i32>* %store_addr, i32 4, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r8, <vscale x 2 x i32>* %store_addr, i32 4, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r9, <vscale x 2 x i32>* %store_addr, i32 4, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r10, <vscale x 2 x i32>* %store_addr, i32 4, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r11, <vscale x 2 x i32>* %store_addr, i32 4, <vscale x 2 x i1> %allones, i32 %n)
  call void @llvm.vp.store.nxv2i32(<vscale x 2 x i32> %r12, <vscale x 2 x i32>* %store_addr, i32 4, <vscale x 2 x i1> %allones, i32 %n)

  ret void
}

define void @test_vp_int_3(<vscale x 2 x i64>* %a0, <vscale x 2 x i64>* %a1, <vscale x 2 x i1> %m, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_int_3:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -48
; CHECK-O0-NEXT:    sd ra, 40(sp)
; CHECK-O0-NEXT:    sd s0, 32(sp)
; CHECK-O0-NEXT:    addi s0, sp, 48
; CHECK-O0-NEXT:    rdvlenb a3
; CHECK-O0-NEXT:    sub sp, sp, a3
; CHECK-O0-NEXT:    sd sp, -40(s0)
; CHECK-O0-NEXT:    # kill: def $x13 killed $x12
; CHECK-O0-NEXT:    lui a3, %hi(scratch)
; CHECK-O0-NEXT:    addi a3, a3, %lo(scratch)
; CHECK-O0-NEXT:    ld a4, -40(s0)
; CHECK-O0-NEXT:    vs1r.v v0, (a4)
; CHECK-O0-NEXT:    # implicit-def: $v2m2
; CHECK-O0-NEXT:    vsetvli a2, a2, e64,m2,tu,mu
; CHECK-O0-NEXT:    vle64.v v2, (a0), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v4m2
; CHECK-O0-NEXT:    vle64.v v4, (a1), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v6m2
; CHECK-O0-NEXT:    vadd.vv v6, v2, v4, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v16m2
; CHECK-O0-NEXT:    vsub.vv v16, v2, v4, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v18m2
; CHECK-O0-NEXT:    vmul.vv v18, v2, v4, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v20m2
; CHECK-O0-NEXT:    vdiv.vv v20, v2, v4, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v22m2
; CHECK-O0-NEXT:    vrem.vv v22, v2, v4, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v8m2
; CHECK-O0-NEXT:    vdivu.vv v8, v2, v4, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v10m2
; CHECK-O0-NEXT:    vremu.vv v10, v2, v4, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v12m2
; CHECK-O0-NEXT:    vand.vv v12, v2, v4, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v14m2
; CHECK-O0-NEXT:    vor.vv v14, v2, v4, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v24m2
; CHECK-O0-NEXT:    vxor.vv v24, v2, v4, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v26m2
; CHECK-O0-NEXT:    vsra.vv v26, v2, v4, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v28m2
; CHECK-O0-NEXT:    vsrl.vv v28, v2, v4, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v30m2
; CHECK-O0-NEXT:    vsll.vv v30, v2, v4, v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v6, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v16, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v18, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v20, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v22, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v8, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v10, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v12, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v14, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v24, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v26, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v28, (a3), v0.t
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    vse64.v v30, (a3), v0.t
; CHECK-O0-NEXT:    addi sp, s0, -48
; CHECK-O0-NEXT:    ld s0, 32(sp)
; CHECK-O0-NEXT:    ld ra, 40(sp)
; CHECK-O0-NEXT:    addi sp, sp, 48
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_int_3:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    vsetvli a2, a2, e64,m2,tu,mu
; CHECK-O2-NEXT:    vle64.v v2, (a0), v0.t
; CHECK-O2-NEXT:    vle64.v v4, (a1), v0.t
; CHECK-O2-NEXT:    lui a0, %hi(scratch)
; CHECK-O2-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O2-NEXT:    vadd.vv v6, v2, v4, v0.t
; CHECK-O2-NEXT:    vsub.vv v16, v2, v4, v0.t
; CHECK-O2-NEXT:    vmul.vv v18, v2, v4, v0.t
; CHECK-O2-NEXT:    vdiv.vv v20, v2, v4, v0.t
; CHECK-O2-NEXT:    vrem.vv v22, v2, v4, v0.t
; CHECK-O2-NEXT:    vdivu.vv v8, v2, v4, v0.t
; CHECK-O2-NEXT:    vremu.vv v10, v2, v4, v0.t
; CHECK-O2-NEXT:    vand.vv v12, v2, v4, v0.t
; CHECK-O2-NEXT:    vor.vv v14, v2, v4, v0.t
; CHECK-O2-NEXT:    vxor.vv v24, v2, v4, v0.t
; CHECK-O2-NEXT:    vsra.vv v26, v2, v4, v0.t
; CHECK-O2-NEXT:    vsrl.vv v28, v2, v4, v0.t
; CHECK-O2-NEXT:    vsll.vv v30, v2, v4, v0.t
; CHECK-O2-NEXT:    vse64.v v6, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v16, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v18, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v20, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v22, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v8, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v10, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v12, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v14, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v24, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v26, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v28, (a0), v0.t
; CHECK-O2-NEXT:    vse64.v v30, (a0), v0.t
; CHECK-O2-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 2 x i64>*

  %i0 = call <vscale x 2 x i64> @llvm.vp.load.nxv2i64(<vscale x 2 x i64>* %a0, i32 8, <vscale x 2 x i1> %m, i32 %n)
  %i1 = call <vscale x 2 x i64> @llvm.vp.load.nxv2i64(<vscale x 2 x i64>* %a1, i32 8, <vscale x 2 x i1> %m, i32 %n)

  %r0 = call <vscale x 2 x i64> @llvm.vp.add.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)
  %r1 = call <vscale x 2 x i64> @llvm.vp.sub.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)
  %r2 = call <vscale x 2 x i64> @llvm.vp.mul.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)
  %r3 = call <vscale x 2 x i64> @llvm.vp.sdiv.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)
  %r4 = call <vscale x 2 x i64> @llvm.vp.srem.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)
  %r5 = call <vscale x 2 x i64> @llvm.vp.udiv.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)
  %r6 = call <vscale x 2 x i64> @llvm.vp.urem.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)
  %r7 = call <vscale x 2 x i64> @llvm.vp.and.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)

  %r8 = call <vscale x 2 x i64> @llvm.vp.or.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)
  %r9 = call <vscale x 2 x i64> @llvm.vp.xor.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)
  %r10 = call <vscale x 2 x i64> @llvm.vp.ashr.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)
  %r11 = call <vscale x 2 x i64> @llvm.vp.lshr.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)
  %r12 = call <vscale x 2 x i64> @llvm.vp.shl.nxv2i64(<vscale x 2 x i64> %i0, <vscale x 2 x i64> %i1, <vscale x 2 x i1> %m, i32 %n)

  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r0, <vscale x 2 x i64>* %store_addr, i32 8, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r1, <vscale x 2 x i64>* %store_addr, i32 8, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r2, <vscale x 2 x i64>* %store_addr, i32 8, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r3, <vscale x 2 x i64>* %store_addr, i32 8, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r4, <vscale x 2 x i64>* %store_addr, i32 8, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r5, <vscale x 2 x i64>* %store_addr, i32 8, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r6, <vscale x 2 x i64>* %store_addr, i32 8, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r7, <vscale x 2 x i64>* %store_addr, i32 8, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r8, <vscale x 2 x i64>* %store_addr, i32 8, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r9, <vscale x 2 x i64>* %store_addr, i32 8, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r10, <vscale x 2 x i64>* %store_addr, i32 8, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r11, <vscale x 2 x i64>* %store_addr, i32 8, <vscale x 2 x i1> %m, i32 %n)
  call void @llvm.vp.store.nxv2i64(<vscale x 2 x i64> %r12, <vscale x 2 x i64>* %store_addr, i32 8, <vscale x 2 x i1> %m, i32 %n)

  ret void
}

; load/store (i64, m1)
declare <vscale x 1 x i64> @llvm.vp.load.nxv1i64(<vscale x 1 x i64>*, i32, <vscale x 1 x i1>, i32)
declare void @llvm.vp.store.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>*, i32, <vscale x 1 x i1>, i32)
; integer arith (i64, m1)
declare <vscale x 1 x i64> @llvm.vp.add.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i64> @llvm.vp.sub.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i64> @llvm.vp.mul.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i64> @llvm.vp.sdiv.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i64> @llvm.vp.srem.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i64> @llvm.vp.udiv.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i64> @llvm.vp.urem.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
; bit arith (i64, m1)
declare <vscale x 1 x i64> @llvm.vp.and.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i64> @llvm.vp.or.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i64> @llvm.vp.xor.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i64> @llvm.vp.ashr.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i64> @llvm.vp.lshr.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i64> @llvm.vp.shl.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i1>, i32)

; load/store (i32, m1)
declare <vscale x 2 x i32> @llvm.vp.load.nxv2i32(<vscale x 2 x i32>*, i32, <vscale x 2 x i1>, i32)
declare void @llvm.vp.store.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>*, i32, <vscale x 2 x i1>, i32)
; integer arith (i32, m1)
declare <vscale x 2 x i32> @llvm.vp.add.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.sub.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.mul.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.sdiv.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.srem.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.udiv.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.urem.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
; bit arith (i32, m1)
declare <vscale x 2 x i32> @llvm.vp.and.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.or.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.xor.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.ashr.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.lshr.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i32> @llvm.vp.shl.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i1>, i32)

; load/store (i64, m2)
declare <vscale x 2 x i64> @llvm.vp.load.nxv2i64(<vscale x 2 x i64>*, i32, <vscale x 2 x i1>, i32)
declare void @llvm.vp.store.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>*, i32, <vscale x 2 x i1>, i32)
; integer arith (i64, m2)
declare <vscale x 2 x i64> @llvm.vp.add.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i64> @llvm.vp.sub.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i64> @llvm.vp.mul.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i64> @llvm.vp.sdiv.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i64> @llvm.vp.srem.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i64> @llvm.vp.udiv.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i64> @llvm.vp.urem.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
; bit arith (i64, m2)
declare <vscale x 2 x i64> @llvm.vp.and.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i64> @llvm.vp.or.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i64> @llvm.vp.xor.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i64> @llvm.vp.ashr.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i64> @llvm.vp.lshr.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i64> @llvm.vp.shl.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, <vscale x 2 x i1>, i32)
