; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv32 < %s | FileCheck --check-prefix=RV32 %s
; RUN: llc -mtriple=riscv64 < %s | FileCheck --check-prefix=RV64 %s

define dso_local void @test1() nounwind {
; RV32-LABEL: test1:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -32
; RV32-NEXT:    sw ra, 28(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 24(sp) # 4-byte Folded Spill
; RV32-NEXT:    addi s0, sp, 32
; RV32-NEXT:    andi sp, sp, -32
; RV32-NEXT:    lw a0, 0(sp)
; RV32-NEXT:    addi a0, a0, 1
; RV32-NEXT:    sw a0, 0(sp)
; RV32-NEXT:    addi sp, s0, -32
; RV32-NEXT:    lw ra, 28(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 24(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 32
; RV32-NEXT:    ret
;
; RV64-LABEL: test1:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -32
; RV64-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    addi s0, sp, 32
; RV64-NEXT:    andi sp, sp, -32
; RV64-NEXT:    lw a0, 0(sp)
; RV64-NEXT:    addiw a0, a0, 1
; RV64-NEXT:    sw a0, 0(sp)
; RV64-NEXT:    addi sp, s0, -32
; RV64-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 32
; RV64-NEXT:    ret
entry:
  %x = alloca i32, align 32
  %x.0.x.0. = load volatile i32, i32* %x, align 32
  %inc = add nsw i32 %x.0.x.0., 1
  store volatile i32 %inc, i32* %x, align 32
  ret void
}

define dso_local void @test2() nounwind {
; RV32-LABEL: test2:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -64
; RV32-NEXT:    sw ra, 60(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 56(sp) # 4-byte Folded Spill
; RV32-NEXT:    addi s0, sp, 64
; RV32-NEXT:    andi sp, sp, -32
; RV32-NEXT:    lw a0, 32(sp)
; RV32-NEXT:    addi a0, a0, 1
; RV32-NEXT:    sw a0, 32(sp)
; RV32-NEXT:    lw a0, 0(sp)
; RV32-NEXT:    addi a0, a0, 1
; RV32-NEXT:    sw a0, 0(sp)
; RV32-NEXT:    addi sp, s0, -64
; RV32-NEXT:    lw ra, 60(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 56(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 64
; RV32-NEXT:    ret
;
; RV64-LABEL: test2:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    addi s0, sp, 64
; RV64-NEXT:    andi sp, sp, -32
; RV64-NEXT:    lw a0, 32(sp)
; RV64-NEXT:    addiw a0, a0, 1
; RV64-NEXT:    sw a0, 32(sp)
; RV64-NEXT:    lw a0, 0(sp)
; RV64-NEXT:    addiw a0, a0, 1
; RV64-NEXT:    sw a0, 0(sp)
; RV64-NEXT:    addi sp, s0, -64
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
entry:
  %x = alloca i32, align 32
  %y = alloca i32, align 32
  %x.0.x.0. = load volatile i32, i32* %x, align 32
  %inc = add nsw i32 %x.0.x.0., 1
  store volatile i32 %inc, i32* %x, align 32
  %y.0.y.0. = load volatile i32, i32* %y, align 32
  %inc1 = add nsw i32 %y.0.y.0., 1
  store volatile i32 %inc1, i32* %y, align 32
  ret void
}

define dso_local void @test3() nounwind {
; RV32-LABEL: test3:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -64
; RV32-NEXT:    sw ra, 60(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 56(sp) # 4-byte Folded Spill
; RV32-NEXT:    addi s0, sp, 64
; RV32-NEXT:    andi sp, sp, -32
; RV32-NEXT:    lw a0, 32(sp)
; RV32-NEXT:    addi a0, a0, 1
; RV32-NEXT:    sw a0, 32(sp)
; RV32-NEXT:    lw a0, 28(sp)
; RV32-NEXT:    addi a0, a0, 1
; RV32-NEXT:    sw a0, 28(sp)
; RV32-NEXT:    addi sp, s0, -64
; RV32-NEXT:    lw ra, 60(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 56(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 64
; RV32-NEXT:    ret
;
; RV64-LABEL: test3:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    addi s0, sp, 64
; RV64-NEXT:    andi sp, sp, -32
; RV64-NEXT:    lw a0, 32(sp)
; RV64-NEXT:    addiw a0, a0, 1
; RV64-NEXT:    sw a0, 32(sp)
; RV64-NEXT:    lw a0, 28(sp)
; RV64-NEXT:    addiw a0, a0, 1
; RV64-NEXT:    sw a0, 28(sp)
; RV64-NEXT:    addi sp, s0, -64
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
entry:
  %x = alloca i32, align 32
  %y = alloca i32, align 4
  %x.0.x.0. = load volatile i32, i32* %x, align 32
  %inc = add nsw i32 %x.0.x.0., 1
  store volatile i32 %inc, i32* %x, align 32
  %y.0.y.0. = load volatile i32, i32* %y, align 4
  %inc1 = add nsw i32 %y.0.y.0., 1
  store volatile i32 %inc1, i32* %y, align 4
  ret void
}

define dso_local void @test4() nounwind {
; RV32-LABEL: test4:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -2032
; RV32-NEXT:    sw ra, 2028(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 2024(sp) # 4-byte Folded Spill
; RV32-NEXT:    addi s0, sp, 2032
; RV32-NEXT:    lui a0, 2
; RV32-NEXT:    addi a0, a0, -2032
; RV32-NEXT:    sub sp, sp, a0
; RV32-NEXT:    srli a0, sp, 12
; RV32-NEXT:    slli sp, a0, 12
; RV32-NEXT:    lui a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    lw a0, 0(a0)
; RV32-NEXT:    addi a0, a0, 1
; RV32-NEXT:    lui a1, 1
; RV32-NEXT:    add a1, sp, a1
; RV32-NEXT:    sw a0, 0(a1)
; RV32-NEXT:    lui a0, 2
; RV32-NEXT:    sub sp, s0, a0
; RV32-NEXT:    lui a0, 2
; RV32-NEXT:    addi a0, a0, -2032
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 2028(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 2024(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 2032
; RV32-NEXT:    ret
;
; RV64-LABEL: test4:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -2032
; RV64-NEXT:    sd ra, 2024(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 2016(sp) # 8-byte Folded Spill
; RV64-NEXT:    addi s0, sp, 2032
; RV64-NEXT:    lui a0, 2
; RV64-NEXT:    addiw a0, a0, -2032
; RV64-NEXT:    sub sp, sp, a0
; RV64-NEXT:    srli a0, sp, 12
; RV64-NEXT:    slli sp, a0, 12
; RV64-NEXT:    lui a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    lw a0, 0(a0)
; RV64-NEXT:    addiw a0, a0, 1
; RV64-NEXT:    lui a1, 1
; RV64-NEXT:    add a1, sp, a1
; RV64-NEXT:    sw a0, 0(a1)
; RV64-NEXT:    lui a0, 2
; RV64-NEXT:    sub sp, s0, a0
; RV64-NEXT:    lui a0, 2
; RV64-NEXT:    addiw a0, a0, -2032
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    ld ra, 2024(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 2016(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 2032
; RV64-NEXT:    ret
entry:
  %x = alloca i32, align 4096
  %x.0.x.0. = load volatile i32, i32* %x, align 4096
  %inc = add nsw i32 %x.0.x.0., 1
  store volatile i32 %inc, i32* %x, align 4096
  ret void
}
