; NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+m,+f,+d,+experimental-v -verify-machineinstrs \
; RUN:    -O2 -epi-pipeline < %s -o - -stop-after=riscv-insert-vsetvli | FileCheck %s

@scratch = global i8 0, align 16

define void @test_lmul_1(<vscale x 1 x double>* %a0, <vscale x 1 x double>* %a1, <vscale x 1 x double>* %a2, <vscale x 1 x i64>* %a3, <vscale x 1 x i1> %m, i32 %n) nounwind {
  ; CHECK-LABEL: name: test_lmul_1
  ; CHECK: bb.0 (%ir-block.0):
  ; CHECK:   liveins: $x10, $x11, $v0, $x14
  ; CHECK:   [[COPY:%[0-9]+]]:gpr = COPY $x14
  ; CHECK:   [[COPY1:%[0-9]+]]:vmv0 = COPY $v0
  ; CHECK:   [[COPY2:%[0-9]+]]:gpr = COPY $x11
  ; CHECK:   [[COPY3:%[0-9]+]]:gpr = COPY $x10
  ; CHECK:   [[LUI:%[0-9]+]]:gpr = LUI target-flags(riscv-hi) @scratch
  ; CHECK:   [[ADDI:%[0-9]+]]:gpr = ADDI killed [[LUI]], target-flags(riscv-lo) @scratch
  ; CHECK:   $v0 = COPY [[COPY1]]
  ; CHECK:   [[DEF:%[0-9]+]]:vr = IMPLICIT_DEF
  ; CHECK:   dead $x0 = PseudoVSETVLI [[COPY]], 88, implicit-def $vl, implicit-def $vtype
  ; CHECK:   [[PseudoEPIVLE64_V_M1_:%[0-9]+]]:vr = PseudoEPIVLE64_V_M1 [[DEF]], [[COPY3]], $v0, $noreg, 64, implicit $vl, implicit $vtype
  ; CHECK:   $v0 = COPY [[COPY1]]
  ; CHECK:   [[DEF1:%[0-9]+]]:vr = IMPLICIT_DEF
  ; CHECK:   [[PseudoEPIVLE64_V_M1_1:%[0-9]+]]:vr = PseudoEPIVLE64_V_M1 [[DEF1]], [[COPY2]], $v0, $noreg, 64, implicit $vl, implicit $vtype
  ; CHECK:   [[SLLI:%[0-9]+]]:gpr = SLLI [[COPY]], 32
  ; CHECK:   [[SRLI:%[0-9]+]]:gpr = SRLI killed [[SLLI]], 32
  ; CHECK:   [[DEF2:%[0-9]+]]:vrnov0 = IMPLICIT_DEF
  ; CHECK:   dead $x0 = PseudoVSETVLI [[SRLI]], 88, implicit-def $vl, implicit-def $vtype
  ; CHECK:   [[PseudoVFADD_VV_M1_MASK:%[0-9]+]]:vrnov0 = PseudoVFADD_VV_M1_MASK [[DEF2]], killed [[PseudoEPIVLE64_V_M1_]], killed [[PseudoEPIVLE64_V_M1_1]], [[COPY1]], $noreg, 6, implicit $vl, implicit $vtype
  ; CHECK:   $v0 = COPY [[COPY1]]
  ; CHECK:   dead $x0 = PseudoVSETVLI [[COPY]], 88, implicit-def $vl, implicit-def $vtype
  ; CHECK:   PseudoEPIVSE64_V_M1 killed [[PseudoVFADD_VV_M1_MASK]], killed [[ADDI]], $v0, $noreg, 64, implicit $vl, implicit $vtype
  ; CHECK:   PseudoRET
  %store_addr = bitcast i8* @scratch to <vscale x 1 x double>*

  %i0 = call <vscale x 1 x double> @llvm.vp.load.nxv1f64(<vscale x 1 x double>* %a0, <vscale x 1 x i1> %m, i32 %n)
  %i1 = call <vscale x 1 x double> @llvm.vp.load.nxv1f64(<vscale x 1 x double>* %a1, <vscale x 1 x i1> %m, i32 %n)

  %r0 = call <vscale x 1 x double> @llvm.vp.fadd.nxv1f64(<vscale x 1 x double> %i0, <vscale x 1 x double> %i1, <vscale x 1 x i1> %m, i32 %n)

  call void @llvm.vp.store.nxv1f64(<vscale x 1 x double> %r0, <vscale x 1 x double>* %store_addr, <vscale x 1 x i1> %m, i32 %n)

  ret void
}

declare <vscale x 1 x double> @llvm.vp.load.nxv1f64(<vscale x 1 x double>*, <vscale x 1 x i1>, i32)
declare void @llvm.vp.store.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>*, <vscale x 1 x i1>, i32)
declare <vscale x 1 x double> @llvm.vp.fadd.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>, i32)

define void @test_lmul_2(<vscale x 2 x double>* %a0, <vscale x 2 x double>* %a1, <vscale x 2 x double>* %a2, <vscale x 2 x i64>* %a3, <vscale x 2 x i1> %m, i32 %n) nounwind {
  ; CHECK-LABEL: name: test_lmul_2
  ; CHECK: bb.0 (%ir-block.0):
  ; CHECK:   liveins: $x10, $x11, $v0, $x14
  ; CHECK:   [[COPY:%[0-9]+]]:gpr = COPY $x14
  ; CHECK:   [[COPY1:%[0-9]+]]:vmv0 = COPY $v0
  ; CHECK:   [[COPY2:%[0-9]+]]:gpr = COPY $x11
  ; CHECK:   [[COPY3:%[0-9]+]]:gpr = COPY $x10
  ; CHECK:   [[LUI:%[0-9]+]]:gpr = LUI target-flags(riscv-hi) @scratch
  ; CHECK:   [[ADDI:%[0-9]+]]:gpr = ADDI killed [[LUI]], target-flags(riscv-lo) @scratch
  ; CHECK:   $v0 = COPY [[COPY1]]
  ; CHECK:   [[DEF:%[0-9]+]]:vrm2 = IMPLICIT_DEF
  ; CHECK:   dead $x0 = PseudoVSETVLI [[COPY]], 89, implicit-def $vl, implicit-def $vtype
  ; CHECK:   [[PseudoEPIVLE64_V_M2_:%[0-9]+]]:vrm2nov0 = PseudoEPIVLE64_V_M2 [[DEF]], [[COPY3]], $v0, $noreg, 64, implicit $vl, implicit $vtype
  ; CHECK:   $v0 = COPY [[COPY1]]
  ; CHECK:   [[DEF1:%[0-9]+]]:vrm2 = IMPLICIT_DEF
  ; CHECK:   [[PseudoEPIVLE64_V_M2_1:%[0-9]+]]:vrm2nov0 = PseudoEPIVLE64_V_M2 [[DEF1]], [[COPY2]], $v0, $noreg, 64, implicit $vl, implicit $vtype
  ; CHECK:   [[SLLI:%[0-9]+]]:gpr = SLLI [[COPY]], 32
  ; CHECK:   [[SRLI:%[0-9]+]]:gpr = SRLI killed [[SLLI]], 32
  ; CHECK:   [[DEF2:%[0-9]+]]:vrm2nov0 = IMPLICIT_DEF
  ; CHECK:   dead $x0 = PseudoVSETVLI [[SRLI]], 89, implicit-def $vl, implicit-def $vtype
  ; CHECK:   [[PseudoVFADD_VV_M2_MASK:%[0-9]+]]:vrm2nov0 = PseudoVFADD_VV_M2_MASK [[DEF2]], killed [[PseudoEPIVLE64_V_M2_]], killed [[PseudoEPIVLE64_V_M2_1]], [[COPY1]], $noreg, 6, implicit $vl, implicit $vtype
  ; CHECK:   $v0 = COPY [[COPY1]]
  ; CHECK:   dead $x0 = PseudoVSETVLI [[COPY]], 89, implicit-def $vl, implicit-def $vtype
  ; CHECK:   PseudoEPIVSE64_V_M2 killed [[PseudoVFADD_VV_M2_MASK]], killed [[ADDI]], $v0, $noreg, 64, implicit $vl, implicit $vtype
  ; CHECK:   PseudoRET
  %store_addr = bitcast i8* @scratch to <vscale x 2 x double>*

  %i0 = call <vscale x 2 x double> @llvm.vp.load.nxv2f64(<vscale x 2 x double>* %a0, <vscale x 2 x i1> %m, i32 %n)
  %i1 = call <vscale x 2 x double> @llvm.vp.load.nxv2f64(<vscale x 2 x double>* %a1, <vscale x 2 x i1> %m, i32 %n)

  %r0 = call <vscale x 2 x double> @llvm.vp.fadd.nxv2f64(<vscale x 2 x double> %i0, <vscale x 2 x double> %i1, <vscale x 2 x i1> %m, i32 %n)

  call void @llvm.vp.store.nxv2f64(<vscale x 2 x double> %r0, <vscale x 2 x double>* %store_addr, <vscale x 2 x i1> %m, i32 %n)

  ret void
}

declare <vscale x 2 x double> @llvm.vp.load.nxv2f64(<vscale x 2 x double>*, <vscale x 2 x i1>, i32)
declare void @llvm.vp.store.nxv2f64(<vscale x 2 x double>, <vscale x 2 x double>*, <vscale x 2 x i1>, i32)
declare <vscale x 2 x double> @llvm.vp.fadd.nxv2f64(<vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x i1>, i32)

define void @test_lmul_4(<vscale x 4 x double>* %a0, <vscale x 4 x double>* %a1, <vscale x 4 x double>* %a2, <vscale x 4 x i64>* %a3, <vscale x 4 x i1> %m, i32 %n) nounwind {
  ; CHECK-LABEL: name: test_lmul_4
  ; CHECK: bb.0 (%ir-block.0):
  ; CHECK:   liveins: $x10, $x11, $v0, $x14
  ; CHECK:   [[COPY:%[0-9]+]]:gpr = COPY $x14
  ; CHECK:   [[COPY1:%[0-9]+]]:vmv0 = COPY $v0
  ; CHECK:   [[COPY2:%[0-9]+]]:gpr = COPY $x11
  ; CHECK:   [[COPY3:%[0-9]+]]:gpr = COPY $x10
  ; CHECK:   [[LUI:%[0-9]+]]:gpr = LUI target-flags(riscv-hi) @scratch
  ; CHECK:   [[ADDI:%[0-9]+]]:gpr = ADDI killed [[LUI]], target-flags(riscv-lo) @scratch
  ; CHECK:   $v0 = COPY [[COPY1]]
  ; CHECK:   [[DEF:%[0-9]+]]:vrm4 = IMPLICIT_DEF
  ; CHECK:   dead $x0 = PseudoVSETVLI [[COPY]], 90, implicit-def $vl, implicit-def $vtype
  ; CHECK:   [[PseudoEPIVLE64_V_M4_:%[0-9]+]]:vrm4nov0 = PseudoEPIVLE64_V_M4 [[DEF]], [[COPY3]], $v0, $noreg, 64, implicit $vl, implicit $vtype
  ; CHECK:   $v0 = COPY [[COPY1]]
  ; CHECK:   [[DEF1:%[0-9]+]]:vrm4 = IMPLICIT_DEF
  ; CHECK:   [[PseudoEPIVLE64_V_M4_1:%[0-9]+]]:vrm4nov0 = PseudoEPIVLE64_V_M4 [[DEF1]], [[COPY2]], $v0, $noreg, 64, implicit $vl, implicit $vtype
  ; CHECK:   [[SLLI:%[0-9]+]]:gpr = SLLI [[COPY]], 32
  ; CHECK:   [[SRLI:%[0-9]+]]:gpr = SRLI killed [[SLLI]], 32
  ; CHECK:   [[DEF2:%[0-9]+]]:vrm4nov0 = IMPLICIT_DEF
  ; CHECK:   dead $x0 = PseudoVSETVLI [[SRLI]], 90, implicit-def $vl, implicit-def $vtype
  ; CHECK:   [[PseudoVFADD_VV_M4_MASK:%[0-9]+]]:vrm4nov0 = PseudoVFADD_VV_M4_MASK [[DEF2]], killed [[PseudoEPIVLE64_V_M4_]], killed [[PseudoEPIVLE64_V_M4_1]], [[COPY1]], $noreg, 6, implicit $vl, implicit $vtype
  ; CHECK:   $v0 = COPY [[COPY1]]
  ; CHECK:   dead $x0 = PseudoVSETVLI [[COPY]], 90, implicit-def $vl, implicit-def $vtype
  ; CHECK:   PseudoEPIVSE64_V_M4 killed [[PseudoVFADD_VV_M4_MASK]], killed [[ADDI]], $v0, $noreg, 64, implicit $vl, implicit $vtype
  ; CHECK:   PseudoRET
  %store_addr = bitcast i8* @scratch to <vscale x 4 x double>*

  %i0 = call <vscale x 4 x double> @llvm.vp.load.nxv4f64(<vscale x 4 x double>* %a0, <vscale x 4 x i1> %m, i32 %n)
  %i1 = call <vscale x 4 x double> @llvm.vp.load.nxv4f64(<vscale x 4 x double>* %a1, <vscale x 4 x i1> %m, i32 %n)

  %r0 = call <vscale x 4 x double> @llvm.vp.fadd.nxv4f64(<vscale x 4 x double> %i0, <vscale x 4 x double> %i1, <vscale x 4 x i1> %m, i32 %n)

  call void @llvm.vp.store.nxv4f64(<vscale x 4 x double> %r0, <vscale x 4 x double>* %store_addr, <vscale x 4 x i1> %m, i32 %n)

  ret void
}

declare <vscale x 4 x double> @llvm.vp.load.nxv4f64(<vscale x 4 x double>*, <vscale x 4 x i1>, i32)
declare void @llvm.vp.store.nxv4f64(<vscale x 4 x double>, <vscale x 4 x double>*, <vscale x 4 x i1>, i32)
declare <vscale x 4 x double> @llvm.vp.fadd.nxv4f64(<vscale x 4 x double>, <vscale x 4 x double>, <vscale x 4 x i1>, i32)

define void @test_lmul_8(<vscale x 8 x double>* %a0, <vscale x 8 x double>* %a1, <vscale x 8 x double>* %a2, <vscale x 8 x i64>* %a3, <vscale x 8 x i1> %m, i32 %n) nounwind {
  ; CHECK-LABEL: name: test_lmul_8
  ; CHECK: bb.0 (%ir-block.0):
  ; CHECK:   liveins: $x10, $x11, $v0, $x14
  ; CHECK:   [[COPY:%[0-9]+]]:gpr = COPY $x14
  ; CHECK:   [[COPY1:%[0-9]+]]:vmv0 = COPY $v0
  ; CHECK:   [[COPY2:%[0-9]+]]:gpr = COPY $x11
  ; CHECK:   [[COPY3:%[0-9]+]]:gpr = COPY $x10
  ; CHECK:   [[LUI:%[0-9]+]]:gpr = LUI target-flags(riscv-hi) @scratch
  ; CHECK:   [[ADDI:%[0-9]+]]:gpr = ADDI killed [[LUI]], target-flags(riscv-lo) @scratch
  ; CHECK:   $v0 = COPY [[COPY1]]
  ; CHECK:   [[DEF:%[0-9]+]]:vrm8 = IMPLICIT_DEF
  ; CHECK:   dead $x0 = PseudoVSETVLI [[COPY]], 91, implicit-def $vl, implicit-def $vtype
  ; CHECK:   [[PseudoEPIVLE64_V_M8_:%[0-9]+]]:vrm8nov0 = PseudoEPIVLE64_V_M8 [[DEF]], [[COPY3]], $v0, $noreg, 64, implicit $vl, implicit $vtype
  ; CHECK:   $v0 = COPY [[COPY1]]
  ; CHECK:   [[DEF1:%[0-9]+]]:vrm8 = IMPLICIT_DEF
  ; CHECK:   [[PseudoEPIVLE64_V_M8_1:%[0-9]+]]:vrm8nov0 = PseudoEPIVLE64_V_M8 [[DEF1]], [[COPY2]], $v0, $noreg, 64, implicit $vl, implicit $vtype
  ; CHECK:   [[SLLI:%[0-9]+]]:gpr = SLLI [[COPY]], 32
  ; CHECK:   [[SRLI:%[0-9]+]]:gpr = SRLI killed [[SLLI]], 32
  ; CHECK:   [[DEF2:%[0-9]+]]:vrm8nov0 = IMPLICIT_DEF
  ; CHECK:   dead $x0 = PseudoVSETVLI [[SRLI]], 91, implicit-def $vl, implicit-def $vtype
  ; CHECK:   [[PseudoVFADD_VV_M8_MASK:%[0-9]+]]:vrm8nov0 = PseudoVFADD_VV_M8_MASK [[DEF2]], killed [[PseudoEPIVLE64_V_M8_]], killed [[PseudoEPIVLE64_V_M8_1]], [[COPY1]], $noreg, 6, implicit $vl, implicit $vtype
  ; CHECK:   $v0 = COPY [[COPY1]]
  ; CHECK:   dead $x0 = PseudoVSETVLI [[COPY]], 91, implicit-def $vl, implicit-def $vtype
  ; CHECK:   PseudoEPIVSE64_V_M8 killed [[PseudoVFADD_VV_M8_MASK]], killed [[ADDI]], $v0, $noreg, 64, implicit $vl, implicit $vtype
  ; CHECK:   PseudoRET
  %store_addr = bitcast i8* @scratch to <vscale x 8 x double>*

  %i0 = call <vscale x 8 x double> @llvm.vp.load.nxv8f64(<vscale x 8 x double>* %a0, <vscale x 8 x i1> %m, i32 %n)
  %i1 = call <vscale x 8 x double> @llvm.vp.load.nxv8f64(<vscale x 8 x double>* %a1, <vscale x 8 x i1> %m, i32 %n)

  %r0 = call <vscale x 8 x double> @llvm.vp.fadd.nxv8f64(<vscale x 8 x double> %i0, <vscale x 8 x double> %i1, <vscale x 8 x i1> %m, i32 %n)

  call void @llvm.vp.store.nxv8f64(<vscale x 8 x double> %r0, <vscale x 8 x double>* %store_addr, <vscale x 8 x i1> %m, i32 %n)

  ret void
}

declare <vscale x 8 x double> @llvm.vp.load.nxv8f64(<vscale x 8 x double>*, <vscale x 8 x i1>, i32)
declare void @llvm.vp.store.nxv8f64(<vscale x 8 x double>, <vscale x 8 x double>*, <vscale x 8 x i1>, i32)
declare <vscale x 8 x double> @llvm.vp.fadd.nxv8f64(<vscale x 8 x double>, <vscale x 8 x double>, <vscale x 8 x i1>, i32)
