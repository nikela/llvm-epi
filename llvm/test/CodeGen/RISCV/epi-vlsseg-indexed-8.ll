; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple riscv64 -o - %s \
; RUN:     -mattr=+experimental-v,+experimental-zvlsseg | FileCheck %s

declare { <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vlseg8.indexed.nxv1f64(double *%a, <vscale x 1 x i64> %index, i64 %avl)

define <vscale x 1 x double> @test_vlseg8_f64(double *%a, <vscale x 1 x i64> %index, i64 %avl) nounwind {
; CHECK-LABEL: test_vlseg8_f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a1, a1, e64,m1,tu,mu
; CHECK-NEXT:    vlxseg8ei64.v v16, (a0), v16
; CHECK-NEXT:    # kill: def $v16 killed $v16 killed $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    ret
  %x = call { <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double> } @llvm.epi.vlseg8.indexed.nxv1f64(double* %a, <vscale x 1 x i64> %index, i64 %avl)
  %y = extractvalue { <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double> } %x, 0
  ret <vscale x 1 x double> %y
}

declare void @llvm.epi.vsseg8.indexed.nxv1f64(<vscale x 1 x double> %v0, <vscale x 1 x double> %v1, <vscale x 1 x double> %v2, <vscale x 1 x double> %v3, <vscale x 1 x double> %v4, <vscale x 1 x double> %v5, <vscale x 1 x double> %v6, <vscale x 1 x double> %v7, double *%a, <vscale x 1 x i64> %index, i64 %avl)

define void @test_vsseg8_f64(<vscale x 1 x double> %v0, <vscale x 1 x double> %v1, <vscale x 1 x double> %v2, <vscale x 1 x double> %v3, <vscale x 1 x double> %v4, <vscale x 1 x double> %v5, <vscale x 1 x double> %v6, <vscale x 1 x double> %v7, double *%a, <vscale x 1 x i64> %index, i64 %avl) nounwind {
; CHECK-LABEL: test_vsseg8_f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    # kill: def $v23 killed $v23 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v22 killed $v22 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v21 killed $v21 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    vsetvli zero, a2, e64,m1,tu,mu
; CHECK-NEXT:    # kill: def $v20 killed $v20 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    vle64.v v1, (a1)
; CHECK-NEXT:    # kill: def $v19 killed $v19 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v18 killed $v18 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v17 killed $v17 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v16 killed $v16 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    vsxseg8ei64.v v16, (a0), v1
; CHECK-NEXT:    ret
  call void @llvm.epi.vsseg8.indexed.nxv1f64(<vscale x 1 x double> %v0, <vscale x 1 x double> %v1, <vscale x 1 x double> %v2, <vscale x 1 x double> %v3, <vscale x 1 x double> %v4, <vscale x 1 x double> %v5, <vscale x 1 x double> %v6, <vscale x 1 x double> %v7, double *%a, <vscale x 1 x i64> %index, i64 %avl)
  ret void
}

declare { <vscale x 2 x float>, <vscale x 2 x float>, <vscale x 2 x float>, <vscale x 2 x float>, <vscale x 2 x float>, <vscale x 2 x float>, <vscale x 2 x float>, <vscale x 2 x float> } @llvm.epi.vlseg8.indexed.nxv2f32(float *%a, <vscale x 2 x i32> %index, i64 %avl)

define <vscale x 2 x float> @test_vlseg8_f32(float *%a, <vscale x 2 x i32> %index, i64 %avl) nounwind {
; CHECK-LABEL: test_vlseg8_f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a1, a1, e32,m1,tu,mu
; CHECK-NEXT:    vlxseg8ei32.v v16, (a0), v16
; CHECK-NEXT:    # kill: def $v16 killed $v16 killed $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    ret
  %x = call { <vscale x 2 x float>, <vscale x 2 x float>, <vscale x 2 x float>, <vscale x 2 x float>, <vscale x 2 x float>, <vscale x 2 x float>, <vscale x 2 x float>, <vscale x 2 x float> } @llvm.epi.vlseg8.indexed.nxv2f32(float* %a, <vscale x 2 x i32> %index, i64 %avl)
  %y = extractvalue { <vscale x 2 x float>, <vscale x 2 x float>, <vscale x 2 x float>, <vscale x 2 x float>, <vscale x 2 x float>, <vscale x 2 x float>, <vscale x 2 x float>, <vscale x 2 x float> } %x, 0
  ret <vscale x 2 x float> %y
}

declare void @llvm.epi.vsseg8.indexed.nxv2f32(<vscale x 2 x float> %v0, <vscale x 2 x float> %v1, <vscale x 2 x float> %v2, <vscale x 2 x float> %v3, <vscale x 2 x float> %v4, <vscale x 2 x float> %v5, <vscale x 2 x float> %v6, <vscale x 2 x float> %v7, float *%a, <vscale x 2 x i32> %index, i64 %avl)

define void @test_vsseg8_f32(<vscale x 2 x float> %v0, <vscale x 2 x float> %v1, <vscale x 2 x float> %v2, <vscale x 2 x float> %v3, <vscale x 2 x float> %v4, <vscale x 2 x float> %v5, <vscale x 2 x float> %v6, <vscale x 2 x float> %v7, float *%a, <vscale x 2 x i32> %index, i64 %avl) nounwind {
; CHECK-LABEL: test_vsseg8_f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    # kill: def $v23 killed $v23 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v22 killed $v22 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v21 killed $v21 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    vsetvli zero, a2, e32,m1,tu,mu
; CHECK-NEXT:    # kill: def $v20 killed $v20 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    vle32.v v1, (a1)
; CHECK-NEXT:    # kill: def $v19 killed $v19 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v18 killed $v18 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v17 killed $v17 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v16 killed $v16 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    vsxseg8ei32.v v16, (a0), v1
; CHECK-NEXT:    ret
  call void @llvm.epi.vsseg8.indexed.nxv2f32(<vscale x 2 x float> %v0, <vscale x 2 x float> %v1, <vscale x 2 x float> %v2, <vscale x 2 x float> %v3, <vscale x 2 x float> %v4, <vscale x 2 x float> %v5, <vscale x 2 x float> %v6, <vscale x 2 x float> %v7, float *%a, <vscale x 2 x i32> %index, i64 %avl)
  ret void
}

declare { <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64> } @llvm.epi.vlseg8.indexed.nxv1i64(i64 *%a, <vscale x 1 x i64> %index, i64 %avl)

define <vscale x 1 x i64> @test_vlseg8_i64(i64 *%a, <vscale x 1 x i64> %index, i64 %avl) nounwind {
; CHECK-LABEL: test_vlseg8_i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a1, a1, e64,m1,tu,mu
; CHECK-NEXT:    vlxseg8ei64.v v16, (a0), v16
; CHECK-NEXT:    # kill: def $v16 killed $v16 killed $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    ret
  %x = call { <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64> } @llvm.epi.vlseg8.indexed.nxv1i64(i64* %a, <vscale x 1 x i64> %index, i64 %avl)
  %y = extractvalue { <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64>, <vscale x 1 x i64> } %x, 0
  ret <vscale x 1 x i64> %y
}

declare void @llvm.epi.vsseg8.indexed.nxv1i64(<vscale x 1 x i64> %v0, <vscale x 1 x i64> %v1, <vscale x 1 x i64> %v2, <vscale x 1 x i64> %v3, <vscale x 1 x i64> %v4, <vscale x 1 x i64> %v5, <vscale x 1 x i64> %v6, <vscale x 1 x i64> %v7, i64 *%a, <vscale x 1 x i64> %index, i64 %avl)

define void @test_vsseg8_i64(<vscale x 1 x i64> %v0, <vscale x 1 x i64> %v1, <vscale x 1 x i64> %v2, <vscale x 1 x i64> %v3, <vscale x 1 x i64> %v4, <vscale x 1 x i64> %v5, <vscale x 1 x i64> %v6, <vscale x 1 x i64> %v7, i64 *%a, <vscale x 1 x i64> %index, i64 %avl) nounwind {
; CHECK-LABEL: test_vsseg8_i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    # kill: def $v23 killed $v23 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v22 killed $v22 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v21 killed $v21 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    vsetvli zero, a2, e64,m1,tu,mu
; CHECK-NEXT:    # kill: def $v20 killed $v20 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    vle64.v v1, (a1)
; CHECK-NEXT:    # kill: def $v19 killed $v19 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v18 killed $v18 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v17 killed $v17 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v16 killed $v16 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    vsxseg8ei64.v v16, (a0), v1
; CHECK-NEXT:    ret
  call void @llvm.epi.vsseg8.indexed.nxv1i64(<vscale x 1 x i64> %v0, <vscale x 1 x i64> %v1, <vscale x 1 x i64> %v2, <vscale x 1 x i64> %v3, <vscale x 1 x i64> %v4, <vscale x 1 x i64> %v5, <vscale x 1 x i64> %v6, <vscale x 1 x i64> %v7, i64 *%a, <vscale x 1 x i64> %index, i64 %avl)
  ret void
}

declare { <vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i32> } @llvm.epi.vlseg8.indexed.nxv2i32(i32 *%a, <vscale x 2 x i32> %index, i64 %avl)

define <vscale x 2 x i32> @test_vlseg8_i32(i32 *%a, <vscale x 2 x i32> %index, i64 %avl) nounwind {
; CHECK-LABEL: test_vlseg8_i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a1, a1, e32,m1,tu,mu
; CHECK-NEXT:    vlxseg8ei32.v v16, (a0), v16
; CHECK-NEXT:    # kill: def $v16 killed $v16 killed $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    ret
  %x = call { <vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i32> } @llvm.epi.vlseg8.indexed.nxv2i32(i32* %a, <vscale x 2 x i32> %index, i64 %avl)
  %y = extractvalue { <vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i32>, <vscale x 2 x i32> } %x, 0
  ret <vscale x 2 x i32> %y
}

declare void @llvm.epi.vsseg8.indexed.nxv2i32(<vscale x 2 x i32> %v0, <vscale x 2 x i32> %v1, <vscale x 2 x i32> %v2, <vscale x 2 x i32> %v3, <vscale x 2 x i32> %v4, <vscale x 2 x i32> %v5, <vscale x 2 x i32> %v6, <vscale x 2 x i32> %v7, i32 *%a, <vscale x 2 x i32> %index, i64 %avl)

define void @test_vsseg8_i32(<vscale x 2 x i32> %v0, <vscale x 2 x i32> %v1, <vscale x 2 x i32> %v2, <vscale x 2 x i32> %v3, <vscale x 2 x i32> %v4, <vscale x 2 x i32> %v5, <vscale x 2 x i32> %v6, <vscale x 2 x i32> %v7, i32 *%a, <vscale x 2 x i32> %index, i64 %avl) nounwind {
; CHECK-LABEL: test_vsseg8_i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    # kill: def $v23 killed $v23 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v22 killed $v22 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v21 killed $v21 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    vsetvli zero, a2, e32,m1,tu,mu
; CHECK-NEXT:    # kill: def $v20 killed $v20 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    vle32.v v1, (a1)
; CHECK-NEXT:    # kill: def $v19 killed $v19 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v18 killed $v18 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v17 killed $v17 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v16 killed $v16 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    vsxseg8ei32.v v16, (a0), v1
; CHECK-NEXT:    ret
  call void @llvm.epi.vsseg8.indexed.nxv2i32(<vscale x 2 x i32> %v0, <vscale x 2 x i32> %v1, <vscale x 2 x i32> %v2, <vscale x 2 x i32> %v3, <vscale x 2 x i32> %v4, <vscale x 2 x i32> %v5, <vscale x 2 x i32> %v6, <vscale x 2 x i32> %v7, i32 *%a, <vscale x 2 x i32> %index, i64 %avl)
  ret void
}

declare { <vscale x 4 x i16>, <vscale x 4 x i16>, <vscale x 4 x i16>, <vscale x 4 x i16>, <vscale x 4 x i16>, <vscale x 4 x i16>, <vscale x 4 x i16>, <vscale x 4 x i16> } @llvm.epi.vlseg8.indexed.nxv4i16(i16 *%a, <vscale x 4 x i16> %index, i64 %avl)

define <vscale x 4 x i16> @test_vlseg8_i16(i16 *%a, <vscale x 4 x i16> %index, i64 %avl) nounwind {
; CHECK-LABEL: test_vlseg8_i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a1, a1, e16,m1,tu,mu
; CHECK-NEXT:    vlxseg8ei16.v v16, (a0), v16
; CHECK-NEXT:    # kill: def $v16 killed $v16 killed $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    ret
  %x = call { <vscale x 4 x i16>, <vscale x 4 x i16>, <vscale x 4 x i16>, <vscale x 4 x i16>, <vscale x 4 x i16>, <vscale x 4 x i16>, <vscale x 4 x i16>, <vscale x 4 x i16> } @llvm.epi.vlseg8.indexed.nxv4i16(i16* %a, <vscale x 4 x i16> %index, i64 %avl)
  %y = extractvalue { <vscale x 4 x i16>, <vscale x 4 x i16>, <vscale x 4 x i16>, <vscale x 4 x i16>, <vscale x 4 x i16>, <vscale x 4 x i16>, <vscale x 4 x i16>, <vscale x 4 x i16> } %x, 0
  ret <vscale x 4 x i16> %y
}

declare void @llvm.epi.vsseg8.indexed.nxv4i16(<vscale x 4 x i16> %v0, <vscale x 4 x i16> %v1, <vscale x 4 x i16> %v2, <vscale x 4 x i16> %v3, <vscale x 4 x i16> %v4, <vscale x 4 x i16> %v5, <vscale x 4 x i16> %v6, <vscale x 4 x i16> %v7, i16 *%a, <vscale x 4 x i16> %index, i64 %avl)

define void @test_vsseg8_i16(<vscale x 4 x i16> %v0, <vscale x 4 x i16> %v1, <vscale x 4 x i16> %v2, <vscale x 4 x i16> %v3, <vscale x 4 x i16> %v4, <vscale x 4 x i16> %v5, <vscale x 4 x i16> %v6, <vscale x 4 x i16> %v7, i16 *%a, <vscale x 4 x i16> %index, i64 %avl) nounwind {
; CHECK-LABEL: test_vsseg8_i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    # kill: def $v23 killed $v23 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v22 killed $v22 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v21 killed $v21 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    vsetvli zero, a2, e16,m1,tu,mu
; CHECK-NEXT:    # kill: def $v20 killed $v20 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    vle16.v v1, (a1)
; CHECK-NEXT:    # kill: def $v19 killed $v19 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v18 killed $v18 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v17 killed $v17 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v16 killed $v16 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    vsxseg8ei16.v v16, (a0), v1
; CHECK-NEXT:    ret
  call void @llvm.epi.vsseg8.indexed.nxv4i16(<vscale x 4 x i16> %v0, <vscale x 4 x i16> %v1, <vscale x 4 x i16> %v2, <vscale x 4 x i16> %v3, <vscale x 4 x i16> %v4, <vscale x 4 x i16> %v5, <vscale x 4 x i16> %v6, <vscale x 4 x i16> %v7, i16 *%a, <vscale x 4 x i16> %index, i64 %avl)
  ret void
}

declare { <vscale x 8 x i8>, <vscale x 8 x i8>, <vscale x 8 x i8>, <vscale x 8 x i8>, <vscale x 8 x i8>, <vscale x 8 x i8>, <vscale x 8 x i8>, <vscale x 8 x i8> } @llvm.epi.vlseg8.indexed.nxv8i8(i8 *%a, <vscale x 8 x i8> %index, i64 %avl)

define <vscale x 8 x i8> @test_vlseg8_i8(i8 *%a, <vscale x 8 x i8> %index, i64 %avl) nounwind {
; CHECK-LABEL: test_vlseg8_i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a1, a1, e8,m1,tu,mu
; CHECK-NEXT:    vlxseg8ei8.v v16, (a0), v16
; CHECK-NEXT:    # kill: def $v16 killed $v16 killed $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    ret
  %x = call { <vscale x 8 x i8>, <vscale x 8 x i8>, <vscale x 8 x i8>, <vscale x 8 x i8>, <vscale x 8 x i8>, <vscale x 8 x i8>, <vscale x 8 x i8>, <vscale x 8 x i8> } @llvm.epi.vlseg8.indexed.nxv8i8(i8* %a, <vscale x 8 x i8> %index, i64 %avl)
  %y = extractvalue { <vscale x 8 x i8>, <vscale x 8 x i8>, <vscale x 8 x i8>, <vscale x 8 x i8>, <vscale x 8 x i8>, <vscale x 8 x i8>, <vscale x 8 x i8>, <vscale x 8 x i8> } %x, 0
  ret <vscale x 8 x i8> %y
}

declare void @llvm.epi.vsseg8.indexed.nxv8i8(<vscale x 8 x i8> %v0, <vscale x 8 x i8> %v1, <vscale x 8 x i8> %v2, <vscale x 8 x i8> %v3, <vscale x 8 x i8> %v4, <vscale x 8 x i8> %v5, <vscale x 8 x i8> %v6, <vscale x 8 x i8> %v7, i8 *%a, <vscale x 8 x i8> %index, i64 %avl)

define void @test_vsseg8_i8(<vscale x 8 x i8> %v0, <vscale x 8 x i8> %v1, <vscale x 8 x i8> %v2, <vscale x 8 x i8> %v3, <vscale x 8 x i8> %v4, <vscale x 8 x i8> %v5, <vscale x 8 x i8> %v6, <vscale x 8 x i8> %v7, i8 *%a, <vscale x 8 x i8> %index, i64 %avl) nounwind {
; CHECK-LABEL: test_vsseg8_i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    # kill: def $v23 killed $v23 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v22 killed $v22 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v21 killed $v21 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    vsetvli zero, a2, e8,m1,tu,mu
; CHECK-NEXT:    # kill: def $v20 killed $v20 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    vle8.v v1, (a1)
; CHECK-NEXT:    # kill: def $v19 killed $v19 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v18 killed $v18 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v17 killed $v17 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    # kill: def $v16 killed $v16 killed $v16_v17_v18_v19_v20_v21_v22_v23 def $v16_v17_v18_v19_v20_v21_v22_v23
; CHECK-NEXT:    vsxseg8ei8.v v16, (a0), v1
; CHECK-NEXT:    ret
  call void @llvm.epi.vsseg8.indexed.nxv8i8(<vscale x 8 x i8> %v0, <vscale x 8 x i8> %v1, <vscale x 8 x i8> %v2, <vscale x 8 x i8> %v3, <vscale x 8 x i8> %v4, <vscale x 8 x i8> %v5, <vscale x 8 x i8> %v6, <vscale x 8 x i8> %v7, i8 *%a, <vscale x 8 x i8> %index, i64 %avl)
  ret void
}
