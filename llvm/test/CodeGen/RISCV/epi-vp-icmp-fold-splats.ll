; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+experimental-v -verify-machineinstrs -O0 \
; RUN:    < %s | FileCheck --check-prefix=CHECK-O0 %s
; RUN: llc -mtriple=riscv64 -mattr=+experimental-v -verify-machineinstrs -O2 \
; RUN:    < %s | FileCheck --check-prefix=CHECK-O2 %s

@scratch = global i8 0, align 16

define void @test_vp_fold_unsigned_greater(<vscale x 1 x i64> %a, i64 %b, <vscale x 1 x i1> %m, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_fold_unsigned_greater:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -48
; CHECK-O0-NEXT:    sd ra, 40(sp)
; CHECK-O0-NEXT:    sd s0, 32(sp)
; CHECK-O0-NEXT:    addi s0, sp, 48
; CHECK-O0-NEXT:    rdvlenb a2
; CHECK-O0-NEXT:    sub sp, sp, a2
; CHECK-O0-NEXT:    sd sp, -40(s0)
; CHECK-O0-NEXT:    # kill: def $x12 killed $x11
; CHECK-O0-NEXT:    lui a2, %hi(scratch)
; CHECK-O0-NEXT:    addi a2, a2, %lo(scratch)
; CHECK-O0-NEXT:    vsetvli a3, zero, e64,m1,tu,mu
; CHECK-O0-NEXT:    vmv.v.x v1, a0
; CHECK-O0-NEXT:    ld a3, -40(s0)
; CHECK-O0-NEXT:    vs1r.v v0, (a3)
; CHECK-O0-NEXT:    # implicit-def: $v2
; CHECK-O0-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O0-NEXT:    vmsgtu.vx v2, v16, a0, v0.t
; CHECK-O0-NEXT:    vsetvli a3, zero, e8,m1,tu,mu
; CHECK-O0-NEXT:    vse8.v v2, (a2)
; CHECK-O0-NEXT:    ld a3, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a3)
; CHECK-O0-NEXT:    # implicit-def: $v2
; CHECK-O0-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O0-NEXT:    vmsltu.vx v2, v16, a0, v0.t
; CHECK-O0-NEXT:    vsetvli a3, zero, e8,m1,tu,mu
; CHECK-O0-NEXT:    vse8.v v2, (a2)
; CHECK-O0-NEXT:    ld a3, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a3)
; CHECK-O0-NEXT:    # implicit-def: $v2
; CHECK-O0-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O0-NEXT:    vmsleu.vv v2, v1, v16, v0.t
; CHECK-O0-NEXT:    vsetvli a3, zero, e8,m1,tu,mu
; CHECK-O0-NEXT:    vse8.v v2, (a2)
; CHECK-O0-NEXT:    ld a3, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a3)
; CHECK-O0-NEXT:    # implicit-def: $v1
; CHECK-O0-NEXT:    vsetvli a1, a1, e64,m1,tu,mu
; CHECK-O0-NEXT:    vmsleu.vx v1, v16, a0, v0.t
; CHECK-O0-NEXT:    vsetvli a0, zero, e8,m1,tu,mu
; CHECK-O0-NEXT:    vse8.v v1, (a2)
; CHECK-O0-NEXT:    addi sp, s0, -48
; CHECK-O0-NEXT:    ld s0, 32(sp)
; CHECK-O0-NEXT:    ld ra, 40(sp)
; CHECK-O0-NEXT:    addi sp, sp, 48
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_fold_unsigned_greater:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    lui a2, %hi(scratch)
; CHECK-O2-NEXT:    addi a2, a2, %lo(scratch)
; CHECK-O2-NEXT:    vsetvli a3, zero, e64,m1,tu,mu
; CHECK-O2-NEXT:    vmv.v.x v1, a0
; CHECK-O2-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O2-NEXT:    vmsgtu.vx v2, v16, a0, v0.t
; CHECK-O2-NEXT:    vsetvli a3, zero, e8,m1,tu,mu
; CHECK-O2-NEXT:    vse8.v v2, (a2)
; CHECK-O2-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O2-NEXT:    vmsltu.vx v2, v16, a0, v0.t
; CHECK-O2-NEXT:    vsetvli a3, zero, e8,m1,tu,mu
; CHECK-O2-NEXT:    vse8.v v2, (a2)
; CHECK-O2-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O2-NEXT:    vmsleu.vv v1, v1, v16, v0.t
; CHECK-O2-NEXT:    vsetvli a3, zero, e8,m1,tu,mu
; CHECK-O2-NEXT:    vse8.v v1, (a2)
; CHECK-O2-NEXT:    vsetvli a1, a1, e64,m1,tu,mu
; CHECK-O2-NEXT:    vmsleu.vx v1, v16, a0, v0.t
; CHECK-O2-NEXT:    vsetvli a0, zero, e8,m1,tu,mu
; CHECK-O2-NEXT:    vse8.v v1, (a2)
; CHECK-O2-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 1 x i1>*

  %head = insertelement <vscale x 1 x i64> undef, i64 %b, i32 0
  %splat = shufflevector <vscale x 1 x i64> %head, <vscale x 1 x i64> undef, <vscale x 1 x i32> zeroinitializer

  ; x > splat(y) → vmsgtu.vx x, y
  %ugt.0 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %splat, i8 34, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %ugt.0, <vscale x 1 x i1>* %store_addr

  ; splat(y) > x → x < splat(y) → vmsltu.vx x, y
  %ugt.1 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %splat, <vscale x 1 x i64> %a, i8 34, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %ugt.1, <vscale x 1 x i1>* %store_addr

  ; x >= splat(y) → cannot be folded (vmsleu.vv)
  %uge.0 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %splat, i8 35, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %uge.0, <vscale x 1 x i1>* %store_addr

  ; splat(y) >= x → x <= splat(y) → vmsleu.vx x, y
  %uge.1 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %splat, <vscale x 1 x i64> %a, i8 35, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %uge.1, <vscale x 1 x i1>* %store_addr

  ret void
}

define void @test_vp_fold_unsigned_lower(<vscale x 1 x i64> %a, i64 %b, <vscale x 1 x i1> %m, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_fold_unsigned_lower:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -48
; CHECK-O0-NEXT:    sd ra, 40(sp)
; CHECK-O0-NEXT:    sd s0, 32(sp)
; CHECK-O0-NEXT:    addi s0, sp, 48
; CHECK-O0-NEXT:    rdvlenb a2
; CHECK-O0-NEXT:    sub sp, sp, a2
; CHECK-O0-NEXT:    sd sp, -40(s0)
; CHECK-O0-NEXT:    # kill: def $x12 killed $x11
; CHECK-O0-NEXT:    lui a2, %hi(scratch)
; CHECK-O0-NEXT:    addi a2, a2, %lo(scratch)
; CHECK-O0-NEXT:    vsetvli a3, zero, e64,m1,tu,mu
; CHECK-O0-NEXT:    vmv.v.x v1, a0
; CHECK-O0-NEXT:    ld a3, -40(s0)
; CHECK-O0-NEXT:    vs1r.v v0, (a3)
; CHECK-O0-NEXT:    # implicit-def: $v2
; CHECK-O0-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O0-NEXT:    vmsltu.vx v2, v16, a0, v0.t
; CHECK-O0-NEXT:    vsetvli a3, zero, e8,m1,tu,mu
; CHECK-O0-NEXT:    vse8.v v2, (a2)
; CHECK-O0-NEXT:    ld a3, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a3)
; CHECK-O0-NEXT:    # implicit-def: $v2
; CHECK-O0-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O0-NEXT:    vmsgtu.vx v2, v16, a0, v0.t
; CHECK-O0-NEXT:    vsetvli a3, zero, e8,m1,tu,mu
; CHECK-O0-NEXT:    vse8.v v2, (a2)
; CHECK-O0-NEXT:    ld a3, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a3)
; CHECK-O0-NEXT:    # implicit-def: $v2
; CHECK-O0-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O0-NEXT:    vmsleu.vx v2, v16, a0, v0.t
; CHECK-O0-NEXT:    vsetvli a0, zero, e8,m1,tu,mu
; CHECK-O0-NEXT:    vse8.v v2, (a2)
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v2
; CHECK-O0-NEXT:    vsetvli a0, a1, e64,m1,tu,mu
; CHECK-O0-NEXT:    vmsleu.vv v2, v1, v16, v0.t
; CHECK-O0-NEXT:    vsetvli a0, zero, e8,m1,tu,mu
; CHECK-O0-NEXT:    vse8.v v2, (a2)
; CHECK-O0-NEXT:    addi sp, s0, -48
; CHECK-O0-NEXT:    ld s0, 32(sp)
; CHECK-O0-NEXT:    ld ra, 40(sp)
; CHECK-O0-NEXT:    addi sp, sp, 48
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_fold_unsigned_lower:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    lui a2, %hi(scratch)
; CHECK-O2-NEXT:    addi a2, a2, %lo(scratch)
; CHECK-O2-NEXT:    vsetvli a3, zero, e64,m1,tu,mu
; CHECK-O2-NEXT:    vmv.v.x v1, a0
; CHECK-O2-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O2-NEXT:    vmsltu.vx v2, v16, a0, v0.t
; CHECK-O2-NEXT:    vsetvli a3, zero, e8,m1,tu,mu
; CHECK-O2-NEXT:    vse8.v v2, (a2)
; CHECK-O2-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O2-NEXT:    vmsgtu.vx v2, v16, a0, v0.t
; CHECK-O2-NEXT:    vsetvli a3, zero, e8,m1,tu,mu
; CHECK-O2-NEXT:    vse8.v v2, (a2)
; CHECK-O2-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O2-NEXT:    vmsleu.vx v2, v16, a0, v0.t
; CHECK-O2-NEXT:    vsetvli a0, zero, e8,m1,tu,mu
; CHECK-O2-NEXT:    vse8.v v2, (a2)
; CHECK-O2-NEXT:    vsetvli a0, a1, e64,m1,tu,mu
; CHECK-O2-NEXT:    vmsleu.vv v1, v1, v16, v0.t
; CHECK-O2-NEXT:    vsetvli a0, zero, e8,m1,tu,mu
; CHECK-O2-NEXT:    vse8.v v1, (a2)
; CHECK-O2-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 1 x i1>*

  %head = insertelement <vscale x 1 x i64> undef, i64 %b, i32 0
  %splat = shufflevector <vscale x 1 x i64> %head, <vscale x 1 x i64> undef, <vscale x 1 x i32> zeroinitializer

  ; x < splat(y) → vmsltu.vx x, y
  %ult.0 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %splat, i8 36, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %ult.0, <vscale x 1 x i1>* %store_addr

  ; splat(y) < x → x > splat(y) → vmsgtu.vx x, y
  %ult.1 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %splat, <vscale x 1 x i64> %a, i8 36, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %ult.1, <vscale x 1 x i1>* %store_addr

  ; x <= splat(y) → vmsleu.vx x, y
  %ule.0 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %splat, i8 37, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %ule.0, <vscale x 1 x i1>* %store_addr

  ; splat(y) <= y → y >= splat(y) → cannot be folded (vmsleu.vv)
  %ule.1 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %splat, <vscale x 1 x i64> %a, i8 37, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %ule.1, <vscale x 1 x i1>* %store_addr

  ret void
}

define void @test_vp_fold_signed_greater(<vscale x 1 x i64> %a, i64 %b, <vscale x 1 x i1> %m, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_fold_signed_greater:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -48
; CHECK-O0-NEXT:    sd ra, 40(sp)
; CHECK-O0-NEXT:    sd s0, 32(sp)
; CHECK-O0-NEXT:    addi s0, sp, 48
; CHECK-O0-NEXT:    rdvlenb a2
; CHECK-O0-NEXT:    sub sp, sp, a2
; CHECK-O0-NEXT:    sd sp, -40(s0)
; CHECK-O0-NEXT:    # kill: def $x12 killed $x11
; CHECK-O0-NEXT:    lui a2, %hi(scratch)
; CHECK-O0-NEXT:    addi a2, a2, %lo(scratch)
; CHECK-O0-NEXT:    vsetvli a3, zero, e64,m1,tu,mu
; CHECK-O0-NEXT:    vmv.v.x v1, a0
; CHECK-O0-NEXT:    ld a3, -40(s0)
; CHECK-O0-NEXT:    vs1r.v v0, (a3)
; CHECK-O0-NEXT:    # implicit-def: $v2
; CHECK-O0-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O0-NEXT:    vmsgt.vx v2, v16, a0, v0.t
; CHECK-O0-NEXT:    vsetvli a3, zero, e8,m1,tu,mu
; CHECK-O0-NEXT:    vse8.v v2, (a2)
; CHECK-O0-NEXT:    ld a3, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a3)
; CHECK-O0-NEXT:    # implicit-def: $v2
; CHECK-O0-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O0-NEXT:    vmslt.vx v2, v16, a0, v0.t
; CHECK-O0-NEXT:    vsetvli a3, zero, e8,m1,tu,mu
; CHECK-O0-NEXT:    vse8.v v2, (a2)
; CHECK-O0-NEXT:    ld a3, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a3)
; CHECK-O0-NEXT:    # implicit-def: $v2
; CHECK-O0-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O0-NEXT:    vmsle.vv v2, v1, v16, v0.t
; CHECK-O0-NEXT:    vsetvli a3, zero, e8,m1,tu,mu
; CHECK-O0-NEXT:    vse8.v v2, (a2)
; CHECK-O0-NEXT:    ld a3, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a3)
; CHECK-O0-NEXT:    # implicit-def: $v1
; CHECK-O0-NEXT:    vsetvli a1, a1, e64,m1,tu,mu
; CHECK-O0-NEXT:    vmsle.vx v1, v16, a0, v0.t
; CHECK-O0-NEXT:    vsetvli a0, zero, e8,m1,tu,mu
; CHECK-O0-NEXT:    vse8.v v1, (a2)
; CHECK-O0-NEXT:    addi sp, s0, -48
; CHECK-O0-NEXT:    ld s0, 32(sp)
; CHECK-O0-NEXT:    ld ra, 40(sp)
; CHECK-O0-NEXT:    addi sp, sp, 48
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_fold_signed_greater:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    lui a2, %hi(scratch)
; CHECK-O2-NEXT:    addi a2, a2, %lo(scratch)
; CHECK-O2-NEXT:    vsetvli a3, zero, e64,m1,tu,mu
; CHECK-O2-NEXT:    vmv.v.x v1, a0
; CHECK-O2-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O2-NEXT:    vmsgt.vx v2, v16, a0, v0.t
; CHECK-O2-NEXT:    vsetvli a3, zero, e8,m1,tu,mu
; CHECK-O2-NEXT:    vse8.v v2, (a2)
; CHECK-O2-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O2-NEXT:    vmslt.vx v2, v16, a0, v0.t
; CHECK-O2-NEXT:    vsetvli a3, zero, e8,m1,tu,mu
; CHECK-O2-NEXT:    vse8.v v2, (a2)
; CHECK-O2-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O2-NEXT:    vmsle.vv v1, v1, v16, v0.t
; CHECK-O2-NEXT:    vsetvli a3, zero, e8,m1,tu,mu
; CHECK-O2-NEXT:    vse8.v v1, (a2)
; CHECK-O2-NEXT:    vsetvli a1, a1, e64,m1,tu,mu
; CHECK-O2-NEXT:    vmsle.vx v1, v16, a0, v0.t
; CHECK-O2-NEXT:    vsetvli a0, zero, e8,m1,tu,mu
; CHECK-O2-NEXT:    vse8.v v1, (a2)
; CHECK-O2-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 1 x i1>*

  %head = insertelement <vscale x 1 x i64> undef, i64 %b, i32 0
  %splat = shufflevector <vscale x 1 x i64> %head, <vscale x 1 x i64> undef, <vscale x 1 x i32> zeroinitializer

  ; x > splat(y) → vmsgt.vx x, y
  %sgt.0 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %splat, i8 38, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %sgt.0, <vscale x 1 x i1>* %store_addr

  ; splat(y) > x → x < splat(y) → vmslt.vx x, y
  %sgt.1 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %splat, <vscale x 1 x i64> %a, i8 38, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %sgt.1, <vscale x 1 x i1>* %store_addr

  ; x >= splat(y) → cannot be folded (vmsle.vv)
  %sge.0 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %splat, i8 39, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %sge.0, <vscale x 1 x i1>* %store_addr

  ; splat(y) >= x → x <= splat(y) → vmsle.vx x, y
  %sge.1 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %splat, <vscale x 1 x i64> %a, i8 39, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %sge.1, <vscale x 1 x i1>* %store_addr

  ret void
}

define void @test_vp_fold_signed_lower(<vscale x 1 x i64> %a, i64 %b, <vscale x 1 x i1> %m, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_fold_signed_lower:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -48
; CHECK-O0-NEXT:    sd ra, 40(sp)
; CHECK-O0-NEXT:    sd s0, 32(sp)
; CHECK-O0-NEXT:    addi s0, sp, 48
; CHECK-O0-NEXT:    rdvlenb a2
; CHECK-O0-NEXT:    sub sp, sp, a2
; CHECK-O0-NEXT:    sd sp, -40(s0)
; CHECK-O0-NEXT:    # kill: def $x12 killed $x11
; CHECK-O0-NEXT:    lui a2, %hi(scratch)
; CHECK-O0-NEXT:    addi a2, a2, %lo(scratch)
; CHECK-O0-NEXT:    vsetvli a3, zero, e64,m1,tu,mu
; CHECK-O0-NEXT:    vmv.v.x v1, a0
; CHECK-O0-NEXT:    ld a3, -40(s0)
; CHECK-O0-NEXT:    vs1r.v v0, (a3)
; CHECK-O0-NEXT:    # implicit-def: $v2
; CHECK-O0-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O0-NEXT:    vmslt.vx v2, v16, a0, v0.t
; CHECK-O0-NEXT:    vsetvli a3, zero, e8,m1,tu,mu
; CHECK-O0-NEXT:    vse8.v v2, (a2)
; CHECK-O0-NEXT:    ld a3, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a3)
; CHECK-O0-NEXT:    # implicit-def: $v2
; CHECK-O0-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O0-NEXT:    vmslt.vv v2, v1, v16, v0.t
; CHECK-O0-NEXT:    vsetvli a3, zero, e8,m1,tu,mu
; CHECK-O0-NEXT:    vse8.v v2, (a2)
; CHECK-O0-NEXT:    ld a3, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a3)
; CHECK-O0-NEXT:    # implicit-def: $v2
; CHECK-O0-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O0-NEXT:    vmsle.vx v2, v16, a0, v0.t
; CHECK-O0-NEXT:    vsetvli a0, zero, e8,m1,tu,mu
; CHECK-O0-NEXT:    vse8.v v2, (a2)
; CHECK-O0-NEXT:    ld a0, -40(s0)
; CHECK-O0-NEXT:    vl1r.v v0, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v2
; CHECK-O0-NEXT:    vsetvli a0, a1, e64,m1,tu,mu
; CHECK-O0-NEXT:    vmsle.vv v2, v1, v16, v0.t
; CHECK-O0-NEXT:    vsetvli a0, zero, e8,m1,tu,mu
; CHECK-O0-NEXT:    vse8.v v2, (a2)
; CHECK-O0-NEXT:    addi sp, s0, -48
; CHECK-O0-NEXT:    ld s0, 32(sp)
; CHECK-O0-NEXT:    ld ra, 40(sp)
; CHECK-O0-NEXT:    addi sp, sp, 48
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_fold_signed_lower:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    lui a2, %hi(scratch)
; CHECK-O2-NEXT:    addi a2, a2, %lo(scratch)
; CHECK-O2-NEXT:    vsetvli a3, zero, e64,m1,tu,mu
; CHECK-O2-NEXT:    vmv.v.x v1, a0
; CHECK-O2-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O2-NEXT:    vmslt.vx v2, v16, a0, v0.t
; CHECK-O2-NEXT:    vsetvli a3, zero, e8,m1,tu,mu
; CHECK-O2-NEXT:    vse8.v v2, (a2)
; CHECK-O2-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O2-NEXT:    vmslt.vv v2, v1, v16, v0.t
; CHECK-O2-NEXT:    vsetvli a3, zero, e8,m1,tu,mu
; CHECK-O2-NEXT:    vse8.v v2, (a2)
; CHECK-O2-NEXT:    vsetvli a3, a1, e64,m1,tu,mu
; CHECK-O2-NEXT:    vmsle.vx v2, v16, a0, v0.t
; CHECK-O2-NEXT:    vsetvli a0, zero, e8,m1,tu,mu
; CHECK-O2-NEXT:    vse8.v v2, (a2)
; CHECK-O2-NEXT:    vsetvli a0, a1, e64,m1,tu,mu
; CHECK-O2-NEXT:    vmsle.vv v1, v1, v16, v0.t
; CHECK-O2-NEXT:    vsetvli a0, zero, e8,m1,tu,mu
; CHECK-O2-NEXT:    vse8.v v1, (a2)
; CHECK-O2-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 1 x i1>*

  %head = insertelement <vscale x 1 x i64> undef, i64 %b, i32 0
  %splat = shufflevector <vscale x 1 x i64> %head, <vscale x 1 x i64> undef, <vscale x 1 x i32> zeroinitializer

  ; y < splat(x) → vmslt.vx x, y
  %slt.0 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %splat, i8 40, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %slt.0, <vscale x 1 x i1>* %store_addr

  ; splat(x) < y → y > splat(x) → cannot be folded (vmslt.vv)
  %slt.1 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %splat, <vscale x 1 x i64> %a, i8 40, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %slt.1, <vscale x 1 x i1>* %store_addr

  ; x <= splat(y) → vmsle.vx x, y
  %sle.0 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %splat, i8 41, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %sle.0, <vscale x 1 x i1>* %store_addr

  ; splat(y) <= x → x >= splat(y) → cannot be folded (vmsle.vv)
  %sle.1 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %splat, <vscale x 1 x i64> %a, i8 41, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %sle.1, <vscale x 1 x i1>* %store_addr

  ret void
}

declare <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %b, i8 %kind, <vscale x 1 x i1> %m, i32 %n)
