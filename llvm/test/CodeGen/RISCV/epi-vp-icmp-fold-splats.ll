; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+m,+v -verify-machineinstrs -O0 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=CHECK-O0 %s
; RUN: llc -mtriple=riscv64 -mattr=+m,+v -verify-machineinstrs -O2 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=CHECK-O2 %s

; NOTE: using volatile in order to avoid instruction selection optimizations.

@scratch = global i8 0, align 16

define void @test_vp_fold_unsigned_greater(<vscale x 1 x i64> %a, i64 %b, <vscale x 1 x i1> %m, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_fold_unsigned_greater:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    csrr a2, vlenb
; CHECK-O0-NEXT:    slli a2, a2, 1
; CHECK-O0-NEXT:    sub sp, sp, a2
; CHECK-O0-NEXT:    mv a2, a1
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vs1r.v v0, (a1) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    mv a1, a0
; CHECK-O0-NEXT:    vmv1r.v v9, v8
; CHECK-O0-NEXT:    # kill: def $x10 killed $x12
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    vsetvli a3, zero, e64, m1, ta, ma
; CHECK-O0-NEXT:    vmv.v.x v10, a1
; CHECK-O0-NEXT:    slli a2, a2, 32
; CHECK-O0-NEXT:    srli a2, a2, 32
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vsetvli zero, a2, e64, m1, ta, ma
; CHECK-O0-NEXT:    vmsgtu.vx v8, v9, a1, v0.t
; CHECK-O0-NEXT:    addi a3, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a3) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a3, zero, e8, mf8, ta, ma
; CHECK-O0-NEXT:    vsm.v v8, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vsetvli zero, a2, e64, m1, ta, ma
; CHECK-O0-NEXT:    vmsltu.vx v8, v9, a1, v0.t
; CHECK-O0-NEXT:    addi a3, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a3) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a3, zero, e8, mf8, ta, ma
; CHECK-O0-NEXT:    vsm.v v8, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vsetvli zero, a2, e64, m1, ta, ma
; CHECK-O0-NEXT:    vmsleu.vv v8, v10, v9, v0.t
; CHECK-O0-NEXT:    addi a3, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a3) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a3, zero, e8, mf8, ta, ma
; CHECK-O0-NEXT:    vsm.v v8, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vsetvli zero, a2, e64, m1, ta, ma
; CHECK-O0-NEXT:    vmsleu.vx v8, v9, a1, v0.t
; CHECK-O0-NEXT:    vsetvli a1, zero, e8, mf8, ta, ma
; CHECK-O0-NEXT:    vsm.v v8, (a0)
; CHECK-O0-NEXT:    csrr a0, vlenb
; CHECK-O0-NEXT:    slli a0, a0, 1
; CHECK-O0-NEXT:    add sp, sp, a0
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_fold_unsigned_greater:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    lui a2, %hi(scratch)
; CHECK-O2-NEXT:    addi a2, a2, %lo(scratch)
; CHECK-O2-NEXT:    vsetvli a3, zero, e64, m1, ta, ma
; CHECK-O2-NEXT:    vmv.v.x v9, a0
; CHECK-O2-NEXT:    slli a1, a1, 32
; CHECK-O2-NEXT:    srli a1, a1, 32
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O2-NEXT:    vmsgtu.vx v10, v8, a0, v0.t
; CHECK-O2-NEXT:    vsetvli a3, zero, e8, mf8, ta, ma
; CHECK-O2-NEXT:    vsm.v v10, (a2)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O2-NEXT:    vmsltu.vx v10, v8, a0, v0.t
; CHECK-O2-NEXT:    vsetvli a3, zero, e8, mf8, ta, ma
; CHECK-O2-NEXT:    vsm.v v10, (a2)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O2-NEXT:    vmsleu.vv v9, v9, v8, v0.t
; CHECK-O2-NEXT:    vsetvli a3, zero, e8, mf8, ta, ma
; CHECK-O2-NEXT:    vsm.v v9, (a2)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O2-NEXT:    vmsleu.vx v8, v8, a0, v0.t
; CHECK-O2-NEXT:    vsetvli a0, zero, e8, mf8, ta, ma
; CHECK-O2-NEXT:    vsm.v v8, (a2)
; CHECK-O2-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 1 x i1>*

  %head = insertelement <vscale x 1 x i64> undef, i64 %b, i32 0
  %splat = shufflevector <vscale x 1 x i64> %head, <vscale x 1 x i64> undef, <vscale x 1 x i32> zeroinitializer

  ; x > splat(y) → vmsgtu.vx x, y
  %ugt.0 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %splat, metadata !"ugt", <vscale x 1 x i1> %m, i32 %n)
  store volatile <vscale x 1 x i1> %ugt.0, <vscale x 1 x i1>* %store_addr

  ; splat(y) > x → x < splat(y) → vmsltu.vx x, y
  %ugt.1 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %splat, <vscale x 1 x i64> %a, metadata !"ugt", <vscale x 1 x i1> %m, i32 %n)
  store volatile <vscale x 1 x i1> %ugt.1, <vscale x 1 x i1>* %store_addr

  ; x >= splat(y) → cannot be folded (vmsleu.vv)
  %uge.0 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %splat, metadata !"uge", <vscale x 1 x i1> %m, i32 %n)
  store volatile <vscale x 1 x i1> %uge.0, <vscale x 1 x i1>* %store_addr

  ; splat(y) >= x → x <= splat(y) → vmsleu.vx x, y
  %uge.1 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %splat, <vscale x 1 x i64> %a, metadata !"uge", <vscale x 1 x i1> %m, i32 %n)
  store volatile <vscale x 1 x i1> %uge.1, <vscale x 1 x i1>* %store_addr

  ret void
}

define void @test_vp_fold_unsigned_lower(<vscale x 1 x i64> %a, i64 %b, <vscale x 1 x i1> %m, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_fold_unsigned_lower:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    csrr a2, vlenb
; CHECK-O0-NEXT:    slli a2, a2, 1
; CHECK-O0-NEXT:    sub sp, sp, a2
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vs1r.v v0, (a2) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    mv a2, a0
; CHECK-O0-NEXT:    vmv1r.v v10, v8
; CHECK-O0-NEXT:    # kill: def $x10 killed $x11
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    vsetvli a3, zero, e64, m1, ta, ma
; CHECK-O0-NEXT:    vmv.v.x v9, a2
; CHECK-O0-NEXT:    slli a1, a1, 32
; CHECK-O0-NEXT:    srli a1, a1, 32
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O0-NEXT:    vmsltu.vx v8, v10, a2, v0.t
; CHECK-O0-NEXT:    addi a3, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a3) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a3, zero, e8, mf8, ta, ma
; CHECK-O0-NEXT:    vsm.v v8, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O0-NEXT:    vmsgtu.vx v8, v10, a2, v0.t
; CHECK-O0-NEXT:    addi a3, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a3) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a3, zero, e8, mf8, ta, ma
; CHECK-O0-NEXT:    vsm.v v8, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O0-NEXT:    vmsleu.vx v8, v10, a2, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf8, ta, ma
; CHECK-O0-NEXT:    vsm.v v8, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O0-NEXT:    vmsleu.vv v8, v9, v10, v0.t
; CHECK-O0-NEXT:    vsetvli a1, zero, e8, mf8, ta, ma
; CHECK-O0-NEXT:    vsm.v v8, (a0)
; CHECK-O0-NEXT:    csrr a0, vlenb
; CHECK-O0-NEXT:    slli a0, a0, 1
; CHECK-O0-NEXT:    add sp, sp, a0
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_fold_unsigned_lower:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    lui a2, %hi(scratch)
; CHECK-O2-NEXT:    addi a2, a2, %lo(scratch)
; CHECK-O2-NEXT:    vsetvli a3, zero, e64, m1, ta, ma
; CHECK-O2-NEXT:    vmv.v.x v9, a0
; CHECK-O2-NEXT:    slli a1, a1, 32
; CHECK-O2-NEXT:    srli a1, a1, 32
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O2-NEXT:    vmsltu.vx v10, v8, a0, v0.t
; CHECK-O2-NEXT:    vsetvli a3, zero, e8, mf8, ta, ma
; CHECK-O2-NEXT:    vsm.v v10, (a2)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O2-NEXT:    vmsgtu.vx v10, v8, a0, v0.t
; CHECK-O2-NEXT:    vsetvli a3, zero, e8, mf8, ta, ma
; CHECK-O2-NEXT:    vsm.v v10, (a2)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O2-NEXT:    vmsleu.vx v10, v8, a0, v0.t
; CHECK-O2-NEXT:    vsetvli a0, zero, e8, mf8, ta, ma
; CHECK-O2-NEXT:    vsm.v v10, (a2)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O2-NEXT:    vmsleu.vv v8, v9, v8, v0.t
; CHECK-O2-NEXT:    vsetvli a0, zero, e8, mf8, ta, ma
; CHECK-O2-NEXT:    vsm.v v8, (a2)
; CHECK-O2-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 1 x i1>*

  %head = insertelement <vscale x 1 x i64> undef, i64 %b, i32 0
  %splat = shufflevector <vscale x 1 x i64> %head, <vscale x 1 x i64> undef, <vscale x 1 x i32> zeroinitializer

  ; x < splat(y) → vmsltu.vx x, y
  %ult.0 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %splat, metadata !"ult", <vscale x 1 x i1> %m, i32 %n)
  store volatile <vscale x 1 x i1> %ult.0, <vscale x 1 x i1>* %store_addr

  ; splat(y) < x → x > splat(y) → vmsgtu.vx x, y
  %ult.1 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %splat, <vscale x 1 x i64> %a, metadata !"ult", <vscale x 1 x i1> %m, i32 %n)
  store volatile <vscale x 1 x i1> %ult.1, <vscale x 1 x i1>* %store_addr

  ; x <= splat(y) → vmsleu.vx x, y
  %ule.0 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %splat, metadata !"ule", <vscale x 1 x i1> %m, i32 %n)
  store volatile <vscale x 1 x i1> %ule.0, <vscale x 1 x i1>* %store_addr

  ; splat(y) <= y → y >= splat(y) → cannot be folded (vmsleu.vv)
  %ule.1 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %splat, <vscale x 1 x i64> %a, metadata !"ule", <vscale x 1 x i1> %m, i32 %n)
  store volatile <vscale x 1 x i1> %ule.1, <vscale x 1 x i1>* %store_addr

  ret void
}

define void @test_vp_fold_signed_greater(<vscale x 1 x i64> %a, i64 %b, <vscale x 1 x i1> %m, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_fold_signed_greater:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    csrr a2, vlenb
; CHECK-O0-NEXT:    slli a2, a2, 1
; CHECK-O0-NEXT:    sub sp, sp, a2
; CHECK-O0-NEXT:    mv a2, a1
; CHECK-O0-NEXT:    addi a1, sp, 16
; CHECK-O0-NEXT:    vs1r.v v0, (a1) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    mv a1, a0
; CHECK-O0-NEXT:    vmv1r.v v9, v8
; CHECK-O0-NEXT:    # kill: def $x10 killed $x12
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    vsetvli a3, zero, e64, m1, ta, ma
; CHECK-O0-NEXT:    vmv.v.x v10, a1
; CHECK-O0-NEXT:    slli a2, a2, 32
; CHECK-O0-NEXT:    srli a2, a2, 32
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vsetvli zero, a2, e64, m1, ta, ma
; CHECK-O0-NEXT:    vmsgt.vx v8, v9, a1, v0.t
; CHECK-O0-NEXT:    addi a3, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a3) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a3, zero, e8, mf8, ta, ma
; CHECK-O0-NEXT:    vsm.v v8, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vsetvli zero, a2, e64, m1, ta, ma
; CHECK-O0-NEXT:    vmslt.vx v8, v9, a1, v0.t
; CHECK-O0-NEXT:    addi a3, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a3) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a3, zero, e8, mf8, ta, ma
; CHECK-O0-NEXT:    vsm.v v8, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vsetvli zero, a2, e64, m1, ta, ma
; CHECK-O0-NEXT:    vmsle.vv v8, v10, v9, v0.t
; CHECK-O0-NEXT:    addi a3, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a3) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a3, zero, e8, mf8, ta, ma
; CHECK-O0-NEXT:    vsm.v v8, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vsetvli zero, a2, e64, m1, ta, ma
; CHECK-O0-NEXT:    vmsle.vx v8, v9, a1, v0.t
; CHECK-O0-NEXT:    vsetvli a1, zero, e8, mf8, ta, ma
; CHECK-O0-NEXT:    vsm.v v8, (a0)
; CHECK-O0-NEXT:    csrr a0, vlenb
; CHECK-O0-NEXT:    slli a0, a0, 1
; CHECK-O0-NEXT:    add sp, sp, a0
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_fold_signed_greater:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    lui a2, %hi(scratch)
; CHECK-O2-NEXT:    addi a2, a2, %lo(scratch)
; CHECK-O2-NEXT:    vsetvli a3, zero, e64, m1, ta, ma
; CHECK-O2-NEXT:    vmv.v.x v9, a0
; CHECK-O2-NEXT:    slli a1, a1, 32
; CHECK-O2-NEXT:    srli a1, a1, 32
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O2-NEXT:    vmsgt.vx v10, v8, a0, v0.t
; CHECK-O2-NEXT:    vsetvli a3, zero, e8, mf8, ta, ma
; CHECK-O2-NEXT:    vsm.v v10, (a2)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O2-NEXT:    vmslt.vx v10, v8, a0, v0.t
; CHECK-O2-NEXT:    vsetvli a3, zero, e8, mf8, ta, ma
; CHECK-O2-NEXT:    vsm.v v10, (a2)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O2-NEXT:    vmsle.vv v9, v9, v8, v0.t
; CHECK-O2-NEXT:    vsetvli a3, zero, e8, mf8, ta, ma
; CHECK-O2-NEXT:    vsm.v v9, (a2)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O2-NEXT:    vmsle.vx v8, v8, a0, v0.t
; CHECK-O2-NEXT:    vsetvli a0, zero, e8, mf8, ta, ma
; CHECK-O2-NEXT:    vsm.v v8, (a2)
; CHECK-O2-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 1 x i1>*

  %head = insertelement <vscale x 1 x i64> undef, i64 %b, i32 0
  %splat = shufflevector <vscale x 1 x i64> %head, <vscale x 1 x i64> undef, <vscale x 1 x i32> zeroinitializer

  ; x > splat(y) → vmsgt.vx x, y
  %sgt.0 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %splat, metadata !"sgt", <vscale x 1 x i1> %m, i32 %n)
  store volatile <vscale x 1 x i1> %sgt.0, <vscale x 1 x i1>* %store_addr

  ; splat(y) > x → x < splat(y) → vmslt.vx x, y
  %sgt.1 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %splat, <vscale x 1 x i64> %a, metadata !"sgt", <vscale x 1 x i1> %m, i32 %n)
  store volatile <vscale x 1 x i1> %sgt.1, <vscale x 1 x i1>* %store_addr

  ; x >= splat(y) → cannot be folded (vmsle.vv)
  %sge.0 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %splat, metadata !"sge", <vscale x 1 x i1> %m, i32 %n)
  store volatile <vscale x 1 x i1> %sge.0, <vscale x 1 x i1>* %store_addr

  ; splat(y) >= x → x <= splat(y) → vmsle.vx x, y
  %sge.1 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %splat, <vscale x 1 x i64> %a, metadata !"sge", <vscale x 1 x i1> %m, i32 %n)
  store volatile <vscale x 1 x i1> %sge.1, <vscale x 1 x i1>* %store_addr

  ret void
}

define void @test_vp_fold_signed_lower(<vscale x 1 x i64> %a, i64 %b, <vscale x 1 x i1> %m, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_fold_signed_lower:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    csrr a2, vlenb
; CHECK-O0-NEXT:    slli a2, a2, 1
; CHECK-O0-NEXT:    sub sp, sp, a2
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vs1r.v v0, (a2) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    mv a2, a0
; CHECK-O0-NEXT:    vmv1r.v v10, v8
; CHECK-O0-NEXT:    # kill: def $x10 killed $x11
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    vsetvli a3, zero, e64, m1, ta, ma
; CHECK-O0-NEXT:    vmv.v.x v9, a2
; CHECK-O0-NEXT:    slli a1, a1, 32
; CHECK-O0-NEXT:    srli a1, a1, 32
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O0-NEXT:    vmslt.vx v8, v10, a2, v0.t
; CHECK-O0-NEXT:    addi a3, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a3) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a3, zero, e8, mf8, ta, ma
; CHECK-O0-NEXT:    vsm.v v8, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O0-NEXT:    vmsgt.vx v8, v10, a2, v0.t
; CHECK-O0-NEXT:    addi a3, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a3) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a3, zero, e8, mf8, ta, ma
; CHECK-O0-NEXT:    vsm.v v8, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O0-NEXT:    vmsle.vx v8, v10, a2, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8, mf8, ta, ma
; CHECK-O0-NEXT:    vsm.v v8, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O0-NEXT:    vmsle.vv v8, v9, v10, v0.t
; CHECK-O0-NEXT:    vsetvli a1, zero, e8, mf8, ta, ma
; CHECK-O0-NEXT:    vsm.v v8, (a0)
; CHECK-O0-NEXT:    csrr a0, vlenb
; CHECK-O0-NEXT:    slli a0, a0, 1
; CHECK-O0-NEXT:    add sp, sp, a0
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_fold_signed_lower:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    lui a2, %hi(scratch)
; CHECK-O2-NEXT:    addi a2, a2, %lo(scratch)
; CHECK-O2-NEXT:    vsetvli a3, zero, e64, m1, ta, ma
; CHECK-O2-NEXT:    vmv.v.x v9, a0
; CHECK-O2-NEXT:    slli a1, a1, 32
; CHECK-O2-NEXT:    srli a1, a1, 32
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O2-NEXT:    vmslt.vx v10, v8, a0, v0.t
; CHECK-O2-NEXT:    vsetvli a3, zero, e8, mf8, ta, ma
; CHECK-O2-NEXT:    vsm.v v10, (a2)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O2-NEXT:    vmsgt.vx v10, v8, a0, v0.t
; CHECK-O2-NEXT:    vsetvli a3, zero, e8, mf8, ta, ma
; CHECK-O2-NEXT:    vsm.v v10, (a2)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O2-NEXT:    vmsle.vx v10, v8, a0, v0.t
; CHECK-O2-NEXT:    vsetvli a0, zero, e8, mf8, ta, ma
; CHECK-O2-NEXT:    vsm.v v10, (a2)
; CHECK-O2-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-O2-NEXT:    vmsle.vv v8, v9, v8, v0.t
; CHECK-O2-NEXT:    vsetvli a0, zero, e8, mf8, ta, ma
; CHECK-O2-NEXT:    vsm.v v8, (a2)
; CHECK-O2-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 1 x i1>*

  %head = insertelement <vscale x 1 x i64> undef, i64 %b, i32 0
  %splat = shufflevector <vscale x 1 x i64> %head, <vscale x 1 x i64> undef, <vscale x 1 x i32> zeroinitializer

  ; y < splat(x) → vmslt.vx x, y
  %slt.0 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %splat, metadata !"slt", <vscale x 1 x i1> %m, i32 %n)
  store volatile <vscale x 1 x i1> %slt.0, <vscale x 1 x i1>* %store_addr

  ; splat(x) < y → y > splat(x) → cannot be folded (vmslt.vv)
  %slt.1 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %splat, <vscale x 1 x i64> %a, metadata !"slt", <vscale x 1 x i1> %m, i32 %n)
  store volatile <vscale x 1 x i1> %slt.1, <vscale x 1 x i1>* %store_addr

  ; x <= splat(y) → vmsle.vx x, y
  %sle.0 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %splat, metadata !"sle", <vscale x 1 x i1> %m, i32 %n)
  store volatile <vscale x 1 x i1> %sle.0, <vscale x 1 x i1>* %store_addr

  ; splat(y) <= x → x >= splat(y) → cannot be folded (vmsle.vv)
  %sle.1 = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %splat, <vscale x 1 x i64> %a, metadata !"sle", <vscale x 1 x i1> %m, i32 %n)
  store volatile <vscale x 1 x i1> %sle.1, <vscale x 1 x i1>* %store_addr

  ret void
}

declare <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %b, metadata, <vscale x 1 x i1> %m, i32 %n)
