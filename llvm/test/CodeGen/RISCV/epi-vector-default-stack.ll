; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple riscv64 -mattr=+m,+a,+f,+d,+v \
; RUN:    -epi-pipeline | FileCheck %s

define void @store(<vscale x 1 x i64> %v1) nounwind
; CHECK-LABEL: store:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -512
; CHECK-NEXT:    mv a0, sp
; CHECK-NEXT:    vs1r.v v8, (a0)
; CHECK-NEXT:    addi sp, sp, 512
; CHECK-NEXT:    ret
{
  %buffer = alloca [64 x i64], align 8

  %v4 = bitcast [64 x i64]* %buffer to <vscale x 1 x i64>*
  store volatile <vscale x 1 x i64> %v1, <vscale x 1 x i64>* %v4, align 8

  ret void
}

define void @intrinsic(<vscale x 1 x i64> %v1, i64 %gvl) nounwind
; CHECK-LABEL: intrinsic:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -512
; CHECK-NEXT:    mv a1, sp
; CHECK-NEXT:    vsetvli zero, a0, e64, m1, ta, mu
; CHECK-NEXT:    vse64.v v8, (a1)
; CHECK-NEXT:    addi sp, sp, 512
; CHECK-NEXT:    ret
{
  %buffer = alloca [64 x i64], align 8

  %v2 = bitcast [64 x i64]* %buffer to <vscale x 1 x i64>*
  call void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64> %v1, <vscale x 1 x i64>* %v2, i64 %gvl)

  ret void
}

declare void @llvm.epi.vstore.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>*, i64 %gvl)
