; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple riscv64 -mattr=+v < %s -epi-pipeline | FileCheck %s

@scratch = global i8 0, align 16

define void @lmul_1(<vscale x 1 x i64> %a, <vscale x 1 x i64> %b) nounwind {
; CHECK-LABEL: lmul_1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(scratch)
; CHECK-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-NEXT:    vsetvli a1, zero, e64, m1, ta, ma
; CHECK-NEXT:    vmseq.vv v10, v8, v9
; CHECK-NEXT:    vsm.v v10, (a0)
; CHECK-NEXT:    vmsne.vv v10, v8, v9
; CHECK-NEXT:    vsm.v v10, (a0)
; CHECK-NEXT:    vmsltu.vv v10, v9, v8
; CHECK-NEXT:    vsm.v v10, (a0)
; CHECK-NEXT:    vmsleu.vv v10, v9, v8
; CHECK-NEXT:    vsm.v v10, (a0)
; CHECK-NEXT:    vmsltu.vv v10, v8, v9
; CHECK-NEXT:    vsm.v v10, (a0)
; CHECK-NEXT:    vmsleu.vv v10, v8, v9
; CHECK-NEXT:    vsm.v v10, (a0)
; CHECK-NEXT:    vmslt.vv v10, v9, v8
; CHECK-NEXT:    vsm.v v10, (a0)
; CHECK-NEXT:    vmsle.vv v10, v9, v8
; CHECK-NEXT:    vsm.v v10, (a0)
; CHECK-NEXT:    vmslt.vv v10, v8, v9
; CHECK-NEXT:    vsm.v v10, (a0)
; CHECK-NEXT:    vmsle.vv v8, v8, v9
; CHECK-NEXT:    vsm.v v8, (a0)
; CHECK-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 1 x i1>*

  %cmp_1 = icmp eq <vscale x 1 x i64> %a, %b
  store <vscale x 1 x i1> %cmp_1, <vscale x 1 x i1>* %store_addr

  %cmp_2 = icmp ne <vscale x 1 x i64> %a, %b
  store <vscale x 1 x i1> %cmp_2, <vscale x 1 x i1>* %store_addr

  %cmp_3 = icmp ugt <vscale x 1 x i64> %a, %b
  store <vscale x 1 x i1> %cmp_3, <vscale x 1 x i1>* %store_addr

  %cmp_4 = icmp uge <vscale x 1 x i64> %a, %b
  store <vscale x 1 x i1> %cmp_4, <vscale x 1 x i1>* %store_addr

  %cmp_5 = icmp ult <vscale x 1 x i64> %a, %b
  store <vscale x 1 x i1> %cmp_5, <vscale x 1 x i1>* %store_addr

  %cmp_6 = icmp ule <vscale x 1 x i64> %a, %b
  store <vscale x 1 x i1> %cmp_6, <vscale x 1 x i1>* %store_addr

  %cmp_7 = icmp sgt <vscale x 1 x i64> %a, %b
  store <vscale x 1 x i1> %cmp_7, <vscale x 1 x i1>* %store_addr

  %cmp_8 = icmp sge <vscale x 1 x i64> %a, %b
  store <vscale x 1 x i1> %cmp_8, <vscale x 1 x i1>* %store_addr

  %cmp_9 = icmp slt <vscale x 1 x i64> %a, %b
  store <vscale x 1 x i1> %cmp_9, <vscale x 1 x i1>* %store_addr

  %cmp_10 = icmp sle <vscale x 1 x i64> %a, %b
  store <vscale x 1 x i1> %cmp_10, <vscale x 1 x i1>* %store_addr

  ret void
}

define void @lmul_2(<vscale x 2 x i64> %a, <vscale x 2 x i64> %b) nounwind {
; CHECK-LABEL: lmul_2:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(scratch)
; CHECK-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-NEXT:    vsetvli a1, zero, e64, m2, ta, ma
; CHECK-NEXT:    vmseq.vv v12, v8, v10
; CHECK-NEXT:    vsm.v v12, (a0)
; CHECK-NEXT:    vmsne.vv v12, v8, v10
; CHECK-NEXT:    vsm.v v12, (a0)
; CHECK-NEXT:    vmsltu.vv v12, v10, v8
; CHECK-NEXT:    vsm.v v12, (a0)
; CHECK-NEXT:    vmsleu.vv v12, v10, v8
; CHECK-NEXT:    vsm.v v12, (a0)
; CHECK-NEXT:    vmsltu.vv v12, v8, v10
; CHECK-NEXT:    vsm.v v12, (a0)
; CHECK-NEXT:    vmsleu.vv v12, v8, v10
; CHECK-NEXT:    vsm.v v12, (a0)
; CHECK-NEXT:    vmslt.vv v12, v10, v8
; CHECK-NEXT:    vsm.v v12, (a0)
; CHECK-NEXT:    vmsle.vv v12, v10, v8
; CHECK-NEXT:    vsm.v v12, (a0)
; CHECK-NEXT:    vmslt.vv v12, v8, v10
; CHECK-NEXT:    vsm.v v12, (a0)
; CHECK-NEXT:    vmsle.vv v12, v8, v10
; CHECK-NEXT:    vsm.v v12, (a0)
; CHECK-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 2 x i1>*

  %cmp_1 = icmp eq <vscale x 2 x i64> %a, %b
  store <vscale x 2 x i1> %cmp_1, <vscale x 2 x i1>* %store_addr

  %cmp_2 = icmp ne <vscale x 2 x i64> %a, %b
  store <vscale x 2 x i1> %cmp_2, <vscale x 2 x i1>* %store_addr

  %cmp_3 = icmp ugt <vscale x 2 x i64> %a, %b
  store <vscale x 2 x i1> %cmp_3, <vscale x 2 x i1>* %store_addr

  %cmp_4 = icmp uge <vscale x 2 x i64> %a, %b
  store <vscale x 2 x i1> %cmp_4, <vscale x 2 x i1>* %store_addr

  %cmp_5 = icmp ult <vscale x 2 x i64> %a, %b
  store <vscale x 2 x i1> %cmp_5, <vscale x 2 x i1>* %store_addr

  %cmp_6 = icmp ule <vscale x 2 x i64> %a, %b
  store <vscale x 2 x i1> %cmp_6, <vscale x 2 x i1>* %store_addr

  %cmp_7 = icmp sgt <vscale x 2 x i64> %a, %b
  store <vscale x 2 x i1> %cmp_7, <vscale x 2 x i1>* %store_addr

  %cmp_8 = icmp sge <vscale x 2 x i64> %a, %b
  store <vscale x 2 x i1> %cmp_8, <vscale x 2 x i1>* %store_addr

  %cmp_9 = icmp slt <vscale x 2 x i64> %a, %b
  store <vscale x 2 x i1> %cmp_9, <vscale x 2 x i1>* %store_addr

  %cmp_10 = icmp sle <vscale x 2 x i64> %a, %b
  store <vscale x 2 x i1> %cmp_10, <vscale x 2 x i1>* %store_addr

  ret void
}

define void @lmul_4(<vscale x 4 x i64> %a, <vscale x 4 x i64> %b) nounwind {
; CHECK-LABEL: lmul_4:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(scratch)
; CHECK-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-NEXT:    vsetvli a1, zero, e64, m4, ta, ma
; CHECK-NEXT:    vmseq.vv v16, v8, v12
; CHECK-NEXT:    vsm.v v16, (a0)
; CHECK-NEXT:    vmsne.vv v16, v8, v12
; CHECK-NEXT:    vsm.v v16, (a0)
; CHECK-NEXT:    vmsltu.vv v16, v12, v8
; CHECK-NEXT:    vsm.v v16, (a0)
; CHECK-NEXT:    vmsleu.vv v16, v12, v8
; CHECK-NEXT:    vsm.v v16, (a0)
; CHECK-NEXT:    vmsltu.vv v16, v8, v12
; CHECK-NEXT:    vsm.v v16, (a0)
; CHECK-NEXT:    vmsleu.vv v16, v8, v12
; CHECK-NEXT:    vsm.v v16, (a0)
; CHECK-NEXT:    vmslt.vv v16, v12, v8
; CHECK-NEXT:    vsm.v v16, (a0)
; CHECK-NEXT:    vmsle.vv v16, v12, v8
; CHECK-NEXT:    vsm.v v16, (a0)
; CHECK-NEXT:    vmslt.vv v16, v8, v12
; CHECK-NEXT:    vsm.v v16, (a0)
; CHECK-NEXT:    vmsle.vv v16, v8, v12
; CHECK-NEXT:    vsm.v v16, (a0)
; CHECK-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 4 x i1>*

  %cmp_1 = icmp eq <vscale x 4 x i64> %a, %b
  store <vscale x 4 x i1> %cmp_1, <vscale x 4 x i1>* %store_addr

  %cmp_2 = icmp ne <vscale x 4 x i64> %a, %b
  store <vscale x 4 x i1> %cmp_2, <vscale x 4 x i1>* %store_addr

  %cmp_3 = icmp ugt <vscale x 4 x i64> %a, %b
  store <vscale x 4 x i1> %cmp_3, <vscale x 4 x i1>* %store_addr

  %cmp_4 = icmp uge <vscale x 4 x i64> %a, %b
  store <vscale x 4 x i1> %cmp_4, <vscale x 4 x i1>* %store_addr

  %cmp_5 = icmp ult <vscale x 4 x i64> %a, %b
  store <vscale x 4 x i1> %cmp_5, <vscale x 4 x i1>* %store_addr

  %cmp_6 = icmp ule <vscale x 4 x i64> %a, %b
  store <vscale x 4 x i1> %cmp_6, <vscale x 4 x i1>* %store_addr

  %cmp_7 = icmp sgt <vscale x 4 x i64> %a, %b
  store <vscale x 4 x i1> %cmp_7, <vscale x 4 x i1>* %store_addr

  %cmp_8 = icmp sge <vscale x 4 x i64> %a, %b
  store <vscale x 4 x i1> %cmp_8, <vscale x 4 x i1>* %store_addr

  %cmp_9 = icmp slt <vscale x 4 x i64> %a, %b
  store <vscale x 4 x i1> %cmp_9, <vscale x 4 x i1>* %store_addr

  %cmp_10 = icmp sle <vscale x 4 x i64> %a, %b
  store <vscale x 4 x i1> %cmp_10, <vscale x 4 x i1>* %store_addr

  ret void
}

define void @lmul_8(<vscale x 8 x i64> %a, <vscale x 8 x i64> %b) nounwind {
; CHECK-LABEL: lmul_8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(scratch)
; CHECK-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-NEXT:    vsetvli a1, zero, e64, m8, ta, ma
; CHECK-NEXT:    vmseq.vv v24, v8, v16
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmsne.vv v24, v8, v16
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmsltu.vv v24, v16, v8
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmsleu.vv v24, v16, v8
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmsltu.vv v24, v8, v16
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmsleu.vv v24, v8, v16
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmslt.vv v24, v16, v8
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmsle.vv v24, v16, v8
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmslt.vv v24, v8, v16
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmsle.vv v24, v8, v16
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 8 x i1>*

  %cmp_1 = icmp eq <vscale x 8 x i64> %a, %b
  store <vscale x 8 x i1> %cmp_1, <vscale x 8 x i1>* %store_addr

  %cmp_2 = icmp ne <vscale x 8 x i64> %a, %b
  store <vscale x 8 x i1> %cmp_2, <vscale x 8 x i1>* %store_addr

  %cmp_3 = icmp ugt <vscale x 8 x i64> %a, %b
  store <vscale x 8 x i1> %cmp_3, <vscale x 8 x i1>* %store_addr

  %cmp_4 = icmp uge <vscale x 8 x i64> %a, %b
  store <vscale x 8 x i1> %cmp_4, <vscale x 8 x i1>* %store_addr

  %cmp_5 = icmp ult <vscale x 8 x i64> %a, %b
  store <vscale x 8 x i1> %cmp_5, <vscale x 8 x i1>* %store_addr

  %cmp_6 = icmp ule <vscale x 8 x i64> %a, %b
  store <vscale x 8 x i1> %cmp_6, <vscale x 8 x i1>* %store_addr

  %cmp_7 = icmp sgt <vscale x 8 x i64> %a, %b
  store <vscale x 8 x i1> %cmp_7, <vscale x 8 x i1>* %store_addr

  %cmp_8 = icmp sge <vscale x 8 x i64> %a, %b
  store <vscale x 8 x i1> %cmp_8, <vscale x 8 x i1>* %store_addr

  %cmp_9 = icmp slt <vscale x 8 x i64> %a, %b
  store <vscale x 8 x i1> %cmp_9, <vscale x 8 x i1>* %store_addr

  %cmp_10 = icmp sle <vscale x 8 x i64> %a, %b
  store <vscale x 8 x i1> %cmp_10, <vscale x 8 x i1>* %store_addr

  ret void
}

define void @lmul_8_i32(<vscale x 16 x i32> %a, <vscale x 16 x i32> %b) nounwind {
; CHECK-LABEL: lmul_8_i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(scratch)
; CHECK-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-NEXT:    vsetvli a1, zero, e32, m8, ta, ma
; CHECK-NEXT:    vmseq.vv v24, v8, v16
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmsne.vv v24, v8, v16
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmsltu.vv v24, v16, v8
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmsleu.vv v24, v16, v8
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmsltu.vv v24, v8, v16
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmsleu.vv v24, v8, v16
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmslt.vv v24, v16, v8
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmsle.vv v24, v16, v8
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmslt.vv v24, v8, v16
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmsle.vv v24, v8, v16
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 16 x i1>*

  %cmp_1 = icmp eq <vscale x 16 x i32> %a, %b
  store <vscale x 16 x i1> %cmp_1, <vscale x 16 x i1>* %store_addr

  %cmp_2 = icmp ne <vscale x 16 x i32> %a, %b
  store <vscale x 16 x i1> %cmp_2, <vscale x 16 x i1>* %store_addr

  %cmp_3 = icmp ugt <vscale x 16 x i32> %a, %b
  store <vscale x 16 x i1> %cmp_3, <vscale x 16 x i1>* %store_addr

  %cmp_4 = icmp uge <vscale x 16 x i32> %a, %b
  store <vscale x 16 x i1> %cmp_4, <vscale x 16 x i1>* %store_addr

  %cmp_5 = icmp ult <vscale x 16 x i32> %a, %b
  store <vscale x 16 x i1> %cmp_5, <vscale x 16 x i1>* %store_addr

  %cmp_6 = icmp ule <vscale x 16 x i32> %a, %b
  store <vscale x 16 x i1> %cmp_6, <vscale x 16 x i1>* %store_addr

  %cmp_7 = icmp sgt <vscale x 16 x i32> %a, %b
  store <vscale x 16 x i1> %cmp_7, <vscale x 16 x i1>* %store_addr

  %cmp_8 = icmp sge <vscale x 16 x i32> %a, %b
  store <vscale x 16 x i1> %cmp_8, <vscale x 16 x i1>* %store_addr

  %cmp_9 = icmp slt <vscale x 16 x i32> %a, %b
  store <vscale x 16 x i1> %cmp_9, <vscale x 16 x i1>* %store_addr

  %cmp_10 = icmp sle <vscale x 16 x i32> %a, %b
  store <vscale x 16 x i1> %cmp_10, <vscale x 16 x i1>* %store_addr

  ret void
}

define void @lmul_8_i16(<vscale x 32 x i16> %a, <vscale x 32 x i16> %b) nounwind {
; CHECK-LABEL: lmul_8_i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(scratch)
; CHECK-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-NEXT:    vsetvli a1, zero, e16, m8, ta, ma
; CHECK-NEXT:    vmseq.vv v24, v8, v16
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmsne.vv v24, v8, v16
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmsltu.vv v24, v16, v8
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmsleu.vv v24, v16, v8
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmsltu.vv v24, v8, v16
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmsleu.vv v24, v8, v16
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmslt.vv v24, v16, v8
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmsle.vv v24, v16, v8
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmslt.vv v24, v8, v16
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    vmsle.vv v24, v8, v16
; CHECK-NEXT:    vsm.v v24, (a0)
; CHECK-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 32 x i1>*
  %store_addr_2 = bitcast i8* @scratch to <vscale x 32 x i1>*

  %cmp_1 = icmp eq <vscale x 32 x i16> %a, %b
  store <vscale x 32 x i1> %cmp_1, <vscale x 32 x i1>* %store_addr

  %cmp_2 = icmp ne <vscale x 32 x i16> %a, %b
  store <vscale x 32 x i1> %cmp_2, <vscale x 32 x i1>* %store_addr

  %cmp_3 = icmp ugt <vscale x 32 x i16> %a, %b
  store <vscale x 32 x i1> %cmp_3, <vscale x 32 x i1>* %store_addr

  %cmp_4 = icmp uge <vscale x 32 x i16> %a, %b
  store <vscale x 32 x i1> %cmp_4, <vscale x 32 x i1>* %store_addr

  %cmp_5 = icmp ult <vscale x 32 x i16> %a, %b
  store <vscale x 32 x i1> %cmp_5, <vscale x 32 x i1>* %store_addr

  %cmp_6 = icmp ule <vscale x 32 x i16> %a, %b
  store <vscale x 32 x i1> %cmp_6, <vscale x 32 x i1>* %store_addr

  %cmp_7 = icmp sgt <vscale x 32 x i16> %a, %b
  store <vscale x 32 x i1> %cmp_7, <vscale x 32 x i1>* %store_addr

  %cmp_8 = icmp sge <vscale x 32 x i16> %a, %b
  store <vscale x 32 x i1> %cmp_8, <vscale x 32 x i1>* %store_addr

  %cmp_9 = icmp slt <vscale x 32 x i16> %a, %b
  store <vscale x 32 x i1> %cmp_9, <vscale x 32 x i1>* %store_addr

  %cmp_10 = icmp sle <vscale x 32 x i16> %a, %b
  store <vscale x 32 x i1> %cmp_10, <vscale x 32 x i1>* %store_addr

  ret void
}

; FIXME enable when nxv64i8 is supported
;define void @lmul_8_i8(<vscale x 64 x i8> %a, <vscale x 64 x i8> %b) nounwind {
;  %store_addr = bitcast i8* @scratch to <vscale x 64 x i1>*
;
;  %cmp_1 = icmp eq <vscale x 64 x i8> %a, %b
;  store <vscale x 64 x i1> %cmp_1, <vscale x 64 x i1>* %store_addr
;
;  %cmp_2 = icmp ne <vscale x 64 x i8> %a, %b
;  store <vscale x 64 x i1> %cmp_2, <vscale x 64 x i1>* %store_addr
;
;  %cmp_3 = icmp ugt <vscale x 64 x i8> %a, %b
;  store <vscale x 64 x i1> %cmp_3, <vscale x 64 x i1>* %store_addr
;
;  %cmp_4 = icmp uge <vscale x 64 x i8> %a, %b
;  store <vscale x 64 x i1> %cmp_4, <vscale x 64 x i1>* %store_addr
;
;  %cmp_5 = icmp ult <vscale x 64 x i8> %a, %b
;  store <vscale x 64 x i1> %cmp_5, <vscale x 64 x i1>* %store_addr
;
;  %cmp_6 = icmp ule <vscale x 64 x i8> %a, %b
;  store <vscale x 64 x i1> %cmp_6, <vscale x 64 x i1>* %store_addr
;
;  %cmp_7 = icmp sgt <vscale x 64 x i8> %a, %b
;  store <vscale x 64 x i1> %cmp_7, <vscale x 64 x i1>* %store_addr
;
;  %cmp_8 = icmp sge <vscale x 64 x i8> %a, %b
;  store <vscale x 64 x i1> %cmp_8, <vscale x 64 x i1>* %store_addr
;
;  %cmp_9 = icmp slt <vscale x 64 x i8> %a, %b
;  store <vscale x 64 x i1> %cmp_9, <vscale x 64 x i1>* %store_addr
;
;  %cmp_10 = icmp sle <vscale x 64 x i8> %a, %b
;  store <vscale x 64 x i1> %cmp_10, <vscale x 64 x i1>* %store_addr
;
;  ret void
;}
