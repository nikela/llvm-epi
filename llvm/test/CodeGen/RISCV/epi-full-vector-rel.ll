; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple riscv64 -mattr=+experimental-v < %s -epi-pipeline | FileCheck %s

@scratch = global i8 0, align 16

define void @lmul_1(<vscale x 1 x i64> %a, <vscale x 1 x i64> %b) nounwind {
; CHECK-LABEL: lmul_1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(scratch)
; CHECK-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-NEXT:    vsetvli a1, zero, e64,m1,ta,mu
; CHECK-NEXT:    vmseq.vv v25, v16, v17
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m1,ta,mu
; CHECK-NEXT:    vmsne.vv v25, v16, v17
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m1,ta,mu
; CHECK-NEXT:    vmsltu.vv v25, v17, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m1,ta,mu
; CHECK-NEXT:    vmsleu.vv v25, v17, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m1,ta,mu
; CHECK-NEXT:    vmsltu.vv v25, v16, v17
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m1,ta,mu
; CHECK-NEXT:    vmsleu.vv v25, v16, v17
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m1,ta,mu
; CHECK-NEXT:    vmslt.vv v25, v17, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m1,ta,mu
; CHECK-NEXT:    vmsle.vv v25, v17, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m1,ta,mu
; CHECK-NEXT:    vmslt.vv v25, v16, v17
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m1,ta,mu
; CHECK-NEXT:    vmsle.vv v25, v16, v17
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 1 x i1>*

  %cmp_1 = icmp eq <vscale x 1 x i64> %a, %b
  store <vscale x 1 x i1> %cmp_1, <vscale x 1 x i1>* %store_addr

  %cmp_2 = icmp ne <vscale x 1 x i64> %a, %b
  store <vscale x 1 x i1> %cmp_2, <vscale x 1 x i1>* %store_addr

  %cmp_3 = icmp ugt <vscale x 1 x i64> %a, %b
  store <vscale x 1 x i1> %cmp_3, <vscale x 1 x i1>* %store_addr

  %cmp_4 = icmp uge <vscale x 1 x i64> %a, %b
  store <vscale x 1 x i1> %cmp_4, <vscale x 1 x i1>* %store_addr

  %cmp_5 = icmp ult <vscale x 1 x i64> %a, %b
  store <vscale x 1 x i1> %cmp_5, <vscale x 1 x i1>* %store_addr

  %cmp_6 = icmp ule <vscale x 1 x i64> %a, %b
  store <vscale x 1 x i1> %cmp_6, <vscale x 1 x i1>* %store_addr

  %cmp_7 = icmp sgt <vscale x 1 x i64> %a, %b
  store <vscale x 1 x i1> %cmp_7, <vscale x 1 x i1>* %store_addr

  %cmp_8 = icmp sge <vscale x 1 x i64> %a, %b
  store <vscale x 1 x i1> %cmp_8, <vscale x 1 x i1>* %store_addr

  %cmp_9 = icmp slt <vscale x 1 x i64> %a, %b
  store <vscale x 1 x i1> %cmp_9, <vscale x 1 x i1>* %store_addr

  %cmp_10 = icmp sle <vscale x 1 x i64> %a, %b
  store <vscale x 1 x i1> %cmp_10, <vscale x 1 x i1>* %store_addr

  ret void
}

define void @lmul_2(<vscale x 2 x i64> %a, <vscale x 2 x i64> %b) nounwind {
; CHECK-LABEL: lmul_2:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(scratch)
; CHECK-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-NEXT:    vsetvli a1, zero, e64,m2,ta,mu
; CHECK-NEXT:    vmseq.vv v25, v16, v18
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m2,ta,mu
; CHECK-NEXT:    vmsne.vv v25, v16, v18
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m2,ta,mu
; CHECK-NEXT:    vmsltu.vv v25, v18, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m2,ta,mu
; CHECK-NEXT:    vmsleu.vv v25, v18, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m2,ta,mu
; CHECK-NEXT:    vmsltu.vv v25, v16, v18
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m2,ta,mu
; CHECK-NEXT:    vmsleu.vv v25, v16, v18
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m2,ta,mu
; CHECK-NEXT:    vmslt.vv v25, v18, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m2,ta,mu
; CHECK-NEXT:    vmsle.vv v25, v18, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m2,ta,mu
; CHECK-NEXT:    vmslt.vv v25, v16, v18
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m2,ta,mu
; CHECK-NEXT:    vmsle.vv v25, v16, v18
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 2 x i1>*

  %cmp_1 = icmp eq <vscale x 2 x i64> %a, %b
  store <vscale x 2 x i1> %cmp_1, <vscale x 2 x i1>* %store_addr

  %cmp_2 = icmp ne <vscale x 2 x i64> %a, %b
  store <vscale x 2 x i1> %cmp_2, <vscale x 2 x i1>* %store_addr

  %cmp_3 = icmp ugt <vscale x 2 x i64> %a, %b
  store <vscale x 2 x i1> %cmp_3, <vscale x 2 x i1>* %store_addr

  %cmp_4 = icmp uge <vscale x 2 x i64> %a, %b
  store <vscale x 2 x i1> %cmp_4, <vscale x 2 x i1>* %store_addr

  %cmp_5 = icmp ult <vscale x 2 x i64> %a, %b
  store <vscale x 2 x i1> %cmp_5, <vscale x 2 x i1>* %store_addr

  %cmp_6 = icmp ule <vscale x 2 x i64> %a, %b
  store <vscale x 2 x i1> %cmp_6, <vscale x 2 x i1>* %store_addr

  %cmp_7 = icmp sgt <vscale x 2 x i64> %a, %b
  store <vscale x 2 x i1> %cmp_7, <vscale x 2 x i1>* %store_addr

  %cmp_8 = icmp sge <vscale x 2 x i64> %a, %b
  store <vscale x 2 x i1> %cmp_8, <vscale x 2 x i1>* %store_addr

  %cmp_9 = icmp slt <vscale x 2 x i64> %a, %b
  store <vscale x 2 x i1> %cmp_9, <vscale x 2 x i1>* %store_addr

  %cmp_10 = icmp sle <vscale x 2 x i64> %a, %b
  store <vscale x 2 x i1> %cmp_10, <vscale x 2 x i1>* %store_addr

  ret void
}

define void @lmul_4(<vscale x 4 x i64> %a, <vscale x 4 x i64> %b) nounwind {
; CHECK-LABEL: lmul_4:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(scratch)
; CHECK-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-NEXT:    vsetvli a1, zero, e64,m4,ta,mu
; CHECK-NEXT:    vmseq.vv v25, v16, v20
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m4,ta,mu
; CHECK-NEXT:    vmsne.vv v25, v16, v20
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m4,ta,mu
; CHECK-NEXT:    vmsltu.vv v25, v20, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m4,ta,mu
; CHECK-NEXT:    vmsleu.vv v25, v20, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m4,ta,mu
; CHECK-NEXT:    vmsltu.vv v25, v16, v20
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m4,ta,mu
; CHECK-NEXT:    vmsleu.vv v25, v16, v20
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m4,ta,mu
; CHECK-NEXT:    vmslt.vv v25, v20, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m4,ta,mu
; CHECK-NEXT:    vmsle.vv v25, v20, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m4,ta,mu
; CHECK-NEXT:    vmslt.vv v25, v16, v20
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m4,ta,mu
; CHECK-NEXT:    vmsle.vv v25, v16, v20
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 4 x i1>*

  %cmp_1 = icmp eq <vscale x 4 x i64> %a, %b
  store <vscale x 4 x i1> %cmp_1, <vscale x 4 x i1>* %store_addr

  %cmp_2 = icmp ne <vscale x 4 x i64> %a, %b
  store <vscale x 4 x i1> %cmp_2, <vscale x 4 x i1>* %store_addr

  %cmp_3 = icmp ugt <vscale x 4 x i64> %a, %b
  store <vscale x 4 x i1> %cmp_3, <vscale x 4 x i1>* %store_addr

  %cmp_4 = icmp uge <vscale x 4 x i64> %a, %b
  store <vscale x 4 x i1> %cmp_4, <vscale x 4 x i1>* %store_addr

  %cmp_5 = icmp ult <vscale x 4 x i64> %a, %b
  store <vscale x 4 x i1> %cmp_5, <vscale x 4 x i1>* %store_addr

  %cmp_6 = icmp ule <vscale x 4 x i64> %a, %b
  store <vscale x 4 x i1> %cmp_6, <vscale x 4 x i1>* %store_addr

  %cmp_7 = icmp sgt <vscale x 4 x i64> %a, %b
  store <vscale x 4 x i1> %cmp_7, <vscale x 4 x i1>* %store_addr

  %cmp_8 = icmp sge <vscale x 4 x i64> %a, %b
  store <vscale x 4 x i1> %cmp_8, <vscale x 4 x i1>* %store_addr

  %cmp_9 = icmp slt <vscale x 4 x i64> %a, %b
  store <vscale x 4 x i1> %cmp_9, <vscale x 4 x i1>* %store_addr

  %cmp_10 = icmp sle <vscale x 4 x i64> %a, %b
  store <vscale x 4 x i1> %cmp_10, <vscale x 4 x i1>* %store_addr

  ret void
}

define void @lmul_8(<vscale x 8 x i64> %a, <vscale x 8 x i64> %b) nounwind {
; CHECK-LABEL: lmul_8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a1, zero, e64,m8,ta,mu
; CHECK-NEXT:    vle64.v v8, (a0)
; CHECK-NEXT:    lui a0, %hi(scratch)
; CHECK-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-NEXT:    vmseq.vv v25, v16, v8
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m8,ta,mu
; CHECK-NEXT:    vmsne.vv v25, v16, v8
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m8,ta,mu
; CHECK-NEXT:    vmsltu.vv v25, v8, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m8,ta,mu
; CHECK-NEXT:    vmsleu.vv v25, v8, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m8,ta,mu
; CHECK-NEXT:    vmsltu.vv v25, v16, v8
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m8,ta,mu
; CHECK-NEXT:    vmsleu.vv v25, v16, v8
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m8,ta,mu
; CHECK-NEXT:    vmslt.vv v25, v8, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m8,ta,mu
; CHECK-NEXT:    vmsle.vv v25, v8, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m8,ta,mu
; CHECK-NEXT:    vmslt.vv v25, v16, v8
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e64,m8,ta,mu
; CHECK-NEXT:    vmsle.vv v25, v16, v8
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 8 x i1>*

  %cmp_1 = icmp eq <vscale x 8 x i64> %a, %b
  store <vscale x 8 x i1> %cmp_1, <vscale x 8 x i1>* %store_addr

  %cmp_2 = icmp ne <vscale x 8 x i64> %a, %b
  store <vscale x 8 x i1> %cmp_2, <vscale x 8 x i1>* %store_addr

  %cmp_3 = icmp ugt <vscale x 8 x i64> %a, %b
  store <vscale x 8 x i1> %cmp_3, <vscale x 8 x i1>* %store_addr

  %cmp_4 = icmp uge <vscale x 8 x i64> %a, %b
  store <vscale x 8 x i1> %cmp_4, <vscale x 8 x i1>* %store_addr

  %cmp_5 = icmp ult <vscale x 8 x i64> %a, %b
  store <vscale x 8 x i1> %cmp_5, <vscale x 8 x i1>* %store_addr

  %cmp_6 = icmp ule <vscale x 8 x i64> %a, %b
  store <vscale x 8 x i1> %cmp_6, <vscale x 8 x i1>* %store_addr

  %cmp_7 = icmp sgt <vscale x 8 x i64> %a, %b
  store <vscale x 8 x i1> %cmp_7, <vscale x 8 x i1>* %store_addr

  %cmp_8 = icmp sge <vscale x 8 x i64> %a, %b
  store <vscale x 8 x i1> %cmp_8, <vscale x 8 x i1>* %store_addr

  %cmp_9 = icmp slt <vscale x 8 x i64> %a, %b
  store <vscale x 8 x i1> %cmp_9, <vscale x 8 x i1>* %store_addr

  %cmp_10 = icmp sle <vscale x 8 x i64> %a, %b
  store <vscale x 8 x i1> %cmp_10, <vscale x 8 x i1>* %store_addr

  ret void
}

define void @lmul_8_i32(<vscale x 16 x i32> %a, <vscale x 16 x i32> %b) nounwind {
; CHECK-LABEL: lmul_8_i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a1, zero, e32,m8,ta,mu
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    lui a0, %hi(scratch)
; CHECK-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-NEXT:    vmseq.vv v25, v16, v8
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e32,m8,ta,mu
; CHECK-NEXT:    vmsne.vv v25, v16, v8
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e32,m8,ta,mu
; CHECK-NEXT:    vmsltu.vv v25, v8, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e32,m8,ta,mu
; CHECK-NEXT:    vmsleu.vv v25, v8, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e32,m8,ta,mu
; CHECK-NEXT:    vmsltu.vv v25, v16, v8
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e32,m8,ta,mu
; CHECK-NEXT:    vmsleu.vv v25, v16, v8
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e32,m8,ta,mu
; CHECK-NEXT:    vmslt.vv v25, v8, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e32,m8,ta,mu
; CHECK-NEXT:    vmsle.vv v25, v8, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e32,m8,ta,mu
; CHECK-NEXT:    vmslt.vv v25, v16, v8
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e32,m8,ta,mu
; CHECK-NEXT:    vmsle.vv v25, v16, v8
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 16 x i1>*

  %cmp_1 = icmp eq <vscale x 16 x i32> %a, %b
  store <vscale x 16 x i1> %cmp_1, <vscale x 16 x i1>* %store_addr

  %cmp_2 = icmp ne <vscale x 16 x i32> %a, %b
  store <vscale x 16 x i1> %cmp_2, <vscale x 16 x i1>* %store_addr

  %cmp_3 = icmp ugt <vscale x 16 x i32> %a, %b
  store <vscale x 16 x i1> %cmp_3, <vscale x 16 x i1>* %store_addr

  %cmp_4 = icmp uge <vscale x 16 x i32> %a, %b
  store <vscale x 16 x i1> %cmp_4, <vscale x 16 x i1>* %store_addr

  %cmp_5 = icmp ult <vscale x 16 x i32> %a, %b
  store <vscale x 16 x i1> %cmp_5, <vscale x 16 x i1>* %store_addr

  %cmp_6 = icmp ule <vscale x 16 x i32> %a, %b
  store <vscale x 16 x i1> %cmp_6, <vscale x 16 x i1>* %store_addr

  %cmp_7 = icmp sgt <vscale x 16 x i32> %a, %b
  store <vscale x 16 x i1> %cmp_7, <vscale x 16 x i1>* %store_addr

  %cmp_8 = icmp sge <vscale x 16 x i32> %a, %b
  store <vscale x 16 x i1> %cmp_8, <vscale x 16 x i1>* %store_addr

  %cmp_9 = icmp slt <vscale x 16 x i32> %a, %b
  store <vscale x 16 x i1> %cmp_9, <vscale x 16 x i1>* %store_addr

  %cmp_10 = icmp sle <vscale x 16 x i32> %a, %b
  store <vscale x 16 x i1> %cmp_10, <vscale x 16 x i1>* %store_addr

  ret void
}

define void @lmul_8_i16(<vscale x 32 x i16> %a, <vscale x 32 x i16> %b) nounwind {
; CHECK-LABEL: lmul_8_i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a1, zero, e16,m8,ta,mu
; CHECK-NEXT:    vle16.v v8, (a0)
; CHECK-NEXT:    lui a0, %hi(scratch)
; CHECK-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-NEXT:    vmseq.vv v25, v16, v8
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e16,m8,ta,mu
; CHECK-NEXT:    vmsne.vv v25, v16, v8
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e16,m8,ta,mu
; CHECK-NEXT:    vmsltu.vv v25, v8, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e16,m8,ta,mu
; CHECK-NEXT:    vmsleu.vv v25, v8, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e16,m8,ta,mu
; CHECK-NEXT:    vmsltu.vv v25, v16, v8
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e16,m8,ta,mu
; CHECK-NEXT:    vmsleu.vv v25, v16, v8
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e16,m8,ta,mu
; CHECK-NEXT:    vmslt.vv v25, v8, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e16,m8,ta,mu
; CHECK-NEXT:    vmsle.vv v25, v8, v16
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e16,m8,ta,mu
; CHECK-NEXT:    vmslt.vv v25, v16, v8
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    vsetvli a1, zero, e16,m8,ta,mu
; CHECK-NEXT:    vmsle.vv v25, v16, v8
; CHECK-NEXT:    vsetvli a1, zero, e8,m1,ta,mu
; CHECK-NEXT:    vse8.v v25, (a0)
; CHECK-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 32 x i1>*
  %store_addr_2 = bitcast i8* @scratch to <vscale x 32 x i1>*

  %cmp_1 = icmp eq <vscale x 32 x i16> %a, %b
  store <vscale x 32 x i1> %cmp_1, <vscale x 32 x i1>* %store_addr

  %cmp_2 = icmp ne <vscale x 32 x i16> %a, %b
  store <vscale x 32 x i1> %cmp_2, <vscale x 32 x i1>* %store_addr

  %cmp_3 = icmp ugt <vscale x 32 x i16> %a, %b
  store <vscale x 32 x i1> %cmp_3, <vscale x 32 x i1>* %store_addr

  %cmp_4 = icmp uge <vscale x 32 x i16> %a, %b
  store <vscale x 32 x i1> %cmp_4, <vscale x 32 x i1>* %store_addr

  %cmp_5 = icmp ult <vscale x 32 x i16> %a, %b
  store <vscale x 32 x i1> %cmp_5, <vscale x 32 x i1>* %store_addr

  %cmp_6 = icmp ule <vscale x 32 x i16> %a, %b
  store <vscale x 32 x i1> %cmp_6, <vscale x 32 x i1>* %store_addr

  %cmp_7 = icmp sgt <vscale x 32 x i16> %a, %b
  store <vscale x 32 x i1> %cmp_7, <vscale x 32 x i1>* %store_addr

  %cmp_8 = icmp sge <vscale x 32 x i16> %a, %b
  store <vscale x 32 x i1> %cmp_8, <vscale x 32 x i1>* %store_addr

  %cmp_9 = icmp slt <vscale x 32 x i16> %a, %b
  store <vscale x 32 x i1> %cmp_9, <vscale x 32 x i1>* %store_addr

  %cmp_10 = icmp sle <vscale x 32 x i16> %a, %b
  store <vscale x 32 x i1> %cmp_10, <vscale x 32 x i1>* %store_addr

  ret void
}

; FIXME enable when nxv64i8 is supported
;define void @lmul_8_i8(<vscale x 64 x i8> %a, <vscale x 64 x i8> %b) nounwind {
;  %store_addr = bitcast i8* @scratch to <vscale x 64 x i1>*
;
;  %cmp_1 = icmp eq <vscale x 64 x i8> %a, %b
;  store <vscale x 64 x i1> %cmp_1, <vscale x 64 x i1>* %store_addr
;
;  %cmp_2 = icmp ne <vscale x 64 x i8> %a, %b
;  store <vscale x 64 x i1> %cmp_2, <vscale x 64 x i1>* %store_addr
;
;  %cmp_3 = icmp ugt <vscale x 64 x i8> %a, %b
;  store <vscale x 64 x i1> %cmp_3, <vscale x 64 x i1>* %store_addr
;
;  %cmp_4 = icmp uge <vscale x 64 x i8> %a, %b
;  store <vscale x 64 x i1> %cmp_4, <vscale x 64 x i1>* %store_addr
;
;  %cmp_5 = icmp ult <vscale x 64 x i8> %a, %b
;  store <vscale x 64 x i1> %cmp_5, <vscale x 64 x i1>* %store_addr
;
;  %cmp_6 = icmp ule <vscale x 64 x i8> %a, %b
;  store <vscale x 64 x i1> %cmp_6, <vscale x 64 x i1>* %store_addr
;
;  %cmp_7 = icmp sgt <vscale x 64 x i8> %a, %b
;  store <vscale x 64 x i1> %cmp_7, <vscale x 64 x i1>* %store_addr
;
;  %cmp_8 = icmp sge <vscale x 64 x i8> %a, %b
;  store <vscale x 64 x i1> %cmp_8, <vscale x 64 x i1>* %store_addr
;
;  %cmp_9 = icmp slt <vscale x 64 x i8> %a, %b
;  store <vscale x 64 x i1> %cmp_9, <vscale x 64 x i1>* %store_addr
;
;  %cmp_10 = icmp sle <vscale x 64 x i8> %a, %b
;  store <vscale x 64 x i1> %cmp_10, <vscale x 64 x i1>* %store_addr
;
;  ret void
;}
