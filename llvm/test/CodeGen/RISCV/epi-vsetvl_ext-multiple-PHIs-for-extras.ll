; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+d,+experimental-v -verify-machineinstrs -epi-pipeline \
; RUN:    < %s | FileCheck %s

define void @multiple_PHIs_for_extras(i64 %N, double* %c, double* %a, double* %b) {
; CHECK-LABEL: multiple_PHIs_for_extras:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    andi a0, a0, 1
; CHECK-NEXT:    bnez a0, .LBB0_2
; CHECK-NEXT:  # %bb.1: # %if.then
; CHECK-NEXT:    addi a6, zero, 64
; CHECK-NEXT:    vsetvli a7, a6, e64, m1, ta, mu, nt
; CHECK-NEXT:    addi a0, zero, 5
; CHECK-NEXT:    ori a4, a0, 88
; CHECK-NEXT:    vsetvl a6, a6, a4
; CHECK-NEXT:    addi a4, zero, 512
; CHECK-NEXT:    j .LBB0_3
; CHECK-NEXT:  .LBB0_2: # %if.else
; CHECK-NEXT:    addi a0, zero, 64
; CHECK-NEXT:    vsetvli a7, a0, e64, m1, ta, mu
; CHECK-NEXT:    vsetvli a6, zero, e64, m1, ta, mu
; CHECK-NEXT:    mv a0, zero
; CHECK-NEXT:    mv a4, zero
; CHECK-NEXT:  .LBB0_3: # %if.end
; CHECK-NEXT:    ori a5, a4, 88
; CHECK-NEXT:    vsetvl a4, a7, a5
; CHECK-NEXT:    vle64.v v8, (a2)
; CHECK-NEXT:    vle64.v v9, (a3)
; CHECK-NEXT:    ori a2, a0, 88
; CHECK-NEXT:    vsetvl a0, a6, a2
; CHECK-NEXT:    vfadd.vv v8, v8, v9
; CHECK-NEXT:    vse64.v v8, (a1)
; CHECK-NEXT:    ret
entry:
  %0 = and i64 %N, 1
  %cmp = icmp eq i64 %0, 0
  br i1 %cmp, label %if.then, label %if.else

if.then:                                          ; preds = %entry
  %1 = tail call i64 @llvm.epi.vsetvl.ext(i64 64, i64 3, i64 0, i64 512)
  %2 = tail call i64 @llvm.epi.vsetvl.ext(i64 64, i64 3, i64 0, i64 5)
  br label %if.end

if.else:                                          ; preds = %entry
  %3 = tail call i64 @llvm.epi.vsetvl(i64 64, i64 3, i64 0)
  %4 = tail call i64 @llvm.epi.vsetvlmax(i64 3, i64 0)
  br label %if.end

if.end:                                           ; preds = %if.else, %if.then
  %gvl2.0 = phi i64 [ %2, %if.then ], [ %4, %if.else ]
  %gvl1.0 = phi i64 [ %1, %if.then ], [ %3, %if.else ]
  %5 = bitcast double* %a to <vscale x 1 x double>*
  %6 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %5, i64 %gvl1.0)
  %7 = bitcast double* %b to <vscale x 1 x double>*
  %8 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %7, i64 %gvl1.0)
  %9 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %6, <vscale x 1 x double> %8, i64 %gvl2.0)
  %10 = bitcast double* %c to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %9, <vscale x 1 x double>* %10, i64 %gvl2.0)
  ret void
}

declare i64 @llvm.epi.vsetvl.ext(i64, i64, i64, i64)
declare i64 @llvm.epi.vsetvl(i64, i64, i64)
declare i64 @llvm.epi.vsetvlmax(i64, i64)

declare <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* nocapture, i64)
declare <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64)
declare void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>* nocapture, i64)
