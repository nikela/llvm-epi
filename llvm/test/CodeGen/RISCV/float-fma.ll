; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv32 -mattr=+f,+d -verify-machineinstrs < %s \
; RUN:   | FileCheck -check-prefix=RV32IFD %s
; RUN: llc -mtriple=riscv64 -mattr=+f,+d -verify-machineinstrs < %s \
; RUN:   | FileCheck -check-prefix=RV64IFD %s

declare float @llvm.fma.f32(float, float, float)
declare float @llvm.fmuladd.f32(float, float, float)

declare double @llvm.fma.f64(double, double, double)
declare double @llvm.fmuladd.f64(double, double, double)

define float @fmadd_s_fma_intrinsic(float %a, float %b, float %c) nounwind {
; RV32IFD-LABEL: fmadd_s_fma_intrinsic:
; RV32IFD:       # %bb.0:
; RV32IFD-NEXT:    fmadd.s fa0, fa0, fa1, fa2
; RV32IFD-NEXT:    ret
;
; RV64IFD-LABEL: fmadd_s_fma_intrinsic:
; RV64IFD:       # %bb.0:
; RV64IFD-NEXT:    fmadd.s fa0, fa0, fa1, fa2
; RV64IFD-NEXT:    ret
  %1 = call float @llvm.fma.f32(float %a, float %b, float %c)
  ret float %1
}

define float @fmadd_s_fmuladd_intrinsic(float %a, float %b, float %c) nounwind {
; Use of fmadd depends on TargetLowering::isFMAFasterthanFMulAndFAdd
; RV32IFD-LABEL: fmadd_s_fmuladd_intrinsic:
; RV32IFD:       # %bb.0:
; RV32IFD-NEXT:    fmadd.s fa0, fa0, fa1, fa2
; RV32IFD-NEXT:    ret
;
; RV64IFD-LABEL: fmadd_s_fmuladd_intrinsic:
; RV64IFD:       # %bb.0:
; RV64IFD-NEXT:    fmadd.s fa0, fa0, fa1, fa2
; RV64IFD-NEXT:    ret
  %1 = call float @llvm.fmuladd.f32(float %a, float %b, float %c)
  ret float %1
}

define float @fmsub_s_fma_intrinsic(float %a, float %b, float %c) nounwind {
; TODO: the DAG combiner converts the fneg of a bitcasted value to a xor,
;       meaning the fmsub pattern fails
; RV32IFD-LABEL: fmsub_s_fma_intrinsic:
; RV32IFD:       # %bb.0:
; RV32IFD-NEXT:    fmsub.s fa0, fa0, fa1, fa2
; RV32IFD-NEXT:    ret
;
; RV64IFD-LABEL: fmsub_s_fma_intrinsic:
; RV64IFD:       # %bb.0:
; RV64IFD-NEXT:    fmsub.s fa0, fa0, fa1, fa2
; RV64IFD-NEXT:    ret
  %1 = fsub float -0.00, %c
  %2 = call float @llvm.fma.f32(float %a, float %b, float %1)
  ret float %2
}

define double @fmadd_d_fma_intrinsic(double %a, double %b, double %c) nounwind {
; RV32IFD-LABEL: fmadd_d_fma_intrinsic:
; RV32IFD:       # %bb.0:
; RV32IFD-NEXT:    fmadd.d fa0, fa0, fa1, fa2
; RV32IFD-NEXT:    ret
;
; RV64IFD-LABEL: fmadd_d_fma_intrinsic:
; RV64IFD:       # %bb.0:
; RV64IFD-NEXT:    fmadd.d fa0, fa0, fa1, fa2
; RV64IFD-NEXT:    ret
  %1 = call double @llvm.fma.f64(double %a, double %b, double %c)
  ret double %1
}

define double @fmadd_d_fmuladd_intrinsic(double %a, double %b, double %c) nounwind {
; RV32IFD-LABEL: fmadd_d_fmuladd_intrinsic:
; RV32IFD:       # %bb.0:
; RV32IFD-NEXT:    fmadd.d fa0, fa0, fa1, fa2
; RV32IFD-NEXT:    ret
;
; RV64IFD-LABEL: fmadd_d_fmuladd_intrinsic:
; RV64IFD:       # %bb.0:
; RV64IFD-NEXT:    fmadd.d fa0, fa0, fa1, fa2
; RV64IFD-NEXT:    ret
  %1 = call double @llvm.fmuladd.f64(double %a, double %b, double %c)
  ret double %1
}

define double @fmsub_d_fma_intrinsic(double %a, double %b, double %c) nounwind {
; TODO: the DAG combiner converts the fneg of a bitcasted value to a xor,
;       meaning the fmsub may not trigger
; RV32IFD-LABEL: fmsub_d_fma_intrinsic:
; RV32IFD:       # %bb.0:
; RV32IFD-NEXT:    fmsub.d fa0, fa0, fa1, fa2
; RV32IFD-NEXT:    ret
;
; RV64IFD-LABEL: fmsub_d_fma_intrinsic:
; RV64IFD:       # %bb.0:
; RV64IFD-NEXT:    fmsub.d fa0, fa0, fa1, fa2
; RV64IFD-NEXT:    ret
  %1 = fsub double -0.00, %c
  %2 = call double @llvm.fma.f64(double %a, double %b, double %1)
  ret double %2
}
