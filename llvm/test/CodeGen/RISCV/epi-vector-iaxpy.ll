; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple riscv64 -mattr=+m,+a,+f,+d,+experimental-v < %s \
; RUN:    -epi-pipeline | FileCheck %s

define void @s16axpy(i16 signext %N, i16* noalias nocapture %y, i16* noalias nocapture readonly %x, i16 signext %alpha) {
; CHECK-LABEL: s16axpy:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    blez a0, .LBB0_8
; CHECK-NEXT:  # %bb.1: # %for.body.preheader
; CHECK-NEXT:    slli a0, a0, 48
; CHECK-NEXT:    srli a6, a0, 48
; CHECK-NEXT:    csrr t0, vlenb
; CHECK-NEXT:    srli t2, t0, 1
; CHECK-NEXT:    bgeu a6, t2, .LBB0_3
; CHECK-NEXT:  # %bb.2:
; CHECK-NEXT:    li t1, 0
; CHECK-NEXT:    j .LBB0_6
; CHECK-NEXT:  .LBB0_3: # %vector.ph
; CHECK-NEXT:    li a4, 0
; CHECK-NEXT:    li a5, 0
; CHECK-NEXT:    remu a7, a6, t2
; CHECK-NEXT:    sub t1, a6, a7
; CHECK-NEXT:  .LBB0_4: # %vector.body
; CHECK-NEXT:    # =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    add a0, a2, a4
; CHECK-NEXT:    vl1re16.v v8, (a0)
; CHECK-NEXT:    add t3, a1, a4
; CHECK-NEXT:    vl1re16.v v9, (t3)
; CHECK-NEXT:    vsetvli a0, zero, e16, m1, ta, mu
; CHECK-NEXT:    vmacc.vx v9, a3, v8
; CHECK-NEXT:    vs1r.v v9, (t3)
; CHECK-NEXT:    add a5, a5, t2
; CHECK-NEXT:    add a4, a4, t0
; CHECK-NEXT:    bne a5, t1, .LBB0_4
; CHECK-NEXT:  # %bb.5: # %middle.block
; CHECK-NEXT:    beqz a7, .LBB0_8
; CHECK-NEXT:  .LBB0_6: # %for.body.preheader23
; CHECK-NEXT:    sub a0, a6, t1
; CHECK-NEXT:    slli a4, t1, 1
; CHECK-NEXT:    add a1, a1, a4
; CHECK-NEXT:    add a2, a2, a4
; CHECK-NEXT:  .LBB0_7: # %for.body
; CHECK-NEXT:    # =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    lh a4, 0(a2)
; CHECK-NEXT:    lh a5, 0(a1)
; CHECK-NEXT:    mulw a4, a4, a3
; CHECK-NEXT:    addw a4, a4, a5
; CHECK-NEXT:    sh a4, 0(a1)
; CHECK-NEXT:    addi a0, a0, -1
; CHECK-NEXT:    addi a1, a1, 2
; CHECK-NEXT:    addi a2, a2, 2
; CHECK-NEXT:    bnez a0, .LBB0_7
; CHECK-NEXT:  .LBB0_8: # %for.cond.cleanup
; CHECK-NEXT:    ret
entry:
  %cmp17 = icmp sgt i16 %N, 0
  br i1 %cmp17, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:                               ; preds = %entry
  %wide.trip.count = zext i16 %N to i64
  %0 = call i64 @llvm.vscale.i64()
  %step.vscale = shl i64 %0, 2
  %min.iters.check = icmp ugt i64 %step.vscale, %wide.trip.count
  br i1 %min.iters.check, label %for.body.preheader23, label %vector.ph

for.body.preheader23:                             ; preds = %middle.block, %for.body.preheader
  %indvars.iv.ph = phi i64 [ 0, %for.body.preheader ], [ %n.vec, %middle.block ]
  br label %for.body

vector.ph:                                        ; preds = %for.body.preheader
  %1 = call i64 @llvm.vscale.i64()
  %step.vscale19 = shl i64 %1, 2
  %n.mod.vf = urem i64 %wide.trip.count, %step.vscale19
  %n.vec = sub nsw i64 %wide.trip.count, %n.mod.vf
  %broadcast.splatinsert20 = insertelement <vscale x 4 x i16> undef, i16 %alpha, i32 0
  %broadcast.splat21 = shufflevector <vscale x 4 x i16> %broadcast.splatinsert20, <vscale x 4 x i16> undef, <vscale x 4 x i32> zeroinitializer
  %2 = call i64 @llvm.vscale.i64()
  %index.vscale = shl i64 %2, 2
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %3 = getelementptr inbounds i16, i16* %x, i64 %index
  %4 = bitcast i16* %3 to <vscale x 4 x i16>*
  %wide.load = load <vscale x 4 x i16>, <vscale x 4 x i16>* %4, align 2
  %5 = mul <vscale x 4 x i16> %wide.load, %broadcast.splat21
  %6 = getelementptr inbounds i16, i16* %y, i64 %index
  %7 = bitcast i16* %6 to <vscale x 4 x i16>*
  %wide.load22 = load <vscale x 4 x i16>, <vscale x 4 x i16>* %7, align 2
  %8 = add <vscale x 4 x i16> %5, %wide.load22
  %9 = bitcast i16* %6 to <vscale x 4 x i16>*
  store <vscale x 4 x i16> %8, <vscale x 4 x i16>* %9, align 2
  %index.next = add i64 %index, %index.vscale
  %10 = icmp eq i64 %index.next, %n.vec
  br i1 %10, label %middle.block, label %vector.body

middle.block:                                     ; preds = %vector.body
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %for.cond.cleanup, label %for.body.preheader23

for.cond.cleanup:                                 ; preds = %for.body, %middle.block, %entry
  ret void

for.body:                                         ; preds = %for.body.preheader23, %for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %for.body ], [ %indvars.iv.ph, %for.body.preheader23 ]
  %arrayidx = getelementptr inbounds i16, i16* %x, i64 %indvars.iv
  %11 = load i16, i16* %arrayidx, align 2
  %mul = mul i16 %11, %alpha
  %arrayidx6 = getelementptr inbounds i16, i16* %y, i64 %indvars.iv
  %12 = load i16, i16* %arrayidx6, align 2
  %add = add i16 %mul, %12
  store i16 %add, i16* %arrayidx6, align 2
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond = icmp eq i64 %indvars.iv.next, %wide.trip.count
  br i1 %exitcond, label %for.cond.cleanup, label %for.body
}

define void @s32axpy(i32 signext %N, i32* noalias nocapture %y, i32* noalias nocapture readonly %x, i32 signext %alpha) {
; CHECK-LABEL: s32axpy:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    blez a0, .LBB1_8
; CHECK-NEXT:  # %bb.1: # %for.body.preheader
; CHECK-NEXT:    slli a0, a0, 32
; CHECK-NEXT:    srli a6, a0, 32
; CHECK-NEXT:    csrr t0, vlenb
; CHECK-NEXT:    srli t2, t0, 2
; CHECK-NEXT:    bgeu a6, t2, .LBB1_3
; CHECK-NEXT:  # %bb.2:
; CHECK-NEXT:    li t1, 0
; CHECK-NEXT:    j .LBB1_6
; CHECK-NEXT:  .LBB1_3: # %vector.ph
; CHECK-NEXT:    li a4, 0
; CHECK-NEXT:    li a5, 0
; CHECK-NEXT:    remu a7, a6, t2
; CHECK-NEXT:    sub t1, a6, a7
; CHECK-NEXT:  .LBB1_4: # %vector.body
; CHECK-NEXT:    # =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    add a0, a2, a4
; CHECK-NEXT:    vl1re32.v v8, (a0)
; CHECK-NEXT:    add t3, a1, a4
; CHECK-NEXT:    vl1re32.v v9, (t3)
; CHECK-NEXT:    vsetvli a0, zero, e32, m1, ta, mu
; CHECK-NEXT:    vmacc.vx v9, a3, v8
; CHECK-NEXT:    vs1r.v v9, (t3)
; CHECK-NEXT:    add a5, a5, t2
; CHECK-NEXT:    add a4, a4, t0
; CHECK-NEXT:    bne a5, t1, .LBB1_4
; CHECK-NEXT:  # %bb.5: # %middle.block
; CHECK-NEXT:    beqz a7, .LBB1_8
; CHECK-NEXT:  .LBB1_6: # %for.body.preheader17
; CHECK-NEXT:    sub a0, a6, t1
; CHECK-NEXT:    slli a4, t1, 2
; CHECK-NEXT:    add a1, a1, a4
; CHECK-NEXT:    add a2, a2, a4
; CHECK-NEXT:  .LBB1_7: # %for.body
; CHECK-NEXT:    # =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    lw a4, 0(a2)
; CHECK-NEXT:    lw a5, 0(a1)
; CHECK-NEXT:    mulw a4, a4, a3
; CHECK-NEXT:    addw a4, a4, a5
; CHECK-NEXT:    sw a4, 0(a1)
; CHECK-NEXT:    addi a0, a0, -1
; CHECK-NEXT:    addi a1, a1, 4
; CHECK-NEXT:    addi a2, a2, 4
; CHECK-NEXT:    bnez a0, .LBB1_7
; CHECK-NEXT:  .LBB1_8: # %for.cond.cleanup
; CHECK-NEXT:    ret
entry:
  %cmp11 = icmp sgt i32 %N, 0
  br i1 %cmp11, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:                               ; preds = %entry
  %wide.trip.count = zext i32 %N to i64
  %0 = call i64 @llvm.vscale.i64()
  %step.vscale = shl i64 %0, 1
  %min.iters.check = icmp ugt i64 %step.vscale, %wide.trip.count
  br i1 %min.iters.check, label %for.body.preheader17, label %vector.ph

for.body.preheader17:                             ; preds = %middle.block, %for.body.preheader
  %indvars.iv.ph = phi i64 [ 0, %for.body.preheader ], [ %n.vec, %middle.block ]
  br label %for.body

vector.ph:                                        ; preds = %for.body.preheader
  %1 = call i64 @llvm.vscale.i64()
  %step.vscale13 = shl i64 %1, 1
  %n.mod.vf = urem i64 %wide.trip.count, %step.vscale13
  %n.vec = sub nsw i64 %wide.trip.count, %n.mod.vf
  %broadcast.splatinsert14 = insertelement <vscale x 2 x i32> undef, i32 %alpha, i32 0
  %broadcast.splat15 = shufflevector <vscale x 2 x i32> %broadcast.splatinsert14, <vscale x 2 x i32> undef, <vscale x 2 x i32> zeroinitializer
  %2 = call i64 @llvm.vscale.i64()
  %index.vscale = shl i64 %2, 1
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %3 = getelementptr inbounds i32, i32* %x, i64 %index
  %4 = bitcast i32* %3 to <vscale x 2 x i32>*
  %wide.load = load <vscale x 2 x i32>, <vscale x 2 x i32>* %4, align 4
  %5 = mul nsw <vscale x 2 x i32> %wide.load, %broadcast.splat15
  %6 = getelementptr inbounds i32, i32* %y, i64 %index
  %7 = bitcast i32* %6 to <vscale x 2 x i32>*
  %wide.load16 = load <vscale x 2 x i32>, <vscale x 2 x i32>* %7, align 4
  %8 = add nsw <vscale x 2 x i32> %5, %wide.load16
  %9 = bitcast i32* %6 to <vscale x 2 x i32>*
  store <vscale x 2 x i32> %8, <vscale x 2 x i32>* %9, align 4
  %index.next = add i64 %index, %index.vscale
  %10 = icmp eq i64 %index.next, %n.vec
  br i1 %10, label %middle.block, label %vector.body

middle.block:                                     ; preds = %vector.body
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %for.cond.cleanup, label %for.body.preheader17

for.cond.cleanup:                                 ; preds = %for.body, %middle.block, %entry
  ret void

for.body:                                         ; preds = %for.body.preheader17, %for.body
  %indvars.iv = phi i64 [ %indvars.iv.next, %for.body ], [ %indvars.iv.ph, %for.body.preheader17 ]
  %arrayidx = getelementptr inbounds i32, i32* %x, i64 %indvars.iv
  %11 = load i32, i32* %arrayidx, align 4
  %mul = mul nsw i32 %11, %alpha
  %arrayidx2 = getelementptr inbounds i32, i32* %y, i64 %indvars.iv
  %12 = load i32, i32* %arrayidx2, align 4
  %add = add nsw i32 %mul, %12
  store i32 %add, i32* %arrayidx2, align 4
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond = icmp eq i64 %indvars.iv.next, %wide.trip.count
  br i1 %exitcond, label %for.cond.cleanup, label %for.body
}

; Function Attrs: nofree norecurse nounwind
define void @s64axpy(i64 %N, i64* noalias nocapture %y, i64* noalias nocapture readonly %x, i64 %alpha) {
; CHECK-LABEL: s64axpy:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    blez a0, .LBB2_8
; CHECK-NEXT:  # %bb.1: # %for.body.preheader
; CHECK-NEXT:    csrr a6, vlenb
; CHECK-NEXT:    srli t1, a6, 3
; CHECK-NEXT:    bgeu a0, t1, .LBB2_3
; CHECK-NEXT:  # %bb.2:
; CHECK-NEXT:    li t0, 0
; CHECK-NEXT:    j .LBB2_6
; CHECK-NEXT:  .LBB2_3: # %vector.ph
; CHECK-NEXT:    li a5, 0
; CHECK-NEXT:    li t2, 0
; CHECK-NEXT:    remu a7, a0, t1
; CHECK-NEXT:    sub t0, a0, a7
; CHECK-NEXT:  .LBB2_4: # %vector.body
; CHECK-NEXT:    # =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    add a4, a2, a5
; CHECK-NEXT:    vl1re64.v v8, (a4)
; CHECK-NEXT:    add t3, a1, a5
; CHECK-NEXT:    vl1re64.v v9, (t3)
; CHECK-NEXT:    vsetvli a4, zero, e64, m1, ta, mu
; CHECK-NEXT:    vmacc.vx v9, a3, v8
; CHECK-NEXT:    vs1r.v v9, (t3)
; CHECK-NEXT:    add t2, t2, t1
; CHECK-NEXT:    add a5, a5, a6
; CHECK-NEXT:    bne t2, t0, .LBB2_4
; CHECK-NEXT:  # %bb.5: # %middle.block
; CHECK-NEXT:    beqz a7, .LBB2_8
; CHECK-NEXT:  .LBB2_6: # %for.body.preheader15
; CHECK-NEXT:    sub a0, a0, t0
; CHECK-NEXT:    slli a4, t0, 3
; CHECK-NEXT:    add a1, a1, a4
; CHECK-NEXT:    add a2, a2, a4
; CHECK-NEXT:  .LBB2_7: # %for.body
; CHECK-NEXT:    # =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    ld a4, 0(a2)
; CHECK-NEXT:    ld a5, 0(a1)
; CHECK-NEXT:    mul a4, a4, a3
; CHECK-NEXT:    add a4, a4, a5
; CHECK-NEXT:    sd a4, 0(a1)
; CHECK-NEXT:    addi a0, a0, -1
; CHECK-NEXT:    addi a1, a1, 8
; CHECK-NEXT:    addi a2, a2, 8
; CHECK-NEXT:    bnez a0, .LBB2_7
; CHECK-NEXT:  .LBB2_8: # %for.cond.cleanup
; CHECK-NEXT:    ret
entry:
  %cmp9 = icmp sgt i64 %N, 0
  br i1 %cmp9, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:                               ; preds = %entry
  %0 = call i64 @llvm.vscale.i64()
  %min.iters.check = icmp ugt i64 %0, %N
  br i1 %min.iters.check, label %for.body.preheader15, label %vector.ph

for.body.preheader15:                             ; preds = %middle.block, %for.body.preheader
  %i.010.ph = phi i64 [ 0, %for.body.preheader ], [ %n.vec, %middle.block ]
  br label %for.body

vector.ph:                                        ; preds = %for.body.preheader
  %1 = call i64 @llvm.vscale.i64()
  %n.mod.vf = urem i64 %N, %1
  %n.vec = sub i64 %N, %n.mod.vf
  %broadcast.splatinsert12 = insertelement <vscale x 1 x i64> undef, i64 %alpha, i32 0
  %broadcast.splat13 = shufflevector <vscale x 1 x i64> %broadcast.splatinsert12, <vscale x 1 x i64> undef, <vscale x 1 x i32> zeroinitializer
  %2 = call i64 @llvm.vscale.i64()
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %3 = getelementptr inbounds i64, i64* %x, i64 %index
  %4 = bitcast i64* %3 to <vscale x 1 x i64>*
  %wide.load = load <vscale x 1 x i64>, <vscale x 1 x i64>* %4, align 8
  %5 = mul nsw <vscale x 1 x i64> %wide.load, %broadcast.splat13
  %6 = getelementptr inbounds i64, i64* %y, i64 %index
  %7 = bitcast i64* %6 to <vscale x 1 x i64>*
  %wide.load14 = load <vscale x 1 x i64>, <vscale x 1 x i64>* %7, align 8
  %8 = add nsw <vscale x 1 x i64> %5, %wide.load14
  %9 = bitcast i64* %6 to <vscale x 1 x i64>*
  store <vscale x 1 x i64> %8, <vscale x 1 x i64>* %9, align 8
  %index.next = add i64 %index, %2
  %10 = icmp eq i64 %index.next, %n.vec
  br i1 %10, label %middle.block, label %vector.body

middle.block:                                     ; preds = %vector.body
  %cmp.n = icmp eq i64 %n.mod.vf, 0
  br i1 %cmp.n, label %for.cond.cleanup, label %for.body.preheader15

for.cond.cleanup:                                 ; preds = %for.body, %middle.block, %entry
  ret void

for.body:                                         ; preds = %for.body.preheader15, %for.body
  %i.010 = phi i64 [ %inc, %for.body ], [ %i.010.ph, %for.body.preheader15 ]
  %arrayidx = getelementptr inbounds i64, i64* %x, i64 %i.010
  %11 = load i64, i64* %arrayidx, align 8
  %mul = mul nsw i64 %11, %alpha
  %arrayidx1 = getelementptr inbounds i64, i64* %y, i64 %i.010
  %12 = load i64, i64* %arrayidx1, align 8
  %add = add nsw i64 %mul, %12
  store i64 %add, i64* %arrayidx1, align 8
  %inc = add nuw nsw i64 %i.010, 1
  %exitcond = icmp eq i64 %inc, %N
  br i1 %exitcond, label %for.cond.cleanup, label %for.body
}

; Function Attrs: nounwind readnone
declare i64 @llvm.vscale.i64() #1
