; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+m,+experimental-v -verify-machineinstrs -O0 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=CHECK-O0 %s
; RUN: llc -mtriple=riscv64 -mattr=+m,+experimental-v -verify-machineinstrs -O2 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=CHECK-O2 %s

@scratch = global i8 0, align 16

define void @test_vp_icmp(<vscale x 1 x i64> %a, <vscale x 1 x i64> %b, <vscale x 1 x i1> %m, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_icmp:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    csrr a1, vlenb
; CHECK-O0-NEXT:    sub sp, sp, a1
; CHECK-O0-NEXT:    mv a1, a0
; CHECK-O0-NEXT:    addi a0, sp, 16
; CHECK-O0-NEXT:    vs1r.v v0, (a0) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    # kill: def $x10 killed $x11
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e64,m1,ta,mu
; CHECK-O0-NEXT:    vmseq.vv v25, v8, v9, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf8,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e64,m1,ta,mu
; CHECK-O0-NEXT:    vmsne.vv v25, v8, v9, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf8,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e64,m1,ta,mu
; CHECK-O0-NEXT:    vmsltu.vv v25, v9, v8, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf8,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e64,m1,ta,mu
; CHECK-O0-NEXT:    vmsleu.vv v25, v9, v8, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf8,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e64,m1,ta,mu
; CHECK-O0-NEXT:    vmsltu.vv v25, v8, v9, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf8,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e64,m1,ta,mu
; CHECK-O0-NEXT:    vmsleu.vv v25, v8, v9, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf8,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e64,m1,ta,mu
; CHECK-O0-NEXT:    vmslt.vv v25, v9, v8, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf8,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e64,m1,ta,mu
; CHECK-O0-NEXT:    vmsle.vv v25, v9, v8, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf8,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e64,m1,ta,mu
; CHECK-O0-NEXT:    vmslt.vv v25, v8, v9, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf8,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e64,m1,ta,mu
; CHECK-O0-NEXT:    vmsle.vv v25, v8, v9, v0.t
; CHECK-O0-NEXT:    vsetvli a1, zero, e8,mf8,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    csrr a0, vlenb
; CHECK-O0-NEXT:    add sp, sp, a0
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_icmp:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    lui a1, %hi(scratch)
; CHECK-O2-NEXT:    addi a1, a1, %lo(scratch)
; CHECK-O2-NEXT:    vsetvli zero, a0, e64,m1,ta,mu
; CHECK-O2-NEXT:    vmseq.vv v25, v8, v9, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf8,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e64,m1,ta,mu
; CHECK-O2-NEXT:    vmsne.vv v25, v8, v9, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf8,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e64,m1,ta,mu
; CHECK-O2-NEXT:    vmsltu.vv v25, v9, v8, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf8,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e64,m1,ta,mu
; CHECK-O2-NEXT:    vmsleu.vv v25, v9, v8, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf8,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e64,m1,ta,mu
; CHECK-O2-NEXT:    vmsltu.vv v25, v8, v9, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf8,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e64,m1,ta,mu
; CHECK-O2-NEXT:    vmsleu.vv v25, v8, v9, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf8,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e64,m1,ta,mu
; CHECK-O2-NEXT:    vmslt.vv v25, v9, v8, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf8,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e64,m1,ta,mu
; CHECK-O2-NEXT:    vmsle.vv v25, v9, v8, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf8,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e64,m1,ta,mu
; CHECK-O2-NEXT:    vmslt.vv v25, v8, v9, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf8,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e64,m1,ta,mu
; CHECK-O2-NEXT:    vmsle.vv v25, v8, v9, v0.t
; CHECK-O2-NEXT:    vsetvli a0, zero, e8,mf8,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 1 x i1>*

  %eq = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %b, i8 32, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %eq, <vscale x 1 x i1>* %store_addr

  %ne = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %b, i8 33, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %ne, <vscale x 1 x i1>* %store_addr

  %ugt = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %b, i8 34, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %ugt, <vscale x 1 x i1>* %store_addr

  %uge = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %b, i8 35, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %uge, <vscale x 1 x i1>* %store_addr

  %ult = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %b, i8 36, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %ult, <vscale x 1 x i1>* %store_addr

  %ule = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %b, i8 37, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %ule, <vscale x 1 x i1>* %store_addr

  %sgt = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %b, i8 38, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %sgt, <vscale x 1 x i1>* %store_addr

  %sge = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %b, i8 39, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %sge, <vscale x 1 x i1>* %store_addr

  %slt = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %b, i8 40, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %slt, <vscale x 1 x i1>* %store_addr

  %sle = call <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64> %a, <vscale x 1 x i64> %b, i8 41, <vscale x 1 x i1> %m, i32 %n)
  store <vscale x 1 x i1> %sle, <vscale x 1 x i1>* %store_addr

  ret void
}

define void @test_vp_icmp_2(<vscale x 2 x i32> %a, <vscale x 2 x i32> %b, <vscale x 2 x i1> %m, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_icmp_2:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    csrr a1, vlenb
; CHECK-O0-NEXT:    sub sp, sp, a1
; CHECK-O0-NEXT:    mv a1, a0
; CHECK-O0-NEXT:    addi a0, sp, 16
; CHECK-O0-NEXT:    vs1r.v v0, (a0) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    # kill: def $x10 killed $x11
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e32,m1,ta,mu
; CHECK-O0-NEXT:    vmseq.vv v25, v8, v9, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e32,m1,ta,mu
; CHECK-O0-NEXT:    vmsne.vv v25, v8, v9, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e32,m1,ta,mu
; CHECK-O0-NEXT:    vmsltu.vv v25, v9, v8, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e32,m1,ta,mu
; CHECK-O0-NEXT:    vmsleu.vv v25, v9, v8, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e32,m1,ta,mu
; CHECK-O0-NEXT:    vmsltu.vv v25, v8, v9, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e32,m1,ta,mu
; CHECK-O0-NEXT:    vmsleu.vv v25, v8, v9, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e32,m1,ta,mu
; CHECK-O0-NEXT:    vmslt.vv v25, v9, v8, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e32,m1,ta,mu
; CHECK-O0-NEXT:    vmsle.vv v25, v9, v8, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e32,m1,ta,mu
; CHECK-O0-NEXT:    vmslt.vv v25, v8, v9, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e32,m1,ta,mu
; CHECK-O0-NEXT:    vmsle.vv v25, v8, v9, v0.t
; CHECK-O0-NEXT:    vsetvli a1, zero, e8,mf4,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    csrr a0, vlenb
; CHECK-O0-NEXT:    add sp, sp, a0
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_icmp_2:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    lui a1, %hi(scratch)
; CHECK-O2-NEXT:    addi a1, a1, %lo(scratch)
; CHECK-O2-NEXT:    vsetvli zero, a0, e32,m1,ta,mu
; CHECK-O2-NEXT:    vmseq.vv v25, v8, v9, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e32,m1,ta,mu
; CHECK-O2-NEXT:    vmsne.vv v25, v8, v9, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e32,m1,ta,mu
; CHECK-O2-NEXT:    vmsltu.vv v25, v9, v8, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e32,m1,ta,mu
; CHECK-O2-NEXT:    vmsleu.vv v25, v9, v8, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e32,m1,ta,mu
; CHECK-O2-NEXT:    vmsltu.vv v25, v8, v9, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e32,m1,ta,mu
; CHECK-O2-NEXT:    vmsleu.vv v25, v8, v9, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e32,m1,ta,mu
; CHECK-O2-NEXT:    vmslt.vv v25, v9, v8, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e32,m1,ta,mu
; CHECK-O2-NEXT:    vmsle.vv v25, v9, v8, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e32,m1,ta,mu
; CHECK-O2-NEXT:    vmslt.vv v25, v8, v9, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e32,m1,ta,mu
; CHECK-O2-NEXT:    vmsle.vv v25, v8, v9, v0.t
; CHECK-O2-NEXT:    vsetvli a0, zero, e8,mf4,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 2 x i1>*

  %eq = call <vscale x 2 x i1> @llvm.vp.icmp.nxv2i32(<vscale x 2 x i32> %a, <vscale x 2 x i32> %b, i8 32, <vscale x 2 x i1> %m, i32 %n)
  store <vscale x 2 x i1> %eq, <vscale x 2 x i1>* %store_addr

  %ne = call <vscale x 2 x i1> @llvm.vp.icmp.nxv2i32(<vscale x 2 x i32> %a, <vscale x 2 x i32> %b, i8 33, <vscale x 2 x i1> %m, i32 %n)
  store <vscale x 2 x i1> %ne, <vscale x 2 x i1>* %store_addr

  %ugt = call <vscale x 2 x i1> @llvm.vp.icmp.nxv2i32(<vscale x 2 x i32> %a, <vscale x 2 x i32> %b, i8 34, <vscale x 2 x i1> %m, i32 %n)
  store <vscale x 2 x i1> %ugt, <vscale x 2 x i1>* %store_addr

  %uge = call <vscale x 2 x i1> @llvm.vp.icmp.nxv2i32(<vscale x 2 x i32> %a, <vscale x 2 x i32> %b, i8 35, <vscale x 2 x i1> %m, i32 %n)
  store <vscale x 2 x i1> %uge, <vscale x 2 x i1>* %store_addr

  %ult = call <vscale x 2 x i1> @llvm.vp.icmp.nxv2i32(<vscale x 2 x i32> %a, <vscale x 2 x i32> %b, i8 36, <vscale x 2 x i1> %m, i32 %n)
  store <vscale x 2 x i1> %ult, <vscale x 2 x i1>* %store_addr

  %ule = call <vscale x 2 x i1> @llvm.vp.icmp.nxv2i32(<vscale x 2 x i32> %a, <vscale x 2 x i32> %b, i8 37, <vscale x 2 x i1> %m, i32 %n)
  store <vscale x 2 x i1> %ule, <vscale x 2 x i1>* %store_addr

  %sgt = call <vscale x 2 x i1> @llvm.vp.icmp.nxv2i32(<vscale x 2 x i32> %a, <vscale x 2 x i32> %b, i8 38, <vscale x 2 x i1> %m, i32 %n)
  store <vscale x 2 x i1> %sgt, <vscale x 2 x i1>* %store_addr

  %sge = call <vscale x 2 x i1> @llvm.vp.icmp.nxv2i32(<vscale x 2 x i32> %a, <vscale x 2 x i32> %b, i8 39, <vscale x 2 x i1> %m, i32 %n)
  store <vscale x 2 x i1> %sge, <vscale x 2 x i1>* %store_addr

  %slt = call <vscale x 2 x i1> @llvm.vp.icmp.nxv2i32(<vscale x 2 x i32> %a, <vscale x 2 x i32> %b, i8 40, <vscale x 2 x i1> %m, i32 %n)
  store <vscale x 2 x i1> %slt, <vscale x 2 x i1>* %store_addr

  %sle = call <vscale x 2 x i1> @llvm.vp.icmp.nxv2i32(<vscale x 2 x i32> %a, <vscale x 2 x i32> %b, i8 41, <vscale x 2 x i1> %m, i32 %n)
  store <vscale x 2 x i1> %sle, <vscale x 2 x i1>* %store_addr

  ret void
}

define void @test_vp_icmp_3(<vscale x 2 x i64> %a, <vscale x 2 x i64> %b, <vscale x 2 x i1> %m, i32 %n) nounwind {
; CHECK-O0-LABEL: test_vp_icmp_3:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    csrr a1, vlenb
; CHECK-O0-NEXT:    sub sp, sp, a1
; CHECK-O0-NEXT:    mv a1, a0
; CHECK-O0-NEXT:    addi a0, sp, 16
; CHECK-O0-NEXT:    vs1r.v v0, (a0) # Unknown-size Folded Spill
; CHECK-O0-NEXT:    # kill: def $x10 killed $x11
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e64,m2,ta,mu
; CHECK-O0-NEXT:    vmseq.vv v25, v8, v10, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e64,m2,ta,mu
; CHECK-O0-NEXT:    vmsne.vv v25, v8, v10, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e64,m2,ta,mu
; CHECK-O0-NEXT:    vmsltu.vv v25, v10, v8, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e64,m2,ta,mu
; CHECK-O0-NEXT:    vmsleu.vv v25, v10, v8, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e64,m2,ta,mu
; CHECK-O0-NEXT:    vmsltu.vv v25, v8, v10, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e64,m2,ta,mu
; CHECK-O0-NEXT:    vmsleu.vv v25, v8, v10, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e64,m2,ta,mu
; CHECK-O0-NEXT:    vmslt.vv v25, v10, v8, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e64,m2,ta,mu
; CHECK-O0-NEXT:    vmsle.vv v25, v10, v8, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e64,m2,ta,mu
; CHECK-O0-NEXT:    vmslt.vv v25, v8, v10, v0.t
; CHECK-O0-NEXT:    addi a2, sp, 16
; CHECK-O0-NEXT:    vl1r.v v0, (a2) # Unknown-size Folded Reload
; CHECK-O0-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v25
; CHECK-O0-NEXT:    vsetvli zero, a1, e64,m2,ta,mu
; CHECK-O0-NEXT:    vmsle.vv v25, v8, v10, v0.t
; CHECK-O0-NEXT:    vsetvli a1, zero, e8,mf4,ta,mu
; CHECK-O0-NEXT:    vse1.v v25, (a0)
; CHECK-O0-NEXT:    csrr a0, vlenb
; CHECK-O0-NEXT:    add sp, sp, a0
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vp_icmp_3:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    lui a1, %hi(scratch)
; CHECK-O2-NEXT:    addi a1, a1, %lo(scratch)
; CHECK-O2-NEXT:    vsetvli zero, a0, e64,m2,ta,mu
; CHECK-O2-NEXT:    vmseq.vv v25, v8, v10, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e64,m2,ta,mu
; CHECK-O2-NEXT:    vmsne.vv v25, v8, v10, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e64,m2,ta,mu
; CHECK-O2-NEXT:    vmsltu.vv v25, v10, v8, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e64,m2,ta,mu
; CHECK-O2-NEXT:    vmsleu.vv v25, v10, v8, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e64,m2,ta,mu
; CHECK-O2-NEXT:    vmsltu.vv v25, v8, v10, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e64,m2,ta,mu
; CHECK-O2-NEXT:    vmsleu.vv v25, v8, v10, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e64,m2,ta,mu
; CHECK-O2-NEXT:    vmslt.vv v25, v10, v8, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e64,m2,ta,mu
; CHECK-O2-NEXT:    vmsle.vv v25, v10, v8, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e64,m2,ta,mu
; CHECK-O2-NEXT:    vmslt.vv v25, v8, v10, v0.t
; CHECK-O2-NEXT:    vsetvli a2, zero, e8,mf4,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a0, e64,m2,ta,mu
; CHECK-O2-NEXT:    vmsle.vv v25, v8, v10, v0.t
; CHECK-O2-NEXT:    vsetvli a0, zero, e8,mf4,ta,mu
; CHECK-O2-NEXT:    vse1.v v25, (a1)
; CHECK-O2-NEXT:    ret
  %store_addr = bitcast i8* @scratch to <vscale x 2 x i1>*

  %eq = call <vscale x 2 x i1> @llvm.vp.icmp.nxv2i64(<vscale x 2 x i64> %a, <vscale x 2 x i64> %b, i8 32, <vscale x 2 x i1> %m, i32 %n)
  store <vscale x 2 x i1> %eq, <vscale x 2 x i1>* %store_addr

  %ne = call <vscale x 2 x i1> @llvm.vp.icmp.nxv2i64(<vscale x 2 x i64> %a, <vscale x 2 x i64> %b, i8 33, <vscale x 2 x i1> %m, i32 %n)
  store <vscale x 2 x i1> %ne, <vscale x 2 x i1>* %store_addr

  %ugt = call <vscale x 2 x i1> @llvm.vp.icmp.nxv2i64(<vscale x 2 x i64> %a, <vscale x 2 x i64> %b, i8 34, <vscale x 2 x i1> %m, i32 %n)
  store <vscale x 2 x i1> %ugt, <vscale x 2 x i1>* %store_addr

  %uge = call <vscale x 2 x i1> @llvm.vp.icmp.nxv2i64(<vscale x 2 x i64> %a, <vscale x 2 x i64> %b, i8 35, <vscale x 2 x i1> %m, i32 %n)
  store <vscale x 2 x i1> %uge, <vscale x 2 x i1>* %store_addr

  %ult = call <vscale x 2 x i1> @llvm.vp.icmp.nxv2i64(<vscale x 2 x i64> %a, <vscale x 2 x i64> %b, i8 36, <vscale x 2 x i1> %m, i32 %n)
  store <vscale x 2 x i1> %ult, <vscale x 2 x i1>* %store_addr

  %ule = call <vscale x 2 x i1> @llvm.vp.icmp.nxv2i64(<vscale x 2 x i64> %a, <vscale x 2 x i64> %b, i8 37, <vscale x 2 x i1> %m, i32 %n)
  store <vscale x 2 x i1> %ule, <vscale x 2 x i1>* %store_addr

  %sgt = call <vscale x 2 x i1> @llvm.vp.icmp.nxv2i64(<vscale x 2 x i64> %a, <vscale x 2 x i64> %b, i8 38, <vscale x 2 x i1> %m, i32 %n)
  store <vscale x 2 x i1> %sgt, <vscale x 2 x i1>* %store_addr

  %sge = call <vscale x 2 x i1> @llvm.vp.icmp.nxv2i64(<vscale x 2 x i64> %a, <vscale x 2 x i64> %b, i8 39, <vscale x 2 x i1> %m, i32 %n)
  store <vscale x 2 x i1> %sge, <vscale x 2 x i1>* %store_addr

  %slt = call <vscale x 2 x i1> @llvm.vp.icmp.nxv2i64(<vscale x 2 x i64> %a, <vscale x 2 x i64> %b, i8 40, <vscale x 2 x i1> %m, i32 %n)
  store <vscale x 2 x i1> %slt, <vscale x 2 x i1>* %store_addr

  %sle = call <vscale x 2 x i1> @llvm.vp.icmp.nxv2i64(<vscale x 2 x i64> %a, <vscale x 2 x i64> %b, i8 41, <vscale x 2 x i1> %m, i32 %n)
  store <vscale x 2 x i1> %sle, <vscale x 2 x i1>* %store_addr

  ret void
}

; FIXME use VP stores
; store
declare void @llvm.vp.store.nxv1i1(<vscale x 1 x i1>, <vscale x 1 x i1>*, i1, <vscale x 1 x i1>, i32)
declare void @llvm.vp.store.nxv2i1(<vscale x 2 x i1>, <vscale x 2 x i1>*, i1, <vscale x 2 x i1>, i32)

; icmp
declare <vscale x 1 x i1> @llvm.vp.icmp.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, i8 immarg, <vscale x 1 x i1>, i32)
declare <vscale x 2 x i1> @llvm.vp.icmp.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>, i8 immarg, <vscale x 2 x i1>, i32)
declare <vscale x 2 x i1> @llvm.vp.icmp.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>, i8 immarg, <vscale x 2 x i1>, i32)
