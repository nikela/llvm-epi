; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+m,+f,+d,+v -verify-machineinstrs < %s | FileCheck %s

define void @test_store_vp_reverse_nxv1f64(<vscale x 1 x double> %src, <vscale x 1 x double> *%ptr, i32 zeroext %evl) {
; CHECK-LABEL: test_store_vp_reverse_nxv1f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    slli a2, a1, 3
; CHECK-NEXT:    add a0, a2, a0
; CHECK-NEXT:    addi a0, a0, -8
; CHECK-NEXT:    li a2, -8
; CHECK-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-NEXT:    vsse64.v v8, (a0), a2
; CHECK-NEXT:    ret
  %head = insertelement <vscale x 1 x i1> undef, i1 1, i32 0
  %allones = shufflevector <vscale x 1 x i1> %head, <vscale x 1 x i1> undef, <vscale x 1 x i32> zeroinitializer

  %dst = call <vscale x 1 x double> @llvm.experimental.vp.reverse.nxv1f64(<vscale x 1 x double> %src, <vscale x 1 x i1> %allones, i32 %evl)
  call void @llvm.vp.store.nxv1f64(<vscale x 1 x double> %dst, <vscale x 1 x double>* %ptr, <vscale x 1 x i1> %allones, i32 %evl)
  ret void
}

define void @test_store_vp_reverse_different_evl_nxv1f64(<vscale x 1 x double> %src, <vscale x 1 x double> *%ptr, i32 zeroext %evl1, i32 zeroext %evl2) {
; CHECK-LABEL: test_store_vp_reverse_different_evl_nxv1f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi a3, a1, -1
; CHECK-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-NEXT:    vid.v v9
; CHECK-NEXT:    vrsub.vx v9, v9, a3
; CHECK-NEXT:    vrgather.vv v10, v8, v9
; CHECK-NEXT:    vsetvli zero, a2, e64, m1, ta, ma
; CHECK-NEXT:    vse64.v v10, (a0)
; CHECK-NEXT:    ret
  %head = insertelement <vscale x 1 x i1> undef, i1 1, i32 0
  %allones = shufflevector <vscale x 1 x i1> %head, <vscale x 1 x i1> undef, <vscale x 1 x i32> zeroinitializer

  %dst = call <vscale x 1 x double> @llvm.experimental.vp.reverse.nxv1f64(<vscale x 1 x double> %src, <vscale x 1 x i1> %allones, i32 %evl1)
  call void @llvm.vp.store.nxv1f64(<vscale x 1 x double> %dst, <vscale x 1 x double>* %ptr, <vscale x 1 x i1> %allones, i32 %evl2)
  ret void
}

define <vscale x 1 x double> @test_store_vp_reverse_many_uses_nxv1f64(<vscale x 1 x double> %src, <vscale x 1 x double> *%ptr, i32 zeroext %evl) {
; CHECK-LABEL: test_store_vp_reverse_many_uses_nxv1f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi a2, a1, -1
; CHECK-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-NEXT:    vid.v v9
; CHECK-NEXT:    vrsub.vx v10, v9, a2
; CHECK-NEXT:    vrgather.vv v9, v8, v10
; CHECK-NEXT:    vse64.v v9, (a0)
; CHECK-NEXT:    vmv.v.v v8, v9
; CHECK-NEXT:    ret
  %head = insertelement <vscale x 1 x i1> undef, i1 1, i32 0
  %allones = shufflevector <vscale x 1 x i1> %head, <vscale x 1 x i1> undef, <vscale x 1 x i32> zeroinitializer

  %dst = call <vscale x 1 x double> @llvm.experimental.vp.reverse.nxv1f64(<vscale x 1 x double> %src, <vscale x 1 x i1> %allones, i32 %evl)
  call void @llvm.vp.store.nxv1f64(<vscale x 1 x double> %dst, <vscale x 1 x double>* %ptr, <vscale x 1 x i1> %allones, i32 %evl)
  ret <vscale x 1 x double> %dst
}

define void @test_store_vp_reverse_nxv1f64_general_mask(<vscale x 1 x double> %src, <vscale x 1 x i1> %mask, <vscale x 1 x double> *%ptr, i32 zeroext %evl) {
; CHECK-LABEL: test_store_vp_reverse_nxv1f64_general_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-NEXT:    vid.v v9, v0.t
; CHECK-NEXT:    addi a1, a1, -1
; CHECK-NEXT:    vrsub.vx v9, v9, a1, v0.t
; CHECK-NEXT:    vrgather.vv v10, v8, v9, v0.t
; CHECK-NEXT:    vse64.v v10, (a0), v0.t
; CHECK-NEXT:    ret
  %dst = call <vscale x 1 x double> @llvm.experimental.vp.reverse.nxv1f64(<vscale x 1 x double> %src, <vscale x 1 x i1> %mask, i32 %evl)
  call void @llvm.vp.store.nxv1f64(<vscale x 1 x double> %dst, <vscale x 1 x double>* %ptr, <vscale x 1 x i1> %mask, i32 %evl)
  ret void
}

define void @test_store_vp_reverse_nxv1f64_reversed_mask(<vscale x 1 x double> %src, <vscale x 1 x i1> %mask, <vscale x 1 x double> *%ptr, i32 zeroext %evl) {
; CHECK-LABEL: test_store_vp_reverse_nxv1f64_reversed_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    slli a2, a1, 3
; CHECK-NEXT:    add a0, a2, a0
; CHECK-NEXT:    addi a0, a0, -8
; CHECK-NEXT:    li a2, -8
; CHECK-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-NEXT:    vsse64.v v8, (a0), a2, v0.t
; CHECK-NEXT:    ret
  %head = insertelement <vscale x 1 x i1> undef, i1 1, i32 0
  %allones = shufflevector <vscale x 1 x i1> %head, <vscale x 1 x i1> undef, <vscale x 1 x i32> zeroinitializer

  %rev.mask = call <vscale x 1 x i1> @llvm.experimental.vp.reverse.nxv1i1(<vscale x 1 x i1> %mask, <vscale x 1 x i1> %allones, i32 %evl)
  %dst = call <vscale x 1 x double> @llvm.experimental.vp.reverse.nxv1f64(<vscale x 1 x double> %src, <vscale x 1 x i1> %allones, i32 %evl)
  call void @llvm.vp.store.nxv1f64(<vscale x 1 x double> %dst, <vscale x 1 x double>* %ptr, <vscale x 1 x i1> %rev.mask, i32 %evl)
  ret void
}

define void @test_store_vp_reverse_nxv1f64_inconsistent_mask(<vscale x 1 x double> %src, <vscale x 1 x i1> %mask, <vscale x 1 x i1> %mask2, <vscale x 1 x double> *%ptr, i32 zeroext %evl) {
; CHECK-LABEL: test_store_vp_reverse_nxv1f64_inconsistent_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmv1r.v v10, v0
; CHECK-NEXT:    vsetvli zero, a1, e64, m1, ta, ma
; CHECK-NEXT:    vmv1r.v v0, v9
; CHECK-NEXT:    vid.v v11, v0.t
; CHECK-NEXT:    addi a1, a1, -1
; CHECK-NEXT:    vrsub.vx v11, v11, a1, v0.t
; CHECK-NEXT:    vmv.v.i v12, 0
; CHECK-NEXT:    vmv1r.v v0, v10
; CHECK-NEXT:    vmerge.vim v10, v12, 1, v0
; CHECK-NEXT:    vmv1r.v v0, v9
; CHECK-NEXT:    vrgather.vv v12, v10, v11, v0.t
; CHECK-NEXT:    vmsne.vi v0, v12, 0, v0.t
; CHECK-NEXT:    vid.v v9
; CHECK-NEXT:    vrsub.vx v9, v9, a1
; CHECK-NEXT:    vrgather.vv v10, v8, v9
; CHECK-NEXT:    vse64.v v10, (a0), v0.t
; CHECK-NEXT:    ret
  %head = insertelement <vscale x 1 x i1> undef, i1 1, i32 0
  %allones = shufflevector <vscale x 1 x i1> %head, <vscale x 1 x i1> undef, <vscale x 1 x i32> zeroinitializer

  %rev.mask = call <vscale x 1 x i1> @llvm.experimental.vp.reverse.nxv1i1(<vscale x 1 x i1> %mask, <vscale x 1 x i1> %mask2, i32 %evl)
  %dst = call <vscale x 1 x double> @llvm.experimental.vp.reverse.nxv1f64(<vscale x 1 x double> %src, <vscale x 1 x i1> %allones, i32 %evl)
  call void @llvm.vp.store.nxv1f64(<vscale x 1 x double> %dst, <vscale x 1 x double>* %ptr, <vscale x 1 x i1> %rev.mask, i32 %evl)
  ret void
}

declare void @llvm.vp.store.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>*, <vscale x 1 x i1>, i32)
declare <vscale x 1 x double> @llvm.experimental.vp.reverse.nxv1f64(<vscale x 1 x double>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x i1> @llvm.experimental.vp.reverse.nxv1i1(<vscale x 1 x i1>, <vscale x 1 x i1>, i32)
