; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple riscv64 -target-abi lp64d \
; RUN:    -epi-pipeline -mattr +m,+a,+f,+d,+experimental-v < %s | FileCheck %s

define void @n1fv_32(double* %ri, double* %ii, double* %ro, double* %io, i64 %is, i64 %os, i64 %v, i64 %ivs, i64 %ovs) nounwind {
; CHECK-LABEL: n1fv_32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addi sp, sp, -64
; CHECK-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; CHECK-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; CHECK-NEXT:    sd s1, 40(sp) # 8-byte Folded Spill
; CHECK-NEXT:    sd s2, 32(sp) # 8-byte Folded Spill
; CHECK-NEXT:    sd s3, 24(sp) # 8-byte Folded Spill
; CHECK-NEXT:    sd s4, 16(sp) # 8-byte Folded Spill
; CHECK-NEXT:    csrr a1, vlenb
; CHECK-NEXT:    slli a3, a1, 2
; CHECK-NEXT:    add a1, a3, a1
; CHECK-NEXT:    sub sp, sp, a1
; CHECK-NEXT:    mv s2, a5
; CHECK-NEXT:    mv s0, a4
; CHECK-NEXT:    mv s3, a2
; CHECK-NEXT:    mv s1, a0
; CHECK-NEXT:    addi s4, zero, 8
; CHECK-NEXT:    vsetvli zero, s4, e64, m1, ta, mu
; CHECK-NEXT:    vid.v v10
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs1r.v v10, (a0) # Unknown-size Folded Spill
; CHECK-NEXT:    vsrl.vi v8, v10, 1
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a1, a0, 1
; CHECK-NEXT:    add a0, a1, a0
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; CHECK-NEXT:    vmul.vx v9, v8, a0
; CHECK-NEXT:    vand.vi v8, v10, 1
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 2
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; CHECK-NEXT:    vmul.vv v10, v8, v8
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vs1r.v v10, (a0) # Unknown-size Folded Spill
; CHECK-NEXT:    vadd.vv v9, v9, v10
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 1
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs1r.v v9, (a0) # Unknown-size Folded Spill
; CHECK-NEXT:    call llvm.epi.mask.cast.nxv1i1.nxv1i64@plt
; CHECK-NEXT:    vsetvli zero, s4, e64, m1, ta, mu
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; CHECK-NEXT:    vxor.vi v9, v8, 1
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a1, a0, 1
; CHECK-NEXT:    add a0, a1, a0
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; CHECK-NEXT:    vmul.vv v8, v8, v8
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vl1r.v v10, (a0) # Unknown-size Folded Reload
; CHECK-NEXT:    vadd.vv v8, v8, v10
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 1
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl1r.v v28, (a0) # Unknown-size Folded Reload
; CHECK-NEXT:    vluxei64.v v19, (a0), v28
; CHECK-NEXT:    slli a0, s0, 7
; CHECK-NEXT:    add a0, s1, a0
; CHECK-NEXT:    vluxei64.v v10, (a0), v28
; CHECK-NEXT:    addi a0, zero, 192
; CHECK-NEXT:    mul a0, s0, a0
; CHECK-NEXT:    add a0, s1, a0
; CHECK-NEXT:    vluxei64.v v11, (a0), v28
; CHECK-NEXT:    slli a1, s0, 4
; CHECK-NEXT:    vfsub.vv v12, v19, v10
; CHECK-NEXT:    vfadd.vv v13, v19, v10
; CHECK-NEXT:    vfsub.vv v10, v8, v11
; CHECK-NEXT:    slli a0, s0, 5
; CHECK-NEXT:    add a0, s1, a0
; CHECK-NEXT:    vluxei64.v v14, (a0), v28
; CHECK-NEXT:    addi a0, zero, 160
; CHECK-NEXT:    mul a0, s0, a0
; CHECK-NEXT:    add a0, s1, a0
; CHECK-NEXT:    vluxei64.v v15, (a0), v28
; CHECK-NEXT:    addi a0, zero, 224
; CHECK-NEXT:    mul a0, s0, a0
; CHECK-NEXT:    add a0, s1, a0
; CHECK-NEXT:    vluxei64.v v16, (a0), v28
; CHECK-NEXT:    addi a0, zero, 96
; CHECK-NEXT:    mul a0, s0, a0
; CHECK-NEXT:    add a0, s1, a0
; CHECK-NEXT:    vluxei64.v v17, (a0), v28
; CHECK-NEXT:    vfadd.vv v11, v8, v11
; CHECK-NEXT:    vfsub.vv v18, v14, v15
; CHECK-NEXT:    vfadd.vv v14, v14, v15
; CHECK-NEXT:    vfsub.vv v15, v16, v17
; CHECK-NEXT:    vfadd.vv v20, v16, v17
; CHECK-NEXT:    vfadd.vv v16, v13, v11
; CHECK-NEXT:    vfadd.vv v17, v14, v20
; CHECK-NEXT:    vfsub.vv v13, v20, v14
; CHECK-NEXT:    vfsub.vv v11, v15, v18
; CHECK-NEXT:    vfmacc.vv v10, v8, v11
; CHECK-NEXT:    vfmsac.vv v12, v8, v8
; CHECK-NEXT:    vfneg.v v11, v12
; CHECK-NEXT:    vfadd.vv v14, v8, v19
; CHECK-NEXT:    vfadd.vv v15, v19, v19
; CHECK-NEXT:    vfadd.vv v18, v8, v8
; CHECK-NEXT:    vfmsac.vv v12, v8, v8
; CHECK-NEXT:    vfneg.v v12, v12
; CHECK-NEXT:    vfadd.vv v20, v15, v18
; CHECK-NEXT:    lui a0, %hi(.LCPI0_0)
; CHECK-NEXT:    fld ft0, %lo(.LCPI0_0)(a0)
; CHECK-NEXT:    vfsub.vv v23, v8, v20
; CHECK-NEXT:    vfsub.vv v14, v8, v14
; CHECK-NEXT:    vfsub.vv v15, v18, v15
; CHECK-NEXT:    vmv1r.v v18, v14
; CHECK-NEXT:    vfmsac.vf v18, ft0, v15
; CHECK-NEXT:    vfneg.v v18, v18
; CHECK-NEXT:    addi a0, zero, 136
; CHECK-NEXT:    mul a0, s0, a0
; CHECK-NEXT:    add a0, s1, a0
; CHECK-NEXT:    vluxei64.v v20, (a0), v28
; CHECK-NEXT:    vluxei64.v v21, (s1), v28
; CHECK-NEXT:    addi a0, zero, 104
; CHECK-NEXT:    mul a2, s0, a0
; CHECK-NEXT:    add a2, s1, a2
; CHECK-NEXT:    vluxei64.v v22, (a2), v28
; CHECK-NEXT:    vfmacc.vf v15, ft0, v14
; CHECK-NEXT:    vfsub.vv v14, v19, v20
; CHECK-NEXT:    vfadd.vv v20, v19, v20
; CHECK-NEXT:    vfsub.vv v24, v21, v22
; CHECK-NEXT:    vfadd.vv v21, v21, v22
; CHECK-NEXT:    vfadd.vv v22, v8, v24
; CHECK-NEXT:    vfmsac.vv v14, v8, v22
; CHECK-NEXT:    vfneg.v v14, v14
; CHECK-NEXT:    vfsub.vv v22, v20, v8
; CHECK-NEXT:    vfsub.vv v20, v8, v21
; CHECK-NEXT:    vmv1r.v v21, v22
; CHECK-NEXT:    vfmsac.vf v21, ft0, v20
; CHECK-NEXT:    vfneg.v v21, v21
; CHECK-NEXT:    add a1, s1, a1
; CHECK-NEXT:    vluxei64.v v24, (a1), v28
; CHECK-NEXT:    addi a1, zero, 48
; CHECK-NEXT:    mul a1, s0, a1
; CHECK-NEXT:    add a1, s1, a1
; CHECK-NEXT:    vluxei64.v v25, (a1), v28
; CHECK-NEXT:    vfmacc.vf v20, ft0, v22
; CHECK-NEXT:    vfsub.vv v22, v24, v19
; CHECK-NEXT:    vfadd.vv v24, v24, v19
; CHECK-NEXT:    vfsub.vv v26, v8, v25
; CHECK-NEXT:    addi a1, zero, 80
; CHECK-NEXT:    mul a1, s0, a1
; CHECK-NEXT:    add a1, s1, a1
; CHECK-NEXT:    vluxei64.v v27, (a1), v28
; CHECK-NEXT:    addi a1, zero, 240
; CHECK-NEXT:    mul a1, s0, a1
; CHECK-NEXT:    add a1, s1, a1
; CHECK-NEXT:    vluxei64.v v28, (a1), v28
; CHECK-NEXT:    vfadd.vv v25, v25, v8
; CHECK-NEXT:    vfsub.vv v29, v27, v8
; CHECK-NEXT:    vfadd.vv v27, v27, v8
; CHECK-NEXT:    vfsub.vv v30, v28, v19
; CHECK-NEXT:    vfadd.vv v19, v28, v19
; CHECK-NEXT:    vfadd.vv v25, v19, v25
; CHECK-NEXT:    vfadd.vv v28, v24, v27
; CHECK-NEXT:    vmv1r.v v19, v22
; CHECK-NEXT:    vfmsac.vf v19, ft0, v29
; CHECK-NEXT:    vfneg.v v19, v19
; CHECK-NEXT:    vmv1r.v v31, v30
; CHECK-NEXT:    vfmsac.vf v31, ft0, v26
; CHECK-NEXT:    vfneg.v v31, v31
; CHECK-NEXT:    vfsub.vv v19, v31, v19
; CHECK-NEXT:    vfmacc.vf v29, ft0, v22
; CHECK-NEXT:    vfmacc.vf v26, ft0, v30
; CHECK-NEXT:    vfadd.vv v22, v29, v26
; CHECK-NEXT:    vfsub.vv v24, v24, v27
; CHECK-NEXT:    vfsub.vv v26, v16, v17
; CHECK-NEXT:    vfsub.vv v24, v8, v24
; CHECK-NEXT:    vmv1r.v v27, v26
; CHECK-NEXT:    vfmsac.vv v27, v8, v8
; CHECK-NEXT:    vfneg.v v27, v27
; CHECK-NEXT:    vfmacc.vv v26, v8, v8
; CHECK-NEXT:    vfsub.vv v29, v25, v28
; CHECK-NEXT:    vfsub.vv v23, v23, v8
; CHECK-NEXT:    vmv1r.v v30, v29
; CHECK-NEXT:    vfmsac.vv v30, v8, v23
; CHECK-NEXT:    vfneg.v v30, v30
; CHECK-NEXT:    vfmacc.vv v29, v8, v23
; CHECK-NEXT:    vfneg.v v30, v30, v0.t
; CHECK-NEXT:    vrgather.vv v23, v30, v9
; CHECK-NEXT:    vfsub.vv v30, v27, v23
; CHECK-NEXT:    csrr a1, vlenb
; CHECK-NEXT:    slli a1, a1, 2
; CHECK-NEXT:    add a1, sp, a1
; CHECK-NEXT:    addi a1, a1, 16
; CHECK-NEXT:    vl1r.v v1, (a1) # Unknown-size Folded Reload
; CHECK-NEXT:    vrgather.vv v31, v30, v1
; CHECK-NEXT:    vsuxei64.v v31, (a0), v8
; CHECK-NEXT:    slli a1, s2, 5
; CHECK-NEXT:    add a1, s3, a1
; CHECK-NEXT:    vfneg.v v29, v29, v0.t
; CHECK-NEXT:    vrgather.vv v30, v29, v9
; CHECK-NEXT:    vfadd.vv v29, v26, v30
; CHECK-NEXT:    vrgather.vv v31, v29, v1
; CHECK-NEXT:    vsuxei64.v v31, (a1), v8
; CHECK-NEXT:    vfadd.vv v23, v27, v23
; CHECK-NEXT:    vrgather.vv v27, v23, v1
; CHECK-NEXT:    vsuxei64.v v27, (a0), v8
; CHECK-NEXT:    vfsub.vv v23, v26, v30
; CHECK-NEXT:    vrgather.vv v26, v23, v1
; CHECK-NEXT:    vsuxei64.v v26, (a0), v8
; CHECK-NEXT:    vfadd.vv v16, v16, v17
; CHECK-NEXT:    vfadd.vv v17, v28, v25
; CHECK-NEXT:    vfadd.vv v16, v16, v17
; CHECK-NEXT:    vrgather.vv v17, v8, v1
; CHECK-NEXT:    vsuxei64.v v8, (a0), v8
; CHECK-NEXT:    vfadd.vv v16, v16, v8
; CHECK-NEXT:    vrgather.vv v23, v16, v1
; CHECK-NEXT:    vsuxei64.v v23, (a0), v8
; CHECK-NEXT:    vsuxei64.v v17, (a0), v8
; CHECK-NEXT:    vsuxei64.v v8, (a0), v8
; CHECK-NEXT:    addi a1, zero, 208
; CHECK-NEXT:    mul a1, s2, a1
; CHECK-NEXT:    lui a2, %hi(.LCPI0_1)
; CHECK-NEXT:    fld ft0, %lo(.LCPI0_1)(a2)
; CHECK-NEXT:    add a1, s3, a1
; CHECK-NEXT:    vsuxei64.v v17, (a1), v8
; CHECK-NEXT:    vfadd.vv v16, v21, v18
; CHECK-NEXT:    vfmsac.vf v16, ft0, v16
; CHECK-NEXT:    vfneg.v v16, v16
; CHECK-NEXT:    vfmacc.vv v13, v8, v24
; CHECK-NEXT:    vfsub.vv v15, v15, v20
; CHECK-NEXT:    vfmacc.vf v13, ft0, v15
; CHECK-NEXT:    addi a1, zero, 112
; CHECK-NEXT:    mul a1, s2, a1
; CHECK-NEXT:    add a1, s3, a1
; CHECK-NEXT:    vrgather.vv v15, v8, v9
; CHECK-NEXT:    vfsub.vv v15, v16, v15
; CHECK-NEXT:    vrgather.vv v16, v15, v1
; CHECK-NEXT:    vsuxei64.v v16, (a1), v8
; CHECK-NEXT:    vfneg.v v13, v13, v0.t
; CHECK-NEXT:    vrgather.vv v15, v13, v9
; CHECK-NEXT:    vfadd.vv v13, v8, v15
; CHECK-NEXT:    vrgather.vv v15, v13, v1
; CHECK-NEXT:    vfmacc.vf v11, ft0, v22
; CHECK-NEXT:    vfmacc.vf v10, ft0, v19
; CHECK-NEXT:    lui a1, %hi(.LCPI0_2)
; CHECK-NEXT:    fld ft0, %lo(.LCPI0_2)(a1)
; CHECK-NEXT:    vsuxei64.v v15, (a0), v8
; CHECK-NEXT:    vsuxei64.v v8, (a0), v8
; CHECK-NEXT:    vfneg.v v13, v8
; CHECK-NEXT:    vfmsac.vf v12, ft0, v8
; CHECK-NEXT:    vfmacc.vf v14, ft0, v8
; CHECK-NEXT:    lui a1, %hi(.LCPI0_3)
; CHECK-NEXT:    fld ft0, %lo(.LCPI0_3)(a1)
; CHECK-NEXT:    vfneg.v v12, v12
; CHECK-NEXT:    vfsub.vv v12, v13, v12
; CHECK-NEXT:    vfadd.vv v13, v14, v8
; CHECK-NEXT:    vfmsac.vf v11, ft0, v13
; CHECK-NEXT:    vfneg.v v11, v11
; CHECK-NEXT:    vfmsac.vf v10, ft0, v12
; CHECK-NEXT:    vfneg.v v10, v10
; CHECK-NEXT:    mul a0, s2, a0
; CHECK-NEXT:    add a0, s3, a0
; CHECK-NEXT:    vfneg.v v10, v10, v0.t
; CHECK-NEXT:    vrgather.vv v12, v10, v9
; CHECK-NEXT:    vfsub.vv v9, v11, v12
; CHECK-NEXT:    vrgather.vv v10, v9, v1
; CHECK-NEXT:    vsuxei64.v v10, (a0), v8
entry:
  %0 = tail call <vscale x 1 x double> @llvm.epi.vfmv.v.f.nxv1f64.f64(double 0x3FEA9B66290EA1A3, i64 8)
  %1 = tail call <vscale x 1 x double> @llvm.epi.vfmv.v.f.nxv1f64.f64(double 0x3FE561B82AB7F990, i64 8)
  %2 = tail call <vscale x 1 x double> @llvm.epi.vfmv.v.f.nxv1f64.f64(double 0x3FED906BCF328D46, i64 8)
  %3 = tail call <vscale x 1 x double> @llvm.epi.vfmv.v.f.nxv1f64.f64(double 0x3FDA827999FCEF32, i64 8)
  %cmp1384 = icmp sgt i64 %v, 0
  %4 = tail call <vscale x 1 x i64> @llvm.epi.vid.nxv1i64(i64 8) #4
  %5 = tail call <vscale x 1 x i64> @llvm.epi.vmv.v.x.nxv1i64.i64(i64 1, i64 8) #4
  %6 = tail call <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64.nxv1i64(<vscale x 1 x i64> %4, <vscale x 1 x i64> %5, i64 8) #4
  %7 = tail call <vscale x 1 x i64> @llvm.epi.vmv.v.x.nxv1i64.i64(i64 undef, i64 8) #4
  %8 = tail call <vscale x 1 x i64> @llvm.epi.vmul.nxv1i64.nxv1i64(<vscale x 1 x i64> %6, <vscale x 1 x i64> %7, i64 8) #4
  %9 = tail call <vscale x 1 x i64> @llvm.epi.vand.nxv1i64.nxv1i64(<vscale x 1 x i64> %4, <vscale x 1 x i64> %5, i64 8) #4
  %10 = tail call <vscale x 1 x i64> @llvm.epi.vmul.nxv1i64.nxv1i64(<vscale x 1 x i64> %9, <vscale x 1 x i64> undef, i64 8) #4
  %11 = tail call <vscale x 1 x i64> @llvm.epi.vadd.nxv1i64.nxv1i64(<vscale x 1 x i64> %8, <vscale x 1 x i64> %10, i64 8) #4
  %12 = tail call <vscale x 1 x i1> @llvm.epi.mask.cast.nxv1i1.nxv1i64(<vscale x 1 x i64> %9) #4
  %13 = tail call <vscale x 1 x i64> @llvm.epi.vxor.nxv1i64.nxv1i64(<vscale x 1 x i64> %4, <vscale x 1 x i64> %5, i64 8) #4
  %14 = tail call <vscale x 1 x i64> @llvm.epi.vmul.nxv1i64.nxv1i64(<vscale x 1 x i64> %6, <vscale x 1 x i64> undef, i64 8) #4
  %15 = tail call <vscale x 1 x i64> @llvm.epi.vadd.nxv1i64.nxv1i64(<vscale x 1 x i64> %14, <vscale x 1 x i64> %10, i64 8) #4
  %mul283 = shl nsw i64 %ovs, 2
  %16 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* undef, <vscale x 1 x i64> %11, i64 8) #4
  %mul = shl nsw i64 %is, 4
  %arrayidx2 = getelementptr inbounds double, double* %ri, i64 %mul
  %17 = bitcast double* %arrayidx2 to <vscale x 1 x double>*
  %18 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %17, <vscale x 1 x i64> %11, i64 8) #4
  %19 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %16, <vscale x 1 x double> %18, i64 8)
  %20 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %16, <vscale x 1 x double> %18, i64 8)
  %mul5 = shl nsw i64 %is, 3
  %arrayidx6 = getelementptr inbounds double, double* %ri, i64 %mul5
  %21 = bitcast double* %arrayidx6 to <vscale x 1 x double>*
  %mul9 = mul nsw i64 %is, 24
  %arrayidx10 = getelementptr inbounds double, double* %ri, i64 %mul9
  %22 = bitcast double* %arrayidx10 to <vscale x 1 x double>*
  %23 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %22, <vscale x 1 x i64> %11, i64 8) #4
  %24 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %23, i64 8)
  %25 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %23, i64 8)
  %mul13 = shl nsw i64 %is, 2
  %arrayidx14 = getelementptr inbounds double, double* %ri, i64 %mul13
  %26 = bitcast double* %arrayidx14 to <vscale x 1 x double>*
  %27 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %26, <vscale x 1 x i64> %11, i64 8) #4
  %mul17 = mul nsw i64 %is, 20
  %arrayidx18 = getelementptr inbounds double, double* %ri, i64 %mul17
  %28 = bitcast double* %arrayidx18 to <vscale x 1 x double>*
  %29 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %28, <vscale x 1 x i64> %11, i64 8) #4
  %30 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %27, <vscale x 1 x double> %29, i64 8)
  %31 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %27, <vscale x 1 x double> %29, i64 8)
  %mul21 = mul nsw i64 %is, 28
  %arrayidx22 = getelementptr inbounds double, double* %ri, i64 %mul21
  %32 = bitcast double* %arrayidx22 to <vscale x 1 x double>*
  %33 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %32, <vscale x 1 x i64> %11, i64 8) #4
  %mul25 = mul nsw i64 %is, 12
  %arrayidx26 = getelementptr inbounds double, double* %ri, i64 %mul25
  %34 = bitcast double* %arrayidx26 to <vscale x 1 x double>*
  %35 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %34, <vscale x 1 x i64> %11, i64 8) #4
  %36 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %33, <vscale x 1 x double> %35, i64 8)
  %37 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %33, <vscale x 1 x double> %35, i64 8)
  %38 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %20, <vscale x 1 x double> %25, i64 8)
  %39 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %31, <vscale x 1 x double> %37, i64 8)
  %40 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %37, <vscale x 1 x double> %31, i64 8)
  %41 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %36, <vscale x 1 x double> %30, i64 8)
  %42 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %24, <vscale x 1 x double> undef, <vscale x 1 x double> %41, i64 8)
  %43 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %24, <vscale x 1 x double> undef, <vscale x 1 x double> %41, i64 8)
  %44 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %19, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 8)
  %45 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %19, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 8)
  %46 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %45, <vscale x 1 x double> %45, i64 8)
  %47 = bitcast double* undef to <vscale x 1 x double>*
  %48 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %47, <vscale x 1 x i64> %11, i64 8) #4
  %arrayidx45 = getelementptr inbounds double, double* %ri, i64 undef
  %49 = bitcast double* %arrayidx45 to <vscale x 1 x double>*
  %50 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %48, i64 8)
  %51 = bitcast double* undef to <vscale x 1 x double>*
  %52 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %51, <vscale x 1 x i64> %11, i64 8) #4
  %mul54 = mul nsw i64 %is, 19
  %53 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* undef, <vscale x 1 x i64> %11, i64 8) #4
  %54 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %52, <vscale x 1 x double> %53, i64 8)
  %55 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %52, <vscale x 1 x double> %53, i64 8)
  %mul64 = mul nsw i64 %is, 11
  %56 = bitcast double* undef to <vscale x 1 x double>*
  %57 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 8)
  %58 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %54, <vscale x 1 x double> undef, i64 8)
  %59 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> %58, i64 8)
  %60 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> %58, i64 8)
  %61 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 8)
  %62 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %61, <vscale x 1 x double> %61, i64 8)
  %63 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %55, <vscale x 1 x double> %57, i64 8)
  %64 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %63, i64 8)
  %65 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %50, i64 8)
  %66 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %57, <vscale x 1 x double> %55, i64 8)
  %67 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %65, <vscale x 1 x double> %3, <vscale x 1 x double> %66, i64 8)
  %68 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %67, <vscale x 1 x double> %67, i64 8)
  %69 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %66, <vscale x 1 x double> %3, <vscale x 1 x double> %65, i64 8)
  %70 = bitcast double* undef to <vscale x 1 x double>*
  %71 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %70, <vscale x 1 x i64> %11, i64 8) #4
  %mul74 = mul nsw i64 %is, 17
  %arrayidx75 = getelementptr inbounds double, double* %ri, i64 %mul74
  %72 = bitcast double* %arrayidx75 to <vscale x 1 x double>*
  %73 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %72, <vscale x 1 x i64> %11, i64 8) #4
  %74 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %71, <vscale x 1 x double> %73, i64 8)
  %75 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %71, <vscale x 1 x double> %73, i64 8)
  %mul79 = mul nsw i64 %is, 9
  %arrayidx85 = getelementptr inbounds double, double* %ri, i64 undef
  %76 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* undef, <vscale x 1 x i64> %11, i64 8) #4
  %77 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* undef, <vscale x 1 x i64> %11, i64 8) #4
  %mul94 = mul nsw i64 %is, 21
  %78 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* undef, <vscale x 1 x i64> %11, i64 8) #4
  %arrayidx100 = getelementptr inbounds double, double* %ri, i64 undef
  %79 = bitcast double* %arrayidx100 to <vscale x 1 x double>*
  %80 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %79, <vscale x 1 x i64> %11, i64 8) #4
  %mul104 = mul nsw i64 %is, 13
  %arrayidx105 = getelementptr inbounds double, double* %ri, i64 %mul104
  %81 = bitcast double* %arrayidx105 to <vscale x 1 x double>*
  %82 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %81, <vscale x 1 x i64> %11, i64 8) #4
  %83 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %80, <vscale x 1 x double> %82, i64 8)
  %84 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %80, <vscale x 1 x double> %82, i64 8)
  %85 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %83, i64 8)
  %86 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %74, <vscale x 1 x double> undef, <vscale x 1 x double> %85, i64 8)
  %87 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %74, <vscale x 1 x double> undef, <vscale x 1 x double> %85, i64 8)
  %88 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %87, <vscale x 1 x double> %87, i64 8)
  %89 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %84, i64 8)
  %90 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %75, <vscale x 1 x double> undef, i64 8)
  %91 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %84, i64 8)
  %92 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %90, <vscale x 1 x double> %3, <vscale x 1 x double> %91, i64 8)
  %93 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %92, <vscale x 1 x double> %92, i64 8)
  %94 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %91, <vscale x 1 x double> %3, <vscale x 1 x double> %90, i64 8)
  %mul109 = shl nsw i64 %is, 1
  %arrayidx110 = getelementptr inbounds double, double* %ri, i64 %mul109
  %95 = bitcast double* %arrayidx110 to <vscale x 1 x double>*
  %96 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %95, <vscale x 1 x i64> %11, i64 8) #4
  %97 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* undef, <vscale x 1 x i64> %11, i64 8) #4
  %98 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %96, <vscale x 1 x double> %97, i64 8)
  %99 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %96, <vscale x 1 x double> %97, i64 8)
  %mul121 = mul nsw i64 %is, 6
  %arrayidx122 = getelementptr inbounds double, double* %ri, i64 %mul121
  %100 = bitcast double* %arrayidx122 to <vscale x 1 x double>*
  %101 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %100, <vscale x 1 x i64> %11, i64 8) #4
  %102 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %101, i64 8)
  %103 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %101, <vscale x 1 x double> undef, i64 8)
  %mul125 = mul nsw i64 %is, 10
  %arrayidx126 = getelementptr inbounds double, double* %ri, i64 %mul125
  %104 = bitcast double* %arrayidx126 to <vscale x 1 x double>*
  %105 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %104, <vscale x 1 x i64> %11, i64 8) #4
  %106 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %105, <vscale x 1 x double> undef, i64 8)
  %107 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %105, <vscale x 1 x double> undef, i64 8)
  %mul133 = mul nsw i64 %is, 30
  %arrayidx134 = getelementptr inbounds double, double* %ri, i64 %mul133
  %108 = bitcast double* %arrayidx134 to <vscale x 1 x double>*
  %109 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %108, <vscale x 1 x i64> %11, i64 8) #4
  %mul137 = mul nsw i64 %is, 14
  %110 = bitcast double* undef to <vscale x 1 x double>*
  %111 = tail call <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* %110, <vscale x 1 x i64> %11, i64 8) #4
  %112 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %109, <vscale x 1 x double> %111, i64 8)
  %113 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %109, <vscale x 1 x double> %111, i64 8)
  %114 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %113, <vscale x 1 x double> %103, i64 8)
  %115 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %99, <vscale x 1 x double> %107, i64 8)
  %116 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %98, <vscale x 1 x double> %3, <vscale x 1 x double> %106, i64 8)
  %117 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %116, <vscale x 1 x double> %116, i64 8)
  %118 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %112, <vscale x 1 x double> %3, <vscale x 1 x double> %102, i64 8)
  %119 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %118, <vscale x 1 x double> %118, i64 8)
  %120 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %119, <vscale x 1 x double> %117, i64 8)
  %121 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %106, <vscale x 1 x double> %3, <vscale x 1 x double> %98, i64 8)
  %122 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %102, <vscale x 1 x double> %3, <vscale x 1 x double> %112, i64 8)
  %123 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %121, <vscale x 1 x double> %122, i64 8)
  %124 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %99, <vscale x 1 x double> %107, i64 8)
  %125 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %124, i64 8)
  %126 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %38, <vscale x 1 x double> %39, i64 8)
  %127 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %126, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 8)
  %128 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %127, <vscale x 1 x double> %127, i64 8)
  %129 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %126, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 8)
  %130 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %114, <vscale x 1 x double> %115, i64 8)
  %131 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %64, <vscale x 1 x double> undef, i64 8)
  %132 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %130, <vscale x 1 x double> undef, <vscale x 1 x double> %131, i64 8)
  %133 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %132, <vscale x 1 x double> %132, i64 8)
  %134 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %130, <vscale x 1 x double> undef, <vscale x 1 x double> %131, i64 8)
  %135 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %133, <vscale x 1 x double> %133, <vscale x 1 x double> %133, <vscale x 1 x i1> %12, i64 8) #4
  %136 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %135, <vscale x 1 x i64> %13, i64 8) #4
  %137 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %128, <vscale x 1 x double> %136, i64 8)
  %138 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %137, <vscale x 1 x i64> %9, i64 8) #4
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> %138, <vscale x 1 x double>* undef, <vscale x 1 x i64> %15, i64 8) #4
  %mul145 = shl nsw i64 %os, 2
  %arrayidx146 = getelementptr inbounds double, double* %ro, i64 %mul145
  %139 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %134, <vscale x 1 x double> %134, <vscale x 1 x double> %134, <vscale x 1 x i1> %12, i64 8) #4
  %140 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %139, <vscale x 1 x i64> %13, i64 8) #4
  %141 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %129, <vscale x 1 x double> %140, i64 8)
  %142 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %141, <vscale x 1 x i64> %9, i64 8) #4
  %143 = bitcast double* %arrayidx146 to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> %142, <vscale x 1 x double>* %143, <vscale x 1 x i64> %15, i64 8) #4
  %144 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %128, <vscale x 1 x double> %136, i64 8)
  %145 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %144, <vscale x 1 x i64> %9, i64 8) #4
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> %145, <vscale x 1 x double>* undef, <vscale x 1 x i64> %15, i64 8) #4
  %146 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %129, <vscale x 1 x double> %140, i64 8)
  %147 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %146, <vscale x 1 x i64> %9, i64 8) #4
  %148 = bitcast double* undef to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> %147, <vscale x 1 x double>* %148, <vscale x 1 x i64> %15, i64 8) #4
  %149 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %38, <vscale x 1 x double> %39, i64 8)
  %150 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %115, <vscale x 1 x double> %114, i64 8)
  %151 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %149, <vscale x 1 x double> %150, i64 8)
  %152 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %149, <vscale x 1 x double> %150, i64 8)
  %153 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %63, i64 8)
  %154 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> undef, <vscale x 1 x i64> %9, i64 8) #4
  %155 = bitcast double* undef to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> undef, <vscale x 1 x double>* %155, <vscale x 1 x i64> %15, i64 8) #4
  %156 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %151, <vscale x 1 x double> undef, i64 8)
  %157 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %156, <vscale x 1 x i64> %9, i64 8) #4
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> %157, <vscale x 1 x double>* undef, <vscale x 1 x i64> %15, i64 8) #4
  %158 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> undef, <vscale x 1 x i64> %9, i64 8) #4
  %mul1661318.pn = mul nsw i64 %os, 24
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> %158, <vscale x 1 x double>* undef, <vscale x 1 x i64> %15, i64 8) #4
  %159 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 8)
  %160 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %159, <vscale x 1 x double> %159, i64 8)
  %161 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %160, <vscale x 1 x double> %2, <vscale x 1 x double> undef, i64 8)
  %162 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %160, <vscale x 1 x double> %2, <vscale x 1 x double> undef, i64 8)
  %163 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %40, <vscale x 1 x double> undef, <vscale x 1 x double> %125, i64 8)
  %164 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %68, <vscale x 1 x double> %93, i64 8)
  %165 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %2, <vscale x 1 x double> %164, i64 8)
  %166 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %2, <vscale x 1 x double> %164, i64 8)
  %167 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %166, <vscale x 1 x double> %166, i64 8)
  %mul170 = mul nsw i64 %os, 10
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> undef, <vscale x 1 x double>* undef, <vscale x 1 x i64> %15, i64 8) #4
  %mul174 = mul nsw i64 %os, 26
  %arrayidx175 = getelementptr inbounds double, double* %ro, i64 %mul174
  %168 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> undef, <vscale x 1 x i64> %9, i64 8) #4
  %169 = bitcast double* %arrayidx175 to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> %168, <vscale x 1 x double>* %169, <vscale x 1 x i64> %15, i64 8) #4
  %mul1781323 = mul nsw i64 %os, 22
  %arrayidx1791324 = getelementptr inbounds double, double* %ro, i64 %mul1781323
  %170 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 8)
  %171 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %93, <vscale x 1 x double> %68, i64 8)
  %172 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %2, <vscale x 1 x double> %171, i64 8)
  %173 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %172, <vscale x 1 x double> %172, i64 8)
  %174 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %40, <vscale x 1 x double> undef, <vscale x 1 x double> %125, i64 8)
  %175 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %69, <vscale x 1 x double> %94, i64 8)
  %176 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %174, <vscale x 1 x double> %2, <vscale x 1 x double> %175, i64 8)
  %177 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %174, <vscale x 1 x double> %2, <vscale x 1 x double> %175, i64 8)
  %mul186 = mul nsw i64 %os, 14
  %arrayidx187 = getelementptr inbounds double, double* %ro, i64 %mul186
  %178 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> undef, <vscale x 1 x i64> %13, i64 8) #4
  %179 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %173, <vscale x 1 x double> %178, i64 8)
  %180 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %179, <vscale x 1 x i64> %9, i64 8) #4
  %181 = bitcast double* %arrayidx187 to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> %180, <vscale x 1 x double>* %181, <vscale x 1 x i64> %15, i64 8) #4
  %182 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %177, <vscale x 1 x double> %177, <vscale x 1 x double> %177, <vscale x 1 x i1> %12, i64 8) #4
  %183 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %182, <vscale x 1 x i64> %13, i64 8) #4
  %184 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %183, i64 8)
  %185 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %184, <vscale x 1 x i64> %9, i64 8) #4
  %186 = bitcast double* undef to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> %185, <vscale x 1 x double>* %186, <vscale x 1 x i64> %15, i64 8) #4
  %187 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %173, <vscale x 1 x double> %178, i64 8)
  %188 = bitcast double* undef to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> undef, <vscale x 1 x double>* %188, <vscale x 1 x i64> %15, i64 8) #4
  %189 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> %183, i64 8)
  %190 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %189, <vscale x 1 x i64> %9, i64 8) #4
  %191 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %46, <vscale x 1 x double> %2, <vscale x 1 x double> %123, i64 8)
  %192 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %43, <vscale x 1 x double> %2, <vscale x 1 x double> %120, i64 8)
  %193 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 8)
  %194 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %46, <vscale x 1 x double> %2, <vscale x 1 x double> %123, i64 8)
  %195 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> undef, <vscale x 1 x double> undef, i64 8)
  %196 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %62, <vscale x 1 x double> %1, <vscale x 1 x double> undef, i64 8)
  %197 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %196, <vscale x 1 x double> %196, i64 8)
  %198 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %195, <vscale x 1 x double> %197, i64 8)
  %199 = tail call <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double> %88, <vscale x 1 x double> %1, <vscale x 1 x double> undef, i64 8)
  %200 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %199, <vscale x 1 x double> undef, i64 8)
  %201 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %191, <vscale x 1 x double> %0, <vscale x 1 x double> %200, i64 8)
  %202 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %201, <vscale x 1 x double> %201, i64 8)
  %203 = tail call <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double> %192, <vscale x 1 x double> %0, <vscale x 1 x double> %198, i64 8)
  %204 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double> %203, <vscale x 1 x double> %203, i64 8)
  %mul202 = mul nsw i64 %os, 13
  %arrayidx203 = getelementptr inbounds double, double* %ro, i64 %mul202
  %205 = tail call <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double> %204, <vscale x 1 x double> %204, <vscale x 1 x double> %204, <vscale x 1 x i1> %12, i64 8) #4
  %206 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %205, <vscale x 1 x i64> %13, i64 8) #4
  %207 = tail call <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double> %202, <vscale x 1 x double> %206, i64 8)
  %arrayidx206 = getelementptr inbounds double, double* %ro, i64 %os
  %208 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %207, <vscale x 1 x i64> %9, i64 8) #4
  %209 = bitcast double* %arrayidx203 to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double> %208, <vscale x 1 x double>* %209, <vscale x 1 x i64> %15, i64 8) #4
  %210 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %202, <vscale x 1 x double> %206, i64 8)
  %211 = tail call <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double> %210, <vscale x 1 x i64> %9, i64 8) #4
  %arrayidx2081347 = getelementptr inbounds double, double* %ro, i64 undef
  %212 = bitcast double* %arrayidx2081347 to <vscale x 1 x double>*
  unreachable
}

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfmv.v.f.nxv1f64.f64(double, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfsub.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfmsac.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfsgnjn.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfmacc.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x i64> @llvm.epi.vid.nxv1i64(i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x i64> @llvm.epi.vmv.v.x.nxv1i64.i64(i64, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x i64> @llvm.epi.vsrl.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x i64> @llvm.epi.vmul.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x i64> @llvm.epi.vand.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x i64> @llvm.epi.vadd.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, i64) #1

; Function Attrs: nounwind readonly
declare <vscale x 1 x double> @llvm.epi.vload.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>* nocapture, <vscale x 1 x i64>, i64) #2

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vrgather.nxv1f64.nxv1i64(<vscale x 1 x double>, <vscale x 1 x i64>, i64) #1

; Function Attrs: nounwind writeonly
declare void @llvm.epi.vstore.indexed.nxv1f64.nxv1i64(<vscale x 1 x double>, <vscale x 1 x double>* nocapture, <vscale x 1 x i64>, i64) #3

; Function Attrs: nounwind readnone
declare <vscale x 1 x i1> @llvm.epi.mask.cast.nxv1i1.nxv1i64(<vscale x 1 x i64>) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x double> @llvm.epi.vfsgnjn.mask.nxv1f64.nxv1f64.nxv1i1(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>, i64) #1

; Function Attrs: nounwind readnone
declare <vscale x 1 x i64> @llvm.epi.vxor.nxv1i64.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>, i64) #1
