; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+m,+f,+d,+a,+c,+experimental-v \
; RUN:    -verify-machineinstrs < %s -epi-pipeline | FileCheck %s

define i32 @red_nxv2i32(<vscale x 2 x i32> %a) nounwind {
; CHECK-LABEL: red_nxv2i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a0, zero, e32,m1,ta,mu
; CHECK-NEXT:    vmv.v.i v25, 0
; CHECK-NEXT:    vredsum.vs v26, v8, v25
; CHECK-NEXT:    vmv.x.s a6, v26
; CHECK-NEXT:    lui a1, 524288
; CHECK-NEXT:    vmv.v.x v26, a1
; CHECK-NEXT:    vredmax.vs v26, v8, v26
; CHECK-NEXT:    vmv.x.s a7, v26
; CHECK-NEXT:    addiw a1, a1, -1
; CHECK-NEXT:    vmv.v.x v26, a1
; CHECK-NEXT:    vredmin.vs v26, v8, v26
; CHECK-NEXT:    vmv.x.s t0, v26
; CHECK-NEXT:    vredmaxu.vs v26, v8, v25
; CHECK-NEXT:    vmv.x.s a3, v26
; CHECK-NEXT:    vmv.v.i v26, -1
; CHECK-NEXT:    vredminu.vs v27, v8, v26
; CHECK-NEXT:    vmv.x.s a4, v27
; CHECK-NEXT:    vredand.vs v26, v8, v26
; CHECK-NEXT:    vmv.x.s a5, v26
; CHECK-NEXT:    vredor.vs v26, v8, v25
; CHECK-NEXT:    vmv.x.s a0, v26
; CHECK-NEXT:    vredxor.vs v25, v8, v25
; CHECK-NEXT:    vmv.x.s a2, v25
; CHECK-NEXT:    add a1, a6, a7
; CHECK-NEXT:    addw a1, a1, t0
; CHECK-NEXT:    add a1, a1, a3
; CHECK-NEXT:    add a1, a1, a4
; CHECK-NEXT:    add a1, a1, a5
; CHECK-NEXT:    add a0, a0, a1
; CHECK-NEXT:    addw a0, a0, a2
; CHECK-NEXT:    ret
  %add = call i32 @llvm.vector.reduce.add.nxv2i32(<vscale x 2 x i32> %a)
  %smax = call i32 @llvm.vector.reduce.smax.nxv2i32(<vscale x 2 x i32> %a)
  %smin = call i32 @llvm.vector.reduce.smin.nxv2i32(<vscale x 2 x i32> %a)
  %umax = call i32 @llvm.vector.reduce.umax.nxv2i32(<vscale x 2 x i32> %a)
  %umin = call i32 @llvm.vector.reduce.umin.nxv2i32(<vscale x 2 x i32> %a)
  %and = call i32 @llvm.vector.reduce.and.nxv2i32(<vscale x 2 x i32> %a)
  %or = call i32 @llvm.vector.reduce.or.nxv2i32(<vscale x 2 x i32> %a)
  %xor = call i32 @llvm.vector.reduce.xor.nxv2i32(<vscale x 2 x i32> %a)

  ; FIXME reduce.mul SDNode needs to be expanded
  ;%mul = call i32 @llvm.vector.reduce.mul.nxv2i32(<vscale x 2 x i32> %a)

  %accum1 = add i32 %add, %smax
  %accum2 = add i32 %accum1, %smin
  %accum3 = add i32 %accum2, %umax
  %accum4 = add i32 %accum3, %umin
  %accum5 = add i32 %accum4, %and
  %accum6 = add i32 %accum5, %or
  %accum7 = add i32 %accum6, %xor

  ret i32 %accum7
}

define i64 @red_nxv1i64(<vscale x 1 x i64> %a) nounwind {
; CHECK-LABEL: red_nxv1i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a0, zero, e64,m1,ta,mu
; CHECK-NEXT:    vmv.v.i v25, 0
; CHECK-NEXT:    vredsum.vs v26, v8, v25
; CHECK-NEXT:    vmv.x.s a6, v26
; CHECK-NEXT:    addi a1, zero, -1
; CHECK-NEXT:    slli a1, a1, 63
; CHECK-NEXT:    vmv.v.x v26, a1
; CHECK-NEXT:    vredmax.vs v26, v8, v26
; CHECK-NEXT:    vmv.x.s a7, v26
; CHECK-NEXT:    addi a1, a1, -1
; CHECK-NEXT:    vmv.v.x v26, a1
; CHECK-NEXT:    vredmin.vs v26, v8, v26
; CHECK-NEXT:    vmv.x.s t0, v26
; CHECK-NEXT:    vredmaxu.vs v26, v8, v25
; CHECK-NEXT:    vmv.x.s a3, v26
; CHECK-NEXT:    vmv.v.i v26, -1
; CHECK-NEXT:    vredminu.vs v27, v8, v26
; CHECK-NEXT:    vmv.x.s a4, v27
; CHECK-NEXT:    vredand.vs v26, v8, v26
; CHECK-NEXT:    vmv.x.s a5, v26
; CHECK-NEXT:    vredor.vs v26, v8, v25
; CHECK-NEXT:    vmv.x.s a0, v26
; CHECK-NEXT:    vredxor.vs v25, v8, v25
; CHECK-NEXT:    vmv.x.s a2, v25
; CHECK-NEXT:    add a1, a6, a7
; CHECK-NEXT:    add a1, a1, t0
; CHECK-NEXT:    add a1, a1, a3
; CHECK-NEXT:    add a1, a1, a4
; CHECK-NEXT:    add a1, a1, a5
; CHECK-NEXT:    add a0, a0, a1
; CHECK-NEXT:    add a0, a0, a2
; CHECK-NEXT:    ret
  %add = call i64 @llvm.vector.reduce.add.nxv1i64(<vscale x 1 x i64> %a)
  %smax = call i64 @llvm.vector.reduce.smax.nxv1i64(<vscale x 1 x i64> %a)
  %smin = call i64 @llvm.vector.reduce.smin.nxv1i64(<vscale x 1 x i64> %a)
  %umax = call i64 @llvm.vector.reduce.umax.nxv1i64(<vscale x 1 x i64> %a)
  %umin = call i64 @llvm.vector.reduce.umin.nxv1i64(<vscale x 1 x i64> %a)
  %and = call i64 @llvm.vector.reduce.and.nxv1i64(<vscale x 1 x i64> %a)
  %or = call i64 @llvm.vector.reduce.or.nxv1i64(<vscale x 1 x i64> %a)
  %xor = call i64 @llvm.vector.reduce.xor.nxv1i64(<vscale x 1 x i64> %a)

  ; FIXME reduce.mul SDNode needs to be expanded
  ;%mul = call i64 @llvm.vector.reduce.mul.nxv1i64(<vscale x 1 x i64> %a)

  %accum1 = add i64 %add, %smax
  %accum2 = add i64 %accum1, %smin
  %accum3 = add i64 %accum2, %umax
  %accum4 = add i64 %accum3, %umin
  %accum5 = add i64 %accum4, %and
  %accum6 = add i64 %accum5, %or
  %accum7 = add i64 %accum6, %xor

  ret i64 %accum7
}

define i64 @red_nxv2i64(<vscale x 2 x i64> %a) nounwind {
; CHECK-LABEL: red_nxv2i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a0, zero, e64,m1,ta,mu
; CHECK-NEXT:    vmv.v.i v25, 0
; CHECK-NEXT:    vsetvli a0, zero, e64,m2,ta,mu
; CHECK-NEXT:    vredsum.vs v26, v8, v25
; CHECK-NEXT:    vsetvli zero, zero, e64,m1,ta,mu
; CHECK-NEXT:    vmv.x.s a6, v26
; CHECK-NEXT:    addi a1, zero, -1
; CHECK-NEXT:    slli a1, a1, 63
; CHECK-NEXT:    vmv.v.x v26, a1
; CHECK-NEXT:    vsetvli a2, zero, e64,m2,ta,mu
; CHECK-NEXT:    vredmax.vs v26, v8, v26
; CHECK-NEXT:    vsetvli zero, zero, e64,m1,ta,mu
; CHECK-NEXT:    vmv.x.s a7, v26
; CHECK-NEXT:    addi a1, a1, -1
; CHECK-NEXT:    vmv.v.x v26, a1
; CHECK-NEXT:    vsetvli a1, zero, e64,m2,ta,mu
; CHECK-NEXT:    vredmin.vs v26, v8, v26
; CHECK-NEXT:    vsetvli zero, zero, e64,m1,ta,mu
; CHECK-NEXT:    vmv.x.s t0, v26
; CHECK-NEXT:    vsetvli a3, zero, e64,m2,ta,mu
; CHECK-NEXT:    vredmaxu.vs v26, v8, v25
; CHECK-NEXT:    vsetvli zero, zero, e64,m1,ta,mu
; CHECK-NEXT:    vmv.x.s a3, v26
; CHECK-NEXT:    vmv.v.i v26, -1
; CHECK-NEXT:    vsetvli a4, zero, e64,m2,ta,mu
; CHECK-NEXT:    vredminu.vs v27, v8, v26
; CHECK-NEXT:    vsetvli zero, zero, e64,m1,ta,mu
; CHECK-NEXT:    vmv.x.s a4, v27
; CHECK-NEXT:    vsetvli a5, zero, e64,m2,ta,mu
; CHECK-NEXT:    vredand.vs v26, v8, v26
; CHECK-NEXT:    vsetvli zero, zero, e64,m1,ta,mu
; CHECK-NEXT:    vmv.x.s a5, v26
; CHECK-NEXT:    vsetvli a0, zero, e64,m2,ta,mu
; CHECK-NEXT:    vredor.vs v26, v8, v25
; CHECK-NEXT:    vsetvli zero, zero, e64,m1,ta,mu
; CHECK-NEXT:    vmv.x.s a0, v26
; CHECK-NEXT:    vsetvli a2, zero, e64,m2,ta,mu
; CHECK-NEXT:    vredxor.vs v25, v8, v25
; CHECK-NEXT:    vsetvli zero, zero, e64,m1,ta,mu
; CHECK-NEXT:    vmv.x.s a2, v25
; CHECK-NEXT:    add a1, a6, a7
; CHECK-NEXT:    add a1, a1, t0
; CHECK-NEXT:    add a1, a1, a3
; CHECK-NEXT:    add a1, a1, a4
; CHECK-NEXT:    add a1, a1, a5
; CHECK-NEXT:    add a0, a0, a1
; CHECK-NEXT:    add a0, a0, a2
; CHECK-NEXT:    ret
  %add = call i64 @llvm.vector.reduce.add.nxv2i64(<vscale x 2 x i64> %a)
  %smax = call i64 @llvm.vector.reduce.smax.nxv2i64(<vscale x 2 x i64> %a)
  %smin = call i64 @llvm.vector.reduce.smin.nxv2i64(<vscale x 2 x i64> %a)
  %umax = call i64 @llvm.vector.reduce.umax.nxv2i64(<vscale x 2 x i64> %a)
  %umin = call i64 @llvm.vector.reduce.umin.nxv2i64(<vscale x 2 x i64> %a)
  %and = call i64 @llvm.vector.reduce.and.nxv2i64(<vscale x 2 x i64> %a)
  %or = call i64 @llvm.vector.reduce.or.nxv2i64(<vscale x 2 x i64> %a)
  %xor = call i64 @llvm.vector.reduce.xor.nxv2i64(<vscale x 2 x i64> %a)

  ; FIXME reduce.mul SDNode needs to be expanded
  ;%mul = call i64 @llvm.vector.reduce.mul.nxv2i64(<vscale x 2 x i64> %a)

  %accum1 = add i64 %add, %smax
  %accum2 = add i64 %accum1, %smin
  %accum3 = add i64 %accum2, %umax
  %accum4 = add i64 %accum3, %umin
  %accum5 = add i64 %accum4, %and
  %accum6 = add i64 %accum5, %or
  %accum7 = add i64 %accum6, %xor

  ret i64 %accum7
}

define float @red_nxv2f32(<vscale x 2 x float> %a) nounwind {
; CHECK-LABEL: red_nxv2f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(.LCPI3_0)
; CHECK-NEXT:    flw ft0, %lo(.LCPI3_0)(a0)
; CHECK-NEXT:    vsetvli a0, zero, e32,m1,ta,mu
; CHECK-NEXT:    vfmv.v.f v25, ft0
; CHECK-NEXT:    vfredosum.vs v25, v8, v25
; CHECK-NEXT:    vfmv.f.s ft1, v25
; CHECK-NEXT:    vmv.v.i v25, 0
; CHECK-NEXT:    vfredsum.vs v25, v8, v25
; CHECK-NEXT:    vfmv.f.s ft2, v25
; CHECK-NEXT:    fadd.s ft0, ft2, ft0
; CHECK-NEXT:    fmv.w.x ft2, zero
; CHECK-NEXT:    vfmv.s.f v25, ft2
; CHECK-NEXT:    vfredmax.vs v26, v8, v25
; CHECK-NEXT:    vfmv.f.s ft2, v26
; CHECK-NEXT:    vfredmin.vs v25, v8, v25
; CHECK-NEXT:    vfmv.f.s ft3, v25
; CHECK-NEXT:    fadd.s ft0, ft1, ft0
; CHECK-NEXT:    fadd.s ft0, ft0, ft2
; CHECK-NEXT:    fadd.s ft0, ft0, ft3
; CHECK-NEXT:    fmv.x.w a0, ft0
; CHECK-NEXT:    ret
  %fadd = call float @llvm.vector.reduce.fadd.f32.nxv2f32(float 4.0, <vscale x 2 x float> %a)
  %unord_fadd = call reassoc float @llvm.vector.reduce.fadd.f32.nxv2f32(float 4.0, <vscale x 2 x float> %a) ; unordered reduction
  %fmax = call float @llvm.vector.reduce.fmax.nxv2f32(<vscale x 2 x float> %a)
  %fmin = call float @llvm.vector.reduce.fmin.nxv2f32(<vscale x 2 x float> %a)

  ; FIXME reduce.fmul SDNode needs to be expanded
  ;%fmul = call float @llvm.vector.reduce.fmul.f32.nxv2f32(float 1.0, <vscale x 2 x float> %a)

  %accum1 = fadd float %fadd, %unord_fadd
  %accum2 = fadd float %accum1, %fmax
  %accum3 = fadd float %accum2, %fmin

  ret float %accum3
}

define double @red_nxv1f64(<vscale x 1 x double> %a) nounwind {
; CHECK-LABEL: red_nxv1f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(.LCPI4_0)
; CHECK-NEXT:    fld ft0, %lo(.LCPI4_0)(a0)
; CHECK-NEXT:    vsetvli a0, zero, e64,m1,ta,mu
; CHECK-NEXT:    vfmv.v.f v25, ft0
; CHECK-NEXT:    vfredosum.vs v25, v8, v25
; CHECK-NEXT:    vfmv.f.s ft1, v25
; CHECK-NEXT:    vmv.v.i v25, 0
; CHECK-NEXT:    vfredsum.vs v25, v8, v25
; CHECK-NEXT:    vfmv.f.s ft2, v25
; CHECK-NEXT:    fadd.d ft0, ft2, ft0
; CHECK-NEXT:    fmv.d.x ft2, zero
; CHECK-NEXT:    vfmv.s.f v25, ft2
; CHECK-NEXT:    vfredmax.vs v26, v8, v25
; CHECK-NEXT:    vfmv.f.s ft2, v26
; CHECK-NEXT:    vfredmin.vs v25, v8, v25
; CHECK-NEXT:    vfmv.f.s ft3, v25
; CHECK-NEXT:    fadd.d ft0, ft1, ft0
; CHECK-NEXT:    fadd.d ft0, ft0, ft2
; CHECK-NEXT:    fadd.d ft0, ft0, ft3
; CHECK-NEXT:    fmv.x.d a0, ft0
; CHECK-NEXT:    ret
  %fadd = call double @llvm.vector.reduce.fadd.f64.nxv1f64(double 4.0, <vscale x 1 x double> %a)
  %unord_fadd = call reassoc double @llvm.vector.reduce.fadd.f64.nxv1f64(double 4.0, <vscale x 1 x double> %a) ; unordered reduction
  %fmax = call double @llvm.vector.reduce.fmax.nxv1f64(<vscale x 1 x double> %a)
  %fmin = call double @llvm.vector.reduce.fmin.nxv1f64(<vscale x 1 x double> %a)

  ; FIXME reduce.fmul SDNode needs to be expanded
  ;%fmul = call double @llvm.vector.reduce.fmul.f64.nxv1f64(double 1.0, <vscale x 1 x double> %a)

  %accum1 = fadd double %fadd, %unord_fadd
  %accum2 = fadd double %accum1, %fmax
  %accum3 = fadd double %accum2, %fmin

  ret double %accum3
}

define double @red_nxv2f64(<vscale x 2 x double> %a) nounwind {
; CHECK-LABEL: red_nxv2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(.LCPI5_0)
; CHECK-NEXT:    fld ft0, %lo(.LCPI5_0)(a0)
; CHECK-NEXT:    vsetvli a0, zero, e64,m1,ta,mu
; CHECK-NEXT:    vfmv.v.f v25, ft0
; CHECK-NEXT:    vsetvli a0, zero, e64,m2,ta,mu
; CHECK-NEXT:    vfredosum.vs v25, v8, v25
; CHECK-NEXT:    vsetvli zero, zero, e64,m1,ta,mu
; CHECK-NEXT:    vfmv.f.s ft1, v25
; CHECK-NEXT:    vmv.v.i v25, 0
; CHECK-NEXT:    vsetvli a0, zero, e64,m2,ta,mu
; CHECK-NEXT:    vfredsum.vs v25, v8, v25
; CHECK-NEXT:    vsetvli zero, zero, e64,m1,ta,mu
; CHECK-NEXT:    vfmv.f.s ft2, v25
; CHECK-NEXT:    fadd.d ft0, ft2, ft0
; CHECK-NEXT:    fmv.d.x ft2, zero
; CHECK-NEXT:    vsetvli a0, zero, e64,m2,ta,mu
; CHECK-NEXT:    vfmv.s.f v26, ft2
; CHECK-NEXT:    vfredmax.vs v28, v8, v26
; CHECK-NEXT:    vfmv.f.s ft2, v28
; CHECK-NEXT:    vfredmin.vs v28, v8, v26
; CHECK-NEXT:    vfmv.f.s ft3, v28
; CHECK-NEXT:    fadd.d ft0, ft1, ft0
; CHECK-NEXT:    fadd.d ft0, ft0, ft2
; CHECK-NEXT:    fadd.d ft0, ft0, ft3
; CHECK-NEXT:    fmv.x.d a0, ft0
; CHECK-NEXT:    ret
  %fadd = call double @llvm.vector.reduce.fadd.f64.nxv2f64(double 4.0, <vscale x 2 x double> %a)
  %unord_fadd = call reassoc double @llvm.vector.reduce.fadd.f64.nxv2f64(double 4.0, <vscale x 2 x double> %a) ; unordered reduction
  %fmax = call double @llvm.vector.reduce.fmax.nxv2f64(<vscale x 2 x double> %a)
  %fmin = call double @llvm.vector.reduce.fmin.nxv2f64(<vscale x 2 x double> %a)

  ; FIXME reduce.fmul SDNode needs to be expanded
  ;%fmul = call double @llvm.vector.reduce.fmul.f64.nxv2f64(double 1.0, <vscale x 2 x double> %a)

  %accum1 = fadd double %fadd, %unord_fadd
  %accum2 = fadd double %accum1, %fmax
  %accum3 = fadd double %accum2, %fmin

  ret double %accum3
}

declare i32 @llvm.vector.reduce.add.nxv2i32(<vscale x 2 x i32>)
declare i32 @llvm.vector.reduce.smax.nxv2i32(<vscale x 2 x i32>)
declare i32 @llvm.vector.reduce.smin.nxv2i32(<vscale x 2 x i32>)
declare i32 @llvm.vector.reduce.umax.nxv2i32(<vscale x 2 x i32>)
declare i32 @llvm.vector.reduce.umin.nxv2i32(<vscale x 2 x i32>)
declare i32 @llvm.vector.reduce.and.nxv2i32(<vscale x 2 x i32>)
declare i32 @llvm.vector.reduce.or.nxv2i32(<vscale x 2 x i32>)
declare i32 @llvm.vector.reduce.xor.nxv2i32(<vscale x 2 x i32>)
;declare i32 @llvm.vector.reduce.mul.nxv2i32(<vscale x 2 x i32>)

declare i64 @llvm.vector.reduce.add.nxv1i64(<vscale x 1 x i64>)
declare i64 @llvm.vector.reduce.smax.nxv1i64(<vscale x 1 x i64>)
declare i64 @llvm.vector.reduce.smin.nxv1i64(<vscale x 1 x i64>)
declare i64 @llvm.vector.reduce.umax.nxv1i64(<vscale x 1 x i64>)
declare i64 @llvm.vector.reduce.umin.nxv1i64(<vscale x 1 x i64>)
declare i64 @llvm.vector.reduce.and.nxv1i64(<vscale x 1 x i64>)
declare i64 @llvm.vector.reduce.or.nxv1i64(<vscale x 1 x i64>)
declare i64 @llvm.vector.reduce.xor.nxv1i64(<vscale x 1 x i64>)
;declare i64 @llvm.vector.reduce.mul.nxv1i64(<vscale x 1 x i64>)

declare i64 @llvm.vector.reduce.add.nxv2i64(<vscale x 2 x i64>)
declare i64 @llvm.vector.reduce.smax.nxv2i64(<vscale x 2 x i64>)
declare i64 @llvm.vector.reduce.smin.nxv2i64(<vscale x 2 x i64>)
declare i64 @llvm.vector.reduce.umax.nxv2i64(<vscale x 2 x i64>)
declare i64 @llvm.vector.reduce.umin.nxv2i64(<vscale x 2 x i64>)
declare i64 @llvm.vector.reduce.and.nxv2i64(<vscale x 2 x i64>)
declare i64 @llvm.vector.reduce.or.nxv2i64(<vscale x 2 x i64>)
declare i64 @llvm.vector.reduce.xor.nxv2i64(<vscale x 2 x i64>)
;declare i64 @llvm.vector.reduce.mul.nxv2i64(<vscale x 2 x i64>)

declare float @llvm.vector.reduce.fadd.f32.nxv2f32(float, <vscale x 2 x float>)
declare float @llvm.vector.reduce.fmax.nxv2f32(<vscale x 2 x float>)
declare float @llvm.vector.reduce.fmin.nxv2f32(<vscale x 2 x float>)
;declare float @llvm.vector.reduce.fmul.f32.nxv2f32(float, <vscale x 2 x float>)

declare double @llvm.vector.reduce.fadd.f64.nxv1f64(double, <vscale x 1 x double>)
declare double @llvm.vector.reduce.fmax.nxv1f64(<vscale x 1 x double>)
declare double @llvm.vector.reduce.fmin.nxv1f64(<vscale x 1 x double>)
;declare double @llvm.vector.reduce.fmul.f64.nxv1f64(double, <vscale x 1 x double>)

declare double @llvm.vector.reduce.fadd.f64.nxv2f64(double, <vscale x 2 x double>)
declare double @llvm.vector.reduce.fmax.nxv2f64(<vscale x 2 x double>)
declare double @llvm.vector.reduce.fmin.nxv2f64(<vscale x 2 x double>)
;declare double @llvm.vector.reduce.fmul.f64.nxv2f64(double, <vscale x 2 x double>)
