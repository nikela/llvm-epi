; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+m,+f,+d,+a,+c,+experimental-v \
; RUN:    -verify-machineinstrs < %s -epi-pipeline | FileCheck %s

define i32 @red_nxv2i32(<vscale x 2 x i32> %a) nounwind {
; CHECK-LABEL: red_nxv2i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 1, e32, m1, ta, mu
; CHECK-NEXT:    vmv.v.i v9, 0
; CHECK-NEXT:    vsetvli a0, zero, e32, m1, ta, mu
; CHECK-NEXT:    vredsum.vs v10, v8, v9
; CHECK-NEXT:    vmv.x.s a6, v10
; CHECK-NEXT:    lui a1, 524288
; CHECK-NEXT:    vsetivli zero, 1, e32, m1, ta, mu
; CHECK-NEXT:    vmv.v.x v10, a1
; CHECK-NEXT:    vsetvli a2, zero, e32, m1, ta, mu
; CHECK-NEXT:    vredmax.vs v10, v8, v10
; CHECK-NEXT:    vmv.x.s a7, v10
; CHECK-NEXT:    addiw a1, a1, -1
; CHECK-NEXT:    vsetivli zero, 1, e32, m1, ta, mu
; CHECK-NEXT:    vmv.v.x v10, a1
; CHECK-NEXT:    vsetvli a1, zero, e32, m1, ta, mu
; CHECK-NEXT:    vredmin.vs v10, v8, v10
; CHECK-NEXT:    vmv.x.s t0, v10
; CHECK-NEXT:    vredmaxu.vs v10, v8, v9
; CHECK-NEXT:    vmv.x.s a3, v10
; CHECK-NEXT:    vsetivli zero, 1, e32, m1, ta, mu
; CHECK-NEXT:    vmv.v.i v10, -1
; CHECK-NEXT:    vsetvli a4, zero, e32, m1, ta, mu
; CHECK-NEXT:    vredminu.vs v11, v8, v10
; CHECK-NEXT:    vmv.x.s a4, v11
; CHECK-NEXT:    vredand.vs v10, v8, v10
; CHECK-NEXT:    vmv.x.s a5, v10
; CHECK-NEXT:    vredor.vs v10, v8, v9
; CHECK-NEXT:    vmv.x.s a0, v10
; CHECK-NEXT:    vredxor.vs v8, v8, v9
; CHECK-NEXT:    vmv.x.s a2, v8
; CHECK-NEXT:    addw a1, a6, a7
; CHECK-NEXT:    addw a1, a1, t0
; CHECK-NEXT:    addw a1, a1, a3
; CHECK-NEXT:    addw a1, a1, a4
; CHECK-NEXT:    addw a1, a1, a5
; CHECK-NEXT:    addw a0, a0, a1
; CHECK-NEXT:    addw a0, a0, a2
; CHECK-NEXT:    ret
  %add = call i32 @llvm.vector.reduce.add.nxv2i32(<vscale x 2 x i32> %a)
  %smax = call i32 @llvm.vector.reduce.smax.nxv2i32(<vscale x 2 x i32> %a)
  %smin = call i32 @llvm.vector.reduce.smin.nxv2i32(<vscale x 2 x i32> %a)
  %umax = call i32 @llvm.vector.reduce.umax.nxv2i32(<vscale x 2 x i32> %a)
  %umin = call i32 @llvm.vector.reduce.umin.nxv2i32(<vscale x 2 x i32> %a)
  %and = call i32 @llvm.vector.reduce.and.nxv2i32(<vscale x 2 x i32> %a)
  %or = call i32 @llvm.vector.reduce.or.nxv2i32(<vscale x 2 x i32> %a)
  %xor = call i32 @llvm.vector.reduce.xor.nxv2i32(<vscale x 2 x i32> %a)

  ; FIXME reduce.mul SDNode needs to be expanded
  ;%mul = call i32 @llvm.vector.reduce.mul.nxv2i32(<vscale x 2 x i32> %a)

  %accum1 = add i32 %add, %smax
  %accum2 = add i32 %accum1, %smin
  %accum3 = add i32 %accum2, %umax
  %accum4 = add i32 %accum3, %umin
  %accum5 = add i32 %accum4, %and
  %accum6 = add i32 %accum5, %or
  %accum7 = add i32 %accum6, %xor

  ret i32 %accum7
}

define i64 @red_nxv1i64(<vscale x 1 x i64> %a) nounwind {
; CHECK-LABEL: red_nxv1i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; CHECK-NEXT:    vmv.v.i v9, 0
; CHECK-NEXT:    vsetvli a0, zero, e64, m1, ta, mu
; CHECK-NEXT:    vredsum.vs v10, v8, v9
; CHECK-NEXT:    vmv.x.s a6, v10
; CHECK-NEXT:    li a1, -1
; CHECK-NEXT:    slli a2, a1, 63
; CHECK-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; CHECK-NEXT:    vmv.v.x v10, a2
; CHECK-NEXT:    vsetvli a2, zero, e64, m1, ta, mu
; CHECK-NEXT:    vredmax.vs v10, v8, v10
; CHECK-NEXT:    vmv.x.s a7, v10
; CHECK-NEXT:    srli a1, a1, 1
; CHECK-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; CHECK-NEXT:    vmv.v.x v10, a1
; CHECK-NEXT:    vsetvli a1, zero, e64, m1, ta, mu
; CHECK-NEXT:    vredmin.vs v10, v8, v10
; CHECK-NEXT:    vmv.x.s t0, v10
; CHECK-NEXT:    vredmaxu.vs v10, v8, v9
; CHECK-NEXT:    vmv.x.s a3, v10
; CHECK-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; CHECK-NEXT:    vmv.v.i v10, -1
; CHECK-NEXT:    vsetvli a4, zero, e64, m1, ta, mu
; CHECK-NEXT:    vredminu.vs v11, v8, v10
; CHECK-NEXT:    vmv.x.s a4, v11
; CHECK-NEXT:    vredand.vs v10, v8, v10
; CHECK-NEXT:    vmv.x.s a5, v10
; CHECK-NEXT:    vredor.vs v10, v8, v9
; CHECK-NEXT:    vmv.x.s a0, v10
; CHECK-NEXT:    vredxor.vs v8, v8, v9
; CHECK-NEXT:    vmv.x.s a2, v8
; CHECK-NEXT:    add a1, a6, a7
; CHECK-NEXT:    add a1, a1, t0
; CHECK-NEXT:    add a1, a1, a3
; CHECK-NEXT:    add a1, a1, a4
; CHECK-NEXT:    add a1, a1, a5
; CHECK-NEXT:    add a0, a0, a1
; CHECK-NEXT:    add a0, a0, a2
; CHECK-NEXT:    ret
  %add = call i64 @llvm.vector.reduce.add.nxv1i64(<vscale x 1 x i64> %a)
  %smax = call i64 @llvm.vector.reduce.smax.nxv1i64(<vscale x 1 x i64> %a)
  %smin = call i64 @llvm.vector.reduce.smin.nxv1i64(<vscale x 1 x i64> %a)
  %umax = call i64 @llvm.vector.reduce.umax.nxv1i64(<vscale x 1 x i64> %a)
  %umin = call i64 @llvm.vector.reduce.umin.nxv1i64(<vscale x 1 x i64> %a)
  %and = call i64 @llvm.vector.reduce.and.nxv1i64(<vscale x 1 x i64> %a)
  %or = call i64 @llvm.vector.reduce.or.nxv1i64(<vscale x 1 x i64> %a)
  %xor = call i64 @llvm.vector.reduce.xor.nxv1i64(<vscale x 1 x i64> %a)

  ; FIXME reduce.mul SDNode needs to be expanded
  ;%mul = call i64 @llvm.vector.reduce.mul.nxv1i64(<vscale x 1 x i64> %a)

  %accum1 = add i64 %add, %smax
  %accum2 = add i64 %accum1, %smin
  %accum3 = add i64 %accum2, %umax
  %accum4 = add i64 %accum3, %umin
  %accum5 = add i64 %accum4, %and
  %accum6 = add i64 %accum5, %or
  %accum7 = add i64 %accum6, %xor

  ret i64 %accum7
}

define i64 @red_nxv2i64(<vscale x 2 x i64> %a) nounwind {
; CHECK-LABEL: red_nxv2i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; CHECK-NEXT:    vmv.v.i v10, 0
; CHECK-NEXT:    vsetvli a0, zero, e64, m2, ta, mu
; CHECK-NEXT:    vredsum.vs v11, v8, v10
; CHECK-NEXT:    vmv.x.s a6, v11
; CHECK-NEXT:    li a1, -1
; CHECK-NEXT:    slli a2, a1, 63
; CHECK-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; CHECK-NEXT:    vmv.v.x v11, a2
; CHECK-NEXT:    vsetvli a2, zero, e64, m2, ta, mu
; CHECK-NEXT:    vredmax.vs v11, v8, v11
; CHECK-NEXT:    vmv.x.s a7, v11
; CHECK-NEXT:    srli a1, a1, 1
; CHECK-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; CHECK-NEXT:    vmv.v.x v11, a1
; CHECK-NEXT:    vsetvli a1, zero, e64, m2, ta, mu
; CHECK-NEXT:    vredmin.vs v11, v8, v11
; CHECK-NEXT:    vmv.x.s t0, v11
; CHECK-NEXT:    vredmaxu.vs v11, v8, v10
; CHECK-NEXT:    vmv.x.s a3, v11
; CHECK-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; CHECK-NEXT:    vmv.v.i v11, -1
; CHECK-NEXT:    vsetvli a4, zero, e64, m2, ta, mu
; CHECK-NEXT:    vredminu.vs v12, v8, v11
; CHECK-NEXT:    vmv.x.s a4, v12
; CHECK-NEXT:    vredand.vs v11, v8, v11
; CHECK-NEXT:    vmv.x.s a5, v11
; CHECK-NEXT:    vredor.vs v11, v8, v10
; CHECK-NEXT:    vmv.x.s a0, v11
; CHECK-NEXT:    vredxor.vs v8, v8, v10
; CHECK-NEXT:    vmv.x.s a2, v8
; CHECK-NEXT:    add a1, a6, a7
; CHECK-NEXT:    add a1, a1, t0
; CHECK-NEXT:    add a1, a1, a3
; CHECK-NEXT:    add a1, a1, a4
; CHECK-NEXT:    add a1, a1, a5
; CHECK-NEXT:    add a0, a0, a1
; CHECK-NEXT:    add a0, a0, a2
; CHECK-NEXT:    ret
  %add = call i64 @llvm.vector.reduce.add.nxv2i64(<vscale x 2 x i64> %a)
  %smax = call i64 @llvm.vector.reduce.smax.nxv2i64(<vscale x 2 x i64> %a)
  %smin = call i64 @llvm.vector.reduce.smin.nxv2i64(<vscale x 2 x i64> %a)
  %umax = call i64 @llvm.vector.reduce.umax.nxv2i64(<vscale x 2 x i64> %a)
  %umin = call i64 @llvm.vector.reduce.umin.nxv2i64(<vscale x 2 x i64> %a)
  %and = call i64 @llvm.vector.reduce.and.nxv2i64(<vscale x 2 x i64> %a)
  %or = call i64 @llvm.vector.reduce.or.nxv2i64(<vscale x 2 x i64> %a)
  %xor = call i64 @llvm.vector.reduce.xor.nxv2i64(<vscale x 2 x i64> %a)

  ; FIXME reduce.mul SDNode needs to be expanded
  ;%mul = call i64 @llvm.vector.reduce.mul.nxv2i64(<vscale x 2 x i64> %a)

  %accum1 = add i64 %add, %smax
  %accum2 = add i64 %accum1, %smin
  %accum3 = add i64 %accum2, %umax
  %accum4 = add i64 %accum3, %umin
  %accum5 = add i64 %accum4, %and
  %accum6 = add i64 %accum5, %or
  %accum7 = add i64 %accum6, %xor

  ret i64 %accum7
}

define float @red_nxv2f32(<vscale x 2 x float> %a) nounwind {
; CHECK-LABEL: red_nxv2f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(.LCPI3_0)
; CHECK-NEXT:    flw ft0, %lo(.LCPI3_0)(a0)
; CHECK-NEXT:    vsetivli zero, 1, e32, m1, ta, mu
; CHECK-NEXT:    vfmv.v.f v9, ft0
; CHECK-NEXT:    vsetvli a0, zero, e32, m1, ta, mu
; CHECK-NEXT:    vfredosum.vs v9, v8, v9
; CHECK-NEXT:    vfmv.f.s ft1, v9
; CHECK-NEXT:    lui a0, %hi(.LCPI3_1)
; CHECK-NEXT:    addi a0, a0, %lo(.LCPI3_1)
; CHECK-NEXT:    vsetivli zero, 1, e32, m1, ta, mu
; CHECK-NEXT:    vlse32.v v9, (a0), zero
; CHECK-NEXT:    vsetvli a0, zero, e32, m1, ta, mu
; CHECK-NEXT:    vfredusum.vs v9, v8, v9
; CHECK-NEXT:    vfmv.f.s ft2, v9
; CHECK-NEXT:    lui a0, %hi(.LCPI3_2)
; CHECK-NEXT:    addi a0, a0, %lo(.LCPI3_2)
; CHECK-NEXT:    vsetivli zero, 1, e32, m1, ta, mu
; CHECK-NEXT:    vlse32.v v9, (a0), zero
; CHECK-NEXT:    vsetvli a0, zero, e32, m1, ta, mu
; CHECK-NEXT:    vfredmax.vs v9, v8, v9
; CHECK-NEXT:    vfmv.f.s ft3, v9
; CHECK-NEXT:    lui a0, %hi(.LCPI3_3)
; CHECK-NEXT:    addi a0, a0, %lo(.LCPI3_3)
; CHECK-NEXT:    vsetivli zero, 1, e32, m1, ta, mu
; CHECK-NEXT:    vlse32.v v9, (a0), zero
; CHECK-NEXT:    fadd.s ft0, ft2, ft0
; CHECK-NEXT:    vsetvli a0, zero, e32, m1, ta, mu
; CHECK-NEXT:    vfredmin.vs v8, v8, v9
; CHECK-NEXT:    vfmv.f.s ft2, v8
; CHECK-NEXT:    fadd.s ft0, ft1, ft0
; CHECK-NEXT:    fadd.s ft0, ft0, ft3
; CHECK-NEXT:    fadd.s ft0, ft0, ft2
; CHECK-NEXT:    fmv.x.w a0, ft0
; CHECK-NEXT:    ret
  %fadd = call float @llvm.vector.reduce.fadd.f32.nxv2f32(float 4.0, <vscale x 2 x float> %a)
  %unord_fadd = call reassoc float @llvm.vector.reduce.fadd.f32.nxv2f32(float 4.0, <vscale x 2 x float> %a) ; unordered reduction
  %fmax = call float @llvm.vector.reduce.fmax.nxv2f32(<vscale x 2 x float> %a)
  %fmin = call float @llvm.vector.reduce.fmin.nxv2f32(<vscale x 2 x float> %a)

  ; FIXME reduce.fmul SDNode needs to be expanded
  ;%fmul = call float @llvm.vector.reduce.fmul.f32.nxv2f32(float 1.0, <vscale x 2 x float> %a)

  %accum1 = fadd float %fadd, %unord_fadd
  %accum2 = fadd float %accum1, %fmax
  %accum3 = fadd float %accum2, %fmin

  ret float %accum3
}

define double @red_nxv1f64(<vscale x 1 x double> %a) nounwind {
; CHECK-LABEL: red_nxv1f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(.LCPI4_0)
; CHECK-NEXT:    fld ft0, %lo(.LCPI4_0)(a0)
; CHECK-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; CHECK-NEXT:    vfmv.v.f v9, ft0
; CHECK-NEXT:    vsetvli a0, zero, e64, m1, ta, mu
; CHECK-NEXT:    vfredosum.vs v9, v8, v9
; CHECK-NEXT:    vfmv.f.s ft1, v9
; CHECK-NEXT:    lui a0, %hi(.LCPI4_1)
; CHECK-NEXT:    addi a0, a0, %lo(.LCPI4_1)
; CHECK-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; CHECK-NEXT:    vlse64.v v9, (a0), zero
; CHECK-NEXT:    vsetvli a0, zero, e64, m1, ta, mu
; CHECK-NEXT:    vfredusum.vs v9, v8, v9
; CHECK-NEXT:    vfmv.f.s ft2, v9
; CHECK-NEXT:    lui a0, %hi(.LCPI4_2)
; CHECK-NEXT:    addi a0, a0, %lo(.LCPI4_2)
; CHECK-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; CHECK-NEXT:    vlse64.v v9, (a0), zero
; CHECK-NEXT:    vsetvli a0, zero, e64, m1, ta, mu
; CHECK-NEXT:    vfredmax.vs v9, v8, v9
; CHECK-NEXT:    vfmv.f.s ft3, v9
; CHECK-NEXT:    lui a0, %hi(.LCPI4_3)
; CHECK-NEXT:    addi a0, a0, %lo(.LCPI4_3)
; CHECK-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; CHECK-NEXT:    vlse64.v v9, (a0), zero
; CHECK-NEXT:    fadd.d ft0, ft2, ft0
; CHECK-NEXT:    vsetvli a0, zero, e64, m1, ta, mu
; CHECK-NEXT:    vfredmin.vs v8, v8, v9
; CHECK-NEXT:    vfmv.f.s ft2, v8
; CHECK-NEXT:    fadd.d ft0, ft1, ft0
; CHECK-NEXT:    fadd.d ft0, ft0, ft3
; CHECK-NEXT:    fadd.d ft0, ft0, ft2
; CHECK-NEXT:    fmv.x.d a0, ft0
; CHECK-NEXT:    ret
  %fadd = call double @llvm.vector.reduce.fadd.f64.nxv1f64(double 4.0, <vscale x 1 x double> %a)
  %unord_fadd = call reassoc double @llvm.vector.reduce.fadd.f64.nxv1f64(double 4.0, <vscale x 1 x double> %a) ; unordered reduction
  %fmax = call double @llvm.vector.reduce.fmax.nxv1f64(<vscale x 1 x double> %a)
  %fmin = call double @llvm.vector.reduce.fmin.nxv1f64(<vscale x 1 x double> %a)

  ; FIXME reduce.fmul SDNode needs to be expanded
  ;%fmul = call double @llvm.vector.reduce.fmul.f64.nxv1f64(double 1.0, <vscale x 1 x double> %a)

  %accum1 = fadd double %fadd, %unord_fadd
  %accum2 = fadd double %accum1, %fmax
  %accum3 = fadd double %accum2, %fmin

  ret double %accum3
}

define double @red_nxv2f64(<vscale x 2 x double> %a) nounwind {
; CHECK-LABEL: red_nxv2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(.LCPI5_0)
; CHECK-NEXT:    fld ft0, %lo(.LCPI5_0)(a0)
; CHECK-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; CHECK-NEXT:    vfmv.v.f v10, ft0
; CHECK-NEXT:    vsetvli a0, zero, e64, m2, ta, mu
; CHECK-NEXT:    vfredosum.vs v10, v8, v10
; CHECK-NEXT:    vfmv.f.s ft1, v10
; CHECK-NEXT:    lui a0, %hi(.LCPI5_1)
; CHECK-NEXT:    addi a0, a0, %lo(.LCPI5_1)
; CHECK-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; CHECK-NEXT:    vlse64.v v10, (a0), zero
; CHECK-NEXT:    vsetvli a0, zero, e64, m2, ta, mu
; CHECK-NEXT:    vfredusum.vs v10, v8, v10
; CHECK-NEXT:    vfmv.f.s ft2, v10
; CHECK-NEXT:    lui a0, %hi(.LCPI5_2)
; CHECK-NEXT:    addi a0, a0, %lo(.LCPI5_2)
; CHECK-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; CHECK-NEXT:    vlse64.v v10, (a0), zero
; CHECK-NEXT:    vsetvli a0, zero, e64, m2, ta, mu
; CHECK-NEXT:    vfredmax.vs v10, v8, v10
; CHECK-NEXT:    vfmv.f.s ft3, v10
; CHECK-NEXT:    lui a0, %hi(.LCPI5_3)
; CHECK-NEXT:    addi a0, a0, %lo(.LCPI5_3)
; CHECK-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; CHECK-NEXT:    vlse64.v v10, (a0), zero
; CHECK-NEXT:    fadd.d ft0, ft2, ft0
; CHECK-NEXT:    vsetvli a0, zero, e64, m2, ta, mu
; CHECK-NEXT:    vfredmin.vs v8, v8, v10
; CHECK-NEXT:    vfmv.f.s ft2, v8
; CHECK-NEXT:    fadd.d ft0, ft1, ft0
; CHECK-NEXT:    fadd.d ft0, ft0, ft3
; CHECK-NEXT:    fadd.d ft0, ft0, ft2
; CHECK-NEXT:    fmv.x.d a0, ft0
; CHECK-NEXT:    ret
  %fadd = call double @llvm.vector.reduce.fadd.f64.nxv2f64(double 4.0, <vscale x 2 x double> %a)
  %unord_fadd = call reassoc double @llvm.vector.reduce.fadd.f64.nxv2f64(double 4.0, <vscale x 2 x double> %a) ; unordered reduction
  %fmax = call double @llvm.vector.reduce.fmax.nxv2f64(<vscale x 2 x double> %a)
  %fmin = call double @llvm.vector.reduce.fmin.nxv2f64(<vscale x 2 x double> %a)

  ; FIXME reduce.fmul SDNode needs to be expanded
  ;%fmul = call double @llvm.vector.reduce.fmul.f64.nxv2f64(double 1.0, <vscale x 2 x double> %a)

  %accum1 = fadd double %fadd, %unord_fadd
  %accum2 = fadd double %accum1, %fmax
  %accum3 = fadd double %accum2, %fmin

  ret double %accum3
}

declare i32 @llvm.vector.reduce.add.nxv2i32(<vscale x 2 x i32>)
declare i32 @llvm.vector.reduce.smax.nxv2i32(<vscale x 2 x i32>)
declare i32 @llvm.vector.reduce.smin.nxv2i32(<vscale x 2 x i32>)
declare i32 @llvm.vector.reduce.umax.nxv2i32(<vscale x 2 x i32>)
declare i32 @llvm.vector.reduce.umin.nxv2i32(<vscale x 2 x i32>)
declare i32 @llvm.vector.reduce.and.nxv2i32(<vscale x 2 x i32>)
declare i32 @llvm.vector.reduce.or.nxv2i32(<vscale x 2 x i32>)
declare i32 @llvm.vector.reduce.xor.nxv2i32(<vscale x 2 x i32>)
;declare i32 @llvm.vector.reduce.mul.nxv2i32(<vscale x 2 x i32>)

declare i64 @llvm.vector.reduce.add.nxv1i64(<vscale x 1 x i64>)
declare i64 @llvm.vector.reduce.smax.nxv1i64(<vscale x 1 x i64>)
declare i64 @llvm.vector.reduce.smin.nxv1i64(<vscale x 1 x i64>)
declare i64 @llvm.vector.reduce.umax.nxv1i64(<vscale x 1 x i64>)
declare i64 @llvm.vector.reduce.umin.nxv1i64(<vscale x 1 x i64>)
declare i64 @llvm.vector.reduce.and.nxv1i64(<vscale x 1 x i64>)
declare i64 @llvm.vector.reduce.or.nxv1i64(<vscale x 1 x i64>)
declare i64 @llvm.vector.reduce.xor.nxv1i64(<vscale x 1 x i64>)
;declare i64 @llvm.vector.reduce.mul.nxv1i64(<vscale x 1 x i64>)

declare i64 @llvm.vector.reduce.add.nxv2i64(<vscale x 2 x i64>)
declare i64 @llvm.vector.reduce.smax.nxv2i64(<vscale x 2 x i64>)
declare i64 @llvm.vector.reduce.smin.nxv2i64(<vscale x 2 x i64>)
declare i64 @llvm.vector.reduce.umax.nxv2i64(<vscale x 2 x i64>)
declare i64 @llvm.vector.reduce.umin.nxv2i64(<vscale x 2 x i64>)
declare i64 @llvm.vector.reduce.and.nxv2i64(<vscale x 2 x i64>)
declare i64 @llvm.vector.reduce.or.nxv2i64(<vscale x 2 x i64>)
declare i64 @llvm.vector.reduce.xor.nxv2i64(<vscale x 2 x i64>)
;declare i64 @llvm.vector.reduce.mul.nxv2i64(<vscale x 2 x i64>)

declare float @llvm.vector.reduce.fadd.f32.nxv2f32(float, <vscale x 2 x float>)
declare float @llvm.vector.reduce.fmax.nxv2f32(<vscale x 2 x float>)
declare float @llvm.vector.reduce.fmin.nxv2f32(<vscale x 2 x float>)
;declare float @llvm.vector.reduce.fmul.f32.nxv2f32(float, <vscale x 2 x float>)

declare double @llvm.vector.reduce.fadd.f64.nxv1f64(double, <vscale x 1 x double>)
declare double @llvm.vector.reduce.fmax.nxv1f64(<vscale x 1 x double>)
declare double @llvm.vector.reduce.fmin.nxv1f64(<vscale x 1 x double>)
;declare double @llvm.vector.reduce.fmul.f64.nxv1f64(double, <vscale x 1 x double>)

declare double @llvm.vector.reduce.fadd.f64.nxv2f64(double, <vscale x 2 x double>)
declare double @llvm.vector.reduce.fmax.nxv2f64(<vscale x 2 x double>)
declare double @llvm.vector.reduce.fmin.nxv2f64(<vscale x 2 x double>)
;declare double @llvm.vector.reduce.fmul.f64.nxv2f64(double, <vscale x 2 x double>)
