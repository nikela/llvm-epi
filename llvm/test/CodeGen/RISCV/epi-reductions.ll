; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+m,+f,+d,+a,+c,+experimental-v \
; RUN:    -verify-machineinstrs < %s | FileCheck %s

define i32 @red_nxv2i32(<vscale x 2 x i32> %a) nounwind {
; CHECK-LABEL: red_nxv2i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a0, zero, e32,m1
; CHECK-NEXT:    vmv.s.x v1, zero
; CHECK-NEXT:    vredsum.vs v2, v16, v1
; CHECK-NEXT:    vmv.x.s a6, v2
; CHECK-NEXT:    vredmax.vs v2, v16, v1
; CHECK-NEXT:    vmv.x.s a7, v2
; CHECK-NEXT:    vredmin.vs v2, v16, v1
; CHECK-NEXT:    vmv.x.s t0, v2
; CHECK-NEXT:    vredmaxu.vs v2, v16, v1
; CHECK-NEXT:    vmv.x.s a3, v2
; CHECK-NEXT:    vredminu.vs v2, v16, v1
; CHECK-NEXT:    vmv.x.s a4, v2
; CHECK-NEXT:    vredand.vs v2, v16, v1
; CHECK-NEXT:    vmv.x.s a5, v2
; CHECK-NEXT:    vredor.vs v2, v16, v1
; CHECK-NEXT:    vmv.x.s a0, v2
; CHECK-NEXT:    vredxor.vs v1, v16, v1
; CHECK-NEXT:    vmv.x.s a1, v1
; CHECK-NEXT:    add a2, a6, a7
; CHECK-NEXT:    addw a2, a2, t0
; CHECK-NEXT:    add a2, a2, a3
; CHECK-NEXT:    add a2, a2, a4
; CHECK-NEXT:    add a2, a2, a5
; CHECK-NEXT:    add a0, a0, a2
; CHECK-NEXT:    addw a0, a0, a1
; CHECK-NEXT:    ret
  %add = call i32 @llvm.vector.reduce.add.nxv2i32(<vscale x 2 x i32> %a)
  %smax = call i32 @llvm.vector.reduce.smax.nxv2i32(<vscale x 2 x i32> %a)
  %smin = call i32 @llvm.vector.reduce.smin.nxv2i32(<vscale x 2 x i32> %a)
  %umax = call i32 @llvm.vector.reduce.umax.nxv2i32(<vscale x 2 x i32> %a)
  %umin = call i32 @llvm.vector.reduce.umin.nxv2i32(<vscale x 2 x i32> %a)
  %and = call i32 @llvm.vector.reduce.and.nxv2i32(<vscale x 2 x i32> %a)
  %or = call i32 @llvm.vector.reduce.or.nxv2i32(<vscale x 2 x i32> %a)
  %xor = call i32 @llvm.vector.reduce.xor.nxv2i32(<vscale x 2 x i32> %a)

  ; FIXME reduce.mul SDNode needs to be expanded
  ;%mul = call i32 @llvm.vector.reduce.mul.nxv2i32(<vscale x 2 x i32> %a)

  %accum1 = add i32 %add, %smax
  %accum2 = add i32 %accum1, %smin
  %accum3 = add i32 %accum2, %umax
  %accum4 = add i32 %accum3, %umin
  %accum5 = add i32 %accum4, %and
  %accum6 = add i32 %accum5, %or
  %accum7 = add i32 %accum6, %xor

  ret i32 %accum7
}

define i64 @red_nxv1i64(<vscale x 1 x i64> %a) nounwind {
; CHECK-LABEL: red_nxv1i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a0, zero, e64,m1
; CHECK-NEXT:    vmv.s.x v1, zero
; CHECK-NEXT:    vredsum.vs v2, v16, v1
; CHECK-NEXT:    vmv.x.s a6, v2
; CHECK-NEXT:    vredmax.vs v2, v16, v1
; CHECK-NEXT:    vmv.x.s a7, v2
; CHECK-NEXT:    vredmin.vs v2, v16, v1
; CHECK-NEXT:    vmv.x.s t0, v2
; CHECK-NEXT:    vredmaxu.vs v2, v16, v1
; CHECK-NEXT:    vmv.x.s a3, v2
; CHECK-NEXT:    vredminu.vs v2, v16, v1
; CHECK-NEXT:    vmv.x.s a4, v2
; CHECK-NEXT:    vredand.vs v2, v16, v1
; CHECK-NEXT:    vmv.x.s a5, v2
; CHECK-NEXT:    vredor.vs v2, v16, v1
; CHECK-NEXT:    vmv.x.s a0, v2
; CHECK-NEXT:    vredxor.vs v1, v16, v1
; CHECK-NEXT:    vmv.x.s a1, v1
; CHECK-NEXT:    add a2, a6, a7
; CHECK-NEXT:    add a2, a2, t0
; CHECK-NEXT:    add a2, a2, a3
; CHECK-NEXT:    add a2, a2, a4
; CHECK-NEXT:    add a2, a2, a5
; CHECK-NEXT:    add a0, a0, a2
; CHECK-NEXT:    add a0, a0, a1
; CHECK-NEXT:    ret
  %add = call i64 @llvm.vector.reduce.add.nxv1i64(<vscale x 1 x i64> %a)
  %smax = call i64 @llvm.vector.reduce.smax.nxv1i64(<vscale x 1 x i64> %a)
  %smin = call i64 @llvm.vector.reduce.smin.nxv1i64(<vscale x 1 x i64> %a)
  %umax = call i64 @llvm.vector.reduce.umax.nxv1i64(<vscale x 1 x i64> %a)
  %umin = call i64 @llvm.vector.reduce.umin.nxv1i64(<vscale x 1 x i64> %a)
  %and = call i64 @llvm.vector.reduce.and.nxv1i64(<vscale x 1 x i64> %a)
  %or = call i64 @llvm.vector.reduce.or.nxv1i64(<vscale x 1 x i64> %a)
  %xor = call i64 @llvm.vector.reduce.xor.nxv1i64(<vscale x 1 x i64> %a)

  ; FIXME reduce.mul SDNode needs to be expanded
  ;%mul = call i64 @llvm.vector.reduce.mul.nxv1i64(<vscale x 1 x i64> %a)

  %accum1 = add i64 %add, %smax
  %accum2 = add i64 %accum1, %smin
  %accum3 = add i64 %accum2, %umax
  %accum4 = add i64 %accum3, %umin
  %accum5 = add i64 %accum4, %and
  %accum6 = add i64 %accum5, %or
  %accum7 = add i64 %accum6, %xor

  ret i64 %accum7
}

define i64 @red_nxv2i64(<vscale x 2 x i64> %a) nounwind {
; CHECK-LABEL: red_nxv2i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a0, zero, e64,m1
; CHECK-NEXT:    vmv.s.x v2, zero
; CHECK-NEXT:    vsetvli a0, zero, e64,m2
; CHECK-NEXT:    vredsum.vs v4, v16, v2
; CHECK-NEXT:    vsetvli zero, zero, e64,m1
; CHECK-NEXT:    vmv.x.s a6, v4
; CHECK-NEXT:    vsetvli a1, zero, e64,m2
; CHECK-NEXT:    vredmax.vs v4, v16, v2
; CHECK-NEXT:    vsetvli zero, zero, e64,m1
; CHECK-NEXT:    vmv.x.s a7, v4
; CHECK-NEXT:    vsetvli a2, zero, e64,m2
; CHECK-NEXT:    vredmin.vs v4, v16, v2
; CHECK-NEXT:    vsetvli zero, zero, e64,m1
; CHECK-NEXT:    vmv.x.s t0, v4
; CHECK-NEXT:    vsetvli a3, zero, e64,m2
; CHECK-NEXT:    vredmaxu.vs v4, v16, v2
; CHECK-NEXT:    vsetvli zero, zero, e64,m1
; CHECK-NEXT:    vmv.x.s a3, v4
; CHECK-NEXT:    vsetvli a4, zero, e64,m2
; CHECK-NEXT:    vredminu.vs v4, v16, v2
; CHECK-NEXT:    vsetvli zero, zero, e64,m1
; CHECK-NEXT:    vmv.x.s a4, v4
; CHECK-NEXT:    vsetvli a5, zero, e64,m2
; CHECK-NEXT:    vredand.vs v4, v16, v2
; CHECK-NEXT:    vsetvli zero, zero, e64,m1
; CHECK-NEXT:    vmv.x.s a5, v4
; CHECK-NEXT:    vsetvli a0, zero, e64,m2
; CHECK-NEXT:    vredor.vs v4, v16, v2
; CHECK-NEXT:    vsetvli zero, zero, e64,m1
; CHECK-NEXT:    vmv.x.s a0, v4
; CHECK-NEXT:    vsetvli a1, zero, e64,m2
; CHECK-NEXT:    vredxor.vs v4, v16, v2
; CHECK-NEXT:    vsetvli zero, zero, e64,m1
; CHECK-NEXT:    vmv.x.s a1, v4
; CHECK-NEXT:    add a2, a6, a7
; CHECK-NEXT:    add a2, a2, t0
; CHECK-NEXT:    add a2, a2, a3
; CHECK-NEXT:    add a2, a2, a4
; CHECK-NEXT:    add a2, a2, a5
; CHECK-NEXT:    add a0, a0, a2
; CHECK-NEXT:    add a0, a0, a1
; CHECK-NEXT:    ret
  %add = call i64 @llvm.vector.reduce.add.nxv2i64(<vscale x 2 x i64> %a)
  %smax = call i64 @llvm.vector.reduce.smax.nxv2i64(<vscale x 2 x i64> %a)
  %smin = call i64 @llvm.vector.reduce.smin.nxv2i64(<vscale x 2 x i64> %a)
  %umax = call i64 @llvm.vector.reduce.umax.nxv2i64(<vscale x 2 x i64> %a)
  %umin = call i64 @llvm.vector.reduce.umin.nxv2i64(<vscale x 2 x i64> %a)
  %and = call i64 @llvm.vector.reduce.and.nxv2i64(<vscale x 2 x i64> %a)
  %or = call i64 @llvm.vector.reduce.or.nxv2i64(<vscale x 2 x i64> %a)
  %xor = call i64 @llvm.vector.reduce.xor.nxv2i64(<vscale x 2 x i64> %a)

  ; FIXME reduce.mul SDNode needs to be expanded
  ;%mul = call i64 @llvm.vector.reduce.mul.nxv2i64(<vscale x 2 x i64> %a)

  %accum1 = add i64 %add, %smax
  %accum2 = add i64 %accum1, %smin
  %accum3 = add i64 %accum2, %umax
  %accum4 = add i64 %accum3, %umin
  %accum5 = add i64 %accum4, %and
  %accum6 = add i64 %accum5, %or
  %accum7 = add i64 %accum6, %xor

  ret i64 %accum7
}

define float @red_nxv2f32(<vscale x 2 x float> %a) nounwind {
; CHECK-LABEL: red_nxv2f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(.LCPI3_0)
; CHECK-NEXT:    flw ft0, %lo(.LCPI3_0)(a0)
; CHECK-NEXT:    vsetvli a0, zero, e32,m1
; CHECK-NEXT:    vfmv.s.f v1, ft0
; CHECK-NEXT:    vfredosum.vs v1, v16, v1
; CHECK-NEXT:    vfmv.f.s ft1, v1
; CHECK-NEXT:    fmv.w.x ft2, zero
; CHECK-NEXT:    vfmv.s.f v1, ft2
; CHECK-NEXT:    vfredsum.vs v2, v16, v1
; CHECK-NEXT:    vfmv.f.s ft2, v2
; CHECK-NEXT:    fadd.s ft0, ft2, ft0
; CHECK-NEXT:    vfredmax.vs v2, v16, v1
; CHECK-NEXT:    vfmv.f.s ft2, v2
; CHECK-NEXT:    vfredmin.vs v1, v16, v1
; CHECK-NEXT:    vfmv.f.s ft3, v1
; CHECK-NEXT:    fadd.s ft0, ft1, ft0
; CHECK-NEXT:    fadd.s ft0, ft0, ft2
; CHECK-NEXT:    fadd.s ft0, ft0, ft3
; CHECK-NEXT:    fmv.x.w a0, ft0
; CHECK-NEXT:    ret
  %fadd = call float @llvm.vector.reduce.fadd.f32.nxv2f32(float 4.0, <vscale x 2 x float> %a)
  %unord_fadd = call reassoc float @llvm.vector.reduce.fadd.f32.nxv2f32(float 4.0, <vscale x 2 x float> %a) ; unordered reduction
  %fmax = call float @llvm.vector.reduce.fmax.nxv2f32(<vscale x 2 x float> %a)
  %fmin = call float @llvm.vector.reduce.fmin.nxv2f32(<vscale x 2 x float> %a)

  ; FIXME reduce.fmul SDNode needs to be expanded
  ;%fmul = call float @llvm.vector.reduce.fmul.f32.nxv2f32(float 1.0, <vscale x 2 x float> %a)

  %accum1 = fadd float %fadd, %unord_fadd
  %accum2 = fadd float %accum1, %fmax
  %accum3 = fadd float %accum2, %fmin

  ret float %accum3
}

define double @red_nxv1f64(<vscale x 1 x double> %a) nounwind {
; CHECK-LABEL: red_nxv1f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(.LCPI4_0)
; CHECK-NEXT:    fld ft0, %lo(.LCPI4_0)(a0)
; CHECK-NEXT:    vsetvli a0, zero, e64,m1
; CHECK-NEXT:    vfmv.s.f v1, ft0
; CHECK-NEXT:    vfredosum.vs v1, v16, v1
; CHECK-NEXT:    vfmv.f.s ft1, v1
; CHECK-NEXT:    fmv.d.x ft2, zero
; CHECK-NEXT:    vfmv.s.f v1, ft2
; CHECK-NEXT:    vfredsum.vs v2, v16, v1
; CHECK-NEXT:    vfmv.f.s ft2, v2
; CHECK-NEXT:    fadd.d ft0, ft2, ft0
; CHECK-NEXT:    vfredmax.vs v2, v16, v1
; CHECK-NEXT:    vfmv.f.s ft2, v2
; CHECK-NEXT:    vfredmin.vs v1, v16, v1
; CHECK-NEXT:    vfmv.f.s ft3, v1
; CHECK-NEXT:    fadd.d ft0, ft1, ft0
; CHECK-NEXT:    fadd.d ft0, ft0, ft2
; CHECK-NEXT:    fadd.d ft0, ft0, ft3
; CHECK-NEXT:    fmv.x.d a0, ft0
; CHECK-NEXT:    ret
  %fadd = call double @llvm.vector.reduce.fadd.f64.nxv1f64(double 4.0, <vscale x 1 x double> %a)
  %unord_fadd = call reassoc double @llvm.vector.reduce.fadd.f64.nxv1f64(double 4.0, <vscale x 1 x double> %a) ; unordered reduction
  %fmax = call double @llvm.vector.reduce.fmax.nxv1f64(<vscale x 1 x double> %a)
  %fmin = call double @llvm.vector.reduce.fmin.nxv1f64(<vscale x 1 x double> %a)

  ; FIXME reduce.fmul SDNode needs to be expanded
  ;%fmul = call double @llvm.vector.reduce.fmul.f64.nxv1f64(double 1.0, <vscale x 1 x double> %a)

  %accum1 = fadd double %fadd, %unord_fadd
  %accum2 = fadd double %accum1, %fmax
  %accum3 = fadd double %accum2, %fmin

  ret double %accum3
}

define double @red_nxv2f64(<vscale x 2 x double> %a) nounwind {
; CHECK-LABEL: red_nxv2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    lui a0, %hi(.LCPI5_0)
; CHECK-NEXT:    fld ft0, %lo(.LCPI5_0)(a0)
; CHECK-NEXT:    vsetvli a0, zero, e64,m1
; CHECK-NEXT:    vfmv.s.f v2, ft0
; CHECK-NEXT:    vsetvli a0, zero, e64,m2
; CHECK-NEXT:    vfredosum.vs v4, v16, v2
; CHECK-NEXT:    vsetvli zero, zero, e64,m1
; CHECK-NEXT:    vfmv.f.s ft1, v4
; CHECK-NEXT:    fmv.d.x ft2, zero
; CHECK-NEXT:    vfmv.s.f v2, ft2
; CHECK-NEXT:    vsetvli a0, zero, e64,m2
; CHECK-NEXT:    vfredsum.vs v4, v16, v2
; CHECK-NEXT:    vsetvli zero, zero, e64,m1
; CHECK-NEXT:    vfmv.f.s ft2, v4
; CHECK-NEXT:    fadd.d ft0, ft2, ft0
; CHECK-NEXT:    vsetvli a0, zero, e64,m2
; CHECK-NEXT:    vfredmax.vs v4, v16, v2
; CHECK-NEXT:    vsetvli zero, zero, e64,m1
; CHECK-NEXT:    vfmv.f.s ft2, v4
; CHECK-NEXT:    vsetvli a0, zero, e64,m2
; CHECK-NEXT:    vfredmin.vs v4, v16, v2
; CHECK-NEXT:    vsetvli zero, zero, e64,m1
; CHECK-NEXT:    vfmv.f.s ft3, v4
; CHECK-NEXT:    fadd.d ft0, ft1, ft0
; CHECK-NEXT:    fadd.d ft0, ft0, ft2
; CHECK-NEXT:    fadd.d ft0, ft0, ft3
; CHECK-NEXT:    fmv.x.d a0, ft0
; CHECK-NEXT:    ret
  %fadd = call double @llvm.vector.reduce.fadd.f64.nxv2f64(double 4.0, <vscale x 2 x double> %a)
  %unord_fadd = call reassoc double @llvm.vector.reduce.fadd.f64.nxv2f64(double 4.0, <vscale x 2 x double> %a) ; unordered reduction
  %fmax = call double @llvm.vector.reduce.fmax.nxv2f64(<vscale x 2 x double> %a)
  %fmin = call double @llvm.vector.reduce.fmin.nxv2f64(<vscale x 2 x double> %a)

  ; FIXME reduce.fmul SDNode needs to be expanded
  ;%fmul = call double @llvm.vector.reduce.fmul.f64.nxv2f64(double 1.0, <vscale x 2 x double> %a)

  %accum1 = fadd double %fadd, %unord_fadd
  %accum2 = fadd double %accum1, %fmax
  %accum3 = fadd double %accum2, %fmin

  ret double %accum3
}

declare i32 @llvm.vector.reduce.add.nxv2i32(<vscale x 2 x i32>)
declare i32 @llvm.vector.reduce.smax.nxv2i32(<vscale x 2 x i32>)
declare i32 @llvm.vector.reduce.smin.nxv2i32(<vscale x 2 x i32>)
declare i32 @llvm.vector.reduce.umax.nxv2i32(<vscale x 2 x i32>)
declare i32 @llvm.vector.reduce.umin.nxv2i32(<vscale x 2 x i32>)
declare i32 @llvm.vector.reduce.and.nxv2i32(<vscale x 2 x i32>)
declare i32 @llvm.vector.reduce.or.nxv2i32(<vscale x 2 x i32>)
declare i32 @llvm.vector.reduce.xor.nxv2i32(<vscale x 2 x i32>)
;declare i32 @llvm.vector.reduce.mul.nxv2i32(<vscale x 2 x i32>)

declare i64 @llvm.vector.reduce.add.nxv1i64(<vscale x 1 x i64>)
declare i64 @llvm.vector.reduce.smax.nxv1i64(<vscale x 1 x i64>)
declare i64 @llvm.vector.reduce.smin.nxv1i64(<vscale x 1 x i64>)
declare i64 @llvm.vector.reduce.umax.nxv1i64(<vscale x 1 x i64>)
declare i64 @llvm.vector.reduce.umin.nxv1i64(<vscale x 1 x i64>)
declare i64 @llvm.vector.reduce.and.nxv1i64(<vscale x 1 x i64>)
declare i64 @llvm.vector.reduce.or.nxv1i64(<vscale x 1 x i64>)
declare i64 @llvm.vector.reduce.xor.nxv1i64(<vscale x 1 x i64>)
;declare i64 @llvm.vector.reduce.mul.nxv1i64(<vscale x 1 x i64>)

declare i64 @llvm.vector.reduce.add.nxv2i64(<vscale x 2 x i64>)
declare i64 @llvm.vector.reduce.smax.nxv2i64(<vscale x 2 x i64>)
declare i64 @llvm.vector.reduce.smin.nxv2i64(<vscale x 2 x i64>)
declare i64 @llvm.vector.reduce.umax.nxv2i64(<vscale x 2 x i64>)
declare i64 @llvm.vector.reduce.umin.nxv2i64(<vscale x 2 x i64>)
declare i64 @llvm.vector.reduce.and.nxv2i64(<vscale x 2 x i64>)
declare i64 @llvm.vector.reduce.or.nxv2i64(<vscale x 2 x i64>)
declare i64 @llvm.vector.reduce.xor.nxv2i64(<vscale x 2 x i64>)
;declare i64 @llvm.vector.reduce.mul.nxv2i64(<vscale x 2 x i64>)

declare float @llvm.vector.reduce.fadd.f32.nxv2f32(float, <vscale x 2 x float>)
declare float @llvm.vector.reduce.fmax.nxv2f32(<vscale x 2 x float>)
declare float @llvm.vector.reduce.fmin.nxv2f32(<vscale x 2 x float>)
;declare float @llvm.vector.reduce.fmul.f32.nxv2f32(float, <vscale x 2 x float>)

declare double @llvm.vector.reduce.fadd.f64.nxv1f64(double, <vscale x 1 x double>)
declare double @llvm.vector.reduce.fmax.nxv1f64(<vscale x 1 x double>)
declare double @llvm.vector.reduce.fmin.nxv1f64(<vscale x 1 x double>)
;declare double @llvm.vector.reduce.fmul.f64.nxv1f64(double, <vscale x 1 x double>)

declare double @llvm.vector.reduce.fadd.f64.nxv2f64(double, <vscale x 2 x double>)
declare double @llvm.vector.reduce.fmax.nxv2f64(<vscale x 2 x double>)
declare double @llvm.vector.reduce.fmin.nxv2f64(<vscale x 2 x double>)
;declare double @llvm.vector.reduce.fmul.f64.nxv2f64(double, <vscale x 2 x double>)
