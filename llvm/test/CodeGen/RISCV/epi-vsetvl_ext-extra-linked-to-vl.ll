; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+d,+v -verify-machineinstrs -O0 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=CHECK-O0 %s
; RUN: llc -mtriple=riscv64 -mattr=+d,+v -verify-machineinstrs -O2 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=CHECK-O2 %s

define void @test_extra_from_gvl(i64 %n, double* %a, double* %b, double* %c) {
; CHECK-O0-LABEL: test_extra_from_gvl:
; CHECK-O0:       # %bb.0: # %entry
; CHECK-O0-NEXT:    addi sp, sp, -64
; CHECK-O0-NEXT:    .cfi_def_cfa_offset 64
; CHECK-O0-NEXT:    sd a3, 16(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a2, 24(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a1, 32(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    mv a1, a0
; CHECK-O0-NEXT:    sd a1, 40(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    li a0, 25
; CHECK-O0-NEXT:    slt a0, a0, a1
; CHECK-O0-NEXT:    slli a0, a0, 9
; CHECK-O0-NEXT:    sd a0, 48(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    li a2, 1024
; CHECK-O0-NEXT:    li a0, 400
; CHECK-O0-NEXT:    sd a2, 56(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    blt a0, a1, .LBB0_2
; CHECK-O0-NEXT:  # %bb.1: # %entry
; CHECK-O0-NEXT:    ld a0, 48(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    sd a0, 56(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:  .LBB0_2: # %entry
; CHECK-O0-NEXT:    ld a1, 40(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a0, 56(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    sd a0, 0(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    li a0, 0
; CHECK-O0-NEXT:    mv a2, a0
; CHECK-O0-NEXT:    sd a2, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    blt a0, a1, .LBB0_4
; CHECK-O0-NEXT:    j .LBB0_3
; CHECK-O0-NEXT:  .LBB0_3: # %for.cond.cleanup
; CHECK-O0-NEXT:    addi sp, sp, 64
; CHECK-O0-NEXT:    ret
; CHECK-O0-NEXT:  .LBB0_4: # %for.body
; CHECK-O0-NEXT:    # =>This Inner Loop Header: Depth=1
; CHECK-O0-NEXT:    ld a1, 40(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a2, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a6, 0(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a3, 16(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a7, 24(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld t0, 32(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    sub a4, a1, a2
; CHECK-O0-NEXT:    ori a5, a6, 88
; CHECK-O0-NEXT:    vsetvl a0, a4, a5
; CHECK-O0-NEXT:    slli a5, a2, 3
; CHECK-O0-NEXT:    add t0, t0, a5
; CHECK-O0-NEXT:    # implicit-def: $v9
; CHECK-O0-NEXT:    vle64.v v9, (t0)
; CHECK-O0-NEXT:    vsetvli t0, a4, e64, m1, ta, mu
; CHECK-O0-NEXT:    add a7, a7, a5
; CHECK-O0-NEXT:    # implicit-def: $v10
; CHECK-O0-NEXT:    vle64.v v10, (a7)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vfadd.vv v8, v9, v10
; CHECK-O0-NEXT:    add a3, a3, a5
; CHECK-O0-NEXT:    ori a5, a6, 88
; CHECK-O0-NEXT:    vsetvl a4, a4, a5
; CHECK-O0-NEXT:    vse64.v v8, (a3)
; CHECK-O0-NEXT:    add a0, a0, a2
; CHECK-O0-NEXT:    mv a2, a0
; CHECK-O0-NEXT:    sd a2, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    blt a0, a1, .LBB0_4
; CHECK-O0-NEXT:    j .LBB0_3
;
; CHECK-O2-LABEL: test_extra_from_gvl:
; CHECK-O2:       # %bb.0: # %entry
; CHECK-O2-NEXT:    li a5, 400
; CHECK-O2-NEXT:    li a4, 1024
; CHECK-O2-NEXT:    blt a5, a0, .LBB0_2
; CHECK-O2-NEXT:  # %bb.1: # %entry
; CHECK-O2-NEXT:    li a4, 25
; CHECK-O2-NEXT:    slt a4, a4, a0
; CHECK-O2-NEXT:    slli a4, a4, 9
; CHECK-O2-NEXT:  .LBB0_2: # %entry
; CHECK-O2-NEXT:    blez a0, .LBB0_5
; CHECK-O2-NEXT:  # %bb.3: # %for.body.preheader
; CHECK-O2-NEXT:    li a5, 0
; CHECK-O2-NEXT:  .LBB0_4: # %for.body
; CHECK-O2-NEXT:    # =>This Inner Loop Header: Depth=1
; CHECK-O2-NEXT:    sub a6, a0, a5
; CHECK-O2-NEXT:    ori t0, a4, 88
; CHECK-O2-NEXT:    vsetvl a7, a6, t0
; CHECK-O2-NEXT:    slli t0, a5, 3
; CHECK-O2-NEXT:    add t1, a1, t0
; CHECK-O2-NEXT:    vle64.v v8, (t1)
; CHECK-O2-NEXT:    vsetvli t1, a6, e64, m1, ta, mu
; CHECK-O2-NEXT:    add t1, a2, t0
; CHECK-O2-NEXT:    vle64.v v9, (t1)
; CHECK-O2-NEXT:    vfadd.vv v8, v8, v9
; CHECK-O2-NEXT:    add t0, a3, t0
; CHECK-O2-NEXT:    ori t1, a4, 88
; CHECK-O2-NEXT:    vsetvl a6, a6, t1
; CHECK-O2-NEXT:    add a5, a7, a5
; CHECK-O2-NEXT:    vse64.v v8, (t0)
; CHECK-O2-NEXT:    blt a5, a0, .LBB0_4
; CHECK-O2-NEXT:  .LBB0_5: # %for.cond.cleanup
; CHECK-O2-NEXT:    ret
entry:
  %cmp1 = icmp sgt i64 %n, 400
  %cmp2 = icmp sgt i64 %n, 25
  %spec.select = select i1 %cmp2, i64 512, i64 0
  %extra.0 = select i1 %cmp1, i64 1024, i64 %spec.select
  %cmp25 = icmp sgt i64 %n, 0
  br i1 %cmp25, label %for.body, label %for.cond.cleanup

for.cond.cleanup:                                 ; preds = %for.body, %entry
  ret void

for.body:                                         ; preds = %entry, %for.body
  %i.026 = phi i64 [ %add, %for.body ], [ 0, %entry ]
  %sub = sub nsw i64 %n, %i.026
  %0 = tail call i64 @llvm.epi.vsetvl.ext(i64 %sub, i64 3, i64 0, i64 %extra.0)
  %arrayidx = getelementptr inbounds double, double* %a, i64 %i.026
  %1 = bitcast double* %arrayidx to <vscale x 1 x double>*
  %2 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %1, i64 %0)
  %3 = tail call i64 @llvm.epi.vsetvl(i64 %sub, i64 3, i64 0)
  %arrayidx6 = getelementptr inbounds double, double* %b, i64 %i.026
  %4 = bitcast double* %arrayidx6 to <vscale x 1 x double>*
  %5 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %4, i64 %3)
  %6 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %2, <vscale x 1 x double> %5, i64 %3)
  %arrayidx7 = getelementptr inbounds double, double* %c, i64 %i.026
  %7 = bitcast double* %arrayidx7 to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %6, <vscale x 1 x double>* %7, i64 %0)
  %add = add nsw i64 %0, %i.026
  %cmp = icmp slt i64 %add, %n
  br i1 %cmp, label %for.body, label %for.cond.cleanup
}

define void @test_extra_from_gvl_from_phi(i64 %n, double* %a, double* %b, double* %c) {
; CHECK-O0-LABEL: test_extra_from_gvl_from_phi:
; CHECK-O0:       # %bb.0: # %entry
; CHECK-O0-NEXT:    addi sp, sp, -96
; CHECK-O0-NEXT:    .cfi_def_cfa_offset 96
; CHECK-O0-NEXT:    sd a3, 48(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a2, 56(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a1, 64(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    mv a1, a0
; CHECK-O0-NEXT:    sd a1, 72(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    li a0, 25
; CHECK-O0-NEXT:    slt a0, a0, a1
; CHECK-O0-NEXT:    slli a0, a0, 9
; CHECK-O0-NEXT:    sd a0, 80(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    li a2, 1024
; CHECK-O0-NEXT:    li a0, 400
; CHECK-O0-NEXT:    sd a2, 88(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    blt a0, a1, .LBB1_2
; CHECK-O0-NEXT:  # %bb.1: # %entry
; CHECK-O0-NEXT:    ld a0, 80(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    sd a0, 88(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:  .LBB1_2: # %entry
; CHECK-O0-NEXT:    ld a1, 72(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a0, 88(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    sd a0, 32(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    li a0, 0
; CHECK-O0-NEXT:    mv a2, a0
; CHECK-O0-NEXT:    sd a2, 40(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    blt a0, a1, .LBB1_4
; CHECK-O0-NEXT:    j .LBB1_3
; CHECK-O0-NEXT:  .LBB1_3: # %for.cond.cleanup
; CHECK-O0-NEXT:    addi sp, sp, 96
; CHECK-O0-NEXT:    ret
; CHECK-O0-NEXT:  .LBB1_4: # %for.body
; CHECK-O0-NEXT:    # =>This Inner Loop Header: Depth=1
; CHECK-O0-NEXT:    ld a1, 72(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a2, 32(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a0, 40(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    sd a0, 0(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sub a0, a1, a0
; CHECK-O0-NEXT:    sd a0, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    ori a3, a2, 88
; CHECK-O0-NEXT:    vsetvl a0, a0, a3
; CHECK-O0-NEXT:    sd a2, 16(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    mv a2, a0
; CHECK-O0-NEXT:    sd a2, 24(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    bne a0, a1, .LBB1_6
; CHECK-O0-NEXT:    j .LBB1_5
; CHECK-O0-NEXT:  .LBB1_5: # %if.then6
; CHECK-O0-NEXT:    # in Loop: Header=BB1_4 Depth=1
; CHECK-O0-NEXT:    ld a0, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    vsetvli a0, a0, e64, m1, ta, mu, nt
; CHECK-O0-NEXT:    li a1, 512
; CHECK-O0-NEXT:    sd a1, 16(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a0, 24(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    j .LBB1_6
; CHECK-O0-NEXT:  .LBB1_6: # %if.end8
; CHECK-O0-NEXT:    # in Loop: Header=BB1_4 Depth=1
; CHECK-O0-NEXT:    ld a1, 72(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a2, 0(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a3, 48(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a6, 56(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a7, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld t0, 64(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a4, 16(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a0, 24(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    slli a5, a2, 3
; CHECK-O0-NEXT:    add t0, t0, a5
; CHECK-O0-NEXT:    # implicit-def: $v9
; CHECK-O0-NEXT:    ori t2, a4, 88
; CHECK-O0-NEXT:    vsetvl t1, a0, t2
; CHECK-O0-NEXT:    vle64.v v9, (t0)
; CHECK-O0-NEXT:    vsetvli a7, a7, e64, m1, ta, mu
; CHECK-O0-NEXT:    add a6, a6, a5
; CHECK-O0-NEXT:    # implicit-def: $v10
; CHECK-O0-NEXT:    vle64.v v10, (a6)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vfadd.vv v8, v9, v10
; CHECK-O0-NEXT:    add a3, a3, a5
; CHECK-O0-NEXT:    ori a5, a4, 88
; CHECK-O0-NEXT:    vsetvl a4, a0, a5
; CHECK-O0-NEXT:    vse64.v v8, (a3)
; CHECK-O0-NEXT:    add a0, a0, a2
; CHECK-O0-NEXT:    mv a2, a0
; CHECK-O0-NEXT:    sd a2, 40(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    blt a0, a1, .LBB1_4
; CHECK-O0-NEXT:    j .LBB1_3
;
; CHECK-O2-LABEL: test_extra_from_gvl_from_phi:
; CHECK-O2:       # %bb.0: # %entry
; CHECK-O2-NEXT:    li a5, 400
; CHECK-O2-NEXT:    li a4, 1024
; CHECK-O2-NEXT:    blt a5, a0, .LBB1_2
; CHECK-O2-NEXT:  # %bb.1: # %entry
; CHECK-O2-NEXT:    li a4, 25
; CHECK-O2-NEXT:    slt a4, a4, a0
; CHECK-O2-NEXT:    slli a4, a4, 9
; CHECK-O2-NEXT:  .LBB1_2: # %entry
; CHECK-O2-NEXT:    blez a0, .LBB1_7
; CHECK-O2-NEXT:  # %bb.3: # %for.body.preheader
; CHECK-O2-NEXT:    li a5, 0
; CHECK-O2-NEXT:    j .LBB1_5
; CHECK-O2-NEXT:  .LBB1_4: # %if.end8
; CHECK-O2-NEXT:    # in Loop: Header=BB1_5 Depth=1
; CHECK-O2-NEXT:    slli t1, a5, 3
; CHECK-O2-NEXT:    add t2, a1, t1
; CHECK-O2-NEXT:    ori t4, a7, 88
; CHECK-O2-NEXT:    vsetvl t3, a6, t4
; CHECK-O2-NEXT:    vle64.v v8, (t2)
; CHECK-O2-NEXT:    vsetvli t0, t0, e64, m1, ta, mu
; CHECK-O2-NEXT:    add t0, a2, t1
; CHECK-O2-NEXT:    vle64.v v9, (t0)
; CHECK-O2-NEXT:    vfadd.vv v8, v8, v9
; CHECK-O2-NEXT:    add t0, a3, t1
; CHECK-O2-NEXT:    ori t1, a7, 88
; CHECK-O2-NEXT:    vsetvl a7, a6, t1
; CHECK-O2-NEXT:    add a5, a6, a5
; CHECK-O2-NEXT:    vse64.v v8, (t0)
; CHECK-O2-NEXT:    bge a5, a0, .LBB1_7
; CHECK-O2-NEXT:  .LBB1_5: # %for.body
; CHECK-O2-NEXT:    # =>This Inner Loop Header: Depth=1
; CHECK-O2-NEXT:    sub t0, a0, a5
; CHECK-O2-NEXT:    ori a7, a4, 88
; CHECK-O2-NEXT:    vsetvl a6, t0, a7
; CHECK-O2-NEXT:    mv a7, a4
; CHECK-O2-NEXT:    bne a6, a0, .LBB1_4
; CHECK-O2-NEXT:  # %bb.6: # %if.then6
; CHECK-O2-NEXT:    # in Loop: Header=BB1_5 Depth=1
; CHECK-O2-NEXT:    vsetvli a6, t0, e64, m1, ta, mu, nt
; CHECK-O2-NEXT:    li a7, 512
; CHECK-O2-NEXT:    j .LBB1_4
; CHECK-O2-NEXT:  .LBB1_7: # %for.cond.cleanup
; CHECK-O2-NEXT:    ret
entry:
  %cmp1 = icmp sgt i64 %n, 400
  %cmp2 = icmp sgt i64 %n, 25
  %spec.select = select i1 %cmp2, i64 512, i64 0
  %extra.0 = select i1 %cmp1, i64 1024, i64 %spec.select
  %cmp33 = icmp sgt i64 %n, 0
  br i1 %cmp33, label %for.body, label %for.cond.cleanup

for.cond.cleanup:                                 ; preds = %if.end8, %entry
  ret void

for.body:                                         ; preds = %entry, %if.end8
  %i.034 = phi i64 [ %add, %if.end8 ], [ 0, %entry ]
  %sub = sub nsw i64 %n, %i.034
  %0 = tail call i64 @llvm.epi.vsetvl.ext(i64 %sub, i64 3, i64 0, i64 %extra.0)
  %cmp5 = icmp eq i64 %0, %n
  br i1 %cmp5, label %if.then6, label %if.end8

if.then6:                                         ; preds = %for.body
  %1 = tail call i64 @llvm.epi.vsetvl.ext(i64 %sub, i64 3, i64 0, i64 512)
  br label %if.end8

if.end8:                                          ; preds = %if.then6, %for.body
  %gvl.0 = phi i64 [ %1, %if.then6 ], [ %0, %for.body ]
  %arrayidx = getelementptr inbounds double, double* %a, i64 %i.034
  %2 = bitcast double* %arrayidx to <vscale x 1 x double>*
  %3 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %2, i64 %gvl.0)
  %4 = tail call i64 @llvm.epi.vsetvl(i64 %sub, i64 3, i64 0)
  %arrayidx10 = getelementptr inbounds double, double* %b, i64 %i.034
  %5 = bitcast double* %arrayidx10 to <vscale x 1 x double>*
  %6 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %5, i64 %4)
  %7 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %3, <vscale x 1 x double> %6, i64 %4)
  %arrayidx11 = getelementptr inbounds double, double* %c, i64 %i.034
  %8 = bitcast double* %arrayidx11 to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %7, <vscale x 1 x double>* %8, i64 %gvl.0)
  %add = add nsw i64 %gvl.0, %i.034
  %cmp = icmp slt i64 %add, %n
  br i1 %cmp, label %for.body, label %for.cond.cleanup
}

define void @test_extra_from_gvl_from_phi_from_temp_register(i64 %n, double* %a, double* %b, double* %c) {
; CHECK-O0-LABEL: test_extra_from_gvl_from_phi_from_temp_register:
; CHECK-O0:       # %bb.0: # %entry
; CHECK-O0-NEXT:    addi sp, sp, -96
; CHECK-O0-NEXT:    .cfi_def_cfa_offset 96
; CHECK-O0-NEXT:    sd a3, 48(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a2, 56(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a1, 64(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    mv a1, a0
; CHECK-O0-NEXT:    sd a1, 72(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    li a0, 25
; CHECK-O0-NEXT:    slt a0, a0, a1
; CHECK-O0-NEXT:    slli a0, a0, 9
; CHECK-O0-NEXT:    sd a0, 80(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    li a2, 1024
; CHECK-O0-NEXT:    li a0, 400
; CHECK-O0-NEXT:    sd a2, 88(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    blt a0, a1, .LBB2_2
; CHECK-O0-NEXT:  # %bb.1: # %entry
; CHECK-O0-NEXT:    ld a0, 80(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    sd a0, 88(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:  .LBB2_2: # %entry
; CHECK-O0-NEXT:    ld a1, 72(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a0, 88(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    sd a0, 32(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    li a0, 0
; CHECK-O0-NEXT:    mv a2, a0
; CHECK-O0-NEXT:    sd a2, 40(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    blt a0, a1, .LBB2_4
; CHECK-O0-NEXT:    j .LBB2_3
; CHECK-O0-NEXT:  .LBB2_3: # %for.cond.cleanup
; CHECK-O0-NEXT:    addi sp, sp, 96
; CHECK-O0-NEXT:    ret
; CHECK-O0-NEXT:  .LBB2_4: # %for.body
; CHECK-O0-NEXT:    # =>This Inner Loop Header: Depth=1
; CHECK-O0-NEXT:    ld a1, 72(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a2, 32(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a0, 40(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    sd a0, 0(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sub a0, a1, a0
; CHECK-O0-NEXT:    sd a0, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    ori a3, a2, 88
; CHECK-O0-NEXT:    vsetvl a0, a0, a3
; CHECK-O0-NEXT:    sd a2, 16(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    mv a2, a0
; CHECK-O0-NEXT:    sd a2, 24(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    bne a0, a1, .LBB2_6
; CHECK-O0-NEXT:    j .LBB2_5
; CHECK-O0-NEXT:  .LBB2_5: # %if.then6
; CHECK-O0-NEXT:    # in Loop: Header=BB2_4 Depth=1
; CHECK-O0-NEXT:    ld a0, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a1, 32(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a2, 0(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    sub a1, a1, a2
; CHECK-O0-NEXT:    ori a2, a1, 88
; CHECK-O0-NEXT:    vsetvl a0, a0, a2
; CHECK-O0-NEXT:    sd a1, 16(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    sd a0, 24(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    j .LBB2_6
; CHECK-O0-NEXT:  .LBB2_6: # %if.end9
; CHECK-O0-NEXT:    # in Loop: Header=BB2_4 Depth=1
; CHECK-O0-NEXT:    ld a1, 72(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a2, 0(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a3, 48(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a6, 56(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a7, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld t0, 64(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a4, 16(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    ld a0, 24(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    slli a5, a2, 3
; CHECK-O0-NEXT:    add t0, t0, a5
; CHECK-O0-NEXT:    # implicit-def: $v9
; CHECK-O0-NEXT:    ori t2, a4, 88
; CHECK-O0-NEXT:    vsetvl t1, a0, t2
; CHECK-O0-NEXT:    vle64.v v9, (t0)
; CHECK-O0-NEXT:    vsetvli a7, a7, e64, m1, ta, mu
; CHECK-O0-NEXT:    add a6, a6, a5
; CHECK-O0-NEXT:    # implicit-def: $v10
; CHECK-O0-NEXT:    vle64.v v10, (a6)
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vfadd.vv v8, v9, v10
; CHECK-O0-NEXT:    add a3, a3, a5
; CHECK-O0-NEXT:    ori a5, a4, 88
; CHECK-O0-NEXT:    vsetvl a4, a0, a5
; CHECK-O0-NEXT:    vse64.v v8, (a3)
; CHECK-O0-NEXT:    add a0, a0, a2
; CHECK-O0-NEXT:    mv a2, a0
; CHECK-O0-NEXT:    sd a2, 40(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    blt a0, a1, .LBB2_4
; CHECK-O0-NEXT:    j .LBB2_3
;
; CHECK-O2-LABEL: test_extra_from_gvl_from_phi_from_temp_register:
; CHECK-O2:       # %bb.0: # %entry
; CHECK-O2-NEXT:    li a5, 400
; CHECK-O2-NEXT:    li a4, 1024
; CHECK-O2-NEXT:    blt a5, a0, .LBB2_2
; CHECK-O2-NEXT:  # %bb.1: # %entry
; CHECK-O2-NEXT:    li a4, 25
; CHECK-O2-NEXT:    slt a4, a4, a0
; CHECK-O2-NEXT:    slli a4, a4, 9
; CHECK-O2-NEXT:  .LBB2_2: # %entry
; CHECK-O2-NEXT:    blez a0, .LBB2_7
; CHECK-O2-NEXT:  # %bb.3: # %for.body.preheader
; CHECK-O2-NEXT:    li a5, 0
; CHECK-O2-NEXT:    j .LBB2_5
; CHECK-O2-NEXT:  .LBB2_4: # %if.end9
; CHECK-O2-NEXT:    # in Loop: Header=BB2_5 Depth=1
; CHECK-O2-NEXT:    slli t1, a5, 3
; CHECK-O2-NEXT:    add t2, a1, t1
; CHECK-O2-NEXT:    ori t4, a7, 88
; CHECK-O2-NEXT:    vsetvl t3, a6, t4
; CHECK-O2-NEXT:    vle64.v v8, (t2)
; CHECK-O2-NEXT:    vsetvli t0, t0, e64, m1, ta, mu
; CHECK-O2-NEXT:    add t0, a2, t1
; CHECK-O2-NEXT:    vle64.v v9, (t0)
; CHECK-O2-NEXT:    vfadd.vv v8, v8, v9
; CHECK-O2-NEXT:    add t0, a3, t1
; CHECK-O2-NEXT:    ori t1, a7, 88
; CHECK-O2-NEXT:    vsetvl a7, a6, t1
; CHECK-O2-NEXT:    add a5, a6, a5
; CHECK-O2-NEXT:    vse64.v v8, (t0)
; CHECK-O2-NEXT:    bge a5, a0, .LBB2_7
; CHECK-O2-NEXT:  .LBB2_5: # %for.body
; CHECK-O2-NEXT:    # =>This Inner Loop Header: Depth=1
; CHECK-O2-NEXT:    sub t0, a0, a5
; CHECK-O2-NEXT:    ori a7, a4, 88
; CHECK-O2-NEXT:    vsetvl a6, t0, a7
; CHECK-O2-NEXT:    mv a7, a4
; CHECK-O2-NEXT:    bne a6, a0, .LBB2_4
; CHECK-O2-NEXT:  # %bb.6: # %if.then6
; CHECK-O2-NEXT:    # in Loop: Header=BB2_5 Depth=1
; CHECK-O2-NEXT:    sub a7, a4, a5
; CHECK-O2-NEXT:    ori t1, a7, 88
; CHECK-O2-NEXT:    vsetvl a6, t0, t1
; CHECK-O2-NEXT:    j .LBB2_4
; CHECK-O2-NEXT:  .LBB2_7: # %for.cond.cleanup
; CHECK-O2-NEXT:    ret
entry:
  %cmp1 = icmp sgt i64 %n, 400
  %cmp2 = icmp sgt i64 %n, 25
  %spec.select = select i1 %cmp2, i64 512, i64 0
  %extra.0 = select i1 %cmp1, i64 1024, i64 %spec.select
  %cmp36 = icmp sgt i64 %n, 0
  br i1 %cmp36, label %for.body, label %for.cond.cleanup

for.cond.cleanup:                                 ; preds = %if.end9, %entry
  ret void

for.body:                                         ; preds = %entry, %if.end9
  %i.037 = phi i64 [ %add, %if.end9 ], [ 0, %entry ]
  %sub = sub nsw i64 %n, %i.037
  %0 = tail call i64 @llvm.epi.vsetvl.ext(i64 %sub, i64 3, i64 0, i64 %extra.0)
  %cmp5 = icmp eq i64 %0, %n
  br i1 %cmp5, label %if.then6, label %if.end9

if.then6:                                         ; preds = %for.body
  %sub8 = sub nsw i64 %extra.0, %i.037
  %1 = tail call i64 @llvm.epi.vsetvl.ext(i64 %sub, i64 3, i64 0, i64 %sub8)
  br label %if.end9

if.end9:                                          ; preds = %if.then6, %for.body
  %gvl.0 = phi i64 [ %1, %if.then6 ], [ %0, %for.body ]
  %arrayidx = getelementptr inbounds double, double* %a, i64 %i.037
  %2 = bitcast double* %arrayidx to <vscale x 1 x double>*
  %3 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %2, i64 %gvl.0)
  %4 = tail call i64 @llvm.epi.vsetvl(i64 %sub, i64 3, i64 0)
  %arrayidx11 = getelementptr inbounds double, double* %b, i64 %i.037
  %5 = bitcast double* %arrayidx11 to <vscale x 1 x double>*
  %6 = tail call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* %5, i64 %4)
  %7 = tail call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double> %3, <vscale x 1 x double> %6, i64 %4)
  %arrayidx12 = getelementptr inbounds double, double* %c, i64 %i.037
  %8 = bitcast double* %arrayidx12 to <vscale x 1 x double>*
  tail call void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double> %7, <vscale x 1 x double>* %8, i64 %gvl.0)
  %add = add nsw i64 %gvl.0, %i.037
  %cmp = icmp slt i64 %add, %n
  br i1 %cmp, label %for.body, label %for.cond.cleanup
}

declare i64 @llvm.epi.vsetvl.ext(i64, i64, i64, i64)
declare <vscale x 1 x double> @llvm.epi.vload.nxv1f64(<vscale x 1 x double>* nocapture, i64)
declare i64 @llvm.epi.vsetvl(i64, i64, i64)
declare <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, i64)
declare void @llvm.epi.vstore.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>* nocapture, i64)
