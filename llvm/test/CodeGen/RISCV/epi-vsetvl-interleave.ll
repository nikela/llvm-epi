; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+f,+d,+v -verify-machineinstrs -O0 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=CHECK-O0 %s
; RUN: llc -mtriple=riscv64 -mattr=+f,+d,+v -verify-machineinstrs -O2 \
; RUN:    < %s -epi-pipeline | FileCheck --check-prefix=CHECK-O2 %s

@scratch = global i8 0, align 16

declare i64 @llvm.epi.vsetvl(
  i64, i64, i64);

declare <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>,
  i64);

declare <vscale x 2 x double> @llvm.epi.vfadd.nxv2f64.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>,
  i64);

declare <vscale x 2 x float> @llvm.epi.vfadd.nxv2f32.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>,
  i64);

declare <vscale x 1 x double> @llvm.epi.vload.nxv1f64(
  <vscale x 1 x double>*,
  i64);

declare <vscale x 2 x double> @llvm.epi.vload.nxv2f64(
  <vscale x 2 x double>*,
  i64);

declare <vscale x 2 x float> @llvm.epi.vload.nxv2f32(
  <vscale x 2 x float>*,
  i64);

declare void @llvm.epi.vstore.nxv1f64(
  <vscale x 1 x double>,
  <vscale x 1 x double>*,
  i64);

declare void @llvm.epi.vstore.nxv2f64(
  <vscale x 2 x double>,
  <vscale x 2 x double>*,
  i64);

declare void @llvm.epi.vstore.nxv2f32(
  <vscale x 2 x float>,
  <vscale x 2 x float>*,
  i64);

define void @test_vsetvl_interleave_sew(<vscale x 1 x double>* %vd, <vscale x 2 x float>* %vf, i64 signext %avl) nounwind
; CHECK-O0-LABEL: test_vsetvl_interleave_sew:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vsetvli zero, a2, e64, m1, ta, mu
; CHECK-O0-NEXT:    vle64.v v8, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v10
; CHECK-O0-NEXT:    vsetvli zero, a2, e32, m1, ta, mu
; CHECK-O0-NEXT:    vle32.v v10, (a1)
; CHECK-O0-NEXT:    # implicit-def: $v9
; CHECK-O0-NEXT:    vsetvli zero, a2, e64, m1, ta, mu
; CHECK-O0-NEXT:    vfadd.vv v9, v8, v8
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vsetvli zero, a2, e32, m1, ta, mu
; CHECK-O0-NEXT:    vfadd.vv v8, v10, v10
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    vsetvli zero, a2, e64, m1, ta, mu
; CHECK-O0-NEXT:    vse64.v v9, (a0)
; CHECK-O0-NEXT:    vsetvli zero, a2, e32, m1, ta, mu
; CHECK-O0-NEXT:    vse32.v v8, (a0)
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_interleave_sew:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    vsetvli zero, a2, e64, m1, ta, mu
; CHECK-O2-NEXT:    vle64.v v8, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a2, e32, m1, ta, mu
; CHECK-O2-NEXT:    vle32.v v9, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a2, e64, m1, ta, mu
; CHECK-O2-NEXT:    vfadd.vv v8, v8, v8
; CHECK-O2-NEXT:    vsetvli zero, a2, e32, m1, ta, mu
; CHECK-O2-NEXT:    vfadd.vv v9, v9, v9
; CHECK-O2-NEXT:    lui a0, %hi(scratch)
; CHECK-O2-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O2-NEXT:    vsetvli zero, a2, e64, m1, ta, mu
; CHECK-O2-NEXT:    vse64.v v8, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a2, e32, m1, ta, mu
; CHECK-O2-NEXT:    vse32.v v9, (a0)
; CHECK-O2-NEXT:    ret
{
  %gvl_d = call i64 @llvm.epi.vsetvl(
    i64 %avl, i64 3, i64 0)

  %gvl_f = call i64 @llvm.epi.vsetvl(
    i64 %avl, i64 2, i64 0)

  %vec_d = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(
    <vscale x 1 x double>* %vd,
    i64 %gvl_d)

  %vec_f = call <vscale x 2 x float> @llvm.epi.vload.nxv2f32(
    <vscale x 2 x float>* %vf,
    i64 %gvl_f)

  %add_d = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(
    <vscale x 1 x double> %vec_d,
    <vscale x 1 x double> %vec_d,
    i64 %gvl_d)

  %add_f = call <vscale x 2 x float> @llvm.epi.vfadd.nxv2f32.nxv2f32(
    <vscale x 2 x float> %vec_f,
    <vscale x 2 x float> %vec_f,
    i64 %gvl_f)

  %store_addr_d = bitcast i8* @scratch to <vscale x 1 x double>*

  %store_addr_f = bitcast i8* @scratch to <vscale x 2 x float>*

  call void @llvm.epi.vstore.nxv1f64(
    <vscale x 1 x double> %add_d,
    <vscale x 1 x double>* %store_addr_d,
    i64 %gvl_d)

  call void @llvm.epi.vstore.nxv2f32(
    <vscale x 2 x float> %add_f,
    <vscale x 2 x float>* %store_addr_f,
    i64 %gvl_f)

    ret void
}

define void @test_vsetvl_interleave_vlmul(<vscale x 1 x double>* %vm1, <vscale x 2 x double>* %vm2, i64 signext %avl) nounwind
; CHECK-O0-LABEL: test_vsetvl_interleave_vlmul:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    # implicit-def: $v8
; CHECK-O0-NEXT:    vsetvli zero, a2, e64, m1, ta, mu
; CHECK-O0-NEXT:    vle64.v v8, (a0)
; CHECK-O0-NEXT:    # implicit-def: $v12m2
; CHECK-O0-NEXT:    vsetvli zero, a2, e64, m2, ta, mu
; CHECK-O0-NEXT:    vle64.v v12, (a1)
; CHECK-O0-NEXT:    # implicit-def: $v10
; CHECK-O0-NEXT:    vsetvli zero, a2, e64, m1, ta, mu
; CHECK-O0-NEXT:    vfadd.vv v10, v8, v8
; CHECK-O0-NEXT:    # implicit-def: $v8m2
; CHECK-O0-NEXT:    vsetvli zero, a2, e64, m2, ta, mu
; CHECK-O0-NEXT:    vfadd.vv v8, v12, v12
; CHECK-O0-NEXT:    lui a0, %hi(scratch)
; CHECK-O0-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O0-NEXT:    vsetvli zero, a2, e64, m1, ta, mu
; CHECK-O0-NEXT:    vse64.v v10, (a0)
; CHECK-O0-NEXT:    vsetvli zero, a2, e64, m2, ta, mu
; CHECK-O0-NEXT:    vse64.v v8, (a0)
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_interleave_vlmul:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    vsetvli zero, a2, e64, m1, ta, mu
; CHECK-O2-NEXT:    vle64.v v8, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a2, e64, m2, ta, mu
; CHECK-O2-NEXT:    vle64.v v10, (a1)
; CHECK-O2-NEXT:    vsetvli zero, a2, e64, m1, ta, mu
; CHECK-O2-NEXT:    vfadd.vv v8, v8, v8
; CHECK-O2-NEXT:    vsetvli zero, a2, e64, m2, ta, mu
; CHECK-O2-NEXT:    vfadd.vv v10, v10, v10
; CHECK-O2-NEXT:    lui a0, %hi(scratch)
; CHECK-O2-NEXT:    addi a0, a0, %lo(scratch)
; CHECK-O2-NEXT:    vsetvli zero, a2, e64, m1, ta, mu
; CHECK-O2-NEXT:    vse64.v v8, (a0)
; CHECK-O2-NEXT:    vsetvli zero, a2, e64, m2, ta, mu
; CHECK-O2-NEXT:    vse64.v v10, (a0)
; CHECK-O2-NEXT:    ret
{
  %gvl_m1 = call i64 @llvm.epi.vsetvl(
    i64 %avl, i64 3, i64 0)

  %gvl_m2 = call i64 @llvm.epi.vsetvl(
    i64 %avl, i64 3, i64 1)

  %vec_m1 = call <vscale x 1 x double> @llvm.epi.vload.nxv1f64(
    <vscale x 1 x double>* %vm1,
    i64 %gvl_m1)

  %vec_m2 = call <vscale x 2 x double> @llvm.epi.vload.nxv2f64(
    <vscale x 2 x double>* %vm2,
    i64 %gvl_m2)

  %add_m1 = call <vscale x 1 x double> @llvm.epi.vfadd.nxv1f64.nxv1f64(
    <vscale x 1 x double> %vec_m1,
    <vscale x 1 x double> %vec_m1,
    i64 %gvl_m1)

  %add_m2 = call <vscale x 2 x double> @llvm.epi.vfadd.nxv2f64.nxv2f64(
    <vscale x 2 x double> %vec_m2,
    <vscale x 2 x double> %vec_m2,
    i64 %gvl_m2)

  %store_addr_m1 = bitcast i8* @scratch to <vscale x 1 x double>*

  %store_addr_m2 = bitcast i8* @scratch to <vscale x 2 x double>*

  call void @llvm.epi.vstore.nxv1f64(
    <vscale x 1 x double> %add_m1,
    <vscale x 1 x double>* %store_addr_m1,
    i64 %gvl_m1)

  call void @llvm.epi.vstore.nxv2f64(
    <vscale x 2 x double> %add_m2,
    <vscale x 2 x double>* %store_addr_m2,
    i64 %gvl_m2)

    ret void
}

define i64 @test_vsetvl_interleave_avl(i64 %avl, i64 %avl2, i64 %avl3, i64 %avl4) nounwind
; CHECK-O0-LABEL: test_vsetvl_interleave_avl:
; CHECK-O0:       # %bb.0:
; CHECK-O0-NEXT:    addi sp, sp, -16
; CHECK-O0-NEXT:    sd a3, 0(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    mv a3, a1
; CHECK-O0-NEXT:    ld a1, 0(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    sd a3, 8(sp) # 8-byte Folded Spill
; CHECK-O0-NEXT:    mv a3, a0
; CHECK-O0-NEXT:    ld a0, 8(sp) # 8-byte Folded Reload
; CHECK-O0-NEXT:    vsetvli a3, a3, e64, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a0, a0, e64, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a2, a2, e64, m1, ta, mu
; CHECK-O0-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O0-NEXT:    add a0, a0, a3
; CHECK-O0-NEXT:    add a0, a0, a2
; CHECK-O0-NEXT:    add a0, a0, a1
; CHECK-O0-NEXT:    addi sp, sp, 16
; CHECK-O0-NEXT:    ret
;
; CHECK-O2-LABEL: test_vsetvl_interleave_avl:
; CHECK-O2:       # %bb.0:
; CHECK-O2-NEXT:    vsetvli a0, a0, e64, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a1, a1, e64, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a2, a2, e64, m1, ta, mu
; CHECK-O2-NEXT:    vsetvli a3, a3, e64, m1, ta, mu
; CHECK-O2-NEXT:    add a0, a1, a0
; CHECK-O2-NEXT:    add a0, a0, a2
; CHECK-O2-NEXT:    add a0, a0, a3
; CHECK-O2-NEXT:    ret
{
  %1 = tail call i64 @llvm.epi.vsetvl(i64 %avl, i64 3, i64 0)
  %2 = tail call i64 @llvm.epi.vsetvl(i64 %avl2, i64 3, i64 0)
  %3 = tail call i64 @llvm.epi.vsetvl(i64 %avl3, i64 3, i64 0)
  %4 = tail call i64 @llvm.epi.vsetvl(i64 %avl4, i64 3, i64 0)
  %add = add nsw i64 %2, %1
  %add1 = add nsw i64 %add, %3
  %add2 = add nsw i64 %add1, %4
  ret i64 %add2
}
