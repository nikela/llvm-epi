//==-- epi-builtins.def - EPI RISCV-V Builtin function database --*- C++ -*-==//
//
//                     The LLVM Compiler Infrastructure
//
// This file is distributed under the University of Illinois Open Source
// License. See LICENSE.TXT for details.
//
//===----------------------------------------------------------------------===//
//
// This file defines builtins from EPI.
//
//===----------------------------------------------------------------------===//

// Each record of the class EPIBuiltin defines a collection of builtins (i.e.
// "def vadd : EPIBuiltin" will be used to define things like "vadd_1xi64",
// "vadd_2xi32", etc).
//
// The elements of this collection are defined by an instantiation process the
// range of which is specified by cross product of the LMUL attribute and every
// element in the attribute TypeRange. By default builtins have LMUL = [1, 2,
// 4, 8] so the process is repeated four times.
//
// LMUL represents the fact that the types of values used by that builtin are
// values generated by instructions that are executed under that LMUL. However,
// this does not mean the builtin is necessarily lowered into an instruction
// that executes under the specified LMUL. An example where this happens are
// loads and stores of masks. A mask like `__epi_8xi1` can be generated, for
// instance, by comparing two `__epi_8xi8` (this is LMUL=1) or comparing two
// `__epi_8xi16` (this is LMUL=2). The actual load or store, however, will be
// performed under LMUL=1 because mask registers are not grouped.
//
// TypeRange is a non-empty sequence of basic types:
//
//   c: signed char (8-bit)
//   s: short (16-bit)
//   i: int (32-bit)
//   l: long (64-bit)
//   f: float (32-bit)
//   d: double (64-bit)
//
// This way, given an LMUL, a record with a TypeRange "sil" will cause the
// definition of 3 builtins. Each type "t" in the TypeRange (in this example
// they are short, int, long) is used as a parameter that drives
// the definition of that particular builtin (for the given LMUL).
//
// During the instantiation, types can be transformed or modified using type
// transformers. Given a type "t" the following primitive type transformers can
// be applied to it to yield another type.
//
//   e: type of "t" as is (identity)
//   v: computes a vector type whose element type is "t" for the current LMUL
//   w: computes a vector type identical to what 'v' computes except for the
//      element type which is twice as wide as the element type of 'v'
//   m: computes a vector type identical to what 'v' computes except for the
//      element type which is bool
//   0: void type, ignores "t"
//   u: unsigned long, ignores "t"
//   s: signed long, ignores "t"
//
// So for instance if t is "i", i.e. int, then "e" will yield int again. "v"
// will yield an EPI vector type (assume LMUL=1), so __epi_2xi32. Accordingly
// "w" would yield __epi_2xi64.
//
// A type transformer can be prefixed by other non-primitive type transformers.
//
//   P: constructs a pointer to the current type
//   C: adds const to the type
//   K: requires the integer type to be a constant expression
//   U: given an integer type, computes its unsigned variant
//   I: given a vector type, compute the vector type with integer type
//      elements of the same width
//   F: given a vector type, compute the vector type with floating-point type
//      elements of the same width
//   T: given a vector type, compute a tuple type with a single element of
//      of the vector type. Given a tuple type, add an additional element to
//      the tuple, of the same type as the existing one. A tuple of a single
//      element (e.g Tv) is not valid, so the minimum valid tuple must have two
//      elements (e.g. TTv)
//
// Following with the example above, if t is "i", then "Ue" will yield unsigned
// int and "Fv" will yield __epi_2xf32 (again assuming LMUL=1), Fw would
// yield __epi_2xf64, etc.
//
// Each builtin is then defined by applying each type in TypeRange against the
// sequence of type transformers described in Suffix and Prototype.
//
// The name of the builtin is defined by the Name attribute (which defaults to
// the name of the class) appended (separated with an underscore) the Suffix
// attribute. For instance with Name="foo", Suffix = "v" and TypeRange = "il",
// the builtin generated will be __builtin_epi_foo_2xi32 and
// __builtin_epi_foo_1xi64 (under LMUL=1). If Suffix contains more than one
// type transformer (say "vv") each of the types is separated with an
// underscore as in "__builtin_epi_foo_1xi32_1xi32".
//
// The C/C++ prototype of the builtin is defined by the Prototype attribute.
// Prototype is a non-empty sequence of type transformers, the first of which
// is the return type of the builtin and the rest are the parameters of the
// builtin, in order. For instance if Prototype is "wvv" and TypeRange is "si"
// a first builtin will have type __epi_4xi32 (__epi_4xi16, __epi_4xi16) and the
// second builtin will have type __epi_2xi64 (__epi_2xi32, __epi_2xi32) (again
// under LMUL=1).
//
// There are a number of attributes that are used to constraint the number and
// shape of the builtins generated. Refer to the comments below for them.
class EPIBuiltin<string suffix, string prototype,
                 string type_range>
{
  // Base name that will be prepended __builtin_epi_ and appended the computed
  // Suffix.
  string Name = NAME;
  // If not empty, each instantiated builtin will have this appended after an
  // underscore (_). Suffix is instantiated like Prototype.
  string Suffix = suffix;
  // For each type described in TypeRange we instantiate this Prototype.
  string Prototype = prototype;
  // The different variants of the builtin, parameterised with a type.
  string TypeRange = type_range;

  // This builtin has a masked form.
  bit HasMask = 1;
  // If HasMask == 1, this flag states that this builtin has a first merge
  // operand.
  bit HasMergeOperand = 1;
  // This builtin has a granted vector length parameter in the last position.
  bit HasVL = 1;

  // Reads or writes "memory".
  bit HasSideEffects = 0;

  // This builtin is valid for the given LMUL.
  list<int> LMUL = [1, 2, 4, 8];

  // Means that we won't emit automatic clang CodeGen code for this builtin
  // and it will have to be provided manually. See IntrinsicTypes below.
  bit HasManualCodegen = 1;

  // The default lowering of clang codegen is an error diagnostic saying
  // that this builtin not supported.
  code ManualCodegen = [{ CGM.ErrorUnsupported(E, "EPI builtin");
                          return llvm::UndefValue::get(ResultType); }];
  code ManualCodegenMask = [{ CGM.ErrorUnsupported(E, "EPI builtin");
                              return llvm::UndefValue::get(ResultType); }];

  // We always assign to the ID variable the LLVM intrinsic identifier value.
  // This allows sharing manual code between builtins. In the case this is not
  // ideal (e.g. the code sets a different value unconditionally and a
  // compiler diagnoses a warning for this automatically emitted assignment)
  // setting this to zero will disable emitting the assignment.
  bit CodegenSetID = 1;

  // If HasManualCodegen = 0 then we emit automatic clang codegen. It describes
  // what types we have to use to obtain the specific LLVM intrinsic.
  //
  // -1 means the return type,
  // -2 the mask operand (only meaningful if HasMask)
  // otherwise, k >= 0 meaning the k-th operand (counting from zero) of the
  // codegen'd parameter of the unmasked version. k can't be the mask operand's
  // position.
  list<int> IntrinsicTypes = [];

  // If these names are not empty, this is the ID of the LLVM intrinsic
  // we want to lower to.
  string IntrinsicName = NAME;
  string IntrinsicNameMask = NAME # "_mask";
}

//===----------------------------------------------------------------------===//
// Basic classes with automatic codegen.
//===----------------------------------------------------------------------===//
class EPIBinBuiltin<string suffix, string prototype, string type_range>
                   : EPIBuiltin<suffix, prototype, type_range>
{
  let HasManualCodegen = 0;
  let IntrinsicTypes = [-1, 1, -2];
}

class EPIWidenBinBuiltin<string suffix, string prototype, string type_range>
                       : EPIBuiltin<suffix, prototype, type_range>
{
  let HasManualCodegen = 0;
  let IntrinsicTypes = [-1, 0, 1, -2];
}

class EPIBinMaskInBuiltin<string suffix, string prototype, string type_range>
                         : EPIBuiltin<suffix, prototype, type_range>
{
  let HasManualCodegen = 0;
  let IntrinsicTypes = [-1, 1, 2];
}

class EPIRelBuiltin<string suffix, string prototype, string type_range>
                   : EPIBuiltin<suffix, prototype, type_range>
{
  let HasManualCodegen = 0;
  let IntrinsicTypes = [-1, 0, 1];
}

class EPITerBuiltin<string suffix, string prototype, string type_range>
                   : EPIBuiltin<suffix, prototype, type_range>
{
  let HasManualCodegen = 0;
  let IntrinsicTypes = [-1, 1, -2];
}

class EPIWidenTerBuiltin<string suffix, string prototype, string type_range>
                   : EPIBuiltin<suffix, prototype, type_range>
{
  let HasManualCodegen = 0;
  let IntrinsicTypes = [-1, 0, 1, -2];
}


class EPIUnaBuiltin<string suffix, string prototype, string type_range>
                   : EPIBuiltin<suffix, prototype, type_range>
{
  let HasManualCodegen = 0;
  let IntrinsicTypes = [-1, 0, -2];
}

class EPIMaskUnaBuiltin<string suffix, string prototype, string type_range>
                         : EPIBuiltin<suffix, prototype, type_range>
{
  let HasManualCodegen = 0;
  let IntrinsicTypes = [-1];
}

class EPIMaskToIntBuiltin<string suffix, string prototype, string type_range>
                         : EPIBuiltin<suffix, prototype, type_range>
{
  let HasManualCodegen = 0;
  let IntrinsicTypes = [-1, 0];
}

class EPIMaskToScalarBuiltin<string suffix, string prototype, string type_range>
                            : EPIBuiltin<suffix, prototype, type_range>
{
  let HasManualCodegen = 0;
  let IntrinsicTypes = [0];
}

class EPIVecScalarBuiltin<string suffix, string prototype, string type_range>
                         : EPIBuiltin<suffix, prototype, type_range>
{
  let HasManualCodegen = 0;
  let IntrinsicTypes = [-1, 1, -2];
}

class EPIScalarToVecBuiltin<string suffix, string prototype,
                            string type_range>
                           : EPIBuiltin<suffix, prototype, type_range>
{
  let HasManualCodegen = 0;
  let IntrinsicTypes = [-1, 0];
}

class EPIScalarToScalarVecBuiltin<string suffix, string prototype,
                                  string type_range>
                                 : EPIBuiltin<suffix, prototype, type_range>
{
  let HasManualCodegen = 0;
  let IntrinsicTypes = [-1, 1];
}

class EPIVecToScalarBuiltin<string suffix, string prototype,
                            string type_range>
                           : EPIBuiltin<suffix, prototype, type_range>
{
  let HasManualCodegen = 0;
  let IntrinsicTypes = [-1, 0];
}

class EPINulBuiltin<string suffix, string prototype, string type_range>
                   : EPIBuiltin<suffix, prototype, type_range>
{
  let HasManualCodegen = 0;
  let IntrinsicTypes = [-1, -2];
}

//===----------------------------------------------------------------------===//
// Builtin definitions.
//===----------------------------------------------------------------------===//
let HasVL = 0, HasMask = 0, HasSideEffects = 1, LMUL = [1],
    ManualCodegen = [{}] in
{
  def vsetvl : EPIBuiltin<"", "UeUeKUeKUe", "l">;
  def vsetvlmax : EPIBuiltin<"", "UeKUeKUe", "l">;
}

let ManualCodegen = [{
  IntrinsicTypes = {ResultType};
  Ops[0] = Builder.CreateBitCast(Ops[0],
      llvm::PointerType::getUnqual(ResultType));
}], ManualCodegenMask = [{
  IntrinsicTypes = {ResultType, Ops[2]->getType()};
  Ops[1] = Builder.CreateBitCast(Ops[1],
      llvm::PointerType::getUnqual(ResultType));
}] in
{
  def vload : EPIBuiltin<"v", "vPCe", "csilfd">;
  def vload_nt : EPIBuiltin<"v", "vPCe", "csilfd">;
  let IntrinsicName = "vload", IntrinsicNameMask = "vload_mask" in
    def vload_unsigned : EPIBuiltin<"v", "vPCUe", "csil">;
  let IntrinsicName = "vload_nt", IntrinsicNameMask = "vload_nt_mask" in
    def vload_nt_unsigned : EPIBuiltin<"v", "vPCUe", "csil">;
}

let ManualCodegen = [{
  IntrinsicTypes = {ResultType};
  Ops[0] = Builder.CreateBitCast(Ops[0],
      llvm::PointerType::getUnqual(ResultType));
}], ManualCodegenMask = [{
  IntrinsicTypes = {ResultType, Ops[3]->getType()};
  Ops[1] = Builder.CreateBitCast(Ops[1],
      llvm::PointerType::getUnqual(ResultType));
}] in
{
  def vload_strided : EPIBuiltin<"v", "vPCes", "csilfd">;
  def vload_nt_strided : EPIBuiltin<"v", "vPCes", "csilfd">;
  let IntrinsicName = "vload_strided",
      IntrinsicNameMask = "vload_strided_mask" in
    def vload_strided_unsigned : EPIBuiltin<"v", "vPCUes", "csil">;
  let IntrinsicName = "vload_nt_strided",
      IntrinsicNameMask = "vload_nt_strided_mask" in
    def vload_nt_strided_unsigned : EPIBuiltin<"v", "vPCUes", "csil">;
}

let ManualCodegen = [{
  IntrinsicTypes = {ResultType, Ops[1]->getType()};
  Ops[0] = Builder.CreateBitCast(Ops[0],
      llvm::PointerType::getUnqual(ResultType));
}], ManualCodegenMask = [{
  IntrinsicTypes = {ResultType, Ops[2]->getType(), Ops[3]->getType()};
  Ops[1] = Builder.CreateBitCast(Ops[1],
      llvm::PointerType::getUnqual(ResultType));
}] in
{
  def vload_indexed : EPIBuiltin<"v", "vPCeIv", "csilfd">;
  def vload_nt_indexed : EPIBuiltin<"v", "vPCeIv", "csilfd">;
  let IntrinsicName = "vload_indexed",
      IntrinsicNameMask = "vload_indexed_mask" in
    def vload_indexed_unsigned : EPIBuiltin<"v", "vPCUeIv", "csil">;
  let IntrinsicName = "vload_nt_indexed",
      IntrinsicNameMask = "vload_nt_indexed_mask" in
    def vload_nt_indexed_unsigned : EPIBuiltin<"v", "vPCUeIv", "csil">;
}

// Load mask.
let Name = "vload", IntrinsicName = "", HasVL = 0, HasMask = 0, LMUL = [1],
    ManualCodegen = [{ {
        clang::CharUnits Align =
            CGM.getNaturalPointeeTypeAlignment(E->getArg(0)->getType());
        llvm::Type *MemType = ConvertTypeForMem(E->getType());
        Ops[0] = Builder.CreateBitCast(Ops[0],
                                       llvm::PointerType::getUnqual(MemType));
        llvm::Value *Load = Builder.CreateAlignedLoad(Ops[0], Align);
        return Load;
    } }] in
def vload_mask : EPIBuiltin<"m", "mPCUe", "csil">;

let HasMergeOperand = 0 in
{
  let ManualCodegen = [{
    assert(Ops.size() >= 2);
    std::swap(Ops[0], Ops[1]);
    Ops[1] = Builder.CreateBitCast(Ops[1],
        llvm::PointerType::getUnqual(Ops[0]->getType()));
    IntrinsicTypes = {Ops[0]->getType()};
  }], ManualCodegenMask = [{
    assert(Ops.size() >= 2);
    std::swap(Ops[0], Ops[1]);
    Ops[1] = Builder.CreateBitCast(Ops[1],
        llvm::PointerType::getUnqual(Ops[0]->getType()));
    IntrinsicTypes = {Ops[0]->getType(), Ops[2]->getType()};
  }] in
  {
    def vstore : EPIBuiltin<"v", "0Pev", "csilfd">;
    def vstore_nt : EPIBuiltin<"v", "0Pev", "csilfd">;
    let IntrinsicName = "vstore", IntrinsicNameMask = "vstore_mask" in
      def vstore_unsigned : EPIBuiltin<"v", "0PUev", "csil">;
    let IntrinsicName = "vstore_nt", IntrinsicNameMask = "vstore_nt_mask" in
      def vstore_nt_unsigned : EPIBuiltin<"v", "0PUev", "csil">;
  }

  let ManualCodegen = [{
    assert(Ops.size() >= 2);
    std::swap(Ops[0], Ops[1]);
    Ops[1] = Builder.CreateBitCast(Ops[1],
        llvm::PointerType::getUnqual(Ops[0]->getType()));
    IntrinsicTypes = {Ops[0]->getType()};
  }], ManualCodegenMask = [{
    assert(Ops.size() >= 2);
    std::swap(Ops[0], Ops[1]);
    Ops[1] = Builder.CreateBitCast(Ops[1],
        llvm::PointerType::getUnqual(Ops[0]->getType()));
    IntrinsicTypes = {Ops[0]->getType(), Ops[3]->getType()};
  }] in
  {
    def vstore_strided : EPIBuiltin<"v", "0Pevs", "csilfd">;
    def vstore_nt_strided : EPIBuiltin<"v", "0Pevs", "csilfd">;
    let IntrinsicName = "vstore_strided",
        IntrinsicNameMask = "vstore_strided_mask" in
      def vstore_strided_unsigned : EPIBuiltin<"v", "0PUevs", "csil">;
    let IntrinsicName = "vstore_nt_strided",
        IntrinsicNameMask = "vstore_nt_strided_mask" in
      def vstore_nt_strided_unsigned : EPIBuiltin<"v", "0PUevs", "csil">;
  }

  let ManualCodegen = [{
    assert(Ops.size() >= 2);
    std::swap(Ops[0], Ops[1]);
    Ops[1] = Builder.CreateBitCast(Ops[1],
        llvm::PointerType::getUnqual(Ops[0]->getType()));
    IntrinsicTypes = {Ops[0]->getType(), Ops[2]->getType()};
  }], ManualCodegenMask = [{
    assert(Ops.size() >= 2);
    std::swap(Ops[0], Ops[1]);
    Ops[1] = Builder.CreateBitCast(Ops[1],
        llvm::PointerType::getUnqual(Ops[0]->getType()));
    IntrinsicTypes = {Ops[0]->getType(), Ops[2]->getType(), Ops[3]->getType()};
  }] in
  {
    def vstore_indexed : EPIBuiltin<"v", "0PevIv", "csilfd">;
    def vstore_nt_indexed : EPIBuiltin<"v", "0PevIv", "csilfd">;
    let IntrinsicName = "vstore_indexed",
        IntrinsicNameMask = "vstore_indexed_mask" in
      def vstore_indexed_unsigned : EPIBuiltin<"v", "0PUevIv", "csil">;
    let IntrinsicName = "vstore_nt_indexed",
        IntrinsicNameMask = "vstore_nt_indexed_mask" in
      def vstore_nt_indexed_unsigned : EPIBuiltin<"v", "0PUevIv", "csil">;
  }

  // Store mask.
  let Name = "vstore", IntrinsicName = "", HasVL = 0, HasMask = 0, LMUL = [1],
  ManualCodegen = [{ {
    assert(Ops.size() == 2);
    clang::CharUnits Align =
      CGM.getNaturalPointeeTypeAlignment(E->getArg(0)->getType());
    llvm::Type *MemType = ConvertTypeForMem(E->getArg(1)->getType());
    std::swap(Ops[0], Ops[1]);
    // Ops[0] = Value
    // Ops[1] = Address
    Ops[1] = Builder.CreateBitCast(Ops[1],
        llvm::PointerType::getUnqual(MemType));
    return Builder.CreateAlignedStore(Ops[0], Ops[1], Align);
  } }] in
  def vstore_mask : EPIBuiltin<"m", "0PUem", "csil">;
}

/* Segmented load/stores */
let LMUL = [1] in {
let ManualCodegen = [{
{
IntrinsicTypes = { cast<StructType>(ResultType)->elements()[0] };

llvm::Function *F = CGM.getIntrinsic(ID, IntrinsicTypes);
llvm::Value *CallValue = Builder.CreateCall(F, Ops, "");
llvm::Value *V = llvm::UndefValue::get(ResultType);
for (unsigned I = 0, E = cast<StructType>(ResultType)->getNumElements();
     I != E; I++) {
  V = Builder.CreateInsertValue(V,
       Builder.CreateExtractValue(CallValue, {I}),
       {I});
}
if (ReturnValue.isNull())
  return V;
else
  return Builder.CreateStore(V, ReturnValue.getValue());
}
}], ManualCodegenMask = [{
{
IntrinsicTypes = { cast<StructType>(ResultType)->elements()[0],
                   Ops[2]->getType() };

// The merge argument needs unpacking.
llvm::SmallVector<Value*, 4> FlatOps;
for (unsigned I = 0, E = cast<StructType>(Ops[0]->getType())->getNumElements();
     I != E; I++) {
  FlatOps.push_back(Builder.CreateExtractValue(Ops[0], {I}));
}
FlatOps.append(Ops.begin() + 1, Ops.end());

llvm::Function *F = CGM.getIntrinsic(ID, IntrinsicTypes);
llvm::Value *CallValue = Builder.CreateCall(F, FlatOps, "");
llvm::Value *V = llvm::UndefValue::get(ResultType);
for (unsigned I = 0, E = cast<StructType>(ResultType)->getNumElements();
     I != E; I++) {
  V = Builder.CreateInsertValue(V,
       Builder.CreateExtractValue(CallValue, {I}),
       {I});
}
if (ReturnValue.isNull())
  return V;
else
  return Builder.CreateStore(V, ReturnValue.getValue());
}
}] in {
def vlseg2 : EPIBuiltin<"TTv", "TTvPCe", "csilfd">;
def vlseg3 : EPIBuiltin<"TTTv", "TTTvPCe", "csilfd">;
def vlseg4 : EPIBuiltin<"TTTTv", "TTTTvPCe", "csilfd">;
def vlseg5 : EPIBuiltin<"TTTTTv", "TTTTTvPCe", "csilfd">;
def vlseg6 : EPIBuiltin<"TTTTTTv", "TTTTTTvPCe", "csilfd">;
def vlseg7 : EPIBuiltin<"TTTTTTTv", "TTTTTTTvPCe", "csilfd">;
def vlseg8 : EPIBuiltin<"TTTTTTTTv", "TTTTTTTTvPCe", "csilfd">;
}

let ManualCodegen = [{
{
IntrinsicTypes = { cast<StructType>(ResultType)->elements()[0] };

llvm::Function *F = CGM.getIntrinsic(ID, IntrinsicTypes);
llvm::Value *CallValue = Builder.CreateCall(F, Ops, "");
llvm::Value *V = llvm::UndefValue::get(ResultType);
for (unsigned I = 0, E = cast<StructType>(ResultType)->getNumElements();
     I != E; I++) {
  V = Builder.CreateInsertValue(V,
       Builder.CreateExtractValue(CallValue, {I}),
       {I});
}
if (ReturnValue.isNull())
  return V;
else
  return Builder.CreateStore(V, ReturnValue.getValue());
}
}], ManualCodegenMask = [{
{
IntrinsicTypes = { cast<StructType>(ResultType)->elements()[0],
                   Ops[3]->getType() };

// The merge argument needs unpacking.
llvm::SmallVector<Value*, 4> FlatOps;
for (unsigned I = 0, E = cast<StructType>(Ops[0]->getType())->getNumElements();
     I != E; I++) {
  FlatOps.push_back(Builder.CreateExtractValue(Ops[0], {I}));
}
FlatOps.append(Ops.begin() + 1, Ops.end());

llvm::Function *F = CGM.getIntrinsic(ID, IntrinsicTypes);
llvm::Value *CallValue = Builder.CreateCall(F, FlatOps, "");
llvm::Value *V = llvm::UndefValue::get(ResultType);
for (unsigned I = 0, E = cast<StructType>(ResultType)->getNumElements();
     I != E; I++) {
  V = Builder.CreateInsertValue(V,
       Builder.CreateExtractValue(CallValue, {I}),
       {I});
}
if (ReturnValue.isNull())
  return V;
else
  return Builder.CreateStore(V, ReturnValue.getValue());
}
}] in {
def vlseg2_strided : EPIBuiltin<"TTv", "TTvPCes", "csilfd">;
def vlseg3_strided : EPIBuiltin<"TTTv", "TTTvPCes", "csilfd">;
def vlseg4_strided : EPIBuiltin<"TTTTv", "TTTTvPCes", "csilfd">;
def vlseg5_strided : EPIBuiltin<"TTTTTv", "TTTTTvPCes", "csilfd">;
def vlseg6_strided : EPIBuiltin<"TTTTTTv", "TTTTTTvPCes", "csilfd">;
def vlseg7_strided : EPIBuiltin<"TTTTTTTv", "TTTTTTTvPCes", "csilfd">;
def vlseg8_strided : EPIBuiltin<"TTTTTTTTv", "TTTTTTTTvPCes", "csilfd">;
}

let ManualCodegen = [{
{
IntrinsicTypes = { cast<StructType>(ResultType)->elements()[0],
                   Ops[1]->getType() };

llvm::Function *F = CGM.getIntrinsic(ID, IntrinsicTypes);
llvm::Value *CallValue = Builder.CreateCall(F, Ops, "");
llvm::Value *V = llvm::UndefValue::get(ResultType);
for (unsigned I = 0, E = cast<StructType>(ResultType)->getNumElements();
     I != E; I++) {
  V = Builder.CreateInsertValue(V,
       Builder.CreateExtractValue(CallValue, {I}),
       {I});
}
if (ReturnValue.isNull())
  return V;
else
  return Builder.CreateStore(V, ReturnValue.getValue());
}
}], ManualCodegenMask = [{
{
IntrinsicTypes = { cast<StructType>(ResultType)->elements()[0],
                   Ops[2]->getType(), Ops[3]->getType() };

// The merge argument needs unpacking.
llvm::SmallVector<Value*, 4> FlatOps;
for (unsigned I = 0, E = cast<StructType>(Ops[0]->getType())->getNumElements();
     I != E; I++) {
  FlatOps.push_back(Builder.CreateExtractValue(Ops[0], {I}));
}
FlatOps.append(Ops.begin() + 1, Ops.end());

llvm::Function *F = CGM.getIntrinsic(ID, IntrinsicTypes);
llvm::Value *CallValue = Builder.CreateCall(F, FlatOps, "");
llvm::Value *V = llvm::UndefValue::get(ResultType);
for (unsigned I = 0, E = cast<StructType>(ResultType)->getNumElements();
     I != E; I++) {
  V = Builder.CreateInsertValue(V,
       Builder.CreateExtractValue(CallValue, {I}),
       {I});
}
if (ReturnValue.isNull())
  return V;
else
  return Builder.CreateStore(V, ReturnValue.getValue());
}
}] in {
def vlseg2_indexed : EPIBuiltin<"TTv", "TTvPCeIv", "csilfd">;
def vlseg3_indexed : EPIBuiltin<"TTTv", "TTTvPCeIv", "csilfd">;
def vlseg4_indexed : EPIBuiltin<"TTTTv", "TTTTvPCeIv", "csilfd">;
def vlseg5_indexed : EPIBuiltin<"TTTTTv", "TTTTTvPCeIv", "csilfd">;
def vlseg6_indexed : EPIBuiltin<"TTTTTTv", "TTTTTTvPCeIv", "csilfd">;
def vlseg7_indexed : EPIBuiltin<"TTTTTTTv", "TTTTTTTvPCeIv", "csilfd">;
def vlseg8_indexed : EPIBuiltin<"TTTTTTTTv", "TTTTTTTTvPCeIv", "csilfd">;
}

let HasMergeOperand = 0 in
{

let ManualCodegen = [{
{
IntrinsicTypes = { cast<StructType>(Ops[1]->getType())->elements()[0] };

llvm::SmallVector<Value*, 4> FlatOps;
for (unsigned I = 0, E = cast<StructType>(Ops[1]->getType())->getNumElements();
     I != E; I++) {
  FlatOps.push_back(Builder.CreateExtractValue(Ops[1], {I}));
}
FlatOps.push_back(Ops[0]);
FlatOps.append(Ops.begin() + 2, Ops.end());

std::swap(Ops, FlatOps);
}
}],
ManualCodegenMask = [{
{
IntrinsicTypes = { cast<StructType>(Ops[1]->getType())->elements()[0],
                   Ops[2]->getType() };

llvm::SmallVector<Value*, 4> FlatOps;
for (unsigned I = 0, E = cast<StructType>(Ops[1]->getType())->getNumElements();
     I != E; I++) {
  FlatOps.push_back(Builder.CreateExtractValue(Ops[1], {I}));
}
FlatOps.push_back(Ops[0]);
FlatOps.append(Ops.begin() + 2, Ops.end());

std::swap(Ops, FlatOps);
}
}] in {
def vsseg2 : EPIBuiltin<"TTv", "0PeTTv", "csilfd">;
def vsseg3 : EPIBuiltin<"TTTv", "0PeTTTv", "csilfd">;
def vsseg4 : EPIBuiltin<"TTTTv", "0PeTTTTv", "csilfd">;
def vsseg5 : EPIBuiltin<"TTTTTv", "0PeTTTTTv", "csilfd">;
def vsseg6 : EPIBuiltin<"TTTTTTv", "0PeTTTTTTv", "csilfd">;
def vsseg7 : EPIBuiltin<"TTTTTTTv", "0PeTTTTTTTv", "csilfd">;
def vsseg8 : EPIBuiltin<"TTTTTTTTv", "0PeTTTTTTTTv", "csilfd">;
}

let ManualCodegen = [{
{
IntrinsicTypes = { cast<StructType>(Ops[1]->getType())->elements()[0] };

llvm::SmallVector<Value*, 4> FlatOps;
for (unsigned I = 0, E = cast<StructType>(Ops[1]->getType())->getNumElements();
     I != E; I++) {
  FlatOps.push_back(Builder.CreateExtractValue(Ops[1], {I}));
}
FlatOps.push_back(Ops[0]);
FlatOps.append(Ops.begin() + 2, Ops.end());

std::swap(Ops, FlatOps);
}
}],
ManualCodegenMask = [{
{
IntrinsicTypes = { cast<StructType>(Ops[1]->getType())->elements()[0],
                   Ops[3]->getType() };

llvm::SmallVector<Value*, 4> FlatOps;
for (unsigned I = 0, E = cast<StructType>(Ops[1]->getType())->getNumElements();
     I != E; I++) {
  FlatOps.push_back(Builder.CreateExtractValue(Ops[1], {I}));
}
FlatOps.push_back(Ops[0]);
FlatOps.append(Ops.begin() + 2, Ops.end());

std::swap(Ops, FlatOps);
}
}] in {
def vsseg2_strided : EPIBuiltin<"TTv", "0PeTTvs", "csilfd">;
def vsseg3_strided : EPIBuiltin<"TTTv", "0PeTTTvs", "csilfd">;
def vsseg4_strided : EPIBuiltin<"TTTTv", "0PeTTTTvs", "csilfd">;
def vsseg5_strided : EPIBuiltin<"TTTTTv", "0PeTTTTTvs", "csilfd">;
def vsseg6_strided : EPIBuiltin<"TTTTTTv", "0PeTTTTTTvs", "csilfd">;
def vsseg7_strided : EPIBuiltin<"TTTTTTTv", "0PeTTTTTTTvs", "csilfd">;
def vsseg8_strided : EPIBuiltin<"TTTTTTTTv", "0PeTTTTTTTTvs", "csilfd">;
}

let ManualCodegen = [{
{
IntrinsicTypes = { cast<StructType>(Ops[1]->getType())->elements()[0],
                   Ops[2]->getType() };

llvm::SmallVector<Value*, 4> FlatOps;
for (unsigned I = 0, E = cast<StructType>(Ops[1]->getType())->getNumElements();
     I != E; I++) {
  FlatOps.push_back(Builder.CreateExtractValue(Ops[1], {I}));
}
FlatOps.push_back(Ops[0]);
FlatOps.append(Ops.begin() + 2, Ops.end());

std::swap(Ops, FlatOps);
}
}],
ManualCodegenMask = [{
{
IntrinsicTypes = { cast<StructType>(Ops[1]->getType())->elements()[0],
                   Ops[2]->getType(), Ops[3]->getType() };

llvm::SmallVector<Value*, 4> FlatOps;
for (unsigned I = 0, E = cast<StructType>(Ops[1]->getType())->getNumElements();
     I != E; I++) {
  FlatOps.push_back(Builder.CreateExtractValue(Ops[1], {I}));
}
FlatOps.push_back(Ops[0]);
FlatOps.append(Ops.begin() + 2, Ops.end());

std::swap(Ops, FlatOps);
}
}] in {
def vsseg2_indexed : EPIBuiltin<"TTv", "0PeTTvIv", "csilfd">;
def vsseg3_indexed : EPIBuiltin<"TTTv", "0PeTTTvIv", "csilfd">;
def vsseg4_indexed : EPIBuiltin<"TTTTv", "0PeTTTTvIv", "csilfd">;
def vsseg5_indexed : EPIBuiltin<"TTTTTv", "0PeTTTTTvIv", "csilfd">;
def vsseg6_indexed : EPIBuiltin<"TTTTTTv", "0PeTTTTTTvIv", "csilfd">;
def vsseg7_indexed : EPIBuiltin<"TTTTTTTv", "0PeTTTTTTTvIv", "csilfd">;
def vsseg8_indexed : EPIBuiltin<"TTTTTTTTv", "0PeTTTTTTTTvIv", "csilfd">;
}

} // HasMergeOperand = 0

} // segmented loads/stores

def vadd : EPIBinBuiltin<"v", "vvv", "csil">;
def vsub : EPIBinBuiltin<"v", "vvv", "csil">;

def vrsub : EPIBinBuiltin<"v", "vve", "csil">;

def vwaddu : EPIWidenBinBuiltin<"w", "wvv", "csi">;
def vwsubu : EPIWidenBinBuiltin<"w", "wvv", "csi">;

def vwadd : EPIWidenBinBuiltin<"w", "wvv", "csi">;
def vwsub : EPIWidenBinBuiltin<"w", "wvv", "csi">;

def vwaddu_w : EPIBinBuiltin<"w", "wwv", "csi">;
def vwsubu_w : EPIBinBuiltin<"w", "wwv", "csi">;

def vwadd_w : EPIBinBuiltin<"w", "wwv", "csi">;
def vwsub_w : EPIBinBuiltin<"w", "wwv", "csi">;

let HasMask = 0 in {
def vadc : EPIBinMaskInBuiltin<"v", "vvvm", "csil">;
def vmadc : EPIRelBuiltin<"v", "mvv", "csil">;
def vmadc_carry_in : EPIRelBuiltin<"v", "mvvm", "csil">;

def vsbc : EPIBinMaskInBuiltin<"v", "vvvm", "csil">;
def vmsbc : EPIRelBuiltin<"v", "mvv", "csil">;
def vmsbc_borrow_in : EPIRelBuiltin<"v", "mvvm", "csil">;
}

def vand : EPIBinBuiltin<"v", "vvv", "csil">;
def vor : EPIBinBuiltin<"v", "vvv", "csil">;
def vxor : EPIBinBuiltin<"v", "vvv", "csil">;

def vsll : EPIBinBuiltin<"v", "vvv", "csil">;
def vsrl : EPIBinBuiltin<"v", "vvv", "csil">;
def vsra : EPIBinBuiltin<"v", "vvv", "csil">;

// These do narrowing but typewise they are like their widen counterparts
def vnsrl : EPIWidenBinBuiltin<"v", "vwv", "csi">;
def vnsra : EPIWidenBinBuiltin<"v", "vwv", "csi">;

def vmseq : EPIRelBuiltin<"v", "mvv", "csil">;
def vmsne : EPIRelBuiltin<"v", "mvv", "csil">;
def vmsltu : EPIRelBuiltin<"v", "mvv", "csil">;
def vmslt : EPIRelBuiltin<"v", "mvv", "csil">;
def vmsleu : EPIRelBuiltin<"v", "mvv", "csil">;
def vmsle : EPIRelBuiltin<"v", "mvv", "csil">;
def vmsgtu : EPIRelBuiltin<"v", "mvv", "csil">;
def vmsgt : EPIRelBuiltin<"v", "mvv", "csil">;

def vminu : EPIBinBuiltin<"v", "vvv", "csil">;
def vmin : EPIBinBuiltin<"v", "vvv", "csil">;

def vmaxu : EPIBinBuiltin<"v", "vvv", "csil">;
def vmax : EPIBinBuiltin<"v", "vvv", "csil">;

def vmul : EPIBinBuiltin<"v", "vvv", "csil">;
def vmulh : EPIBinBuiltin<"v", "vvv", "csil">;
def vmulhu : EPIBinBuiltin<"v", "vvv", "csil">;
def vmulhsu : EPIBinBuiltin<"v", "vvv", "csil">;

def vwmul : EPIWidenBinBuiltin<"w", "wvv", "csi">;
def vwmulu : EPIWidenBinBuiltin<"w", "wvv", "csi">;
def vwmulsu : EPIWidenBinBuiltin<"w", "wvv", "csi">;

let HasMergeOperand = 0 in
{
  def vmacc : EPIBinBuiltin<"v", "vvvv", "csil">;
  def vnmsac : EPIBinBuiltin<"v", "vvvv", "csil">;
  def vmadd : EPIBinBuiltin<"v", "vvvv", "csil">;
  def vnmsub : EPIBinBuiltin<"v", "vvvv", "csil">;

  def vwmaccu : EPIWidenTerBuiltin<"w", "wvvw", "csi">;
  def vwmacc : EPIWidenTerBuiltin<"w", "wvvw", "csi">;
  def vwmsacu : EPIWidenTerBuiltin<"w", "wvvw", "csi">;
  def vwmsac : EPIWidenTerBuiltin<"w", "wvvw", "csi">;

  def vwsmaccu : EPIWidenTerBuiltin<"w", "wvvw", "csi">;
  def vwsmacc : EPIWidenTerBuiltin<"w", "wvvw", "csi">;
  def vwsmsacu : EPIWidenTerBuiltin<"w", "wvvw", "csi">;
  def vwsmsac : EPIWidenTerBuiltin<"w", "wvvw", "csi">;
}

def vdivu : EPIBinBuiltin<"v", "vvv", "csil">;
def vdiv : EPIBinBuiltin<"v", "vvv", "csil">;
def vremu : EPIBinBuiltin<"v", "vvv", "csil">;
def vrem : EPIBinBuiltin<"v", "vvv", "csil">;

let HasMask = 0 in
def vmerge : EPIBinMaskInBuiltin<"v", "vvvm", "csil">;

def vsaddu : EPIBinBuiltin<"v", "vvv", "csil">;
def vsadd : EPIBinBuiltin<"v", "vvv", "csil">;
def vssub : EPIBinBuiltin<"v", "vvv", "csil">;
def vssubu : EPIBinBuiltin<"v", "vvv", "csil">;

def vaadd : EPIBinBuiltin<"v", "vvv", "csil">;
def vasub : EPIBinBuiltin<"v", "vvv", "csil">;

def vsmul : EPIBinBuiltin<"v", "vvv", "csil">;


def vssrl : EPIBinBuiltin<"v", "vvv", "csil">;
def vssra : EPIBinBuiltin<"v", "vvv", "csil">;

def vnclipu : EPIBuiltin<"v", "vww", "csi">;
def vnclip : EPIBuiltin<"v", "vww", "csi">;

def vfadd : EPIBinBuiltin<"v", "vvv", "fd">;
def vfsub : EPIBinBuiltin<"v", "vvv", "fd">;

def vfwadd : EPIWidenBinBuiltin<"w", "wvv", "f">;
def vfwsub : EPIWidenBinBuiltin<"w", "wvv", "f">;
def vfwadd_w : EPIBinBuiltin<"w", "wwv", "f">;
def vfwsub_w : EPIBinBuiltin<"w", "wwv", "f">;

def vfmul : EPIBinBuiltin<"v", "vvv", "fd">;
def vfdiv : EPIBinBuiltin<"v", "vvv", "fd">;

def vfwmul : EPIWidenBinBuiltin<"w", "wvv", "f">;

let HasMergeOperand = 0 in
{
  def vfmadd : EPITerBuiltin<"v", "vvvv", "fd">;
  def vfnmadd : EPITerBuiltin<"v", "vvvv", "fd">;
  def vfmsub : EPITerBuiltin<"v", "vvvv", "fd">;
  def vfnmsub : EPITerBuiltin<"v", "vvvv", "fd">;
  def vfmacc : EPITerBuiltin<"v", "vvvv", "fd">;
  def vfnmacc : EPITerBuiltin<"v", "vvvv", "fd">;
  def vfmsac : EPITerBuiltin<"v", "vvvv", "fd">;
  def vfnmsac : EPITerBuiltin<"v", "vvvv", "fd">;

  def vfwmacc : EPIWidenTerBuiltin<"w", "wvvw", "f">;
  def vfwnmacc : EPIWidenTerBuiltin<"w", "wvvw", "f">;
  def vfwmsac : EPIWidenTerBuiltin<"w", "wvvw", "f">;
  def vfwnmsac : EPIWidenTerBuiltin<"w", "wvvw", "f">;
}

def vfsqrt : EPIUnaBuiltin<"v", "vv", "fd">;

def vfmin : EPIBinBuiltin<"v", "vvv", "fd">;
def vfmax : EPIBinBuiltin<"v", "vvv", "fd">;

def vfsgnj : EPIBinBuiltin<"v", "vvv", "fd">;
def vfsgnjn : EPIBinBuiltin<"v", "vvv", "fd">;
def vfsgnjx : EPIBinBuiltin<"v", "vvv", "fd">;

def vmfeq : EPIRelBuiltin<"v", "mvv", "fd">;
def vmfne : EPIRelBuiltin<"v", "mvv", "fd">;
def vmflt : EPIRelBuiltin<"v", "mvv", "fd">;
def vmfle : EPIRelBuiltin<"v", "mvv", "fd">;
def vmfgt : EPIRelBuiltin<"v", "mvv", "fd">;
def vmfge : EPIRelBuiltin<"v", "mvv", "fd">;

let HasMask = 0 in
def vfmerge : EPIBinMaskInBuiltin<"v", "vvvm", "fd">;

def vfcvt_xu_f : EPIUnaBuiltin<"Ivv", "Ivv", "fd">;
def vfcvt_x_f : EPIUnaBuiltin<"Ivv", "Ivv", "fd">;

def vfcvt_f_xu : EPIUnaBuiltin<"Fvv", "Fvv", "il">;
def vfcvt_f_x : EPIUnaBuiltin<"Fvv", "Fvv", "il">;

def vfwcvt_xu_f : EPIUnaBuiltin<"Iwv", "Iwv", "f">;
def vfwcvt_x_f : EPIUnaBuiltin<"Iwv", "Iwv", "f">;

def vfwcvt_f_xu : EPIUnaBuiltin<"Fwv", "Fwv", "si">;
def vfwcvt_f_x : EPIUnaBuiltin<"Fwv", "Fwv", "si">;

// Note that we use "vFw" + "si" here because "Ivw" + "f" would miss f32→i16
// We have no way to represent f16 in the TypeRange yet.
def vfncvt_xu_f : EPIUnaBuiltin<"vFw", "vFw", "si">;
def vfncvt_x_f : EPIUnaBuiltin<"vFw", "vFw", "si">;

def vfncvt_f_xu : EPIUnaBuiltin<"Fvw", "Fvw", "i">;
def vfncvt_f_x : EPIUnaBuiltin<"Fvw", "Fvw", "i">;

def vfwcvt_f_f : EPIUnaBuiltin<"wv", "wv", "f">;
def vfncvt_f_f : EPIUnaBuiltin<"vw", "vw", "f">;

def vredsum : EPIBinBuiltin<"v", "vvv", "csil">;
def vredmaxu : EPIBinBuiltin<"v", "vvv", "csil">;
def vredmax : EPIBinBuiltin<"v", "vvv", "csil">;
def vredmin : EPIBinBuiltin<"v", "vvv", "csil">;
def vredminu : EPIBinBuiltin<"v", "vvv", "csil">;
def vredand : EPIBinBuiltin<"v", "vvv", "csil">;
def vredor : EPIBinBuiltin<"v", "vvv", "csil">;
def vredxor : EPIBinBuiltin<"v", "vvv", "csil">;

// FIXME: we forgot these ones
// def vwredsum : EPIWidenBinBuiltin<"w", "wvw", "csi">;
// def vwredsumu : EPIWidenBinBuiltin<"w", "wvw", "csi">;

def vfredosum : EPIBinBuiltin<"v", "vvv", "fd">;
def vfredsum : EPIBinBuiltin<"v", "vvv", "fd">;
def vfredmax : EPIBinBuiltin<"v", "vvv", "fd">;
def vfredmin : EPIBinBuiltin<"v", "vvv", "fd">;

def vfwredosum : EPIWidenBinBuiltin<"w", "wvw", "f">;
def vfwredsum : EPIWidenBinBuiltin<"w", "wvw", "f">;

let HasMask = 0 in
{
  def vmand : EPIBinBuiltin<"m", "mmm", "csil">;
  def vmnand : EPIBinBuiltin<"m", "mmm", "csil">;
  def vmandnot : EPIBinBuiltin<"m", "mmm", "csil">;
  def vmxor : EPIBinBuiltin<"m", "mmm", "csil">;
  def vmor : EPIBinBuiltin<"m", "mmm", "csil">;
  def vmnor : EPIBinBuiltin<"m", "mmm", "csil">;
  def vmornot : EPIBinBuiltin<"m", "mmm", "csil">;
  def vmxnor : EPIBinBuiltin<"m", "mmm", "csil">;
}

let HasMergeOperand = 0 in
{
  def vpopc : EPIMaskToScalarBuiltin<"m", "sm", "csil">;
  def vfirst : EPIMaskToScalarBuiltin<"m", "sm", "csil">;
}

let HasMergeOperand = 0 in {
def vmsbf : EPIMaskUnaBuiltin<"m", "mm", "csil">;
def vmsif : EPIMaskUnaBuiltin<"m", "mm", "csil">;
def vmsof : EPIMaskUnaBuiltin<"m", "mm", "csil">;
}

def viota : EPIMaskToIntBuiltin<"v", "vm", "csil">;

def vid : EPINulBuiltin<"v", "v", "csil">;

let LMUL = [1], HasMask = 0 in
def vmv_s_x : EPIScalarToScalarVecBuiltin<"v", "vve", "csil">;

let LMUL = [1], HasMask = 0 in
def vfmv_s_f : EPIScalarToScalarVecBuiltin<"v", "vve", "fd">;

let LMUL = [1], HasMask = 0, HasVL = 0 in
def vmv_x_s : EPIVecToScalarBuiltin<"v", "ev", "csil">;

let LMUL = [1], HasMask = 0, HasVL = 0 in
def vfmv_f_s : EPIVecToScalarBuiltin<"v", "ev", "fd">;

def vslideup : EPIVecScalarBuiltin<"v", "vvu", "csilfd">;
def vslidedown : EPIVecScalarBuiltin<"v", "vvu", "csilfd">;
def vslide1up : EPIVecScalarBuiltin<"v", "vvu", "csilfd">;
def vslide1down : EPIVecScalarBuiltin<"v", "vvu", "csilfd">;

def vrgather : EPIBinBuiltin<"v", "vvIv", "csilfd">;
let IntrinsicName = "vrgather", IntrinsicNameMask = "vrgather_mask" in
def vsplat : EPIVecScalarBuiltin<"v", "vvu", "csilfd">;

let HasMask = 0 in
def vcompress : EPIBinBuiltin<"v", "vvm", "csilfd">;

// FIXME EDIV intrinsics disabled
//def vdot : EPIBinBuiltin<"v", "vvv", "csil">;
//def vdotu : EPIBinBuiltin<"v", "vvv", "csil">;
//def vfdot : EPIBinBuiltin<"v", "vvv", "fd">;

let HasMask = 0 in
def vmv_v_x : EPIScalarToVecBuiltin<"v", "ve", "csil">;

let HasMask = 0 in
def vfmv_v_f : EPIScalarToVecBuiltin<"v", "ve", "fd">;

let LMUL = [1], HasMask = 0, HasVL = 0, Name = "cast", IntrinsicName = "" in
{
  let ManualCodegen = [{ {
      return Builder.CreateTrunc(Ops[0], ResultType);
  } }] in
  def cast_to_mask : EPIBuiltin<"mv", "mv", "csil">;

  let ManualCodegen = [{ {
      return Builder.CreateZExt(Ops[0], ResultType);
  } }] in
  def cast_from_mask : EPIBuiltin<"vm", "vm", "csil">;
}

// EPI experimental extensions
let LMUL = [1], HasMask = 0,
    ManualCodegen = [{
{
IntrinsicTypes = { Ops[0]->getType() };

llvm::Function *F = CGM.getIntrinsic(ID, IntrinsicTypes);
llvm::Value *CallValue = Builder.CreateCall(F, Ops, "");
llvm::Value *V = Builder.CreateInsertValue(
     llvm::UndefValue::get(ResultType),
     Builder.CreateExtractValue(CallValue, {0}),
     {0});
V = Builder.CreateInsertValue(
     V,
     Builder.CreateExtractValue(CallValue, {1}),
     {1});
if (ReturnValue.isNull())
  return V;
else
  return Builder.CreateStore(V, ReturnValue.getValue());
}
}]
in {

let IntrinsicName = "vzip2" in
def vzip2 : EPIBuiltin<"TTv", "TTvvv", "csildf">;

let IntrinsicName = "vunzip2" in
def vunzip2 : EPIBuiltin<"TTv", "TTvvv", "csildf">;

// Like a zip but first zip the elements in even-numbered positions and
// then the elements in odd-numbered positions.
let IntrinsicName = "vtrn" in
def vtrn : EPIBuiltin<"TTv", "TTvvv", "csildf">;

}
