// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple riscv64 -target-abi lp64 -emit-llvm %s -o - | FileCheck %s --check-prefix=RISCV64-LP64
// RUN: %clang_cc1 -triple riscv64 -target-feature +f -target-abi lp64f -emit-llvm %s -o - | FileCheck %s --check-prefix=RISCV64-LP64F
// RUN: %clang_cc1 -triple riscv64 -target-feature +f,+d -target-abi lp64d -emit-llvm %s -o - | FileCheck %s --check-prefix=RISCV64-LP64D

struct pair_of_float_homogeneous_t {
  float a, b;
} a_pair_of_float_homogeneous;

void pair_of_float_homogeneous(struct pair_of_float_homogeneous_t);

// RISCV64-LP64-LABEL: @test_pair_of_float_homogeneous(
// RISCV64-LP64-NEXT:  entry:
// RISCV64-LP64-NEXT:    [[TMP0:%.*]] = load i64, i64* bitcast (%struct.pair_of_float_homogeneous_t* @a_pair_of_float_homogeneous to i64*), align 4
// RISCV64-LP64-NEXT:    call void @pair_of_float_homogeneous(i64 [[TMP0]])
// RISCV64-LP64-NEXT:    ret void
//
// RISCV64-LP64F-LABEL: @test_pair_of_float_homogeneous(
// RISCV64-LP64F-NEXT:  entry:
// RISCV64-LP64F-NEXT:    [[TMP0:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_homogeneous_t* @a_pair_of_float_homogeneous to { float, float }*), i32 0, i32 0), align 4
// RISCV64-LP64F-NEXT:    [[TMP1:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_homogeneous_t* @a_pair_of_float_homogeneous to { float, float }*), i32 0, i32 1), align 4
// RISCV64-LP64F-NEXT:    call void @pair_of_float_homogeneous(float [[TMP0]], float [[TMP1]])
// RISCV64-LP64F-NEXT:    ret void
//
// RISCV64-LP64D-LABEL: @test_pair_of_float_homogeneous(
// RISCV64-LP64D-NEXT:  entry:
// RISCV64-LP64D-NEXT:    [[TMP0:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_homogeneous_t* @a_pair_of_float_homogeneous to { float, float }*), i32 0, i32 0), align 4
// RISCV64-LP64D-NEXT:    [[TMP1:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_homogeneous_t* @a_pair_of_float_homogeneous to { float, float }*), i32 0, i32 1), align 4
// RISCV64-LP64D-NEXT:    call void @pair_of_float_homogeneous(float [[TMP0]], float [[TMP1]])
// RISCV64-LP64D-NEXT:    ret void
//
void test_pair_of_float_homogeneous(void) {
  pair_of_float_homogeneous(a_pair_of_float_homogeneous);
}

void last_still_ok_for_homogeneous_pair(float a1, float a2, float a3, float a4, float a5, float a6, struct pair_of_float_homogeneous_t);

// RISCV64-LP64-LABEL: @test_last_still_ok_for_homogeneous_pair(
// RISCV64-LP64-NEXT:  entry:
// RISCV64-LP64-NEXT:    [[TMP0:%.*]] = load i64, i64* bitcast (%struct.pair_of_float_homogeneous_t* @a_pair_of_float_homogeneous to i64*), align 4
// RISCV64-LP64-NEXT:    call void @last_still_ok_for_homogeneous_pair(float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, i64 [[TMP0]])
// RISCV64-LP64-NEXT:    ret void
//
// RISCV64-LP64F-LABEL: @test_last_still_ok_for_homogeneous_pair(
// RISCV64-LP64F-NEXT:  entry:
// RISCV64-LP64F-NEXT:    [[TMP0:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_homogeneous_t* @a_pair_of_float_homogeneous to { float, float }*), i32 0, i32 0), align 4
// RISCV64-LP64F-NEXT:    [[TMP1:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_homogeneous_t* @a_pair_of_float_homogeneous to { float, float }*), i32 0, i32 1), align 4
// RISCV64-LP64F-NEXT:    call void @last_still_ok_for_homogeneous_pair(float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float [[TMP0]], float [[TMP1]])
// RISCV64-LP64F-NEXT:    ret void
//
// RISCV64-LP64D-LABEL: @test_last_still_ok_for_homogeneous_pair(
// RISCV64-LP64D-NEXT:  entry:
// RISCV64-LP64D-NEXT:    [[TMP0:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_homogeneous_t* @a_pair_of_float_homogeneous to { float, float }*), i32 0, i32 0), align 4
// RISCV64-LP64D-NEXT:    [[TMP1:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_homogeneous_t* @a_pair_of_float_homogeneous to { float, float }*), i32 0, i32 1), align 4
// RISCV64-LP64D-NEXT:    call void @last_still_ok_for_homogeneous_pair(float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float [[TMP0]], float [[TMP1]])
// RISCV64-LP64D-NEXT:    ret void
//
void test_last_still_ok_for_homogeneous_pair(void) {
  last_still_ok_for_homogeneous_pair(0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, a_pair_of_float_homogeneous);
}

void too_many_floats_1_for_homogeneous_pair(float a1, float a2, float a3, float a4, float a5, float a6, float a7, struct pair_of_float_homogeneous_t);

// RISCV64-LP64-LABEL: @test_too_many_floats_1_for_homogeneous_pair(
// RISCV64-LP64-NEXT:  entry:
// RISCV64-LP64-NEXT:    [[TMP0:%.*]] = load i64, i64* bitcast (%struct.pair_of_float_homogeneous_t* @a_pair_of_float_homogeneous to i64*), align 4
// RISCV64-LP64-NEXT:    call void @too_many_floats_1_for_homogeneous_pair(float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, i64 [[TMP0]])
// RISCV64-LP64-NEXT:    ret void
//
// RISCV64-LP64F-LABEL: @test_too_many_floats_1_for_homogeneous_pair(
// RISCV64-LP64F-NEXT:  entry:
// RISCV64-LP64F-NEXT:    [[TMP0:%.*]] = load i64, i64* bitcast (%struct.pair_of_float_homogeneous_t* @a_pair_of_float_homogeneous to i64*), align 4
// RISCV64-LP64F-NEXT:    call void @too_many_floats_1_for_homogeneous_pair(float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, i64 [[TMP0]])
// RISCV64-LP64F-NEXT:    ret void
//
// RISCV64-LP64D-LABEL: @test_too_many_floats_1_for_homogeneous_pair(
// RISCV64-LP64D-NEXT:  entry:
// RISCV64-LP64D-NEXT:    [[TMP0:%.*]] = load i64, i64* bitcast (%struct.pair_of_float_homogeneous_t* @a_pair_of_float_homogeneous to i64*), align 4
// RISCV64-LP64D-NEXT:    call void @too_many_floats_1_for_homogeneous_pair(float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, i64 [[TMP0]])
// RISCV64-LP64D-NEXT:    ret void
//
void test_too_many_floats_1_for_homogeneous_pair(void) {
  too_many_floats_1_for_homogeneous_pair(0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, 0.0f, a_pair_of_float_homogeneous);
}

struct pair_of_float_nesting1_t {
  float a;
  struct {
    float b;
  };
} a_pair_of_float_nesting1;

void pair_of_float_nesting1(struct pair_of_float_nesting1_t);

// RISCV64-LP64-LABEL: @test_pair_of_float_nesting1(
// RISCV64-LP64-NEXT:  entry:
// RISCV64-LP64-NEXT:    [[TMP0:%.*]] = load i64, i64* bitcast (%struct.pair_of_float_nesting1_t* @a_pair_of_float_nesting1 to i64*), align 4
// RISCV64-LP64-NEXT:    call void @pair_of_float_nesting1(i64 [[TMP0]])
// RISCV64-LP64-NEXT:    ret void
//
// RISCV64-LP64F-LABEL: @test_pair_of_float_nesting1(
// RISCV64-LP64F-NEXT:  entry:
// RISCV64-LP64F-NEXT:    [[TMP0:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_nesting1_t* @a_pair_of_float_nesting1 to { float, float }*), i32 0, i32 0), align 4
// RISCV64-LP64F-NEXT:    [[TMP1:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_nesting1_t* @a_pair_of_float_nesting1 to { float, float }*), i32 0, i32 1), align 4
// RISCV64-LP64F-NEXT:    call void @pair_of_float_nesting1(float [[TMP0]], float [[TMP1]])
// RISCV64-LP64F-NEXT:    ret void
//
// RISCV64-LP64D-LABEL: @test_pair_of_float_nesting1(
// RISCV64-LP64D-NEXT:  entry:
// RISCV64-LP64D-NEXT:    [[TMP0:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_nesting1_t* @a_pair_of_float_nesting1 to { float, float }*), i32 0, i32 0), align 4
// RISCV64-LP64D-NEXT:    [[TMP1:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_nesting1_t* @a_pair_of_float_nesting1 to { float, float }*), i32 0, i32 1), align 4
// RISCV64-LP64D-NEXT:    call void @pair_of_float_nesting1(float [[TMP0]], float [[TMP1]])
// RISCV64-LP64D-NEXT:    ret void
//
void test_pair_of_float_nesting1(void) {
  pair_of_float_nesting1(a_pair_of_float_nesting1);
}

struct pair_of_float_nesting2_t {
  float a[2];
} a_pair_of_float_nesting2;

void pair_of_float_nesting2(struct pair_of_float_nesting2_t);

// RISCV64-LP64-LABEL: @test_pair_of_float_nesting2(
// RISCV64-LP64-NEXT:  entry:
// RISCV64-LP64-NEXT:    [[TMP0:%.*]] = load i64, i64* bitcast (%struct.pair_of_float_nesting2_t* @a_pair_of_float_nesting2 to i64*), align 4
// RISCV64-LP64-NEXT:    call void @pair_of_float_nesting2(i64 [[TMP0]])
// RISCV64-LP64-NEXT:    ret void
//
// RISCV64-LP64F-LABEL: @test_pair_of_float_nesting2(
// RISCV64-LP64F-NEXT:  entry:
// RISCV64-LP64F-NEXT:    [[TMP0:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_nesting2_t* @a_pair_of_float_nesting2 to { float, float }*), i32 0, i32 0), align 4
// RISCV64-LP64F-NEXT:    [[TMP1:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_nesting2_t* @a_pair_of_float_nesting2 to { float, float }*), i32 0, i32 1), align 4
// RISCV64-LP64F-NEXT:    call void @pair_of_float_nesting2(float [[TMP0]], float [[TMP1]])
// RISCV64-LP64F-NEXT:    ret void
//
// RISCV64-LP64D-LABEL: @test_pair_of_float_nesting2(
// RISCV64-LP64D-NEXT:  entry:
// RISCV64-LP64D-NEXT:    [[TMP0:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_nesting2_t* @a_pair_of_float_nesting2 to { float, float }*), i32 0, i32 0), align 4
// RISCV64-LP64D-NEXT:    [[TMP1:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_nesting2_t* @a_pair_of_float_nesting2 to { float, float }*), i32 0, i32 1), align 4
// RISCV64-LP64D-NEXT:    call void @pair_of_float_nesting2(float [[TMP0]], float [[TMP1]])
// RISCV64-LP64D-NEXT:    ret void
//
void test_pair_of_float_nesting2(void) {
  pair_of_float_nesting2(a_pair_of_float_nesting2);
}

struct pair_of_float_nesting3_t {
  float a1[1];
  float a2[1];
} a_pair_of_float_nesting3;

void pair_of_float_nesting3(struct pair_of_float_nesting3_t);

struct pair_of_float_nesting4_t {
  float a1[1];
  struct {
    float b;
  } c;
} a_pair_of_float_nesting4;

void pair_of_float_nesting4(struct pair_of_float_nesting4_t);

// RISCV64-LP64-LABEL: @test_pair_of_float_nesting4(
// RISCV64-LP64-NEXT:  entry:
// RISCV64-LP64-NEXT:    [[TMP0:%.*]] = load i64, i64* bitcast (%struct.pair_of_float_nesting4_t* @a_pair_of_float_nesting4 to i64*), align 4
// RISCV64-LP64-NEXT:    call void @pair_of_float_nesting4(i64 [[TMP0]])
// RISCV64-LP64-NEXT:    ret void
//
// RISCV64-LP64F-LABEL: @test_pair_of_float_nesting4(
// RISCV64-LP64F-NEXT:  entry:
// RISCV64-LP64F-NEXT:    [[TMP0:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_nesting4_t* @a_pair_of_float_nesting4 to { float, float }*), i32 0, i32 0), align 4
// RISCV64-LP64F-NEXT:    [[TMP1:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_nesting4_t* @a_pair_of_float_nesting4 to { float, float }*), i32 0, i32 1), align 4
// RISCV64-LP64F-NEXT:    call void @pair_of_float_nesting4(float [[TMP0]], float [[TMP1]])
// RISCV64-LP64F-NEXT:    ret void
//
// RISCV64-LP64D-LABEL: @test_pair_of_float_nesting4(
// RISCV64-LP64D-NEXT:  entry:
// RISCV64-LP64D-NEXT:    [[TMP0:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_nesting4_t* @a_pair_of_float_nesting4 to { float, float }*), i32 0, i32 0), align 4
// RISCV64-LP64D-NEXT:    [[TMP1:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_nesting4_t* @a_pair_of_float_nesting4 to { float, float }*), i32 0, i32 1), align 4
// RISCV64-LP64D-NEXT:    call void @pair_of_float_nesting4(float [[TMP0]], float [[TMP1]])
// RISCV64-LP64D-NEXT:    ret void
//
void test_pair_of_float_nesting4(void) {
  pair_of_float_nesting4(a_pair_of_float_nesting4);
}

struct pair_of_float_nesting5_t {
  float a1[1];
  struct {
    float b;
  } c[1];
} a_pair_of_float_nesting5;

void pair_of_float_nesting5(struct pair_of_float_nesting5_t);

// RISCV64-LP64-LABEL: @test_pair_of_float_nesting5(
// RISCV64-LP64-NEXT:  entry:
// RISCV64-LP64-NEXT:    [[TMP0:%.*]] = load i64, i64* bitcast (%struct.pair_of_float_nesting5_t* @a_pair_of_float_nesting5 to i64*), align 4
// RISCV64-LP64-NEXT:    call void @pair_of_float_nesting5(i64 [[TMP0]])
// RISCV64-LP64-NEXT:    ret void
//
// RISCV64-LP64F-LABEL: @test_pair_of_float_nesting5(
// RISCV64-LP64F-NEXT:  entry:
// RISCV64-LP64F-NEXT:    [[TMP0:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_nesting5_t* @a_pair_of_float_nesting5 to { float, float }*), i32 0, i32 0), align 4
// RISCV64-LP64F-NEXT:    [[TMP1:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_nesting5_t* @a_pair_of_float_nesting5 to { float, float }*), i32 0, i32 1), align 4
// RISCV64-LP64F-NEXT:    call void @pair_of_float_nesting5(float [[TMP0]], float [[TMP1]])
// RISCV64-LP64F-NEXT:    ret void
//
// RISCV64-LP64D-LABEL: @test_pair_of_float_nesting5(
// RISCV64-LP64D-NEXT:  entry:
// RISCV64-LP64D-NEXT:    [[TMP0:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_nesting5_t* @a_pair_of_float_nesting5 to { float, float }*), i32 0, i32 0), align 4
// RISCV64-LP64D-NEXT:    [[TMP1:%.*]] = load float, float* getelementptr inbounds ({ float, float }, { float, float }* bitcast (%struct.pair_of_float_nesting5_t* @a_pair_of_float_nesting5 to { float, float }*), i32 0, i32 1), align 4
// RISCV64-LP64D-NEXT:    call void @pair_of_float_nesting5(float [[TMP0]], float [[TMP1]])
// RISCV64-LP64D-NEXT:    ret void
//
void test_pair_of_float_nesting5(void) {
  pair_of_float_nesting5(a_pair_of_float_nesting5);
}

struct float_and_double_t {
  float a;
  double b;
} a_float_and_double;

void float_and_double(struct float_and_double_t);

// RISCV64-LP64-LABEL: @test_float_and_double(
// RISCV64-LP64-NEXT:  entry:
// RISCV64-LP64-NEXT:    [[TMP0:%.*]] = load [2 x i64], [2 x i64]* bitcast (%struct.float_and_double_t* @a_float_and_double to [2 x i64]*), align 8
// RISCV64-LP64-NEXT:    call void @float_and_double([2 x i64] [[TMP0]])
// RISCV64-LP64-NEXT:    ret void
//
// RISCV64-LP64F-LABEL: @test_float_and_double(
// RISCV64-LP64F-NEXT:  entry:
// RISCV64-LP64F-NEXT:    [[TMP0:%.*]] = load [2 x i64], [2 x i64]* bitcast (%struct.float_and_double_t* @a_float_and_double to [2 x i64]*), align 8
// RISCV64-LP64F-NEXT:    call void @float_and_double([2 x i64] [[TMP0]])
// RISCV64-LP64F-NEXT:    ret void
//
// RISCV64-LP64D-LABEL: @test_float_and_double(
// RISCV64-LP64D-NEXT:  entry:
// RISCV64-LP64D-NEXT:    [[TMP0:%.*]] = load float, float* getelementptr inbounds ({ float, double }, { float, double }* bitcast (%struct.float_and_double_t* @a_float_and_double to { float, double }*), i32 0, i32 0), align 8
// RISCV64-LP64D-NEXT:    [[TMP1:%.*]] = load double, double* getelementptr inbounds ({ float, double }, { float, double }* bitcast (%struct.float_and_double_t* @a_float_and_double to { float, double }*), i32 0, i32 1), align 8
// RISCV64-LP64D-NEXT:    call void @float_and_double(float [[TMP0]], double [[TMP1]])
// RISCV64-LP64D-NEXT:    ret void
//
void test_float_and_double(void) {
  float_and_double(a_float_and_double);
}

struct float_and_double_nesting1_t {
  float a;
  struct {
    double b;
  } c[1];
} a_float_and_double_nesting1;

void float_and_double_nesting1(struct float_and_double_nesting1_t);

// RISCV64-LP64-LABEL: @test_float_and_double_nesting1(
// RISCV64-LP64-NEXT:  entry:
// RISCV64-LP64-NEXT:    [[TMP0:%.*]] = load [2 x i64], [2 x i64]* bitcast (%struct.float_and_double_nesting1_t* @a_float_and_double_nesting1 to [2 x i64]*), align 8
// RISCV64-LP64-NEXT:    call void @float_and_double_nesting1([2 x i64] [[TMP0]])
// RISCV64-LP64-NEXT:    ret void
//
// RISCV64-LP64F-LABEL: @test_float_and_double_nesting1(
// RISCV64-LP64F-NEXT:  entry:
// RISCV64-LP64F-NEXT:    [[TMP0:%.*]] = load [2 x i64], [2 x i64]* bitcast (%struct.float_and_double_nesting1_t* @a_float_and_double_nesting1 to [2 x i64]*), align 8
// RISCV64-LP64F-NEXT:    call void @float_and_double_nesting1([2 x i64] [[TMP0]])
// RISCV64-LP64F-NEXT:    ret void
//
// RISCV64-LP64D-LABEL: @test_float_and_double_nesting1(
// RISCV64-LP64D-NEXT:  entry:
// RISCV64-LP64D-NEXT:    [[TMP0:%.*]] = load float, float* getelementptr inbounds ({ float, double }, { float, double }* bitcast (%struct.float_and_double_nesting1_t* @a_float_and_double_nesting1 to { float, double }*), i32 0, i32 0), align 8
// RISCV64-LP64D-NEXT:    [[TMP1:%.*]] = load double, double* getelementptr inbounds ({ float, double }, { float, double }* bitcast (%struct.float_and_double_nesting1_t* @a_float_and_double_nesting1 to { float, double }*), i32 0, i32 1), align 8
// RISCV64-LP64D-NEXT:    call void @float_and_double_nesting1(float [[TMP0]], double [[TMP1]])
// RISCV64-LP64D-NEXT:    ret void
//
void test_float_and_double_nesting1(void) {
  float_and_double_nesting1(a_float_and_double_nesting1);
}

struct mixed_float_int_t {
    float a;
    int b;
} a_mixed_float_int;

void mixed_float_int(struct mixed_float_int_t);

// RISCV64-LP64-LABEL: @test_mixed_float_int(
// RISCV64-LP64-NEXT:  entry:
// RISCV64-LP64-NEXT:    [[TMP0:%.*]] = load i64, i64* bitcast (%struct.mixed_float_int_t* @a_mixed_float_int to i64*), align 4
// RISCV64-LP64-NEXT:    call void @mixed_float_int(i64 [[TMP0]])
// RISCV64-LP64-NEXT:    ret void
//
// RISCV64-LP64F-LABEL: @test_mixed_float_int(
// RISCV64-LP64F-NEXT:  entry:
// RISCV64-LP64F-NEXT:    [[TMP0:%.*]] = load float, float* getelementptr inbounds ({ float, i32 }, { float, i32 }* bitcast (%struct.mixed_float_int_t* @a_mixed_float_int to { float, i32 }*), i32 0, i32 0), align 4
// RISCV64-LP64F-NEXT:    [[TMP1:%.*]] = load i32, i32* getelementptr inbounds ({ float, i32 }, { float, i32 }* bitcast (%struct.mixed_float_int_t* @a_mixed_float_int to { float, i32 }*), i32 0, i32 1), align 4
// RISCV64-LP64F-NEXT:    call void @mixed_float_int(float [[TMP0]], i32 [[TMP1]])
// RISCV64-LP64F-NEXT:    ret void
//
// RISCV64-LP64D-LABEL: @test_mixed_float_int(
// RISCV64-LP64D-NEXT:  entry:
// RISCV64-LP64D-NEXT:    [[TMP0:%.*]] = load float, float* getelementptr inbounds ({ float, i32 }, { float, i32 }* bitcast (%struct.mixed_float_int_t* @a_mixed_float_int to { float, i32 }*), i32 0, i32 0), align 4
// RISCV64-LP64D-NEXT:    [[TMP1:%.*]] = load i32, i32* getelementptr inbounds ({ float, i32 }, { float, i32 }* bitcast (%struct.mixed_float_int_t* @a_mixed_float_int to { float, i32 }*), i32 0, i32 1), align 4
// RISCV64-LP64D-NEXT:    call void @mixed_float_int(float [[TMP0]], i32 [[TMP1]])
// RISCV64-LP64D-NEXT:    ret void
//
void test_mixed_float_int(void)
{
    mixed_float_int(a_mixed_float_int);
}

struct mixed_float_short_t {
    float a;
    unsigned short b;
} a_mixed_float_short;

void mixed_float_short(struct mixed_float_short_t, short c);

// RISCV64-LP64-LABEL: @test_mixed_float_short(
// RISCV64-LP64-NEXT:  entry:
// RISCV64-LP64-NEXT:    [[S:%.*]] = alloca i16, align 2
// RISCV64-LP64-NEXT:    store i16 1, i16* [[S]], align 2
// RISCV64-LP64-NEXT:    [[TMP0:%.*]] = load i16, i16* [[S]], align 2
// RISCV64-LP64-NEXT:    [[TMP1:%.*]] = load i64, i64* bitcast (%struct.mixed_float_short_t* @a_mixed_float_short to i64*), align 4
// RISCV64-LP64-NEXT:    call void @mixed_float_short(i64 [[TMP1]], i16 signext [[TMP0]])
// RISCV64-LP64-NEXT:    ret void
//
// RISCV64-LP64F-LABEL: @test_mixed_float_short(
// RISCV64-LP64F-NEXT:  entry:
// RISCV64-LP64F-NEXT:    [[S:%.*]] = alloca i16, align 2
// RISCV64-LP64F-NEXT:    store i16 1, i16* [[S]], align 2
// RISCV64-LP64F-NEXT:    [[TMP0:%.*]] = load i16, i16* [[S]], align 2
// RISCV64-LP64F-NEXT:    [[TMP1:%.*]] = load float, float* getelementptr inbounds ({ float, i16 }, { float, i16 }* bitcast (%struct.mixed_float_short_t* @a_mixed_float_short to { float, i16 }*), i32 0, i32 0), align 4
// RISCV64-LP64F-NEXT:    [[TMP2:%.*]] = load i16, i16* getelementptr inbounds ({ float, i16 }, { float, i16 }* bitcast (%struct.mixed_float_short_t* @a_mixed_float_short to { float, i16 }*), i32 0, i32 1), align 4
// RISCV64-LP64F-NEXT:    call void @mixed_float_short(float [[TMP1]], i16 [[TMP2]], i16 signext [[TMP0]])
// RISCV64-LP64F-NEXT:    ret void
//
// RISCV64-LP64D-LABEL: @test_mixed_float_short(
// RISCV64-LP64D-NEXT:  entry:
// RISCV64-LP64D-NEXT:    [[S:%.*]] = alloca i16, align 2
// RISCV64-LP64D-NEXT:    store i16 1, i16* [[S]], align 2
// RISCV64-LP64D-NEXT:    [[TMP0:%.*]] = load i16, i16* [[S]], align 2
// RISCV64-LP64D-NEXT:    [[TMP1:%.*]] = load float, float* getelementptr inbounds ({ float, i16 }, { float, i16 }* bitcast (%struct.mixed_float_short_t* @a_mixed_float_short to { float, i16 }*), i32 0, i32 0), align 4
// RISCV64-LP64D-NEXT:    [[TMP2:%.*]] = load i16, i16* getelementptr inbounds ({ float, i16 }, { float, i16 }* bitcast (%struct.mixed_float_short_t* @a_mixed_float_short to { float, i16 }*), i32 0, i32 1), align 4
// RISCV64-LP64D-NEXT:    call void @mixed_float_short(float [[TMP1]], i16 [[TMP2]], i16 signext [[TMP0]])
// RISCV64-LP64D-NEXT:    ret void
//
void test_mixed_float_short(void)
{
    short s = 1;
    mixed_float_short(a_mixed_float_short, s);
}

struct mixed_double_short_t {
    double a;
    unsigned short b;
} a_mixed_double_short;

void mixed_double_short(struct mixed_double_short_t, short c);

// RISCV64-LP64-LABEL: @test_mixed_double_short(
// RISCV64-LP64-NEXT:  entry:
// RISCV64-LP64-NEXT:    [[S:%.*]] = alloca i16, align 2
// RISCV64-LP64-NEXT:    store i16 1, i16* [[S]], align 2
// RISCV64-LP64-NEXT:    [[TMP0:%.*]] = load i16, i16* [[S]], align 2
// RISCV64-LP64-NEXT:    [[TMP1:%.*]] = load [2 x i64], [2 x i64]* bitcast (%struct.mixed_double_short_t* @a_mixed_double_short to [2 x i64]*), align 8
// RISCV64-LP64-NEXT:    call void @mixed_double_short([2 x i64] [[TMP1]], i16 signext [[TMP0]])
// RISCV64-LP64-NEXT:    ret void
//
// RISCV64-LP64F-LABEL: @test_mixed_double_short(
// RISCV64-LP64F-NEXT:  entry:
// RISCV64-LP64F-NEXT:    [[S:%.*]] = alloca i16, align 2
// RISCV64-LP64F-NEXT:    store i16 1, i16* [[S]], align 2
// RISCV64-LP64F-NEXT:    [[TMP0:%.*]] = load i16, i16* [[S]], align 2
// RISCV64-LP64F-NEXT:    [[TMP1:%.*]] = load [2 x i64], [2 x i64]* bitcast (%struct.mixed_double_short_t* @a_mixed_double_short to [2 x i64]*), align 8
// RISCV64-LP64F-NEXT:    call void @mixed_double_short([2 x i64] [[TMP1]], i16 signext [[TMP0]])
// RISCV64-LP64F-NEXT:    ret void
//
// RISCV64-LP64D-LABEL: @test_mixed_double_short(
// RISCV64-LP64D-NEXT:  entry:
// RISCV64-LP64D-NEXT:    [[S:%.*]] = alloca i16, align 2
// RISCV64-LP64D-NEXT:    store i16 1, i16* [[S]], align 2
// RISCV64-LP64D-NEXT:    [[TMP0:%.*]] = load i16, i16* [[S]], align 2
// RISCV64-LP64D-NEXT:    [[TMP1:%.*]] = load double, double* getelementptr inbounds ({ double, i16 }, { double, i16 }* bitcast (%struct.mixed_double_short_t* @a_mixed_double_short to { double, i16 }*), i32 0, i32 0), align 8
// RISCV64-LP64D-NEXT:    [[TMP2:%.*]] = load i16, i16* getelementptr inbounds ({ double, i16 }, { double, i16 }* bitcast (%struct.mixed_double_short_t* @a_mixed_double_short to { double, i16 }*), i32 0, i32 1), align 8
// RISCV64-LP64D-NEXT:    call void @mixed_double_short(double [[TMP1]], i16 [[TMP2]], i16 signext [[TMP0]])
// RISCV64-LP64D-NEXT:    ret void
//
void test_mixed_double_short(void)
{
    short s = 1;
    mixed_double_short(a_mixed_double_short, s);
}

void complex_float(_Complex float d);

// RISCV64-LP64-LABEL: @test_complex_float(
// RISCV64-LP64-NEXT:  entry:
// RISCV64-LP64-NEXT:    [[D:%.*]] = alloca { float, float }, align 4
// RISCV64-LP64-NEXT:    [[COERCE:%.*]] = alloca { float, float }, align 4
// RISCV64-LP64-NEXT:    [[D_REALP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[D]], i32 0, i32 0
// RISCV64-LP64-NEXT:    [[D_IMAGP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[D]], i32 0, i32 1
// RISCV64-LP64-NEXT:    store float 1.250000e+00, float* [[D_REALP]], align 4
// RISCV64-LP64-NEXT:    store float 0.000000e+00, float* [[D_IMAGP]], align 4
// RISCV64-LP64-NEXT:    [[D_REALP1:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[D]], i32 0, i32 0
// RISCV64-LP64-NEXT:    [[D_REAL:%.*]] = load float, float* [[D_REALP1]], align 4
// RISCV64-LP64-NEXT:    [[D_IMAGP2:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[D]], i32 0, i32 1
// RISCV64-LP64-NEXT:    [[D_IMAG:%.*]] = load float, float* [[D_IMAGP2]], align 4
// RISCV64-LP64-NEXT:    [[COERCE_REALP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[COERCE]], i32 0, i32 0
// RISCV64-LP64-NEXT:    [[COERCE_IMAGP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[COERCE]], i32 0, i32 1
// RISCV64-LP64-NEXT:    store float [[D_REAL]], float* [[COERCE_REALP]], align 4
// RISCV64-LP64-NEXT:    store float [[D_IMAG]], float* [[COERCE_IMAGP]], align 4
// RISCV64-LP64-NEXT:    [[TMP0:%.*]] = bitcast { float, float }* [[COERCE]] to i64*
// RISCV64-LP64-NEXT:    [[TMP1:%.*]] = load i64, i64* [[TMP0]], align 4
// RISCV64-LP64-NEXT:    call void @complex_float(i64 [[TMP1]])
// RISCV64-LP64-NEXT:    ret void
//
// RISCV64-LP64F-LABEL: @test_complex_float(
// RISCV64-LP64F-NEXT:  entry:
// RISCV64-LP64F-NEXT:    [[D:%.*]] = alloca { float, float }, align 4
// RISCV64-LP64F-NEXT:    [[COERCE:%.*]] = alloca { float, float }, align 4
// RISCV64-LP64F-NEXT:    [[D_REALP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[D]], i32 0, i32 0
// RISCV64-LP64F-NEXT:    [[D_IMAGP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[D]], i32 0, i32 1
// RISCV64-LP64F-NEXT:    store float 1.250000e+00, float* [[D_REALP]], align 4
// RISCV64-LP64F-NEXT:    store float 0.000000e+00, float* [[D_IMAGP]], align 4
// RISCV64-LP64F-NEXT:    [[D_REALP1:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[D]], i32 0, i32 0
// RISCV64-LP64F-NEXT:    [[D_REAL:%.*]] = load float, float* [[D_REALP1]], align 4
// RISCV64-LP64F-NEXT:    [[D_IMAGP2:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[D]], i32 0, i32 1
// RISCV64-LP64F-NEXT:    [[D_IMAG:%.*]] = load float, float* [[D_IMAGP2]], align 4
// RISCV64-LP64F-NEXT:    [[COERCE_REALP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[COERCE]], i32 0, i32 0
// RISCV64-LP64F-NEXT:    [[COERCE_IMAGP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[COERCE]], i32 0, i32 1
// RISCV64-LP64F-NEXT:    store float [[D_REAL]], float* [[COERCE_REALP]], align 4
// RISCV64-LP64F-NEXT:    store float [[D_IMAG]], float* [[COERCE_IMAGP]], align 4
// RISCV64-LP64F-NEXT:    [[TMP0:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[COERCE]], i32 0, i32 0
// RISCV64-LP64F-NEXT:    [[TMP1:%.*]] = load float, float* [[TMP0]], align 4
// RISCV64-LP64F-NEXT:    [[TMP2:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[COERCE]], i32 0, i32 1
// RISCV64-LP64F-NEXT:    [[TMP3:%.*]] = load float, float* [[TMP2]], align 4
// RISCV64-LP64F-NEXT:    call void @complex_float(float [[TMP1]], float [[TMP3]])
// RISCV64-LP64F-NEXT:    ret void
//
// RISCV64-LP64D-LABEL: @test_complex_float(
// RISCV64-LP64D-NEXT:  entry:
// RISCV64-LP64D-NEXT:    [[D:%.*]] = alloca { float, float }, align 4
// RISCV64-LP64D-NEXT:    [[COERCE:%.*]] = alloca { float, float }, align 4
// RISCV64-LP64D-NEXT:    [[D_REALP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[D]], i32 0, i32 0
// RISCV64-LP64D-NEXT:    [[D_IMAGP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[D]], i32 0, i32 1
// RISCV64-LP64D-NEXT:    store float 1.250000e+00, float* [[D_REALP]], align 4
// RISCV64-LP64D-NEXT:    store float 0.000000e+00, float* [[D_IMAGP]], align 4
// RISCV64-LP64D-NEXT:    [[D_REALP1:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[D]], i32 0, i32 0
// RISCV64-LP64D-NEXT:    [[D_REAL:%.*]] = load float, float* [[D_REALP1]], align 4
// RISCV64-LP64D-NEXT:    [[D_IMAGP2:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[D]], i32 0, i32 1
// RISCV64-LP64D-NEXT:    [[D_IMAG:%.*]] = load float, float* [[D_IMAGP2]], align 4
// RISCV64-LP64D-NEXT:    [[COERCE_REALP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[COERCE]], i32 0, i32 0
// RISCV64-LP64D-NEXT:    [[COERCE_IMAGP:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[COERCE]], i32 0, i32 1
// RISCV64-LP64D-NEXT:    store float [[D_REAL]], float* [[COERCE_REALP]], align 4
// RISCV64-LP64D-NEXT:    store float [[D_IMAG]], float* [[COERCE_IMAGP]], align 4
// RISCV64-LP64D-NEXT:    [[TMP0:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[COERCE]], i32 0, i32 0
// RISCV64-LP64D-NEXT:    [[TMP1:%.*]] = load float, float* [[TMP0]], align 4
// RISCV64-LP64D-NEXT:    [[TMP2:%.*]] = getelementptr inbounds { float, float }, { float, float }* [[COERCE]], i32 0, i32 1
// RISCV64-LP64D-NEXT:    [[TMP3:%.*]] = load float, float* [[TMP2]], align 4
// RISCV64-LP64D-NEXT:    call void @complex_float(float [[TMP1]], float [[TMP3]])
// RISCV64-LP64D-NEXT:    ret void
//
void test_complex_float(void)
{
  _Complex float d = 1.25f;
  complex_float(d);
}

void complex_double(_Complex double d);

// RISCV64-LP64-LABEL: @test_complex_double(
// RISCV64-LP64-NEXT:  entry:
// RISCV64-LP64-NEXT:    [[D:%.*]] = alloca { double, double }, align 8
// RISCV64-LP64-NEXT:    [[COERCE:%.*]] = alloca { double, double }, align 8
// RISCV64-LP64-NEXT:    [[D_REALP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[D]], i32 0, i32 0
// RISCV64-LP64-NEXT:    [[D_IMAGP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[D]], i32 0, i32 1
// RISCV64-LP64-NEXT:    store double 1.250000e+00, double* [[D_REALP]], align 8
// RISCV64-LP64-NEXT:    store double 0.000000e+00, double* [[D_IMAGP]], align 8
// RISCV64-LP64-NEXT:    [[D_REALP1:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[D]], i32 0, i32 0
// RISCV64-LP64-NEXT:    [[D_REAL:%.*]] = load double, double* [[D_REALP1]], align 8
// RISCV64-LP64-NEXT:    [[D_IMAGP2:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[D]], i32 0, i32 1
// RISCV64-LP64-NEXT:    [[D_IMAG:%.*]] = load double, double* [[D_IMAGP2]], align 8
// RISCV64-LP64-NEXT:    [[COERCE_REALP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[COERCE]], i32 0, i32 0
// RISCV64-LP64-NEXT:    [[COERCE_IMAGP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[COERCE]], i32 0, i32 1
// RISCV64-LP64-NEXT:    store double [[D_REAL]], double* [[COERCE_REALP]], align 8
// RISCV64-LP64-NEXT:    store double [[D_IMAG]], double* [[COERCE_IMAGP]], align 8
// RISCV64-LP64-NEXT:    [[TMP0:%.*]] = bitcast { double, double }* [[COERCE]] to [2 x i64]*
// RISCV64-LP64-NEXT:    [[TMP1:%.*]] = load [2 x i64], [2 x i64]* [[TMP0]], align 8
// RISCV64-LP64-NEXT:    call void @complex_double([2 x i64] [[TMP1]])
// RISCV64-LP64-NEXT:    ret void
//
// RISCV64-LP64F-LABEL: @test_complex_double(
// RISCV64-LP64F-NEXT:  entry:
// RISCV64-LP64F-NEXT:    [[D:%.*]] = alloca { double, double }, align 8
// RISCV64-LP64F-NEXT:    [[COERCE:%.*]] = alloca { double, double }, align 8
// RISCV64-LP64F-NEXT:    [[D_REALP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[D]], i32 0, i32 0
// RISCV64-LP64F-NEXT:    [[D_IMAGP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[D]], i32 0, i32 1
// RISCV64-LP64F-NEXT:    store double 1.250000e+00, double* [[D_REALP]], align 8
// RISCV64-LP64F-NEXT:    store double 0.000000e+00, double* [[D_IMAGP]], align 8
// RISCV64-LP64F-NEXT:    [[D_REALP1:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[D]], i32 0, i32 0
// RISCV64-LP64F-NEXT:    [[D_REAL:%.*]] = load double, double* [[D_REALP1]], align 8
// RISCV64-LP64F-NEXT:    [[D_IMAGP2:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[D]], i32 0, i32 1
// RISCV64-LP64F-NEXT:    [[D_IMAG:%.*]] = load double, double* [[D_IMAGP2]], align 8
// RISCV64-LP64F-NEXT:    [[COERCE_REALP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[COERCE]], i32 0, i32 0
// RISCV64-LP64F-NEXT:    [[COERCE_IMAGP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[COERCE]], i32 0, i32 1
// RISCV64-LP64F-NEXT:    store double [[D_REAL]], double* [[COERCE_REALP]], align 8
// RISCV64-LP64F-NEXT:    store double [[D_IMAG]], double* [[COERCE_IMAGP]], align 8
// RISCV64-LP64F-NEXT:    [[TMP0:%.*]] = bitcast { double, double }* [[COERCE]] to [2 x i64]*
// RISCV64-LP64F-NEXT:    [[TMP1:%.*]] = load [2 x i64], [2 x i64]* [[TMP0]], align 8
// RISCV64-LP64F-NEXT:    call void @complex_double([2 x i64] [[TMP1]])
// RISCV64-LP64F-NEXT:    ret void
//
// RISCV64-LP64D-LABEL: @test_complex_double(
// RISCV64-LP64D-NEXT:  entry:
// RISCV64-LP64D-NEXT:    [[D:%.*]] = alloca { double, double }, align 8
// RISCV64-LP64D-NEXT:    [[COERCE:%.*]] = alloca { double, double }, align 8
// RISCV64-LP64D-NEXT:    [[D_REALP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[D]], i32 0, i32 0
// RISCV64-LP64D-NEXT:    [[D_IMAGP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[D]], i32 0, i32 1
// RISCV64-LP64D-NEXT:    store double 1.250000e+00, double* [[D_REALP]], align 8
// RISCV64-LP64D-NEXT:    store double 0.000000e+00, double* [[D_IMAGP]], align 8
// RISCV64-LP64D-NEXT:    [[D_REALP1:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[D]], i32 0, i32 0
// RISCV64-LP64D-NEXT:    [[D_REAL:%.*]] = load double, double* [[D_REALP1]], align 8
// RISCV64-LP64D-NEXT:    [[D_IMAGP2:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[D]], i32 0, i32 1
// RISCV64-LP64D-NEXT:    [[D_IMAG:%.*]] = load double, double* [[D_IMAGP2]], align 8
// RISCV64-LP64D-NEXT:    [[COERCE_REALP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[COERCE]], i32 0, i32 0
// RISCV64-LP64D-NEXT:    [[COERCE_IMAGP:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[COERCE]], i32 0, i32 1
// RISCV64-LP64D-NEXT:    store double [[D_REAL]], double* [[COERCE_REALP]], align 8
// RISCV64-LP64D-NEXT:    store double [[D_IMAG]], double* [[COERCE_IMAGP]], align 8
// RISCV64-LP64D-NEXT:    [[TMP0:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[COERCE]], i32 0, i32 0
// RISCV64-LP64D-NEXT:    [[TMP1:%.*]] = load double, double* [[TMP0]], align 8
// RISCV64-LP64D-NEXT:    [[TMP2:%.*]] = getelementptr inbounds { double, double }, { double, double }* [[COERCE]], i32 0, i32 1
// RISCV64-LP64D-NEXT:    [[TMP3:%.*]] = load double, double* [[TMP2]], align 8
// RISCV64-LP64D-NEXT:    call void @complex_double(double [[TMP1]], double [[TMP3]])
// RISCV64-LP64D-NEXT:    ret void
//
void test_complex_double(void)
{
  _Complex double d = 1.25;
  complex_double(d);
}
