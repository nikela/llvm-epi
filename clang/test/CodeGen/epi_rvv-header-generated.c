// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang -ffreestanding --target=riscv64-unknown-linux-gnu -mepi -S -emit-llvm -O2 -o - -Xclang -no-opaque-pointers %s \
// RUN:       | FileCheck --check-prefix=CHECK-O2 %s

#include <epi_rvv.h>

// CHECK-O2-LABEL: @test_vadc_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vadc.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vadc_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vadc_8xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vadc_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vadc.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vadc_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vadc_4xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vadc_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vadc.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vadc_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vadc_2xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vadc_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vadc.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vadc_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vadc_1xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vadc_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vadc.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vadc_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vadc_16xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vadc_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vadc.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vadc_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vadc_8xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vadc_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vadc.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vadc_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vadc_4xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vadc_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vadc.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vadc_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vadc_2xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vadc_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vadc.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vadc_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vadc_32xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vadc_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vadc.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vadc_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vadc_16xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vadc_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vadc.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vadc_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vadc_8xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vadc_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vadc.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vadc_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vadc_4xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vadd_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vadd.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vadd_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vadd_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vadd_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vadd.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vadd_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vadd_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vadd_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vadd.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vadd_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vadd_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vadd_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vadd.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vadd_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vadd_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vadd_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vadd.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vadd_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vadd_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vadd_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vadd.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vadd_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vadd_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vadd_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vadd.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vadd_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vadd_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vadd_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vadd.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vadd_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vadd_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vadd_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vadd.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vadd_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vadd_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vadd_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vadd.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vadd_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vadd_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vadd_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vadd.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vadd_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vadd_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vadd_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vadd.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vadd_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vadd_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vadd_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vadd.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vadd_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vadd_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vadd_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vadd.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vadd_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vadd_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vadd_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vadd.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vadd_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vadd_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vadd_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vadd.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vadd_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vadd_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vadd_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vadd.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vadd_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vadd_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vadd_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vadd.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vadd_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vadd_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vadd_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vadd.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vadd_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vadd_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vadd_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vadd.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vadd_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vadd_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vadd_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vadd.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vadd_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vadd_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vadd_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vadd.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vadd_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vadd_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vadd_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vadd.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vadd_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vadd_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vadd_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vadd.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vadd_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vadd_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vand_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vand.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vand_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vand_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vand_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vand.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vand_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vand_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vand_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vand.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vand_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vand_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vand_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vand.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vand_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vand_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vand_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vand.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vand_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vand_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vand_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vand.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vand_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vand_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vand_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vand.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vand_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vand_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vand_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vand.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vand_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vand_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vand_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vand.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vand_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vand_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vand_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vand.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vand_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vand_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vand_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vand.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vand_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vand_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vand_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vand.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vand_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vand_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vand_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vand.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vand_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vand_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vand_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vand.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vand_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vand_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vand_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vand.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vand_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vand_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vand_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vand.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vand_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vand_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vand_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vand.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vand_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vand_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vand_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vand.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vand_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vand_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vand_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vand.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vand_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vand_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vand_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vand.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vand_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vand_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vand_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vand.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vand_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vand_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vand_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vand.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vand_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vand_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vand_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vand.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vand_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vand_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vand_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vand.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vand_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vand_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vcompress_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vcompress.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vcompress_8xi8(__epi_8xi8 arg_0, __epi_8xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vcompress_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vcompress_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vcompress.nxv4i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vcompress_4xi16(__epi_4xi16 arg_0, __epi_4xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vcompress_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vcompress_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vcompress.nxv2i32.i64(<vscale x 2 x i32> poison, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vcompress_2xi32(__epi_2xi32 arg_0, __epi_2xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vcompress_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vcompress_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vcompress.nxv1i64.i64(<vscale x 1 x i64> poison, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vcompress_1xi64(__epi_1xi64 arg_0, __epi_1xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vcompress_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vcompress_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vcompress.nxv2f32.i64(<vscale x 2 x float> poison, <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vcompress_2xf32(__epi_2xf32 arg_0, __epi_2xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vcompress_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vcompress_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vcompress.nxv1f64.i64(<vscale x 1 x double> poison, <vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vcompress_1xf64(__epi_1xf64 arg_0, __epi_1xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vcompress_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vcompress_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vcompress.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vcompress_16xi8(__epi_16xi8 arg_0, __epi_16xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vcompress_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vcompress_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vcompress.nxv8i16.i64(<vscale x 8 x i16> poison, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vcompress_8xi16(__epi_8xi16 arg_0, __epi_8xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vcompress_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vcompress_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vcompress.nxv4i32.i64(<vscale x 4 x i32> poison, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vcompress_4xi32(__epi_4xi32 arg_0, __epi_4xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vcompress_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vcompress_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vcompress.nxv2i64.i64(<vscale x 2 x i64> poison, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vcompress_2xi64(__epi_2xi64 arg_0, __epi_2xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vcompress_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vcompress_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vcompress.nxv4f32.i64(<vscale x 4 x float> poison, <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vcompress_4xf32(__epi_4xf32 arg_0, __epi_4xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vcompress_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vcompress_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vcompress.nxv2f64.i64(<vscale x 2 x double> poison, <vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vcompress_2xf64(__epi_2xf64 arg_0, __epi_2xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vcompress_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vcompress_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vcompress.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vcompress_32xi8(__epi_32xi8 arg_0, __epi_32xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vcompress_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vcompress_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vcompress.nxv16i16.i64(<vscale x 16 x i16> poison, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vcompress_16xi16(__epi_16xi16 arg_0, __epi_16xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vcompress_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vcompress_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vcompress.nxv8i32.i64(<vscale x 8 x i32> poison, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vcompress_8xi32(__epi_8xi32 arg_0, __epi_8xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vcompress_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vcompress_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vcompress.nxv4i64.i64(<vscale x 4 x i64> poison, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vcompress_4xi64(__epi_4xi64 arg_0, __epi_4xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vcompress_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vcompress_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vcompress.nxv8f32.i64(<vscale x 8 x float> poison, <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vcompress_8xf32(__epi_8xf32 arg_0, __epi_8xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vcompress_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vcompress_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vcompress.nxv4f64.i64(<vscale x 4 x double> poison, <vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vcompress_4xf64(__epi_4xf64 arg_0, __epi_4xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vcompress_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdiv_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vdiv.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vdiv_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdiv_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdiv_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vdiv.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vdiv_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdiv_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vdiv_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vdiv.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vdiv_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdiv_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdiv_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vdiv.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vdiv_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdiv_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vdiv_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vdiv.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vdiv_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdiv_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdiv_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vdiv.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vdiv_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdiv_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vdiv_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vdiv.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vdiv_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdiv_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdiv_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vdiv.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vdiv_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdiv_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vdiv_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vdiv.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vdiv_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdiv_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdiv_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vdiv.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vdiv_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdiv_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vdiv_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vdiv.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vdiv_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdiv_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdiv_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vdiv.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vdiv_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdiv_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vdiv_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vdiv.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vdiv_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdiv_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdiv_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vdiv.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vdiv_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdiv_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vdiv_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vdiv.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vdiv_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdiv_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdiv_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vdiv.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vdiv_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdiv_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vdiv_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vdiv.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vdiv_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdiv_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdiv_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vdiv.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vdiv_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdiv_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vdiv_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vdiv.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vdiv_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdiv_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdiv_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vdiv.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vdiv_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdiv_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vdiv_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vdiv.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vdiv_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdiv_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdiv_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vdiv.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vdiv_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdiv_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vdiv_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vdiv.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vdiv_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdiv_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdiv_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vdiv.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vdiv_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdiv_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vdivu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vdivu.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vdivu_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdivu_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdivu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vdivu.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vdivu_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdivu_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vdivu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vdivu.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vdivu_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdivu_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdivu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vdivu.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vdivu_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdivu_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vdivu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vdivu.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vdivu_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdivu_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdivu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vdivu.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vdivu_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdivu_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vdivu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vdivu.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vdivu_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdivu_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdivu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vdivu.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vdivu_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdivu_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vdivu_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vdivu.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vdivu_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdivu_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdivu_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vdivu.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vdivu_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdivu_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vdivu_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vdivu.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vdivu_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdivu_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdivu_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vdivu.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vdivu_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdivu_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vdivu_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vdivu.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vdivu_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdivu_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdivu_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vdivu.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vdivu_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdivu_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vdivu_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vdivu.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vdivu_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdivu_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdivu_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vdivu.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vdivu_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdivu_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vdivu_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vdivu.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vdivu_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdivu_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdivu_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vdivu.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vdivu_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdivu_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vdivu_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vdivu.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vdivu_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdivu_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdivu_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vdivu.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vdivu_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdivu_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vdivu_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vdivu.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vdivu_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdivu_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdivu_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vdivu.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vdivu_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdivu_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vdivu_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vdivu.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vdivu_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vdivu_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vdivu_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vdivu.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vdivu_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vdivu_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfadd_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfadd.nxv2f32.nxv2f32.i64(<vscale x 2 x float> undef, <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfadd_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfadd_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfadd_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfadd.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfadd_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfadd_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfadd_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfadd.nxv1f64.nxv1f64.i64(<vscale x 1 x double> undef, <vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfadd_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfadd_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfadd_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfadd.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfadd_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfadd_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfadd_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfadd.nxv4f32.nxv4f32.i64(<vscale x 4 x float> undef, <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfadd_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfadd_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfadd_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfadd.mask.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfadd_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfadd_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfadd_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfadd.nxv2f64.nxv2f64.i64(<vscale x 2 x double> undef, <vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfadd_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfadd_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfadd_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfadd.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfadd_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfadd_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfadd_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfadd.nxv8f32.nxv8f32.i64(<vscale x 8 x float> undef, <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfadd_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfadd_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfadd_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfadd.mask.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfadd_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfadd_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfadd_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfadd.nxv4f64.nxv4f64.i64(<vscale x 4 x double> undef, <vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfadd_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfadd_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfadd_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfadd.mask.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfadd_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfadd_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfcvt_f_x_2xf32_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfcvt.f.x.v.nxv2f32.nxv2i32.i64(<vscale x 2 x float> undef, <vscale x 2 x i32> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfcvt_f_x_2xf32_2xi32(__epi_2xi32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_f_x_2xf32_2xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_f_x_2xf32_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfcvt.f.x.v.mask.nxv2f32.nxv2i32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfcvt_f_x_2xf32_2xi32_mask(__epi_2xf32 arg_0, __epi_2xi32 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_f_x_2xf32_2xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfcvt_f_x_1xf64_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfcvt.f.x.v.nxv1f64.nxv1i64.i64(<vscale x 1 x double> undef, <vscale x 1 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfcvt_f_x_1xf64_1xi64(__epi_1xi64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_f_x_1xf64_1xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_f_x_1xf64_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfcvt.f.x.v.mask.nxv1f64.nxv1i64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfcvt_f_x_1xf64_1xi64_mask(__epi_1xf64 arg_0, __epi_1xi64 arg_1, __epi_1xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_f_x_1xf64_1xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfcvt_f_x_4xf32_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfcvt.f.x.v.nxv4f32.nxv4i32.i64(<vscale x 4 x float> undef, <vscale x 4 x i32> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfcvt_f_x_4xf32_4xi32(__epi_4xi32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_f_x_4xf32_4xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_f_x_4xf32_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfcvt.f.x.v.mask.nxv4f32.nxv4i32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfcvt_f_x_4xf32_4xi32_mask(__epi_4xf32 arg_0, __epi_4xi32 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_f_x_4xf32_4xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfcvt_f_x_2xf64_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfcvt.f.x.v.nxv2f64.nxv2i64.i64(<vscale x 2 x double> undef, <vscale x 2 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfcvt_f_x_2xf64_2xi64(__epi_2xi64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_f_x_2xf64_2xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_f_x_2xf64_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfcvt.f.x.v.mask.nxv2f64.nxv2i64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfcvt_f_x_2xf64_2xi64_mask(__epi_2xf64 arg_0, __epi_2xi64 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_f_x_2xf64_2xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfcvt_f_x_8xf32_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfcvt.f.x.v.nxv8f32.nxv8i32.i64(<vscale x 8 x float> undef, <vscale x 8 x i32> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfcvt_f_x_8xf32_8xi32(__epi_8xi32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_f_x_8xf32_8xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_f_x_8xf32_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfcvt.f.x.v.mask.nxv8f32.nxv8i32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfcvt_f_x_8xf32_8xi32_mask(__epi_8xf32 arg_0, __epi_8xi32 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_f_x_8xf32_8xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfcvt_f_x_4xf64_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfcvt.f.x.v.nxv4f64.nxv4i64.i64(<vscale x 4 x double> undef, <vscale x 4 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfcvt_f_x_4xf64_4xi64(__epi_4xi64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_f_x_4xf64_4xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_f_x_4xf64_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfcvt.f.x.v.mask.nxv4f64.nxv4i64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfcvt_f_x_4xf64_4xi64_mask(__epi_4xf64 arg_0, __epi_4xi64 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_f_x_4xf64_4xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfcvt_f_xu_2xf32_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfcvt.f.xu.v.nxv2f32.nxv2i32.i64(<vscale x 2 x float> undef, <vscale x 2 x i32> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfcvt_f_xu_2xf32_2xi32(__epi_2xi32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_f_xu_2xf32_2xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_f_xu_2xf32_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfcvt.f.xu.v.mask.nxv2f32.nxv2i32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfcvt_f_xu_2xf32_2xi32_mask(__epi_2xf32 arg_0, __epi_2xi32 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_f_xu_2xf32_2xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfcvt_f_xu_1xf64_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfcvt.f.xu.v.nxv1f64.nxv1i64.i64(<vscale x 1 x double> undef, <vscale x 1 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfcvt_f_xu_1xf64_1xi64(__epi_1xi64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_f_xu_1xf64_1xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_f_xu_1xf64_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfcvt.f.xu.v.mask.nxv1f64.nxv1i64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfcvt_f_xu_1xf64_1xi64_mask(__epi_1xf64 arg_0, __epi_1xi64 arg_1, __epi_1xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_f_xu_1xf64_1xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfcvt_f_xu_4xf32_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfcvt.f.xu.v.nxv4f32.nxv4i32.i64(<vscale x 4 x float> undef, <vscale x 4 x i32> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfcvt_f_xu_4xf32_4xi32(__epi_4xi32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_f_xu_4xf32_4xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_f_xu_4xf32_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfcvt.f.xu.v.mask.nxv4f32.nxv4i32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfcvt_f_xu_4xf32_4xi32_mask(__epi_4xf32 arg_0, __epi_4xi32 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_f_xu_4xf32_4xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfcvt_f_xu_2xf64_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfcvt.f.xu.v.nxv2f64.nxv2i64.i64(<vscale x 2 x double> undef, <vscale x 2 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfcvt_f_xu_2xf64_2xi64(__epi_2xi64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_f_xu_2xf64_2xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_f_xu_2xf64_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfcvt.f.xu.v.mask.nxv2f64.nxv2i64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfcvt_f_xu_2xf64_2xi64_mask(__epi_2xf64 arg_0, __epi_2xi64 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_f_xu_2xf64_2xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfcvt_f_xu_8xf32_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfcvt.f.xu.v.nxv8f32.nxv8i32.i64(<vscale x 8 x float> undef, <vscale x 8 x i32> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfcvt_f_xu_8xf32_8xi32(__epi_8xi32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_f_xu_8xf32_8xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_f_xu_8xf32_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfcvt.f.xu.v.mask.nxv8f32.nxv8i32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfcvt_f_xu_8xf32_8xi32_mask(__epi_8xf32 arg_0, __epi_8xi32 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_f_xu_8xf32_8xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfcvt_f_xu_4xf64_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfcvt.f.xu.v.nxv4f64.nxv4i64.i64(<vscale x 4 x double> undef, <vscale x 4 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfcvt_f_xu_4xf64_4xi64(__epi_4xi64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_f_xu_4xf64_4xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_f_xu_4xf64_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfcvt.f.xu.v.mask.nxv4f64.nxv4i64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfcvt_f_xu_4xf64_4xi64_mask(__epi_4xf64 arg_0, __epi_4xi64 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_f_xu_4xf64_4xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfcvt_x_f_2xi32_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vfcvt.x.f.v.nxv2i32.nxv2f32.i64(<vscale x 2 x i32> undef, <vscale x 2 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vfcvt_x_f_2xi32_2xf32(__epi_2xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_x_f_2xi32_2xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_x_f_2xi32_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vfcvt.x.f.v.mask.nxv2i32.nxv2f32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vfcvt_x_f_2xi32_2xf32_mask(__epi_2xi32 arg_0, __epi_2xf32 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_x_f_2xi32_2xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfcvt_x_f_1xi64_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vfcvt.x.f.v.nxv1i64.nxv1f64.i64(<vscale x 1 x i64> undef, <vscale x 1 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vfcvt_x_f_1xi64_1xf64(__epi_1xf64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_x_f_1xi64_1xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_x_f_1xi64_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vfcvt.x.f.v.mask.nxv1i64.nxv1f64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vfcvt_x_f_1xi64_1xf64_mask(__epi_1xi64 arg_0, __epi_1xf64 arg_1, __epi_1xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_x_f_1xi64_1xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfcvt_x_f_4xi32_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vfcvt.x.f.v.nxv4i32.nxv4f32.i64(<vscale x 4 x i32> undef, <vscale x 4 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vfcvt_x_f_4xi32_4xf32(__epi_4xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_x_f_4xi32_4xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_x_f_4xi32_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vfcvt.x.f.v.mask.nxv4i32.nxv4f32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vfcvt_x_f_4xi32_4xf32_mask(__epi_4xi32 arg_0, __epi_4xf32 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_x_f_4xi32_4xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfcvt_x_f_2xi64_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vfcvt.x.f.v.nxv2i64.nxv2f64.i64(<vscale x 2 x i64> undef, <vscale x 2 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vfcvt_x_f_2xi64_2xf64(__epi_2xf64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_x_f_2xi64_2xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_x_f_2xi64_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vfcvt.x.f.v.mask.nxv2i64.nxv2f64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vfcvt_x_f_2xi64_2xf64_mask(__epi_2xi64 arg_0, __epi_2xf64 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_x_f_2xi64_2xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfcvt_x_f_8xi32_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vfcvt.x.f.v.nxv8i32.nxv8f32.i64(<vscale x 8 x i32> undef, <vscale x 8 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vfcvt_x_f_8xi32_8xf32(__epi_8xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_x_f_8xi32_8xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_x_f_8xi32_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vfcvt.x.f.v.mask.nxv8i32.nxv8f32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vfcvt_x_f_8xi32_8xf32_mask(__epi_8xi32 arg_0, __epi_8xf32 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_x_f_8xi32_8xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfcvt_x_f_4xi64_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vfcvt.x.f.v.nxv4i64.nxv4f64.i64(<vscale x 4 x i64> undef, <vscale x 4 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vfcvt_x_f_4xi64_4xf64(__epi_4xf64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_x_f_4xi64_4xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_x_f_4xi64_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vfcvt.x.f.v.mask.nxv4i64.nxv4f64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vfcvt_x_f_4xi64_4xf64_mask(__epi_4xi64 arg_0, __epi_4xf64 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_x_f_4xi64_4xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfcvt_xu_f_2xi32_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vfcvt.xu.f.v.nxv2i32.nxv2f32.i64(<vscale x 2 x i32> undef, <vscale x 2 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vfcvt_xu_f_2xi32_2xf32(__epi_2xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_xu_f_2xi32_2xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_xu_f_2xi32_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vfcvt.xu.f.v.mask.nxv2i32.nxv2f32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vfcvt_xu_f_2xi32_2xf32_mask(__epi_2xi32 arg_0, __epi_2xf32 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_xu_f_2xi32_2xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfcvt_xu_f_1xi64_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vfcvt.xu.f.v.nxv1i64.nxv1f64.i64(<vscale x 1 x i64> undef, <vscale x 1 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vfcvt_xu_f_1xi64_1xf64(__epi_1xf64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_xu_f_1xi64_1xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_xu_f_1xi64_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vfcvt.xu.f.v.mask.nxv1i64.nxv1f64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vfcvt_xu_f_1xi64_1xf64_mask(__epi_1xi64 arg_0, __epi_1xf64 arg_1, __epi_1xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_xu_f_1xi64_1xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfcvt_xu_f_4xi32_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vfcvt.xu.f.v.nxv4i32.nxv4f32.i64(<vscale x 4 x i32> undef, <vscale x 4 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vfcvt_xu_f_4xi32_4xf32(__epi_4xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_xu_f_4xi32_4xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_xu_f_4xi32_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vfcvt.xu.f.v.mask.nxv4i32.nxv4f32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vfcvt_xu_f_4xi32_4xf32_mask(__epi_4xi32 arg_0, __epi_4xf32 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_xu_f_4xi32_4xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfcvt_xu_f_2xi64_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vfcvt.xu.f.v.nxv2i64.nxv2f64.i64(<vscale x 2 x i64> undef, <vscale x 2 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vfcvt_xu_f_2xi64_2xf64(__epi_2xf64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_xu_f_2xi64_2xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_xu_f_2xi64_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vfcvt.xu.f.v.mask.nxv2i64.nxv2f64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vfcvt_xu_f_2xi64_2xf64_mask(__epi_2xi64 arg_0, __epi_2xf64 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_xu_f_2xi64_2xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfcvt_xu_f_8xi32_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vfcvt.xu.f.v.nxv8i32.nxv8f32.i64(<vscale x 8 x i32> undef, <vscale x 8 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vfcvt_xu_f_8xi32_8xf32(__epi_8xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_xu_f_8xi32_8xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_xu_f_8xi32_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vfcvt.xu.f.v.mask.nxv8i32.nxv8f32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vfcvt_xu_f_8xi32_8xf32_mask(__epi_8xi32 arg_0, __epi_8xf32 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_xu_f_8xi32_8xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfcvt_xu_f_4xi64_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vfcvt.xu.f.v.nxv4i64.nxv4f64.i64(<vscale x 4 x i64> undef, <vscale x 4 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vfcvt_xu_f_4xi64_4xf64(__epi_4xf64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfcvt_xu_f_4xi64_4xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfcvt_xu_f_4xi64_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vfcvt.xu.f.v.mask.nxv4i64.nxv4f64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vfcvt_xu_f_4xi64_4xf64_mask(__epi_4xi64 arg_0, __epi_4xf64 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfcvt_xu_f_4xi64_4xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfdiv_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfdiv.nxv2f32.nxv2f32.i64(<vscale x 2 x float> undef, <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfdiv_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfdiv_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfdiv_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfdiv.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfdiv_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfdiv_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfdiv_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfdiv.nxv1f64.nxv1f64.i64(<vscale x 1 x double> undef, <vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfdiv_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfdiv_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfdiv_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfdiv.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfdiv_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfdiv_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfdiv_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfdiv.nxv4f32.nxv4f32.i64(<vscale x 4 x float> undef, <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfdiv_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfdiv_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfdiv_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfdiv.mask.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfdiv_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfdiv_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfdiv_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfdiv.nxv2f64.nxv2f64.i64(<vscale x 2 x double> undef, <vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfdiv_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfdiv_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfdiv_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfdiv.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfdiv_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfdiv_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfdiv_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfdiv.nxv8f32.nxv8f32.i64(<vscale x 8 x float> undef, <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfdiv_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfdiv_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfdiv_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfdiv.mask.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfdiv_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfdiv_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfdiv_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfdiv.nxv4f64.nxv4f64.i64(<vscale x 4 x double> undef, <vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfdiv_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfdiv_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfdiv_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfdiv.mask.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfdiv_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfdiv_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfirst_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vfirst.nxv8i1.i64(<vscale x 8 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vfirst_8xi1(__epi_8xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfirst_8xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfirst_8xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vfirst.mask.nxv8i1.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vfirst_8xi1_mask(__epi_8xi1 arg_0, __epi_8xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfirst_8xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfirst_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vfirst.nxv4i1.i64(<vscale x 4 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vfirst_4xi1(__epi_4xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfirst_4xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfirst_4xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vfirst.mask.nxv4i1.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vfirst_4xi1_mask(__epi_4xi1 arg_0, __epi_4xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfirst_4xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfirst_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vfirst.nxv2i1.i64(<vscale x 2 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vfirst_2xi1(__epi_2xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfirst_2xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfirst_2xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vfirst.mask.nxv2i1.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vfirst_2xi1_mask(__epi_2xi1 arg_0, __epi_2xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfirst_2xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfirst_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vfirst.nxv1i1.i64(<vscale x 1 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vfirst_1xi1(__epi_1xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfirst_1xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfirst_1xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vfirst.mask.nxv1i1.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vfirst_1xi1_mask(__epi_1xi1 arg_0, __epi_1xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfirst_1xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfirst_16xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vfirst.nxv16i1.i64(<vscale x 16 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vfirst_16xi1(__epi_16xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfirst_16xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfirst_16xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vfirst.mask.nxv16i1.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vfirst_16xi1_mask(__epi_16xi1 arg_0, __epi_16xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfirst_16xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfirst_32xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vfirst.nxv32i1.i64(<vscale x 32 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vfirst_32xi1(__epi_32xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfirst_32xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfirst_32xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vfirst.mask.nxv32i1.i64(<vscale x 32 x i1> [[ARG_0:%.*]], <vscale x 32 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vfirst_32xi1_mask(__epi_32xi1 arg_0, __epi_32xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfirst_32xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmacc_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfmacc.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfmacc_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmacc_2xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmacc_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfmacc.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfmacc_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmacc_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmacc_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfmacc.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfmacc_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmacc_1xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmacc_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfmacc.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfmacc_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmacc_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmacc_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfmacc.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfmacc_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmacc_4xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmacc_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfmacc.mask.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfmacc_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmacc_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmacc_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfmacc.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfmacc_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmacc_2xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmacc_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfmacc.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfmacc_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmacc_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmacc_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfmacc.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfmacc_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmacc_8xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmacc_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfmacc.mask.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfmacc_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmacc_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmacc_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfmacc.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfmacc_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmacc_4xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmacc_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfmacc.mask.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfmacc_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmacc_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmadd_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfmadd.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfmadd_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmadd_2xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmadd_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfmadd.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfmadd_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmadd_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmadd_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfmadd.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfmadd_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmadd_1xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmadd_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfmadd.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfmadd_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmadd_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmadd_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfmadd.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfmadd_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmadd_4xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmadd_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfmadd.mask.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfmadd_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmadd_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmadd_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfmadd.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfmadd_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmadd_2xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmadd_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfmadd.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfmadd_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmadd_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmadd_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfmadd.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfmadd_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmadd_8xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmadd_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfmadd.mask.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfmadd_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmadd_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmadd_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfmadd.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfmadd_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmadd_4xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmadd_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfmadd.mask.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfmadd_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmadd_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmax_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfmax.nxv2f32.nxv2f32.i64(<vscale x 2 x float> undef, <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfmax_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmax_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmax_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfmax.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfmax_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmax_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmax_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfmax.nxv1f64.nxv1f64.i64(<vscale x 1 x double> undef, <vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfmax_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmax_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmax_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfmax.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfmax_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmax_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmax_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfmax.nxv4f32.nxv4f32.i64(<vscale x 4 x float> undef, <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfmax_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmax_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmax_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfmax.mask.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfmax_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmax_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmax_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfmax.nxv2f64.nxv2f64.i64(<vscale x 2 x double> undef, <vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfmax_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmax_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmax_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfmax.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfmax_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmax_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmax_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfmax.nxv8f32.nxv8f32.i64(<vscale x 8 x float> undef, <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfmax_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmax_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmax_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfmax.mask.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfmax_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmax_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmax_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfmax.nxv4f64.nxv4f64.i64(<vscale x 4 x double> undef, <vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfmax_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmax_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmax_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfmax.mask.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfmax_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmax_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmerge_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vmerge.nxv2f32.nxv2f32.i64(<vscale x 2 x float> poison, <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfmerge_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmerge_2xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmerge_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vmerge.nxv1f64.nxv1f64.i64(<vscale x 1 x double> poison, <vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfmerge_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmerge_1xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmerge_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vmerge.nxv4f32.nxv4f32.i64(<vscale x 4 x float> poison, <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfmerge_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmerge_4xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmerge_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vmerge.nxv2f64.nxv2f64.i64(<vscale x 2 x double> poison, <vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfmerge_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmerge_2xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmerge_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vmerge.nxv8f32.nxv8f32.i64(<vscale x 8 x float> poison, <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfmerge_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmerge_8xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmerge_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vmerge.nxv4f64.nxv4f64.i64(<vscale x 4 x double> poison, <vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfmerge_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmerge_4xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmin_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfmin.nxv2f32.nxv2f32.i64(<vscale x 2 x float> undef, <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfmin_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmin_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmin_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfmin.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfmin_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmin_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmin_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfmin.nxv1f64.nxv1f64.i64(<vscale x 1 x double> undef, <vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfmin_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmin_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmin_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfmin.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfmin_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmin_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmin_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfmin.nxv4f32.nxv4f32.i64(<vscale x 4 x float> undef, <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfmin_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmin_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmin_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfmin.mask.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfmin_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmin_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmin_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfmin.nxv2f64.nxv2f64.i64(<vscale x 2 x double> undef, <vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfmin_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmin_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmin_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfmin.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfmin_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmin_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmin_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfmin.nxv8f32.nxv8f32.i64(<vscale x 8 x float> undef, <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfmin_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmin_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmin_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfmin.mask.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfmin_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmin_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmin_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfmin.nxv4f64.nxv4f64.i64(<vscale x 4 x double> undef, <vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfmin_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmin_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmin_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfmin.mask.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfmin_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmin_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmsac_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfmsac.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfmsac_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmsac_2xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmsac_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfmsac.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfmsac_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmsac_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmsac_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfmsac.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfmsac_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmsac_1xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmsac_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfmsac.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfmsac_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmsac_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmsac_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfmsac.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfmsac_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmsac_4xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmsac_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfmsac.mask.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfmsac_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmsac_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmsac_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfmsac.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfmsac_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmsac_2xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmsac_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfmsac.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfmsac_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmsac_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmsac_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfmsac.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfmsac_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmsac_8xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmsac_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfmsac.mask.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfmsac_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmsac_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmsac_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfmsac.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfmsac_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmsac_4xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmsac_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfmsac.mask.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfmsac_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmsac_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmsub_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfmsub.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfmsub_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmsub_2xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmsub_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfmsub.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfmsub_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmsub_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmsub_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfmsub.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfmsub_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmsub_1xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmsub_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfmsub.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfmsub_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmsub_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmsub_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfmsub.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfmsub_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmsub_4xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmsub_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfmsub.mask.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfmsub_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmsub_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmsub_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfmsub.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfmsub_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmsub_2xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmsub_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfmsub.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfmsub_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmsub_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmsub_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfmsub.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfmsub_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmsub_8xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmsub_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfmsub.mask.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfmsub_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmsub_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmsub_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfmsub.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfmsub_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfmsub_4xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfmsub_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfmsub.mask.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfmsub_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmsub_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmul_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfmul.nxv2f32.nxv2f32.i64(<vscale x 2 x float> undef, <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfmul_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmul_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmul_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfmul.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfmul_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmul_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmul_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfmul.nxv1f64.nxv1f64.i64(<vscale x 1 x double> undef, <vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfmul_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmul_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmul_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfmul.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfmul_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmul_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmul_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfmul.nxv4f32.nxv4f32.i64(<vscale x 4 x float> undef, <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfmul_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmul_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmul_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfmul.mask.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfmul_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmul_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmul_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfmul.nxv2f64.nxv2f64.i64(<vscale x 2 x double> undef, <vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfmul_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmul_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmul_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfmul.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfmul_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmul_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmul_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfmul.nxv8f32.nxv8f32.i64(<vscale x 8 x float> undef, <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfmul_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmul_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmul_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfmul.mask.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfmul_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmul_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmul_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfmul.nxv4f64.nxv4f64.i64(<vscale x 4 x double> undef, <vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfmul_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmul_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmul_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfmul.mask.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfmul_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfmul_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfmv_f_s_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call float @llvm.riscv.vfmv.f.s.nxv2f32(<vscale x 2 x float> [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret float [[TMP0]]
//
float test_vfmv_f_s_2xf32(__epi_2xf32 arg_0)
{
    return __builtin_epi_vfmv_f_s_2xf32(arg_0);
}

// CHECK-O2-LABEL: @test_vfmv_f_s_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call double @llvm.riscv.vfmv.f.s.nxv1f64(<vscale x 1 x double> [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret double [[TMP0]]
//
double test_vfmv_f_s_1xf64(__epi_1xf64 arg_0)
{
    return __builtin_epi_vfmv_f_s_1xf64(arg_0);
}

// CHECK-O2-LABEL: @test_vfmv_f_s_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call float @llvm.riscv.vfmv.f.s.nxv4f32(<vscale x 4 x float> [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret float [[TMP0]]
//
float test_vfmv_f_s_4xf32(__epi_4xf32 arg_0)
{
    return __builtin_epi_vfmv_f_s_4xf32(arg_0);
}

// CHECK-O2-LABEL: @test_vfmv_f_s_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call double @llvm.riscv.vfmv.f.s.nxv2f64(<vscale x 2 x double> [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret double [[TMP0]]
//
double test_vfmv_f_s_2xf64(__epi_2xf64 arg_0)
{
    return __builtin_epi_vfmv_f_s_2xf64(arg_0);
}

// CHECK-O2-LABEL: @test_vfmv_f_s_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call float @llvm.riscv.vfmv.f.s.nxv8f32(<vscale x 8 x float> [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret float [[TMP0]]
//
float test_vfmv_f_s_8xf32(__epi_8xf32 arg_0)
{
    return __builtin_epi_vfmv_f_s_8xf32(arg_0);
}

// CHECK-O2-LABEL: @test_vfmv_f_s_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call double @llvm.riscv.vfmv.f.s.nxv4f64(<vscale x 4 x double> [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret double [[TMP0]]
//
double test_vfmv_f_s_4xf64(__epi_4xf64 arg_0)
{
    return __builtin_epi_vfmv_f_s_4xf64(arg_0);
}

// CHECK-O2-LABEL: @test_vfmv_s_f_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfmv.s.f.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], float [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfmv_s_f_2xf32(__epi_2xf32 arg_0, float arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmv_s_f_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmv_s_f_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfmv.s.f.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], double [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfmv_s_f_1xf64(__epi_1xf64 arg_0, double arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmv_s_f_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmv_s_f_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfmv.s.f.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], float [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfmv_s_f_4xf32(__epi_4xf32 arg_0, float arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmv_s_f_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmv_s_f_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfmv.s.f.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], double [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfmv_s_f_2xf64(__epi_2xf64 arg_0, double arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmv_s_f_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmv_s_f_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfmv.s.f.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], float [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfmv_s_f_8xf32(__epi_8xf32 arg_0, float arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmv_s_f_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmv_s_f_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfmv.s.f.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], double [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfmv_s_f_4xf64(__epi_4xf64 arg_0, double arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfmv_s_f_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfmv_v_f_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfmv.v.f.nxv2f32.i64(<vscale x 2 x float> undef, float [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfmv_v_f_2xf32(float arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfmv_v_f_2xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfmv_v_f_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfmv.v.f.nxv1f64.i64(<vscale x 1 x double> undef, double [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfmv_v_f_1xf64(double arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfmv_v_f_1xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfmv_v_f_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfmv.v.f.nxv4f32.i64(<vscale x 4 x float> undef, float [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfmv_v_f_4xf32(float arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfmv_v_f_4xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfmv_v_f_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfmv.v.f.nxv2f64.i64(<vscale x 2 x double> undef, double [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfmv_v_f_2xf64(double arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfmv_v_f_2xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfmv_v_f_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfmv.v.f.nxv8f32.i64(<vscale x 8 x float> undef, float [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfmv_v_f_8xf32(float arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfmv_v_f_8xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfmv_v_f_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfmv.v.f.nxv4f64.i64(<vscale x 4 x double> undef, double [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfmv_v_f_4xf64(double arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfmv_v_f_4xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfncvt_f_f_2xf32_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfncvt.f.f.w.nxv2f32.nxv2f64.i64(<vscale x 2 x float> undef, <vscale x 2 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfncvt_f_f_2xf32_2xf64(__epi_2xf64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfncvt_f_f_2xf32_2xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfncvt_f_f_2xf32_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfncvt.f.f.w.mask.nxv2f32.nxv2f64.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfncvt_f_f_2xf32_2xf64_mask(__epi_2xf32 arg_0, __epi_2xf64 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfncvt_f_f_2xf32_2xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfncvt_f_f_4xf32_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfncvt.f.f.w.nxv4f32.nxv4f64.i64(<vscale x 4 x float> undef, <vscale x 4 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfncvt_f_f_4xf32_4xf64(__epi_4xf64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfncvt_f_f_4xf32_4xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfncvt_f_f_4xf32_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfncvt.f.f.w.mask.nxv4f32.nxv4f64.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfncvt_f_f_4xf32_4xf64_mask(__epi_4xf32 arg_0, __epi_4xf64 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfncvt_f_f_4xf32_4xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfncvt_f_f_8xf32_8xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfncvt.f.f.w.nxv8f32.nxv8f64.i64(<vscale x 8 x float> undef, <vscale x 8 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfncvt_f_f_8xf32_8xf64(__epi_8xf64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfncvt_f_f_8xf32_8xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfncvt_f_f_8xf32_8xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfncvt.f.f.w.mask.nxv8f32.nxv8f64.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x double> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfncvt_f_f_8xf32_8xf64_mask(__epi_8xf32 arg_0, __epi_8xf64 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfncvt_f_f_8xf32_8xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfncvt_f_x_2xf32_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfncvt.f.x.w.nxv2f32.nxv2i64.i64(<vscale x 2 x float> undef, <vscale x 2 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfncvt_f_x_2xf32_2xi64(__epi_2xi64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfncvt_f_x_2xf32_2xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfncvt_f_x_2xf32_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfncvt.f.x.w.mask.nxv2f32.nxv2i64.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfncvt_f_x_2xf32_2xi64_mask(__epi_2xf32 arg_0, __epi_2xi64 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfncvt_f_x_2xf32_2xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfncvt_f_x_4xf32_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfncvt.f.x.w.nxv4f32.nxv4i64.i64(<vscale x 4 x float> undef, <vscale x 4 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfncvt_f_x_4xf32_4xi64(__epi_4xi64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfncvt_f_x_4xf32_4xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfncvt_f_x_4xf32_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfncvt.f.x.w.mask.nxv4f32.nxv4i64.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfncvt_f_x_4xf32_4xi64_mask(__epi_4xf32 arg_0, __epi_4xi64 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfncvt_f_x_4xf32_4xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfncvt_f_x_8xf32_8xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfncvt.f.x.w.nxv8f32.nxv8i64.i64(<vscale x 8 x float> undef, <vscale x 8 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfncvt_f_x_8xf32_8xi64(__epi_8xi64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfncvt_f_x_8xf32_8xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfncvt_f_x_8xf32_8xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfncvt.f.x.w.mask.nxv8f32.nxv8i64.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x i64> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfncvt_f_x_8xf32_8xi64_mask(__epi_8xf32 arg_0, __epi_8xi64 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfncvt_f_x_8xf32_8xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfncvt_f_xu_2xf32_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfncvt.f.xu.w.nxv2f32.nxv2i64.i64(<vscale x 2 x float> undef, <vscale x 2 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfncvt_f_xu_2xf32_2xi64(__epi_2xi64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfncvt_f_xu_2xf32_2xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfncvt_f_xu_2xf32_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfncvt.f.xu.w.mask.nxv2f32.nxv2i64.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfncvt_f_xu_2xf32_2xi64_mask(__epi_2xf32 arg_0, __epi_2xi64 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfncvt_f_xu_2xf32_2xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfncvt_f_xu_4xf32_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfncvt.f.xu.w.nxv4f32.nxv4i64.i64(<vscale x 4 x float> undef, <vscale x 4 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfncvt_f_xu_4xf32_4xi64(__epi_4xi64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfncvt_f_xu_4xf32_4xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfncvt_f_xu_4xf32_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfncvt.f.xu.w.mask.nxv4f32.nxv4i64.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfncvt_f_xu_4xf32_4xi64_mask(__epi_4xf32 arg_0, __epi_4xi64 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfncvt_f_xu_4xf32_4xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfncvt_f_xu_8xf32_8xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfncvt.f.xu.w.nxv8f32.nxv8i64.i64(<vscale x 8 x float> undef, <vscale x 8 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfncvt_f_xu_8xf32_8xi64(__epi_8xi64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfncvt_f_xu_8xf32_8xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfncvt_f_xu_8xf32_8xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfncvt.f.xu.w.mask.nxv8f32.nxv8i64.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x i64> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfncvt_f_xu_8xf32_8xi64_mask(__epi_8xf32 arg_0, __epi_8xi64 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfncvt_f_xu_8xf32_8xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfncvt_x_f_4xi16_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vfncvt.x.f.w.nxv4i16.nxv4f32.i64(<vscale x 4 x i16> undef, <vscale x 4 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vfncvt_x_f_4xi16_4xf32(__epi_4xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfncvt_x_f_4xi16_4xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfncvt_x_f_4xi16_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vfncvt.x.f.w.mask.nxv4i16.nxv4f32.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vfncvt_x_f_4xi16_4xf32_mask(__epi_4xi16 arg_0, __epi_4xf32 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfncvt_x_f_4xi16_4xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfncvt_x_f_2xi32_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vfncvt.x.f.w.nxv2i32.nxv2f64.i64(<vscale x 2 x i32> undef, <vscale x 2 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vfncvt_x_f_2xi32_2xf64(__epi_2xf64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfncvt_x_f_2xi32_2xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfncvt_x_f_2xi32_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vfncvt.x.f.w.mask.nxv2i32.nxv2f64.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vfncvt_x_f_2xi32_2xf64_mask(__epi_2xi32 arg_0, __epi_2xf64 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfncvt_x_f_2xi32_2xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfncvt_x_f_8xi16_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vfncvt.x.f.w.nxv8i16.nxv8f32.i64(<vscale x 8 x i16> undef, <vscale x 8 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vfncvt_x_f_8xi16_8xf32(__epi_8xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfncvt_x_f_8xi16_8xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfncvt_x_f_8xi16_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vfncvt.x.f.w.mask.nxv8i16.nxv8f32.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vfncvt_x_f_8xi16_8xf32_mask(__epi_8xi16 arg_0, __epi_8xf32 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfncvt_x_f_8xi16_8xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfncvt_x_f_4xi32_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vfncvt.x.f.w.nxv4i32.nxv4f64.i64(<vscale x 4 x i32> undef, <vscale x 4 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vfncvt_x_f_4xi32_4xf64(__epi_4xf64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfncvt_x_f_4xi32_4xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfncvt_x_f_4xi32_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vfncvt.x.f.w.mask.nxv4i32.nxv4f64.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vfncvt_x_f_4xi32_4xf64_mask(__epi_4xi32 arg_0, __epi_4xf64 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfncvt_x_f_4xi32_4xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfncvt_x_f_16xi16_16xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vfncvt.x.f.w.nxv16i16.nxv16f32.i64(<vscale x 16 x i16> undef, <vscale x 16 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vfncvt_x_f_16xi16_16xf32(__epi_16xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfncvt_x_f_16xi16_16xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfncvt_x_f_16xi16_16xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vfncvt.x.f.w.mask.nxv16i16.nxv16f32.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x float> [[ARG_1:%.*]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vfncvt_x_f_16xi16_16xf32_mask(__epi_16xi16 arg_0, __epi_16xf32 arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfncvt_x_f_16xi16_16xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfncvt_x_f_8xi32_8xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vfncvt.x.f.w.nxv8i32.nxv8f64.i64(<vscale x 8 x i32> undef, <vscale x 8 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vfncvt_x_f_8xi32_8xf64(__epi_8xf64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfncvt_x_f_8xi32_8xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfncvt_x_f_8xi32_8xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vfncvt.x.f.w.mask.nxv8i32.nxv8f64.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x double> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vfncvt_x_f_8xi32_8xf64_mask(__epi_8xi32 arg_0, __epi_8xf64 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfncvt_x_f_8xi32_8xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfncvt_xu_f_4xi16_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vfncvt.xu.f.w.nxv4i16.nxv4f32.i64(<vscale x 4 x i16> undef, <vscale x 4 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vfncvt_xu_f_4xi16_4xf32(__epi_4xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfncvt_xu_f_4xi16_4xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfncvt_xu_f_4xi16_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vfncvt.xu.f.w.mask.nxv4i16.nxv4f32.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vfncvt_xu_f_4xi16_4xf32_mask(__epi_4xi16 arg_0, __epi_4xf32 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfncvt_xu_f_4xi16_4xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfncvt_xu_f_2xi32_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vfncvt.xu.f.w.nxv2i32.nxv2f64.i64(<vscale x 2 x i32> undef, <vscale x 2 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vfncvt_xu_f_2xi32_2xf64(__epi_2xf64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfncvt_xu_f_2xi32_2xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfncvt_xu_f_2xi32_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vfncvt.xu.f.w.mask.nxv2i32.nxv2f64.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vfncvt_xu_f_2xi32_2xf64_mask(__epi_2xi32 arg_0, __epi_2xf64 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfncvt_xu_f_2xi32_2xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfncvt_xu_f_8xi16_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vfncvt.xu.f.w.nxv8i16.nxv8f32.i64(<vscale x 8 x i16> undef, <vscale x 8 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vfncvt_xu_f_8xi16_8xf32(__epi_8xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfncvt_xu_f_8xi16_8xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfncvt_xu_f_8xi16_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vfncvt.xu.f.w.mask.nxv8i16.nxv8f32.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vfncvt_xu_f_8xi16_8xf32_mask(__epi_8xi16 arg_0, __epi_8xf32 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfncvt_xu_f_8xi16_8xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfncvt_xu_f_4xi32_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vfncvt.xu.f.w.nxv4i32.nxv4f64.i64(<vscale x 4 x i32> undef, <vscale x 4 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vfncvt_xu_f_4xi32_4xf64(__epi_4xf64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfncvt_xu_f_4xi32_4xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfncvt_xu_f_4xi32_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vfncvt.xu.f.w.mask.nxv4i32.nxv4f64.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vfncvt_xu_f_4xi32_4xf64_mask(__epi_4xi32 arg_0, __epi_4xf64 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfncvt_xu_f_4xi32_4xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfncvt_xu_f_16xi16_16xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vfncvt.xu.f.w.nxv16i16.nxv16f32.i64(<vscale x 16 x i16> undef, <vscale x 16 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vfncvt_xu_f_16xi16_16xf32(__epi_16xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfncvt_xu_f_16xi16_16xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfncvt_xu_f_16xi16_16xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vfncvt.xu.f.w.mask.nxv16i16.nxv16f32.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x float> [[ARG_1:%.*]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vfncvt_xu_f_16xi16_16xf32_mask(__epi_16xi16 arg_0, __epi_16xf32 arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfncvt_xu_f_16xi16_16xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfncvt_xu_f_8xi32_8xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vfncvt.xu.f.w.nxv8i32.nxv8f64.i64(<vscale x 8 x i32> undef, <vscale x 8 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vfncvt_xu_f_8xi32_8xf64(__epi_8xf64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfncvt_xu_f_8xi32_8xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfncvt_xu_f_8xi32_8xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vfncvt.xu.f.w.mask.nxv8i32.nxv8f64.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x double> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vfncvt_xu_f_8xi32_8xf64_mask(__epi_8xi32 arg_0, __epi_8xf64 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfncvt_xu_f_8xi32_8xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmacc_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfnmacc.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfnmacc_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmacc_2xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmacc_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfnmacc.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfnmacc_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmacc_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfnmacc_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfnmacc.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfnmacc_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmacc_1xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmacc_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfnmacc.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfnmacc_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmacc_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfnmacc_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfnmacc.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfnmacc_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmacc_4xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmacc_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfnmacc.mask.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfnmacc_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmacc_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfnmacc_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfnmacc.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfnmacc_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmacc_2xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmacc_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfnmacc.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfnmacc_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmacc_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfnmacc_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfnmacc.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfnmacc_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmacc_8xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmacc_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfnmacc.mask.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfnmacc_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmacc_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfnmacc_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfnmacc.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfnmacc_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmacc_4xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmacc_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfnmacc.mask.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfnmacc_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmacc_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfnmadd_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfnmadd.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfnmadd_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmadd_2xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmadd_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfnmadd.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfnmadd_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmadd_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfnmadd_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfnmadd.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfnmadd_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmadd_1xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmadd_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfnmadd.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfnmadd_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmadd_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfnmadd_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfnmadd.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfnmadd_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmadd_4xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmadd_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfnmadd.mask.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfnmadd_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmadd_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfnmadd_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfnmadd.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfnmadd_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmadd_2xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmadd_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfnmadd.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfnmadd_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmadd_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfnmadd_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfnmadd.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfnmadd_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmadd_8xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmadd_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfnmadd.mask.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfnmadd_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmadd_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfnmadd_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfnmadd.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfnmadd_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmadd_4xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmadd_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfnmadd.mask.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfnmadd_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmadd_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfnmsac_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfnmsac.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfnmsac_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmsac_2xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmsac_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfnmsac.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfnmsac_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmsac_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfnmsac_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfnmsac.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfnmsac_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmsac_1xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmsac_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfnmsac.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfnmsac_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmsac_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfnmsac_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfnmsac.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfnmsac_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmsac_4xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmsac_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfnmsac.mask.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfnmsac_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmsac_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfnmsac_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfnmsac.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfnmsac_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmsac_2xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmsac_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfnmsac.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfnmsac_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmsac_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfnmsac_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfnmsac.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfnmsac_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmsac_8xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmsac_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfnmsac.mask.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfnmsac_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmsac_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfnmsac_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfnmsac.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfnmsac_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmsac_4xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmsac_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfnmsac.mask.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfnmsac_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmsac_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfnmsub_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfnmsub.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfnmsub_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmsub_2xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmsub_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfnmsub.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfnmsub_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmsub_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfnmsub_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfnmsub.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfnmsub_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmsub_1xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmsub_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfnmsub.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfnmsub_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmsub_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfnmsub_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfnmsub.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfnmsub_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmsub_4xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmsub_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfnmsub.mask.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfnmsub_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmsub_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfnmsub_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfnmsub.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfnmsub_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmsub_2xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmsub_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfnmsub.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfnmsub_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmsub_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfnmsub_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfnmsub.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfnmsub_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmsub_8xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmsub_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfnmsub.mask.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfnmsub_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmsub_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfnmsub_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfnmsub.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfnmsub_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfnmsub_4xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfnmsub_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfnmsub.mask.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfnmsub_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfnmsub_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredmax_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredmax.nxv2f32.nxv2f32.i64(<vscale x 2 x float> poison, <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfredmax_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredmax_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredmax_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredmax.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfredmax_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredmax_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredmax_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredmax.nxv1f64.nxv1f64.i64(<vscale x 1 x double> poison, <vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfredmax_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredmax_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredmax_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredmax.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfredmax_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredmax_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredmax_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv4f32(<vscale x 4 x float> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredmax.nxv2f32.nxv4f32.i64(<vscale x 2 x float> poison, <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x float> @llvm.vector.insert.nxv4f32.nxv2f32(<vscale x 4 x float> poison, <vscale x 2 x float> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP2]]
//
__epi_4xf32 test_vfredmax_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredmax_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredmax_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv4f32(<vscale x 4 x float> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv4f32(<vscale x 4 x float> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredmax.mask.nxv2f32.nxv4f32.i64(<vscale x 2 x float> [[TMP0]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x float> @llvm.vector.insert.nxv4f32.nxv2f32(<vscale x 4 x float> poison, <vscale x 2 x float> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP3]]
//
__epi_4xf32 test_vfredmax_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredmax_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredmax_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv2f64(<vscale x 2 x double> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredmax.nxv1f64.nxv2f64.i64(<vscale x 1 x double> poison, <vscale x 2 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x double> @llvm.vector.insert.nxv2f64.nxv1f64(<vscale x 2 x double> poison, <vscale x 1 x double> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP2]]
//
__epi_2xf64 test_vfredmax_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredmax_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredmax_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv2f64(<vscale x 2 x double> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv2f64(<vscale x 2 x double> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredmax.mask.nxv1f64.nxv2f64.i64(<vscale x 1 x double> [[TMP0]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[TMP1]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 2 x double> @llvm.vector.insert.nxv2f64.nxv1f64(<vscale x 2 x double> poison, <vscale x 1 x double> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP3]]
//
__epi_2xf64 test_vfredmax_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredmax_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredmax_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv8f32(<vscale x 8 x float> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredmax.nxv2f32.nxv8f32.i64(<vscale x 2 x float> poison, <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x float> @llvm.vector.insert.nxv8f32.nxv2f32(<vscale x 8 x float> poison, <vscale x 2 x float> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP2]]
//
__epi_8xf32 test_vfredmax_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredmax_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredmax_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv8f32(<vscale x 8 x float> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv8f32(<vscale x 8 x float> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredmax.mask.nxv2f32.nxv8f32.i64(<vscale x 2 x float> [[TMP0]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x float> @llvm.vector.insert.nxv8f32.nxv2f32(<vscale x 8 x float> poison, <vscale x 2 x float> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP3]]
//
__epi_8xf32 test_vfredmax_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredmax_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredmax_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv4f64(<vscale x 4 x double> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredmax.nxv1f64.nxv4f64.i64(<vscale x 1 x double> poison, <vscale x 4 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x double> @llvm.vector.insert.nxv4f64.nxv1f64(<vscale x 4 x double> poison, <vscale x 1 x double> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP2]]
//
__epi_4xf64 test_vfredmax_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredmax_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredmax_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv4f64(<vscale x 4 x double> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv4f64(<vscale x 4 x double> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredmax.mask.nxv1f64.nxv4f64.i64(<vscale x 1 x double> [[TMP0]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x double> @llvm.vector.insert.nxv4f64.nxv1f64(<vscale x 4 x double> poison, <vscale x 1 x double> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP3]]
//
__epi_4xf64 test_vfredmax_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredmax_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredmin_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredmin.nxv2f32.nxv2f32.i64(<vscale x 2 x float> poison, <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfredmin_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredmin_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredmin_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredmin.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfredmin_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredmin_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredmin_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredmin.nxv1f64.nxv1f64.i64(<vscale x 1 x double> poison, <vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfredmin_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredmin_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredmin_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredmin.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfredmin_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredmin_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredmin_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv4f32(<vscale x 4 x float> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredmin.nxv2f32.nxv4f32.i64(<vscale x 2 x float> poison, <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x float> @llvm.vector.insert.nxv4f32.nxv2f32(<vscale x 4 x float> poison, <vscale x 2 x float> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP2]]
//
__epi_4xf32 test_vfredmin_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredmin_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredmin_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv4f32(<vscale x 4 x float> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv4f32(<vscale x 4 x float> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredmin.mask.nxv2f32.nxv4f32.i64(<vscale x 2 x float> [[TMP0]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x float> @llvm.vector.insert.nxv4f32.nxv2f32(<vscale x 4 x float> poison, <vscale x 2 x float> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP3]]
//
__epi_4xf32 test_vfredmin_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredmin_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredmin_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv2f64(<vscale x 2 x double> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredmin.nxv1f64.nxv2f64.i64(<vscale x 1 x double> poison, <vscale x 2 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x double> @llvm.vector.insert.nxv2f64.nxv1f64(<vscale x 2 x double> poison, <vscale x 1 x double> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP2]]
//
__epi_2xf64 test_vfredmin_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredmin_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredmin_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv2f64(<vscale x 2 x double> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv2f64(<vscale x 2 x double> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredmin.mask.nxv1f64.nxv2f64.i64(<vscale x 1 x double> [[TMP0]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[TMP1]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 2 x double> @llvm.vector.insert.nxv2f64.nxv1f64(<vscale x 2 x double> poison, <vscale x 1 x double> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP3]]
//
__epi_2xf64 test_vfredmin_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredmin_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredmin_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv8f32(<vscale x 8 x float> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredmin.nxv2f32.nxv8f32.i64(<vscale x 2 x float> poison, <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x float> @llvm.vector.insert.nxv8f32.nxv2f32(<vscale x 8 x float> poison, <vscale x 2 x float> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP2]]
//
__epi_8xf32 test_vfredmin_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredmin_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredmin_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv8f32(<vscale x 8 x float> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv8f32(<vscale x 8 x float> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredmin.mask.nxv2f32.nxv8f32.i64(<vscale x 2 x float> [[TMP0]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x float> @llvm.vector.insert.nxv8f32.nxv2f32(<vscale x 8 x float> poison, <vscale x 2 x float> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP3]]
//
__epi_8xf32 test_vfredmin_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredmin_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredmin_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv4f64(<vscale x 4 x double> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredmin.nxv1f64.nxv4f64.i64(<vscale x 1 x double> poison, <vscale x 4 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x double> @llvm.vector.insert.nxv4f64.nxv1f64(<vscale x 4 x double> poison, <vscale x 1 x double> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP2]]
//
__epi_4xf64 test_vfredmin_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredmin_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredmin_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv4f64(<vscale x 4 x double> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv4f64(<vscale x 4 x double> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredmin.mask.nxv1f64.nxv4f64.i64(<vscale x 1 x double> [[TMP0]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x double> @llvm.vector.insert.nxv4f64.nxv1f64(<vscale x 4 x double> poison, <vscale x 1 x double> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP3]]
//
__epi_4xf64 test_vfredmin_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredmin_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredosum_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredosum.nxv2f32.nxv2f32.i64(<vscale x 2 x float> poison, <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfredosum_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredosum_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredosum_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredosum.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfredosum_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredosum_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredosum_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredosum.nxv1f64.nxv1f64.i64(<vscale x 1 x double> poison, <vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfredosum_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredosum_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredosum_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredosum.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfredosum_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredosum_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredosum_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv4f32(<vscale x 4 x float> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredosum.nxv2f32.nxv4f32.i64(<vscale x 2 x float> poison, <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x float> @llvm.vector.insert.nxv4f32.nxv2f32(<vscale x 4 x float> poison, <vscale x 2 x float> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP2]]
//
__epi_4xf32 test_vfredosum_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredosum_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredosum_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv4f32(<vscale x 4 x float> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv4f32(<vscale x 4 x float> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredosum.mask.nxv2f32.nxv4f32.i64(<vscale x 2 x float> [[TMP0]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x float> @llvm.vector.insert.nxv4f32.nxv2f32(<vscale x 4 x float> poison, <vscale x 2 x float> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP3]]
//
__epi_4xf32 test_vfredosum_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredosum_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredosum_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv2f64(<vscale x 2 x double> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredosum.nxv1f64.nxv2f64.i64(<vscale x 1 x double> poison, <vscale x 2 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x double> @llvm.vector.insert.nxv2f64.nxv1f64(<vscale x 2 x double> poison, <vscale x 1 x double> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP2]]
//
__epi_2xf64 test_vfredosum_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredosum_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredosum_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv2f64(<vscale x 2 x double> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv2f64(<vscale x 2 x double> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredosum.mask.nxv1f64.nxv2f64.i64(<vscale x 1 x double> [[TMP0]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[TMP1]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 2 x double> @llvm.vector.insert.nxv2f64.nxv1f64(<vscale x 2 x double> poison, <vscale x 1 x double> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP3]]
//
__epi_2xf64 test_vfredosum_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredosum_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredosum_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv8f32(<vscale x 8 x float> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredosum.nxv2f32.nxv8f32.i64(<vscale x 2 x float> poison, <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x float> @llvm.vector.insert.nxv8f32.nxv2f32(<vscale x 8 x float> poison, <vscale x 2 x float> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP2]]
//
__epi_8xf32 test_vfredosum_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredosum_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredosum_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv8f32(<vscale x 8 x float> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv8f32(<vscale x 8 x float> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredosum.mask.nxv2f32.nxv8f32.i64(<vscale x 2 x float> [[TMP0]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x float> @llvm.vector.insert.nxv8f32.nxv2f32(<vscale x 8 x float> poison, <vscale x 2 x float> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP3]]
//
__epi_8xf32 test_vfredosum_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredosum_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredosum_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv4f64(<vscale x 4 x double> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredosum.nxv1f64.nxv4f64.i64(<vscale x 1 x double> poison, <vscale x 4 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x double> @llvm.vector.insert.nxv4f64.nxv1f64(<vscale x 4 x double> poison, <vscale x 1 x double> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP2]]
//
__epi_4xf64 test_vfredosum_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredosum_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredosum_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv4f64(<vscale x 4 x double> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv4f64(<vscale x 4 x double> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredosum.mask.nxv1f64.nxv4f64.i64(<vscale x 1 x double> [[TMP0]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x double> @llvm.vector.insert.nxv4f64.nxv1f64(<vscale x 4 x double> poison, <vscale x 1 x double> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP3]]
//
__epi_4xf64 test_vfredosum_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredosum_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredsum_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredusum.nxv2f32.nxv2f32.i64(<vscale x 2 x float> poison, <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfredsum_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredsum_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredsum_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredusum.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfredsum_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredsum_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredsum_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredusum.nxv1f64.nxv1f64.i64(<vscale x 1 x double> poison, <vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfredsum_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredsum_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredsum_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredusum.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfredsum_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredsum_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredsum_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv4f32(<vscale x 4 x float> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredusum.nxv2f32.nxv4f32.i64(<vscale x 2 x float> poison, <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x float> @llvm.vector.insert.nxv4f32.nxv2f32(<vscale x 4 x float> poison, <vscale x 2 x float> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP2]]
//
__epi_4xf32 test_vfredsum_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredsum_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredsum_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv4f32(<vscale x 4 x float> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv4f32(<vscale x 4 x float> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredusum.mask.nxv2f32.nxv4f32.i64(<vscale x 2 x float> [[TMP0]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x float> @llvm.vector.insert.nxv4f32.nxv2f32(<vscale x 4 x float> poison, <vscale x 2 x float> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP3]]
//
__epi_4xf32 test_vfredsum_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredsum_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredsum_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv2f64(<vscale x 2 x double> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredusum.nxv1f64.nxv2f64.i64(<vscale x 1 x double> poison, <vscale x 2 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x double> @llvm.vector.insert.nxv2f64.nxv1f64(<vscale x 2 x double> poison, <vscale x 1 x double> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP2]]
//
__epi_2xf64 test_vfredsum_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredsum_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredsum_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv2f64(<vscale x 2 x double> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv2f64(<vscale x 2 x double> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredusum.mask.nxv1f64.nxv2f64.i64(<vscale x 1 x double> [[TMP0]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[TMP1]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 2 x double> @llvm.vector.insert.nxv2f64.nxv1f64(<vscale x 2 x double> poison, <vscale x 1 x double> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP3]]
//
__epi_2xf64 test_vfredsum_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredsum_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredsum_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv8f32(<vscale x 8 x float> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredusum.nxv2f32.nxv8f32.i64(<vscale x 2 x float> poison, <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x float> @llvm.vector.insert.nxv8f32.nxv2f32(<vscale x 8 x float> poison, <vscale x 2 x float> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP2]]
//
__epi_8xf32 test_vfredsum_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredsum_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredsum_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv8f32(<vscale x 8 x float> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.vector.extract.nxv2f32.nxv8f32(<vscale x 8 x float> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfredusum.mask.nxv2f32.nxv8f32.i64(<vscale x 2 x float> [[TMP0]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x float> @llvm.vector.insert.nxv8f32.nxv2f32(<vscale x 8 x float> poison, <vscale x 2 x float> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP3]]
//
__epi_8xf32 test_vfredsum_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredsum_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfredsum_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv4f64(<vscale x 4 x double> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredusum.nxv1f64.nxv4f64.i64(<vscale x 1 x double> poison, <vscale x 4 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x double> @llvm.vector.insert.nxv4f64.nxv1f64(<vscale x 4 x double> poison, <vscale x 1 x double> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP2]]
//
__epi_4xf64 test_vfredsum_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfredsum_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfredsum_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv4f64(<vscale x 4 x double> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv4f64(<vscale x 4 x double> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfredusum.mask.nxv1f64.nxv4f64.i64(<vscale x 1 x double> [[TMP0]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x double> @llvm.vector.insert.nxv4f64.nxv1f64(<vscale x 4 x double> poison, <vscale x 1 x double> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP3]]
//
__epi_4xf64 test_vfredsum_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfredsum_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsgnj_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfsgnj.nxv2f32.nxv2f32.i64(<vscale x 2 x float> undef, <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfsgnj_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsgnj_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsgnj_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfsgnj.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfsgnj_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsgnj_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsgnj_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfsgnj.nxv1f64.nxv1f64.i64(<vscale x 1 x double> undef, <vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfsgnj_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsgnj_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsgnj_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfsgnj.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfsgnj_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsgnj_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsgnj_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfsgnj.nxv4f32.nxv4f32.i64(<vscale x 4 x float> undef, <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfsgnj_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsgnj_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsgnj_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfsgnj.mask.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfsgnj_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsgnj_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsgnj_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfsgnj.nxv2f64.nxv2f64.i64(<vscale x 2 x double> undef, <vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfsgnj_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsgnj_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsgnj_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfsgnj.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfsgnj_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsgnj_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsgnj_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfsgnj.nxv8f32.nxv8f32.i64(<vscale x 8 x float> undef, <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfsgnj_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsgnj_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsgnj_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfsgnj.mask.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfsgnj_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsgnj_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsgnj_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfsgnj.nxv4f64.nxv4f64.i64(<vscale x 4 x double> undef, <vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfsgnj_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsgnj_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsgnj_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfsgnj.mask.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfsgnj_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsgnj_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsgnjn_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfsgnjn.nxv2f32.nxv2f32.i64(<vscale x 2 x float> undef, <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfsgnjn_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsgnjn_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsgnjn_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfsgnjn.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfsgnjn_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsgnjn_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsgnjn_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfsgnjn.nxv1f64.nxv1f64.i64(<vscale x 1 x double> undef, <vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfsgnjn_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsgnjn_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsgnjn_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfsgnjn.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfsgnjn_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsgnjn_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsgnjn_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfsgnjn.nxv4f32.nxv4f32.i64(<vscale x 4 x float> undef, <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfsgnjn_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsgnjn_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsgnjn_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfsgnjn.mask.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfsgnjn_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsgnjn_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsgnjn_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfsgnjn.nxv2f64.nxv2f64.i64(<vscale x 2 x double> undef, <vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfsgnjn_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsgnjn_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsgnjn_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfsgnjn.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfsgnjn_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsgnjn_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsgnjn_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfsgnjn.nxv8f32.nxv8f32.i64(<vscale x 8 x float> undef, <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfsgnjn_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsgnjn_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsgnjn_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfsgnjn.mask.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfsgnjn_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsgnjn_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsgnjn_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfsgnjn.nxv4f64.nxv4f64.i64(<vscale x 4 x double> undef, <vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfsgnjn_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsgnjn_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsgnjn_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfsgnjn.mask.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfsgnjn_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsgnjn_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsgnjx_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfsgnjx.nxv2f32.nxv2f32.i64(<vscale x 2 x float> undef, <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfsgnjx_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsgnjx_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsgnjx_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfsgnjx.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfsgnjx_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsgnjx_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsgnjx_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfsgnjx.nxv1f64.nxv1f64.i64(<vscale x 1 x double> undef, <vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfsgnjx_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsgnjx_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsgnjx_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfsgnjx.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfsgnjx_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsgnjx_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsgnjx_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfsgnjx.nxv4f32.nxv4f32.i64(<vscale x 4 x float> undef, <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfsgnjx_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsgnjx_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsgnjx_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfsgnjx.mask.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfsgnjx_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsgnjx_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsgnjx_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfsgnjx.nxv2f64.nxv2f64.i64(<vscale x 2 x double> undef, <vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfsgnjx_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsgnjx_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsgnjx_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfsgnjx.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfsgnjx_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsgnjx_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsgnjx_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfsgnjx.nxv8f32.nxv8f32.i64(<vscale x 8 x float> undef, <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfsgnjx_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsgnjx_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsgnjx_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfsgnjx.mask.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfsgnjx_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsgnjx_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsgnjx_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfsgnjx.nxv4f64.nxv4f64.i64(<vscale x 4 x double> undef, <vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfsgnjx_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsgnjx_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsgnjx_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfsgnjx.mask.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfsgnjx_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsgnjx_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsqrt_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfsqrt.nxv2f32.i64(<vscale x 2 x float> undef, <vscale x 2 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfsqrt_2xf32(__epi_2xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfsqrt_2xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfsqrt_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfsqrt.mask.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfsqrt_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfsqrt_2xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfsqrt_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfsqrt.nxv1f64.i64(<vscale x 1 x double> undef, <vscale x 1 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfsqrt_1xf64(__epi_1xf64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfsqrt_1xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfsqrt_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfsqrt.mask.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfsqrt_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfsqrt_1xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfsqrt_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfsqrt.nxv4f32.i64(<vscale x 4 x float> undef, <vscale x 4 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfsqrt_4xf32(__epi_4xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfsqrt_4xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfsqrt_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfsqrt.mask.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfsqrt_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfsqrt_4xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfsqrt_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfsqrt.nxv2f64.i64(<vscale x 2 x double> undef, <vscale x 2 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfsqrt_2xf64(__epi_2xf64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfsqrt_2xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfsqrt_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfsqrt.mask.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfsqrt_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfsqrt_2xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfsqrt_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfsqrt.nxv8f32.i64(<vscale x 8 x float> undef, <vscale x 8 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfsqrt_8xf32(__epi_8xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfsqrt_8xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfsqrt_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfsqrt.mask.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfsqrt_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfsqrt_8xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfsqrt_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfsqrt.nxv4f64.i64(<vscale x 4 x double> undef, <vscale x 4 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfsqrt_4xf64(__epi_4xf64 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfsqrt_4xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfsqrt_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfsqrt.mask.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfsqrt_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfsqrt_4xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfsub_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfsub.nxv2f32.nxv2f32.i64(<vscale x 2 x float> undef, <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfsub_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsub_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsub_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfsub.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vfsub_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsub_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsub_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfsub.nxv1f64.nxv1f64.i64(<vscale x 1 x double> undef, <vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfsub_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsub_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsub_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfsub.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vfsub_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsub_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsub_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfsub.nxv4f32.nxv4f32.i64(<vscale x 4 x float> undef, <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfsub_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsub_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsub_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfsub.mask.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfsub_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsub_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsub_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfsub.nxv2f64.nxv2f64.i64(<vscale x 2 x double> undef, <vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfsub_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsub_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsub_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfsub.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfsub_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsub_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsub_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfsub.nxv8f32.nxv8f32.i64(<vscale x 8 x float> undef, <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfsub_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsub_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsub_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfsub.mask.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfsub_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsub_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfsub_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfsub.nxv4f64.nxv4f64.i64(<vscale x 4 x double> undef, <vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfsub_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfsub_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfsub_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfsub.mask.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfsub_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfsub_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwadd_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwadd.nxv2f64.nxv2f32.nxv2f32.i64(<vscale x 2 x double> undef, <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwadd_2xf64(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfwadd_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfwadd_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwadd.mask.nxv2f64.nxv2f32.nxv2f32.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwadd_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwadd_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwadd_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwadd.nxv4f64.nxv4f32.nxv4f32.i64(<vscale x 4 x double> undef, <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwadd_4xf64(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfwadd_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfwadd_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwadd.mask.nxv4f64.nxv4f32.nxv4f32.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwadd_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwadd_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwadd_8xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwadd.nxv8f64.nxv8f32.nxv8f32.i64(<vscale x 8 x double> undef, <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwadd_8xf64(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfwadd_8xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfwadd_8xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwadd.mask.nxv8f64.nxv8f32.nxv8f32.i64(<vscale x 8 x double> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwadd_8xf64_mask(__epi_8xf64 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwadd_8xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwadd_w_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwadd.w.nxv2f64.nxv2f32.i64(<vscale x 2 x double> undef, <vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwadd_w_2xf64(__epi_2xf64 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfwadd_w_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfwadd_w_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwadd.w.mask.nxv2f64.nxv2f32.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwadd_w_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwadd_w_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwadd_w_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwadd.w.nxv4f64.nxv4f32.i64(<vscale x 4 x double> undef, <vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwadd_w_4xf64(__epi_4xf64 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfwadd_w_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfwadd_w_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwadd.w.mask.nxv4f64.nxv4f32.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwadd_w_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwadd_w_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwadd_w_8xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwadd.w.nxv8f64.nxv8f32.i64(<vscale x 8 x double> undef, <vscale x 8 x double> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwadd_w_8xf64(__epi_8xf64 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfwadd_w_8xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfwadd_w_8xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwadd.w.mask.nxv8f64.nxv8f32.i64(<vscale x 8 x double> [[ARG_0:%.*]], <vscale x 8 x double> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwadd_w_8xf64_mask(__epi_8xf64 arg_0, __epi_8xf64 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwadd_w_8xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_f_2xf64_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwcvt.f.f.v.nxv2f64.nxv2f32.i64(<vscale x 2 x double> undef, <vscale x 2 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwcvt_f_f_2xf64_2xf32(__epi_2xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfwcvt_f_f_2xf64_2xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_f_2xf64_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwcvt.f.f.v.mask.nxv2f64.nxv2f32.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwcvt_f_f_2xf64_2xf32_mask(__epi_2xf64 arg_0, __epi_2xf32 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwcvt_f_f_2xf64_2xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_f_4xf64_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwcvt.f.f.v.nxv4f64.nxv4f32.i64(<vscale x 4 x double> undef, <vscale x 4 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwcvt_f_f_4xf64_4xf32(__epi_4xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfwcvt_f_f_4xf64_4xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_f_4xf64_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwcvt.f.f.v.mask.nxv4f64.nxv4f32.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwcvt_f_f_4xf64_4xf32_mask(__epi_4xf64 arg_0, __epi_4xf32 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwcvt_f_f_4xf64_4xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_f_8xf64_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwcvt.f.f.v.nxv8f64.nxv8f32.i64(<vscale x 8 x double> undef, <vscale x 8 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwcvt_f_f_8xf64_8xf32(__epi_8xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfwcvt_f_f_8xf64_8xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_f_8xf64_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwcvt.f.f.v.mask.nxv8f64.nxv8f32.i64(<vscale x 8 x double> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwcvt_f_f_8xf64_8xf32_mask(__epi_8xf64 arg_0, __epi_8xf32 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwcvt_f_f_8xf64_8xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_x_4xf32_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfwcvt.f.x.v.nxv4f32.nxv4i16.i64(<vscale x 4 x float> undef, <vscale x 4 x i16> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfwcvt_f_x_4xf32_4xi16(__epi_4xi16 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfwcvt_f_x_4xf32_4xi16(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_x_4xf32_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfwcvt.f.x.v.mask.nxv4f32.nxv4i16.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfwcvt_f_x_4xf32_4xi16_mask(__epi_4xf32 arg_0, __epi_4xi16 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwcvt_f_x_4xf32_4xi16_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_x_2xf64_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwcvt.f.x.v.nxv2f64.nxv2i32.i64(<vscale x 2 x double> undef, <vscale x 2 x i32> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwcvt_f_x_2xf64_2xi32(__epi_2xi32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfwcvt_f_x_2xf64_2xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_x_2xf64_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwcvt.f.x.v.mask.nxv2f64.nxv2i32.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwcvt_f_x_2xf64_2xi32_mask(__epi_2xf64 arg_0, __epi_2xi32 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwcvt_f_x_2xf64_2xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_x_8xf32_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfwcvt.f.x.v.nxv8f32.nxv8i16.i64(<vscale x 8 x float> undef, <vscale x 8 x i16> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfwcvt_f_x_8xf32_8xi16(__epi_8xi16 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfwcvt_f_x_8xf32_8xi16(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_x_8xf32_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfwcvt.f.x.v.mask.nxv8f32.nxv8i16.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfwcvt_f_x_8xf32_8xi16_mask(__epi_8xf32 arg_0, __epi_8xi16 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwcvt_f_x_8xf32_8xi16_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_x_4xf64_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwcvt.f.x.v.nxv4f64.nxv4i32.i64(<vscale x 4 x double> undef, <vscale x 4 x i32> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwcvt_f_x_4xf64_4xi32(__epi_4xi32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfwcvt_f_x_4xf64_4xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_x_4xf64_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwcvt.f.x.v.mask.nxv4f64.nxv4i32.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwcvt_f_x_4xf64_4xi32_mask(__epi_4xf64 arg_0, __epi_4xi32 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwcvt_f_x_4xf64_4xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_x_16xf32_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x float> @llvm.riscv.vfwcvt.f.x.v.nxv16f32.nxv16i16.i64(<vscale x 16 x float> undef, <vscale x 16 x i16> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x float> [[TMP0]]
//
__epi_16xf32 test_vfwcvt_f_x_16xf32_16xi16(__epi_16xi16 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfwcvt_f_x_16xf32_16xi16(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_x_16xf32_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x float> @llvm.riscv.vfwcvt.f.x.v.mask.nxv16f32.nxv16i16.i64(<vscale x 16 x float> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x float> [[TMP0]]
//
__epi_16xf32 test_vfwcvt_f_x_16xf32_16xi16_mask(__epi_16xf32 arg_0, __epi_16xi16 arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwcvt_f_x_16xf32_16xi16_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_x_8xf64_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwcvt.f.x.v.nxv8f64.nxv8i32.i64(<vscale x 8 x double> undef, <vscale x 8 x i32> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwcvt_f_x_8xf64_8xi32(__epi_8xi32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfwcvt_f_x_8xf64_8xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_x_8xf64_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwcvt.f.x.v.mask.nxv8f64.nxv8i32.i64(<vscale x 8 x double> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwcvt_f_x_8xf64_8xi32_mask(__epi_8xf64 arg_0, __epi_8xi32 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwcvt_f_x_8xf64_8xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_xu_4xf32_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfwcvt.f.xu.v.nxv4f32.nxv4i16.i64(<vscale x 4 x float> undef, <vscale x 4 x i16> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfwcvt_f_xu_4xf32_4xi16(__epi_4xi16 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfwcvt_f_xu_4xf32_4xi16(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_xu_4xf32_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfwcvt.f.xu.v.mask.nxv4f32.nxv4i16.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vfwcvt_f_xu_4xf32_4xi16_mask(__epi_4xf32 arg_0, __epi_4xi16 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwcvt_f_xu_4xf32_4xi16_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_xu_2xf64_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwcvt.f.xu.v.nxv2f64.nxv2i32.i64(<vscale x 2 x double> undef, <vscale x 2 x i32> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwcvt_f_xu_2xf64_2xi32(__epi_2xi32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfwcvt_f_xu_2xf64_2xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_xu_2xf64_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwcvt.f.xu.v.mask.nxv2f64.nxv2i32.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwcvt_f_xu_2xf64_2xi32_mask(__epi_2xf64 arg_0, __epi_2xi32 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwcvt_f_xu_2xf64_2xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_xu_8xf32_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfwcvt.f.xu.v.nxv8f32.nxv8i16.i64(<vscale x 8 x float> undef, <vscale x 8 x i16> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfwcvt_f_xu_8xf32_8xi16(__epi_8xi16 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfwcvt_f_xu_8xf32_8xi16(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_xu_8xf32_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfwcvt.f.xu.v.mask.nxv8f32.nxv8i16.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vfwcvt_f_xu_8xf32_8xi16_mask(__epi_8xf32 arg_0, __epi_8xi16 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwcvt_f_xu_8xf32_8xi16_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_xu_4xf64_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwcvt.f.xu.v.nxv4f64.nxv4i32.i64(<vscale x 4 x double> undef, <vscale x 4 x i32> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwcvt_f_xu_4xf64_4xi32(__epi_4xi32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfwcvt_f_xu_4xf64_4xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_xu_4xf64_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwcvt.f.xu.v.mask.nxv4f64.nxv4i32.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwcvt_f_xu_4xf64_4xi32_mask(__epi_4xf64 arg_0, __epi_4xi32 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwcvt_f_xu_4xf64_4xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_xu_16xf32_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x float> @llvm.riscv.vfwcvt.f.xu.v.nxv16f32.nxv16i16.i64(<vscale x 16 x float> undef, <vscale x 16 x i16> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x float> [[TMP0]]
//
__epi_16xf32 test_vfwcvt_f_xu_16xf32_16xi16(__epi_16xi16 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfwcvt_f_xu_16xf32_16xi16(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_xu_16xf32_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x float> @llvm.riscv.vfwcvt.f.xu.v.mask.nxv16f32.nxv16i16.i64(<vscale x 16 x float> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x float> [[TMP0]]
//
__epi_16xf32 test_vfwcvt_f_xu_16xf32_16xi16_mask(__epi_16xf32 arg_0, __epi_16xi16 arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwcvt_f_xu_16xf32_16xi16_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_xu_8xf64_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwcvt.f.xu.v.nxv8f64.nxv8i32.i64(<vscale x 8 x double> undef, <vscale x 8 x i32> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwcvt_f_xu_8xf64_8xi32(__epi_8xi32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfwcvt_f_xu_8xf64_8xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfwcvt_f_xu_8xf64_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwcvt.f.xu.v.mask.nxv8f64.nxv8i32.i64(<vscale x 8 x double> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwcvt_f_xu_8xf64_8xi32_mask(__epi_8xf64 arg_0, __epi_8xi32 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwcvt_f_xu_8xf64_8xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwcvt_x_f_2xi64_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vfwcvt.x.f.v.nxv2i64.nxv2f32.i64(<vscale x 2 x i64> undef, <vscale x 2 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vfwcvt_x_f_2xi64_2xf32(__epi_2xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfwcvt_x_f_2xi64_2xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfwcvt_x_f_2xi64_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vfwcvt.x.f.v.mask.nxv2i64.nxv2f32.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vfwcvt_x_f_2xi64_2xf32_mask(__epi_2xi64 arg_0, __epi_2xf32 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwcvt_x_f_2xi64_2xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwcvt_x_f_4xi64_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vfwcvt.x.f.v.nxv4i64.nxv4f32.i64(<vscale x 4 x i64> undef, <vscale x 4 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vfwcvt_x_f_4xi64_4xf32(__epi_4xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfwcvt_x_f_4xi64_4xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfwcvt_x_f_4xi64_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vfwcvt.x.f.v.mask.nxv4i64.nxv4f32.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vfwcvt_x_f_4xi64_4xf32_mask(__epi_4xi64 arg_0, __epi_4xf32 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwcvt_x_f_4xi64_4xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwcvt_x_f_8xi64_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vfwcvt.x.f.v.nxv8i64.nxv8f32.i64(<vscale x 8 x i64> undef, <vscale x 8 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vfwcvt_x_f_8xi64_8xf32(__epi_8xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfwcvt_x_f_8xi64_8xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfwcvt_x_f_8xi64_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vfwcvt.x.f.v.mask.nxv8i64.nxv8f32.i64(<vscale x 8 x i64> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vfwcvt_x_f_8xi64_8xf32_mask(__epi_8xi64 arg_0, __epi_8xf32 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwcvt_x_f_8xi64_8xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwcvt_xu_f_2xi64_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vfwcvt.xu.f.v.nxv2i64.nxv2f32.i64(<vscale x 2 x i64> undef, <vscale x 2 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vfwcvt_xu_f_2xi64_2xf32(__epi_2xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfwcvt_xu_f_2xi64_2xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfwcvt_xu_f_2xi64_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vfwcvt.xu.f.v.mask.nxv2i64.nxv2f32.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vfwcvt_xu_f_2xi64_2xf32_mask(__epi_2xi64 arg_0, __epi_2xf32 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwcvt_xu_f_2xi64_2xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwcvt_xu_f_4xi64_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vfwcvt.xu.f.v.nxv4i64.nxv4f32.i64(<vscale x 4 x i64> undef, <vscale x 4 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vfwcvt_xu_f_4xi64_4xf32(__epi_4xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfwcvt_xu_f_4xi64_4xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfwcvt_xu_f_4xi64_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vfwcvt.xu.f.v.mask.nxv4i64.nxv4f32.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vfwcvt_xu_f_4xi64_4xf32_mask(__epi_4xi64 arg_0, __epi_4xf32 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwcvt_xu_f_4xi64_4xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwcvt_xu_f_8xi64_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vfwcvt.xu.f.v.nxv8i64.nxv8f32.i64(<vscale x 8 x i64> undef, <vscale x 8 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vfwcvt_xu_f_8xi64_8xf32(__epi_8xf32 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vfwcvt_xu_f_8xi64_8xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vfwcvt_xu_f_8xi64_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vfwcvt.xu.f.v.mask.nxv8i64.nxv8f32.i64(<vscale x 8 x i64> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vfwcvt_xu_f_8xi64_8xf32_mask(__epi_8xi64 arg_0, __epi_8xf32 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwcvt_xu_f_8xi64_8xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwmacc_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwmacc.nxv2f64.nxv2f32.nxv2f32.i64(<vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwmacc_2xf64(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwmacc_2xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwmacc_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwmacc.mask.nxv2f64.nxv2f32.nxv2f32.i64(<vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwmacc_2xf64_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwmacc_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwmacc_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwmacc.nxv4f64.nxv4f32.nxv4f32.i64(<vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwmacc_4xf64(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwmacc_4xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwmacc_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwmacc.mask.nxv4f64.nxv4f32.nxv4f32.i64(<vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwmacc_4xf64_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwmacc_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwmacc_8xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwmacc.nxv8f64.nxv8f32.nxv8f32.i64(<vscale x 8 x double> [[ARG_2:%.*]], <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwmacc_8xf64(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwmacc_8xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwmacc_8xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwmacc.mask.nxv8f64.nxv8f32.nxv8f32.i64(<vscale x 8 x double> [[ARG_2:%.*]], <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwmacc_8xf64_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf64 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwmacc_8xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwmsac_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwmsac.nxv2f64.nxv2f32.nxv2f32.i64(<vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwmsac_2xf64(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwmsac_2xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwmsac_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwmsac.mask.nxv2f64.nxv2f32.nxv2f32.i64(<vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwmsac_2xf64_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwmsac_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwmsac_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwmsac.nxv4f64.nxv4f32.nxv4f32.i64(<vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwmsac_4xf64(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwmsac_4xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwmsac_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwmsac.mask.nxv4f64.nxv4f32.nxv4f32.i64(<vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwmsac_4xf64_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwmsac_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwmsac_8xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwmsac.nxv8f64.nxv8f32.nxv8f32.i64(<vscale x 8 x double> [[ARG_2:%.*]], <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwmsac_8xf64(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwmsac_8xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwmsac_8xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwmsac.mask.nxv8f64.nxv8f32.nxv8f32.i64(<vscale x 8 x double> [[ARG_2:%.*]], <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwmsac_8xf64_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf64 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwmsac_8xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwmul_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwmul.nxv2f64.nxv2f32.nxv2f32.i64(<vscale x 2 x double> undef, <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwmul_2xf64(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfwmul_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfwmul_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwmul.mask.nxv2f64.nxv2f32.nxv2f32.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwmul_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwmul_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwmul_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwmul.nxv4f64.nxv4f32.nxv4f32.i64(<vscale x 4 x double> undef, <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwmul_4xf64(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfwmul_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfwmul_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwmul.mask.nxv4f64.nxv4f32.nxv4f32.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwmul_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwmul_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwmul_8xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwmul.nxv8f64.nxv8f32.nxv8f32.i64(<vscale x 8 x double> undef, <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwmul_8xf64(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfwmul_8xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfwmul_8xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwmul.mask.nxv8f64.nxv8f32.nxv8f32.i64(<vscale x 8 x double> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwmul_8xf64_mask(__epi_8xf64 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwmul_8xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwnmacc_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwnmacc.nxv2f64.nxv2f32.nxv2f32.i64(<vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwnmacc_2xf64(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwnmacc_2xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwnmacc_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwnmacc.mask.nxv2f64.nxv2f32.nxv2f32.i64(<vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwnmacc_2xf64_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwnmacc_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwnmacc_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwnmacc.nxv4f64.nxv4f32.nxv4f32.i64(<vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwnmacc_4xf64(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwnmacc_4xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwnmacc_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwnmacc.mask.nxv4f64.nxv4f32.nxv4f32.i64(<vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwnmacc_4xf64_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwnmacc_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwnmacc_8xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwnmacc.nxv8f64.nxv8f32.nxv8f32.i64(<vscale x 8 x double> [[ARG_2:%.*]], <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwnmacc_8xf64(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwnmacc_8xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwnmacc_8xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwnmacc.mask.nxv8f64.nxv8f32.nxv8f32.i64(<vscale x 8 x double> [[ARG_2:%.*]], <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwnmacc_8xf64_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf64 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwnmacc_8xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwnmsac_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwnmsac.nxv2f64.nxv2f32.nxv2f32.i64(<vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwnmsac_2xf64(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwnmsac_2xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwnmsac_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwnmsac.mask.nxv2f64.nxv2f32.nxv2f32.i64(<vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwnmsac_2xf64_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwnmsac_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwnmsac_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwnmsac.nxv4f64.nxv4f32.nxv4f32.i64(<vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwnmsac_4xf64(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwnmsac_4xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwnmsac_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwnmsac.mask.nxv4f64.nxv4f32.nxv4f32.i64(<vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwnmsac_4xf64_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwnmsac_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwnmsac_8xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwnmsac.nxv8f64.nxv8f32.nxv8f32.i64(<vscale x 8 x double> [[ARG_2:%.*]], <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwnmsac_8xf64(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vfwnmsac_8xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vfwnmsac_8xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwnmsac.mask.nxv8f64.nxv8f32.nxv8f32.i64(<vscale x 8 x double> [[ARG_2:%.*]], <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwnmsac_8xf64_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xf64 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwnmsac_8xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwredosum_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv2f64(<vscale x 2 x double> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfwredosum.nxv1f64.nxv2f32.i64(<vscale x 1 x double> poison, <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 1 x double> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x double> @llvm.vector.insert.nxv2f64.nxv1f64(<vscale x 2 x double> poison, <vscale x 1 x double> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP2]]
//
__epi_2xf64 test_vfwredosum_2xf64(__epi_2xf32 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfwredosum_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfwredosum_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv2f64(<vscale x 2 x double> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv2f64(<vscale x 2 x double> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfwredosum.mask.nxv1f64.nxv2f32.i64(<vscale x 1 x double> [[TMP0]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 1 x double> [[TMP1]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 2 x double> @llvm.vector.insert.nxv2f64.nxv1f64(<vscale x 2 x double> poison, <vscale x 1 x double> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP3]]
//
__epi_2xf64 test_vfwredosum_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf32 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwredosum_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwredosum_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv4f64(<vscale x 4 x double> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfwredosum.nxv1f64.nxv4f32.i64(<vscale x 1 x double> poison, <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 1 x double> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x double> @llvm.vector.insert.nxv4f64.nxv1f64(<vscale x 4 x double> poison, <vscale x 1 x double> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP2]]
//
__epi_4xf64 test_vfwredosum_4xf64(__epi_4xf32 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfwredosum_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfwredosum_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv4f64(<vscale x 4 x double> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv4f64(<vscale x 4 x double> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfwredosum.mask.nxv1f64.nxv4f32.i64(<vscale x 1 x double> [[TMP0]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 1 x double> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x double> @llvm.vector.insert.nxv4f64.nxv1f64(<vscale x 4 x double> poison, <vscale x 1 x double> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP3]]
//
__epi_4xf64 test_vfwredosum_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf32 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwredosum_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwredosum_8xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv8f64(<vscale x 8 x double> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfwredosum.nxv1f64.nxv8f32.i64(<vscale x 1 x double> poison, <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 1 x double> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nxv1f64(<vscale x 8 x double> poison, <vscale x 1 x double> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP2]]
//
__epi_8xf64 test_vfwredosum_8xf64(__epi_8xf32 arg_0, __epi_8xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfwredosum_8xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfwredosum_8xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv8f64(<vscale x 8 x double> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv8f64(<vscale x 8 x double> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfwredosum.mask.nxv1f64.nxv8f32.i64(<vscale x 1 x double> [[TMP0]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 1 x double> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nxv1f64(<vscale x 8 x double> poison, <vscale x 1 x double> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP3]]
//
__epi_8xf64 test_vfwredosum_8xf64_mask(__epi_8xf64 arg_0, __epi_8xf32 arg_1, __epi_8xf64 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwredosum_8xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwredsum_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv2f64(<vscale x 2 x double> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfwredusum.nxv1f64.nxv2f32.i64(<vscale x 1 x double> poison, <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 1 x double> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x double> @llvm.vector.insert.nxv2f64.nxv1f64(<vscale x 2 x double> poison, <vscale x 1 x double> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP2]]
//
__epi_2xf64 test_vfwredsum_2xf64(__epi_2xf32 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfwredsum_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfwredsum_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv2f64(<vscale x 2 x double> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv2f64(<vscale x 2 x double> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfwredusum.mask.nxv1f64.nxv2f32.i64(<vscale x 1 x double> [[TMP0]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 1 x double> [[TMP1]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 2 x double> @llvm.vector.insert.nxv2f64.nxv1f64(<vscale x 2 x double> poison, <vscale x 1 x double> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP3]]
//
__epi_2xf64 test_vfwredsum_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf32 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwredsum_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwredsum_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv4f64(<vscale x 4 x double> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfwredusum.nxv1f64.nxv4f32.i64(<vscale x 1 x double> poison, <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 1 x double> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x double> @llvm.vector.insert.nxv4f64.nxv1f64(<vscale x 4 x double> poison, <vscale x 1 x double> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP2]]
//
__epi_4xf64 test_vfwredsum_4xf64(__epi_4xf32 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfwredsum_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfwredsum_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv4f64(<vscale x 4 x double> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv4f64(<vscale x 4 x double> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfwredusum.mask.nxv1f64.nxv4f32.i64(<vscale x 1 x double> [[TMP0]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 1 x double> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x double> @llvm.vector.insert.nxv4f64.nxv1f64(<vscale x 4 x double> poison, <vscale x 1 x double> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP3]]
//
__epi_4xf64 test_vfwredsum_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf32 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwredsum_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwredsum_8xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv8f64(<vscale x 8 x double> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfwredusum.nxv1f64.nxv8f32.i64(<vscale x 1 x double> poison, <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 1 x double> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nxv1f64(<vscale x 8 x double> poison, <vscale x 1 x double> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP2]]
//
__epi_8xf64 test_vfwredsum_8xf64(__epi_8xf32 arg_0, __epi_8xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfwredsum_8xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfwredsum_8xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv8f64(<vscale x 8 x double> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.vector.extract.nxv1f64.nxv8f64(<vscale x 8 x double> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfwredusum.mask.nxv1f64.nxv8f32.i64(<vscale x 1 x double> [[TMP0]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 1 x double> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nxv1f64(<vscale x 8 x double> poison, <vscale x 1 x double> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP3]]
//
__epi_8xf64 test_vfwredsum_8xf64_mask(__epi_8xf64 arg_0, __epi_8xf32 arg_1, __epi_8xf64 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwredsum_8xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwsub_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwsub.nxv2f64.nxv2f32.nxv2f32.i64(<vscale x 2 x double> undef, <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwsub_2xf64(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfwsub_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfwsub_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwsub.mask.nxv2f64.nxv2f32.nxv2f32.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwsub_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwsub_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwsub_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwsub.nxv4f64.nxv4f32.nxv4f32.i64(<vscale x 4 x double> undef, <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwsub_4xf64(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfwsub_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfwsub_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwsub.mask.nxv4f64.nxv4f32.nxv4f32.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwsub_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwsub_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwsub_8xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwsub.nxv8f64.nxv8f32.nxv8f32.i64(<vscale x 8 x double> undef, <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwsub_8xf64(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfwsub_8xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfwsub_8xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwsub.mask.nxv8f64.nxv8f32.nxv8f32.i64(<vscale x 8 x double> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwsub_8xf64_mask(__epi_8xf64 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwsub_8xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwsub_w_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwsub.w.nxv2f64.nxv2f32.i64(<vscale x 2 x double> undef, <vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwsub_w_2xf64(__epi_2xf64 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfwsub_w_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfwsub_w_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfwsub.w.mask.nxv2f64.nxv2f32.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vfwsub_w_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwsub_w_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwsub_w_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwsub.w.nxv4f64.nxv4f32.i64(<vscale x 4 x double> undef, <vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwsub_w_4xf64(__epi_4xf64 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfwsub_w_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfwsub_w_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfwsub.w.mask.nxv4f64.nxv4f32.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vfwsub_w_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwsub_w_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vfwsub_w_8xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwsub.w.nxv8f64.nxv8f32.i64(<vscale x 8 x double> undef, <vscale x 8 x double> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwsub_w_8xf64(__epi_8xf64 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vfwsub_w_8xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vfwsub_w_8xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x double> @llvm.riscv.vfwsub.w.mask.nxv8f64.nxv8f32.i64(<vscale x 8 x double> [[ARG_0:%.*]], <vscale x 8 x double> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x double> [[TMP0]]
//
__epi_8xf64 test_vfwsub_w_8xf64_mask(__epi_8xf64 arg_0, __epi_8xf64 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vfwsub_w_8xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vid_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vid.nxv8i8.i64(<vscale x 8 x i8> undef, i64 [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vid_8xi8(unsigned long int arg_0)
{
    return __builtin_epi_vid_8xi8(arg_0);
}

// CHECK-O2-LABEL: @test_vid_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vid.mask.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vid_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vid_8xi8_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vid_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vid.nxv4i16.i64(<vscale x 4 x i16> undef, i64 [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vid_4xi16(unsigned long int arg_0)
{
    return __builtin_epi_vid_4xi16(arg_0);
}

// CHECK-O2-LABEL: @test_vid_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vid.mask.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vid_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vid_4xi16_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vid_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vid.nxv2i32.i64(<vscale x 2 x i32> undef, i64 [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vid_2xi32(unsigned long int arg_0)
{
    return __builtin_epi_vid_2xi32(arg_0);
}

// CHECK-O2-LABEL: @test_vid_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vid.mask.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vid_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vid_2xi32_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vid_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vid.nxv1i64.i64(<vscale x 1 x i64> undef, i64 [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vid_1xi64(unsigned long int arg_0)
{
    return __builtin_epi_vid_1xi64(arg_0);
}

// CHECK-O2-LABEL: @test_vid_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vid.mask.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vid_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vid_1xi64_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vid_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vid.nxv16i8.i64(<vscale x 16 x i8> undef, i64 [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vid_16xi8(unsigned long int arg_0)
{
    return __builtin_epi_vid_16xi8(arg_0);
}

// CHECK-O2-LABEL: @test_vid_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vid.mask.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vid_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vid_16xi8_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vid_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vid.nxv8i16.i64(<vscale x 8 x i16> undef, i64 [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vid_8xi16(unsigned long int arg_0)
{
    return __builtin_epi_vid_8xi16(arg_0);
}

// CHECK-O2-LABEL: @test_vid_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vid.mask.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vid_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vid_8xi16_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vid_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vid.nxv4i32.i64(<vscale x 4 x i32> undef, i64 [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vid_4xi32(unsigned long int arg_0)
{
    return __builtin_epi_vid_4xi32(arg_0);
}

// CHECK-O2-LABEL: @test_vid_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vid.mask.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vid_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vid_4xi32_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vid_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vid.nxv2i64.i64(<vscale x 2 x i64> undef, i64 [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vid_2xi64(unsigned long int arg_0)
{
    return __builtin_epi_vid_2xi64(arg_0);
}

// CHECK-O2-LABEL: @test_vid_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vid.mask.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vid_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vid_2xi64_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vid_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vid.nxv32i8.i64(<vscale x 32 x i8> undef, i64 [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vid_32xi8(unsigned long int arg_0)
{
    return __builtin_epi_vid_32xi8(arg_0);
}

// CHECK-O2-LABEL: @test_vid_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vid.mask.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vid_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vid_32xi8_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vid_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vid.nxv16i16.i64(<vscale x 16 x i16> undef, i64 [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vid_16xi16(unsigned long int arg_0)
{
    return __builtin_epi_vid_16xi16(arg_0);
}

// CHECK-O2-LABEL: @test_vid_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vid.mask.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vid_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vid_16xi16_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vid_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vid.nxv8i32.i64(<vscale x 8 x i32> undef, i64 [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vid_8xi32(unsigned long int arg_0)
{
    return __builtin_epi_vid_8xi32(arg_0);
}

// CHECK-O2-LABEL: @test_vid_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vid.mask.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vid_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vid_8xi32_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vid_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vid.nxv4i64.i64(<vscale x 4 x i64> undef, i64 [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vid_4xi64(unsigned long int arg_0)
{
    return __builtin_epi_vid_4xi64(arg_0);
}

// CHECK-O2-LABEL: @test_vid_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vid.mask.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vid_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vid_4xi64_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_viota_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.viota.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_viota_8xi8(__epi_8xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_viota_8xi8(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_viota_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.viota.mask.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i1> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_viota_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi1 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_viota_8xi8_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_viota_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.viota.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_viota_4xi16(__epi_4xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_viota_4xi16(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_viota_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.viota.mask.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_viota_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi1 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_viota_4xi16_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_viota_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.viota.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_viota_2xi32(__epi_2xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_viota_2xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_viota_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.viota.mask.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i1> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_viota_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi1 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_viota_2xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_viota_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.viota.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_viota_1xi64(__epi_1xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_viota_1xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_viota_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.viota.mask.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i1> [[ARG_1:%.*]], <vscale x 1 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_viota_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi1 arg_1, __epi_1xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_viota_1xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_viota_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.viota.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_viota_16xi8(__epi_16xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_viota_16xi8(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_viota_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.viota.mask.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i1> [[ARG_1:%.*]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_viota_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi1 arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_viota_16xi8_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_viota_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.viota.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_viota_8xi16(__epi_8xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_viota_8xi16(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_viota_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.viota.mask.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i1> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_viota_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi1 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_viota_8xi16_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_viota_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.viota.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_viota_4xi32(__epi_4xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_viota_4xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_viota_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.viota.mask.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_viota_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi1 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_viota_4xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_viota_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.viota.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_viota_2xi64(__epi_2xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_viota_2xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_viota_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.viota.mask.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i1> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_viota_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi1 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_viota_2xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_viota_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.viota.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_viota_32xi8(__epi_32xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_viota_32xi8(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_viota_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.viota.mask.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i1> [[ARG_1:%.*]], <vscale x 32 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_viota_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi1 arg_1, __epi_32xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_viota_32xi8_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_viota_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.viota.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_viota_16xi16(__epi_16xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_viota_16xi16(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_viota_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.viota.mask.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i1> [[ARG_1:%.*]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_viota_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi1 arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_viota_16xi16_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_viota_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.viota.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_viota_8xi32(__epi_8xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_viota_8xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_viota_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.viota.mask.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i1> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_viota_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi1 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_viota_8xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_viota_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.viota.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_viota_4xi64(__epi_4xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_viota_4xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_viota_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.viota.mask.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_viota_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi1 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_viota_4xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vle.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP1]]
//
__epi_8xi8 test_vload_8xi8(const signed char*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_8xi8(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_1:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vle.mask.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8>* [[TMP0]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP1]]
//
__epi_8xi8 test_vload_8xi8_mask(__epi_8xi8 arg_0, const signed char*  arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_8xi8_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vle.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP1]]
//
__epi_4xi16 test_vload_4xi16(const signed short int*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_4xi16(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_1:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vle.mask.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16>* [[TMP0]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP1]]
//
__epi_4xi16 test_vload_4xi16_mask(__epi_4xi16 arg_0, const signed short int*  arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_4xi16_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vle.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP1]]
//
__epi_2xi32 test_vload_2xi32(const signed int*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_2xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_1:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vle.mask.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32>* [[TMP0]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP1]]
//
__epi_2xi32 test_vload_2xi32_mask(__epi_2xi32 arg_0, const signed int*  arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_2xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vle.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP1]]
//
__epi_1xi64 test_vload_1xi64(const signed long int*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_1xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_1:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vle.mask.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64>* [[TMP0]], <vscale x 1 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP1]]
//
__epi_1xi64 test_vload_1xi64_mask(__epi_1xi64 arg_0, const signed long int*  arg_1, __epi_1xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_1xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 2 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vle.nxv2f32.i64(<vscale x 2 x float> undef, <vscale x 2 x float>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP1]]
//
__epi_2xf32 test_vload_2xf32(const float*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_2xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_1:%.*]] to <vscale x 2 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vle.mask.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float>* [[TMP0]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP1]]
//
__epi_2xf32 test_vload_2xf32_mask(__epi_2xf32 arg_0, const float*  arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_2xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 1 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vle.nxv1f64.i64(<vscale x 1 x double> undef, <vscale x 1 x double>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP1]]
//
__epi_1xf64 test_vload_1xf64(const double*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_1xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_1:%.*]] to <vscale x 1 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vle.mask.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double>* [[TMP0]], <vscale x 1 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP1]]
//
__epi_1xf64 test_vload_1xf64_mask(__epi_1xf64 arg_0, const double*  arg_1, __epi_1xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_1xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vle.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP1]]
//
__epi_16xi8 test_vload_16xi8(const signed char*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_16xi8(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_1:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vle.mask.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8>* [[TMP0]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP1]]
//
__epi_16xi8 test_vload_16xi8_mask(__epi_16xi8 arg_0, const signed char*  arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_16xi8_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vle.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP1]]
//
__epi_8xi16 test_vload_8xi16(const signed short int*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_8xi16(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_1:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vle.mask.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16>* [[TMP0]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP1]]
//
__epi_8xi16 test_vload_8xi16_mask(__epi_8xi16 arg_0, const signed short int*  arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_8xi16_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vle.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP1]]
//
__epi_4xi32 test_vload_4xi32(const signed int*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_4xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_1:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vle.mask.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32>* [[TMP0]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP1]]
//
__epi_4xi32 test_vload_4xi32_mask(__epi_4xi32 arg_0, const signed int*  arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_4xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vle.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP1]]
//
__epi_2xi64 test_vload_2xi64(const signed long int*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_2xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_1:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vle.mask.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64>* [[TMP0]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP1]]
//
__epi_2xi64 test_vload_2xi64_mask(__epi_2xi64 arg_0, const signed long int*  arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_2xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 4 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vle.nxv4f32.i64(<vscale x 4 x float> undef, <vscale x 4 x float>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP1]]
//
__epi_4xf32 test_vload_4xf32(const float*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_4xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_1:%.*]] to <vscale x 4 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vle.mask.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float>* [[TMP0]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP1]]
//
__epi_4xf32 test_vload_4xf32_mask(__epi_4xf32 arg_0, const float*  arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_4xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 2 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vle.nxv2f64.i64(<vscale x 2 x double> undef, <vscale x 2 x double>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP1]]
//
__epi_2xf64 test_vload_2xf64(const double*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_2xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_1:%.*]] to <vscale x 2 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vle.mask.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double>* [[TMP0]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP1]]
//
__epi_2xf64 test_vload_2xf64_mask(__epi_2xf64 arg_0, const double*  arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_2xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vle.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP1]]
//
__epi_32xi8 test_vload_32xi8(const signed char*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_32xi8(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_1:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vle.mask.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8>* [[TMP0]], <vscale x 32 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP1]]
//
__epi_32xi8 test_vload_32xi8_mask(__epi_32xi8 arg_0, const signed char*  arg_1, __epi_32xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_32xi8_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vle.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP1]]
//
__epi_16xi16 test_vload_16xi16(const signed short int*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_16xi16(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_1:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vle.mask.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16>* [[TMP0]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP1]]
//
__epi_16xi16 test_vload_16xi16_mask(__epi_16xi16 arg_0, const signed short int*  arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_16xi16_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vle.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP1]]
//
__epi_8xi32 test_vload_8xi32(const signed int*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_8xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_1:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vle.mask.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32>* [[TMP0]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP1]]
//
__epi_8xi32 test_vload_8xi32_mask(__epi_8xi32 arg_0, const signed int*  arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_8xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vle.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP1]]
//
__epi_4xi64 test_vload_4xi64(const signed long int*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_4xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_1:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vle.mask.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64>* [[TMP0]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP1]]
//
__epi_4xi64 test_vload_4xi64_mask(__epi_4xi64 arg_0, const signed long int*  arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_4xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 8 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vle.nxv8f32.i64(<vscale x 8 x float> undef, <vscale x 8 x float>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP1]]
//
__epi_8xf32 test_vload_8xf32(const float*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_8xf32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_1:%.*]] to <vscale x 8 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vle.mask.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float>* [[TMP0]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP1]]
//
__epi_8xf32 test_vload_8xf32_mask(__epi_8xf32 arg_0, const float*  arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_8xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 4 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vle.nxv4f64.i64(<vscale x 4 x double> undef, <vscale x 4 x double>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP1]]
//
__epi_4xf64 test_vload_4xf64(const double*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_4xf64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_1:%.*]] to <vscale x 4 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vle.mask.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double>* [[TMP0]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP1]]
//
__epi_4xf64 test_vload_4xf64_mask(__epi_4xf64 arg_0, const double*  arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_4xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_indexed_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vluxei.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8>* [[TMP0]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP1]]
//
__epi_8xi8 test_vload_indexed_8xi8(const signed char*  arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_1:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vluxei.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8>* [[TMP0]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP1]]
//
__epi_8xi8 test_vload_indexed_8xi8_mask(__epi_8xi8 arg_0, const signed char*  arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vluxei.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16>* [[TMP0]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP1]]
//
__epi_4xi16 test_vload_indexed_4xi16(const signed short int*  arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_1:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vluxei.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16>* [[TMP0]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP1]]
//
__epi_4xi16 test_vload_indexed_4xi16_mask(__epi_4xi16 arg_0, const signed short int*  arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vluxei.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32>* [[TMP0]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP1]]
//
__epi_2xi32 test_vload_indexed_2xi32(const signed int*  arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_1:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vluxei.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32>* [[TMP0]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP1]]
//
__epi_2xi32 test_vload_indexed_2xi32_mask(__epi_2xi32 arg_0, const signed int*  arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vluxei.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64>* [[TMP0]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP1]]
//
__epi_1xi64 test_vload_indexed_1xi64(const signed long int*  arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_1:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vluxei.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64>* [[TMP0]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP1]]
//
__epi_1xi64 test_vload_indexed_1xi64_mask(__epi_1xi64 arg_0, const signed long int*  arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 2 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vluxei.nxv2f32.nxv2i32.i64(<vscale x 2 x float> undef, <vscale x 2 x float>* [[TMP0]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP1]]
//
__epi_2xf32 test_vload_indexed_2xf32(const float*  arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_1:%.*]] to <vscale x 2 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vluxei.mask.nxv2f32.nxv2i32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float>* [[TMP0]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP1]]
//
__epi_2xf32 test_vload_indexed_2xf32_mask(__epi_2xf32 arg_0, const float*  arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 1 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vluxei.nxv1f64.nxv1i64.i64(<vscale x 1 x double> undef, <vscale x 1 x double>* [[TMP0]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP1]]
//
__epi_1xf64 test_vload_indexed_1xf64(const double*  arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_1:%.*]] to <vscale x 1 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vluxei.mask.nxv1f64.nxv1i64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double>* [[TMP0]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP1]]
//
__epi_1xf64 test_vload_indexed_1xf64_mask(__epi_1xf64 arg_0, const double*  arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vluxei.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8>* [[TMP0]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP1]]
//
__epi_16xi8 test_vload_indexed_16xi8(const signed char*  arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_1:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vluxei.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8>* [[TMP0]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP1]]
//
__epi_16xi8 test_vload_indexed_16xi8_mask(__epi_16xi8 arg_0, const signed char*  arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vluxei.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16>* [[TMP0]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP1]]
//
__epi_8xi16 test_vload_indexed_8xi16(const signed short int*  arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_1:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vluxei.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16>* [[TMP0]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP1]]
//
__epi_8xi16 test_vload_indexed_8xi16_mask(__epi_8xi16 arg_0, const signed short int*  arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vluxei.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32>* [[TMP0]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP1]]
//
__epi_4xi32 test_vload_indexed_4xi32(const signed int*  arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_1:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vluxei.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32>* [[TMP0]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP1]]
//
__epi_4xi32 test_vload_indexed_4xi32_mask(__epi_4xi32 arg_0, const signed int*  arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vluxei.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64>* [[TMP0]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP1]]
//
__epi_2xi64 test_vload_indexed_2xi64(const signed long int*  arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_1:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vluxei.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64>* [[TMP0]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP1]]
//
__epi_2xi64 test_vload_indexed_2xi64_mask(__epi_2xi64 arg_0, const signed long int*  arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 4 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vluxei.nxv4f32.nxv4i32.i64(<vscale x 4 x float> undef, <vscale x 4 x float>* [[TMP0]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP1]]
//
__epi_4xf32 test_vload_indexed_4xf32(const float*  arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_1:%.*]] to <vscale x 4 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vluxei.mask.nxv4f32.nxv4i32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float>* [[TMP0]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP1]]
//
__epi_4xf32 test_vload_indexed_4xf32_mask(__epi_4xf32 arg_0, const float*  arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 2 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vluxei.nxv2f64.nxv2i64.i64(<vscale x 2 x double> undef, <vscale x 2 x double>* [[TMP0]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP1]]
//
__epi_2xf64 test_vload_indexed_2xf64(const double*  arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_1:%.*]] to <vscale x 2 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vluxei.mask.nxv2f64.nxv2i64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double>* [[TMP0]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP1]]
//
__epi_2xf64 test_vload_indexed_2xf64_mask(__epi_2xf64 arg_0, const double*  arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vluxei.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8>* [[TMP0]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP1]]
//
__epi_32xi8 test_vload_indexed_32xi8(const signed char*  arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_1:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vluxei.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8>* [[TMP0]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP1]]
//
__epi_32xi8 test_vload_indexed_32xi8_mask(__epi_32xi8 arg_0, const signed char*  arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vluxei.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16>* [[TMP0]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP1]]
//
__epi_16xi16 test_vload_indexed_16xi16(const signed short int*  arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_1:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vluxei.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16>* [[TMP0]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP1]]
//
__epi_16xi16 test_vload_indexed_16xi16_mask(__epi_16xi16 arg_0, const signed short int*  arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vluxei.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32>* [[TMP0]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP1]]
//
__epi_8xi32 test_vload_indexed_8xi32(const signed int*  arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_1:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vluxei.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32>* [[TMP0]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP1]]
//
__epi_8xi32 test_vload_indexed_8xi32_mask(__epi_8xi32 arg_0, const signed int*  arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vluxei.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64>* [[TMP0]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP1]]
//
__epi_4xi64 test_vload_indexed_4xi64(const signed long int*  arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_1:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vluxei.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64>* [[TMP0]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP1]]
//
__epi_4xi64 test_vload_indexed_4xi64_mask(__epi_4xi64 arg_0, const signed long int*  arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 8 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vluxei.nxv8f32.nxv8i32.i64(<vscale x 8 x float> undef, <vscale x 8 x float>* [[TMP0]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP1]]
//
__epi_8xf32 test_vload_indexed_8xf32(const float*  arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_1:%.*]] to <vscale x 8 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vluxei.mask.nxv8f32.nxv8i32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float>* [[TMP0]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP1]]
//
__epi_8xf32 test_vload_indexed_8xf32_mask(__epi_8xf32 arg_0, const float*  arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 4 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vluxei.nxv4f64.nxv4i64.i64(<vscale x 4 x double> undef, <vscale x 4 x double>* [[TMP0]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP1]]
//
__epi_4xf64 test_vload_indexed_4xf64(const double*  arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_1:%.*]] to <vscale x 4 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vluxei.mask.nxv4f64.nxv4i64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double>* [[TMP0]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP1]]
//
__epi_4xf64 test_vload_indexed_4xf64_mask(__epi_4xf64 arg_0, const double*  arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vluxei.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8>* [[TMP0]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP1]]
//
__epi_8xi8 test_vload_indexed_unsigned_8xi8(const unsigned char*  arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_unsigned_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_1:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vluxei.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8>* [[TMP0]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP1]]
//
__epi_8xi8 test_vload_indexed_unsigned_8xi8_mask(__epi_8xi8 arg_0, const unsigned char*  arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_unsigned_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vluxei.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16>* [[TMP0]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP1]]
//
__epi_4xi16 test_vload_indexed_unsigned_4xi16(const unsigned short int*  arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_unsigned_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_1:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vluxei.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16>* [[TMP0]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP1]]
//
__epi_4xi16 test_vload_indexed_unsigned_4xi16_mask(__epi_4xi16 arg_0, const unsigned short int*  arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_unsigned_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vluxei.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32>* [[TMP0]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP1]]
//
__epi_2xi32 test_vload_indexed_unsigned_2xi32(const unsigned int*  arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_unsigned_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_1:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vluxei.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32>* [[TMP0]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP1]]
//
__epi_2xi32 test_vload_indexed_unsigned_2xi32_mask(__epi_2xi32 arg_0, const unsigned int*  arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_unsigned_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vluxei.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64>* [[TMP0]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP1]]
//
__epi_1xi64 test_vload_indexed_unsigned_1xi64(const unsigned long int*  arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_unsigned_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_1:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vluxei.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64>* [[TMP0]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP1]]
//
__epi_1xi64 test_vload_indexed_unsigned_1xi64_mask(__epi_1xi64 arg_0, const unsigned long int*  arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_unsigned_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vluxei.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8>* [[TMP0]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP1]]
//
__epi_16xi8 test_vload_indexed_unsigned_16xi8(const unsigned char*  arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_unsigned_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_1:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vluxei.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8>* [[TMP0]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP1]]
//
__epi_16xi8 test_vload_indexed_unsigned_16xi8_mask(__epi_16xi8 arg_0, const unsigned char*  arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_unsigned_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vluxei.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16>* [[TMP0]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP1]]
//
__epi_8xi16 test_vload_indexed_unsigned_8xi16(const unsigned short int*  arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_unsigned_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_1:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vluxei.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16>* [[TMP0]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP1]]
//
__epi_8xi16 test_vload_indexed_unsigned_8xi16_mask(__epi_8xi16 arg_0, const unsigned short int*  arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_unsigned_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vluxei.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32>* [[TMP0]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP1]]
//
__epi_4xi32 test_vload_indexed_unsigned_4xi32(const unsigned int*  arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_unsigned_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_1:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vluxei.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32>* [[TMP0]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP1]]
//
__epi_4xi32 test_vload_indexed_unsigned_4xi32_mask(__epi_4xi32 arg_0, const unsigned int*  arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_unsigned_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vluxei.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64>* [[TMP0]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP1]]
//
__epi_2xi64 test_vload_indexed_unsigned_2xi64(const unsigned long int*  arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_unsigned_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_1:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vluxei.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64>* [[TMP0]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP1]]
//
__epi_2xi64 test_vload_indexed_unsigned_2xi64_mask(__epi_2xi64 arg_0, const unsigned long int*  arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_unsigned_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vluxei.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8>* [[TMP0]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP1]]
//
__epi_32xi8 test_vload_indexed_unsigned_32xi8(const unsigned char*  arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_unsigned_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_1:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vluxei.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8>* [[TMP0]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP1]]
//
__epi_32xi8 test_vload_indexed_unsigned_32xi8_mask(__epi_32xi8 arg_0, const unsigned char*  arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_unsigned_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vluxei.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16>* [[TMP0]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP1]]
//
__epi_16xi16 test_vload_indexed_unsigned_16xi16(const unsigned short int*  arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_unsigned_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_1:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vluxei.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16>* [[TMP0]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP1]]
//
__epi_16xi16 test_vload_indexed_unsigned_16xi16_mask(__epi_16xi16 arg_0, const unsigned short int*  arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_unsigned_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vluxei.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32>* [[TMP0]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP1]]
//
__epi_8xi32 test_vload_indexed_unsigned_8xi32(const unsigned int*  arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_unsigned_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_1:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vluxei.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32>* [[TMP0]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP1]]
//
__epi_8xi32 test_vload_indexed_unsigned_8xi32_mask(__epi_8xi32 arg_0, const unsigned int*  arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_unsigned_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vluxei.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64>* [[TMP0]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP1]]
//
__epi_4xi64 test_vload_indexed_unsigned_4xi64(const unsigned long int*  arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_indexed_unsigned_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_indexed_unsigned_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_1:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vluxei.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64>* [[TMP0]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP1]]
//
__epi_4xi64 test_vload_indexed_unsigned_4xi64_mask(__epi_4xi64 arg_0, const unsigned long int*  arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_indexed_unsigned_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vlse.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP1]]
//
__epi_8xi8 test_vload_strided_8xi8(const signed char*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_1:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vlse.mask.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP1]]
//
__epi_8xi8 test_vload_strided_8xi8_mask(__epi_8xi8 arg_0, const signed char*  arg_1, signed long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vlse.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP1]]
//
__epi_4xi16 test_vload_strided_4xi16(const signed short int*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_1:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vlse.mask.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP1]]
//
__epi_4xi16 test_vload_strided_4xi16_mask(__epi_4xi16 arg_0, const signed short int*  arg_1, signed long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vlse.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP1]]
//
__epi_2xi32 test_vload_strided_2xi32(const signed int*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_1:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vlse.mask.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP1]]
//
__epi_2xi32 test_vload_strided_2xi32_mask(__epi_2xi32 arg_0, const signed int*  arg_1, signed long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vlse.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP1]]
//
__epi_1xi64 test_vload_strided_1xi64(const signed long int*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_1:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vlse.mask.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP1]]
//
__epi_1xi64 test_vload_strided_1xi64_mask(__epi_1xi64 arg_0, const signed long int*  arg_1, signed long int arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 2 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vlse.nxv2f32.i64(<vscale x 2 x float> undef, <vscale x 2 x float>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP1]]
//
__epi_2xf32 test_vload_strided_2xf32(const float*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_1:%.*]] to <vscale x 2 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vlse.mask.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP1]]
//
__epi_2xf32 test_vload_strided_2xf32_mask(__epi_2xf32 arg_0, const float*  arg_1, signed long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 1 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vlse.nxv1f64.i64(<vscale x 1 x double> undef, <vscale x 1 x double>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP1]]
//
__epi_1xf64 test_vload_strided_1xf64(const double*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_1:%.*]] to <vscale x 1 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vlse.mask.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP1]]
//
__epi_1xf64 test_vload_strided_1xf64_mask(__epi_1xf64 arg_0, const double*  arg_1, signed long int arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vlse.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP1]]
//
__epi_16xi8 test_vload_strided_16xi8(const signed char*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_1:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vlse.mask.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP1]]
//
__epi_16xi8 test_vload_strided_16xi8_mask(__epi_16xi8 arg_0, const signed char*  arg_1, signed long int arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vlse.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP1]]
//
__epi_8xi16 test_vload_strided_8xi16(const signed short int*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_1:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vlse.mask.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP1]]
//
__epi_8xi16 test_vload_strided_8xi16_mask(__epi_8xi16 arg_0, const signed short int*  arg_1, signed long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vlse.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP1]]
//
__epi_4xi32 test_vload_strided_4xi32(const signed int*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_1:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vlse.mask.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP1]]
//
__epi_4xi32 test_vload_strided_4xi32_mask(__epi_4xi32 arg_0, const signed int*  arg_1, signed long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vlse.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP1]]
//
__epi_2xi64 test_vload_strided_2xi64(const signed long int*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_1:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vlse.mask.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP1]]
//
__epi_2xi64 test_vload_strided_2xi64_mask(__epi_2xi64 arg_0, const signed long int*  arg_1, signed long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 4 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vlse.nxv4f32.i64(<vscale x 4 x float> undef, <vscale x 4 x float>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP1]]
//
__epi_4xf32 test_vload_strided_4xf32(const float*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_1:%.*]] to <vscale x 4 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vlse.mask.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP1]]
//
__epi_4xf32 test_vload_strided_4xf32_mask(__epi_4xf32 arg_0, const float*  arg_1, signed long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 2 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vlse.nxv2f64.i64(<vscale x 2 x double> undef, <vscale x 2 x double>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP1]]
//
__epi_2xf64 test_vload_strided_2xf64(const double*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_1:%.*]] to <vscale x 2 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vlse.mask.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP1]]
//
__epi_2xf64 test_vload_strided_2xf64_mask(__epi_2xf64 arg_0, const double*  arg_1, signed long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vlse.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP1]]
//
__epi_32xi8 test_vload_strided_32xi8(const signed char*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_1:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vlse.mask.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP1]]
//
__epi_32xi8 test_vload_strided_32xi8_mask(__epi_32xi8 arg_0, const signed char*  arg_1, signed long int arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vlse.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP1]]
//
__epi_16xi16 test_vload_strided_16xi16(const signed short int*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_1:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vlse.mask.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP1]]
//
__epi_16xi16 test_vload_strided_16xi16_mask(__epi_16xi16 arg_0, const signed short int*  arg_1, signed long int arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vlse.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP1]]
//
__epi_8xi32 test_vload_strided_8xi32(const signed int*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_1:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vlse.mask.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP1]]
//
__epi_8xi32 test_vload_strided_8xi32_mask(__epi_8xi32 arg_0, const signed int*  arg_1, signed long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vlse.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP1]]
//
__epi_4xi64 test_vload_strided_4xi64(const signed long int*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_1:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vlse.mask.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP1]]
//
__epi_4xi64 test_vload_strided_4xi64_mask(__epi_4xi64 arg_0, const signed long int*  arg_1, signed long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 8 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vlse.nxv8f32.i64(<vscale x 8 x float> undef, <vscale x 8 x float>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP1]]
//
__epi_8xf32 test_vload_strided_8xf32(const float*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_1:%.*]] to <vscale x 8 x float>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vlse.mask.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP1]]
//
__epi_8xf32 test_vload_strided_8xf32_mask(__epi_8xf32 arg_0, const float*  arg_1, signed long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 4 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vlse.nxv4f64.i64(<vscale x 4 x double> undef, <vscale x 4 x double>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP1]]
//
__epi_4xf64 test_vload_strided_4xf64(const double*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_1:%.*]] to <vscale x 4 x double>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vlse.mask.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP1]]
//
__epi_4xf64 test_vload_strided_4xf64_mask(__epi_4xf64 arg_0, const double*  arg_1, signed long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vlse.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP1]]
//
__epi_8xi8 test_vload_strided_unsigned_8xi8(const unsigned char*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_unsigned_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_1:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vlse.mask.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP1]]
//
__epi_8xi8 test_vload_strided_unsigned_8xi8_mask(__epi_8xi8 arg_0, const unsigned char*  arg_1, signed long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_unsigned_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vlse.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP1]]
//
__epi_4xi16 test_vload_strided_unsigned_4xi16(const unsigned short int*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_unsigned_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_1:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vlse.mask.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP1]]
//
__epi_4xi16 test_vload_strided_unsigned_4xi16_mask(__epi_4xi16 arg_0, const unsigned short int*  arg_1, signed long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_unsigned_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vlse.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP1]]
//
__epi_2xi32 test_vload_strided_unsigned_2xi32(const unsigned int*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_unsigned_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_1:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vlse.mask.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP1]]
//
__epi_2xi32 test_vload_strided_unsigned_2xi32_mask(__epi_2xi32 arg_0, const unsigned int*  arg_1, signed long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_unsigned_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vlse.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP1]]
//
__epi_1xi64 test_vload_strided_unsigned_1xi64(const unsigned long int*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_unsigned_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_1:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vlse.mask.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP1]]
//
__epi_1xi64 test_vload_strided_unsigned_1xi64_mask(__epi_1xi64 arg_0, const unsigned long int*  arg_1, signed long int arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_unsigned_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vlse.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP1]]
//
__epi_16xi8 test_vload_strided_unsigned_16xi8(const unsigned char*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_unsigned_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_1:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vlse.mask.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP1]]
//
__epi_16xi8 test_vload_strided_unsigned_16xi8_mask(__epi_16xi8 arg_0, const unsigned char*  arg_1, signed long int arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_unsigned_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vlse.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP1]]
//
__epi_8xi16 test_vload_strided_unsigned_8xi16(const unsigned short int*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_unsigned_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_1:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vlse.mask.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP1]]
//
__epi_8xi16 test_vload_strided_unsigned_8xi16_mask(__epi_8xi16 arg_0, const unsigned short int*  arg_1, signed long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_unsigned_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vlse.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP1]]
//
__epi_4xi32 test_vload_strided_unsigned_4xi32(const unsigned int*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_unsigned_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_1:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vlse.mask.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP1]]
//
__epi_4xi32 test_vload_strided_unsigned_4xi32_mask(__epi_4xi32 arg_0, const unsigned int*  arg_1, signed long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_unsigned_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vlse.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP1]]
//
__epi_2xi64 test_vload_strided_unsigned_2xi64(const unsigned long int*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_unsigned_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_1:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vlse.mask.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP1]]
//
__epi_2xi64 test_vload_strided_unsigned_2xi64_mask(__epi_2xi64 arg_0, const unsigned long int*  arg_1, signed long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_unsigned_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vlse.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP1]]
//
__epi_32xi8 test_vload_strided_unsigned_32xi8(const unsigned char*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_unsigned_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_1:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vlse.mask.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP1]]
//
__epi_32xi8 test_vload_strided_unsigned_32xi8_mask(__epi_32xi8 arg_0, const unsigned char*  arg_1, signed long int arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_unsigned_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vlse.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP1]]
//
__epi_16xi16 test_vload_strided_unsigned_16xi16(const unsigned short int*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_unsigned_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_1:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vlse.mask.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP1]]
//
__epi_16xi16 test_vload_strided_unsigned_16xi16_mask(__epi_16xi16 arg_0, const unsigned short int*  arg_1, signed long int arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_unsigned_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vlse.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP1]]
//
__epi_8xi32 test_vload_strided_unsigned_8xi32(const unsigned int*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_unsigned_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_1:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vlse.mask.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP1]]
//
__epi_8xi32 test_vload_strided_unsigned_8xi32_mask(__epi_8xi32 arg_0, const unsigned int*  arg_1, signed long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_unsigned_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vlse.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64>* [[TMP0]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP1]]
//
__epi_4xi64 test_vload_strided_unsigned_4xi64(const unsigned long int*  arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vload_strided_unsigned_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vload_strided_unsigned_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_1:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vlse.mask.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP1]]
//
__epi_4xi64 test_vload_strided_unsigned_4xi64_mask(__epi_4xi64 arg_0, const unsigned long int*  arg_1, signed long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vload_strided_unsigned_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vload_unsigned_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vle.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP1]]
//
__epi_8xi8 test_vload_unsigned_8xi8(const unsigned char*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_unsigned_8xi8(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_unsigned_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_1:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vle.mask.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8>* [[TMP0]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP1]]
//
__epi_8xi8 test_vload_unsigned_8xi8_mask(__epi_8xi8 arg_0, const unsigned char*  arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_unsigned_8xi8_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_unsigned_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vle.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP1]]
//
__epi_4xi16 test_vload_unsigned_4xi16(const unsigned short int*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_unsigned_4xi16(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_unsigned_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_1:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vle.mask.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16>* [[TMP0]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP1]]
//
__epi_4xi16 test_vload_unsigned_4xi16_mask(__epi_4xi16 arg_0, const unsigned short int*  arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_unsigned_4xi16_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_unsigned_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vle.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP1]]
//
__epi_2xi32 test_vload_unsigned_2xi32(const unsigned int*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_unsigned_2xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_unsigned_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_1:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vle.mask.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32>* [[TMP0]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP1]]
//
__epi_2xi32 test_vload_unsigned_2xi32_mask(__epi_2xi32 arg_0, const unsigned int*  arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_unsigned_2xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_unsigned_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vle.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP1]]
//
__epi_1xi64 test_vload_unsigned_1xi64(const unsigned long int*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_unsigned_1xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_unsigned_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_1:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vle.mask.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64>* [[TMP0]], <vscale x 1 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP1]]
//
__epi_1xi64 test_vload_unsigned_1xi64_mask(__epi_1xi64 arg_0, const unsigned long int*  arg_1, __epi_1xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_unsigned_1xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_unsigned_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vle.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP1]]
//
__epi_16xi8 test_vload_unsigned_16xi8(const unsigned char*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_unsigned_16xi8(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_unsigned_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_1:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vle.mask.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8>* [[TMP0]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP1]]
//
__epi_16xi8 test_vload_unsigned_16xi8_mask(__epi_16xi8 arg_0, const unsigned char*  arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_unsigned_16xi8_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_unsigned_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vle.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP1]]
//
__epi_8xi16 test_vload_unsigned_8xi16(const unsigned short int*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_unsigned_8xi16(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_unsigned_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_1:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vle.mask.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16>* [[TMP0]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP1]]
//
__epi_8xi16 test_vload_unsigned_8xi16_mask(__epi_8xi16 arg_0, const unsigned short int*  arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_unsigned_8xi16_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_unsigned_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vle.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP1]]
//
__epi_4xi32 test_vload_unsigned_4xi32(const unsigned int*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_unsigned_4xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_unsigned_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_1:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vle.mask.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32>* [[TMP0]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP1]]
//
__epi_4xi32 test_vload_unsigned_4xi32_mask(__epi_4xi32 arg_0, const unsigned int*  arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_unsigned_4xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_unsigned_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vle.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP1]]
//
__epi_2xi64 test_vload_unsigned_2xi64(const unsigned long int*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_unsigned_2xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_unsigned_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_1:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vle.mask.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64>* [[TMP0]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP1]]
//
__epi_2xi64 test_vload_unsigned_2xi64_mask(__epi_2xi64 arg_0, const unsigned long int*  arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_unsigned_2xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_unsigned_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vle.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP1]]
//
__epi_32xi8 test_vload_unsigned_32xi8(const unsigned char*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_unsigned_32xi8(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_unsigned_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_1:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vle.mask.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8>* [[TMP0]], <vscale x 32 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP1]]
//
__epi_32xi8 test_vload_unsigned_32xi8_mask(__epi_32xi8 arg_0, const unsigned char*  arg_1, __epi_32xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_unsigned_32xi8_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_unsigned_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vle.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP1]]
//
__epi_16xi16 test_vload_unsigned_16xi16(const unsigned short int*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_unsigned_16xi16(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_unsigned_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_1:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vle.mask.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16>* [[TMP0]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP1]]
//
__epi_16xi16 test_vload_unsigned_16xi16_mask(__epi_16xi16 arg_0, const unsigned short int*  arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_unsigned_16xi16_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_unsigned_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vle.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP1]]
//
__epi_8xi32 test_vload_unsigned_8xi32(const unsigned int*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_unsigned_8xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_unsigned_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_1:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vle.mask.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32>* [[TMP0]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP1]]
//
__epi_8xi32 test_vload_unsigned_8xi32_mask(__epi_8xi32 arg_0, const unsigned int*  arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_unsigned_8xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vload_unsigned_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vle.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64>* [[TMP0]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP1]]
//
__epi_4xi64 test_vload_unsigned_4xi64(const unsigned long int*  arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vload_unsigned_4xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vload_unsigned_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_1:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vle.mask.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64>* [[TMP0]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP1]]
//
__epi_4xi64 test_vload_unsigned_4xi64_mask(__epi_4xi64 arg_0, const unsigned long int*  arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vload_unsigned_4xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmadc_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmadc.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmadc_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmadc_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmadc_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmadc.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmadc_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmadc_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmadc_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmadc.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmadc_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmadc_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmadc_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmadc.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmadc_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmadc_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmadc_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmadc.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmadc_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmadc_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmadc_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmadc.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmadc_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmadc_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmadc_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmadc.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmadc_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmadc_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmadc_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmadc.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmadc_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmadc_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmadc_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmadc.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmadc_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmadc_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmadc_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmadc.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmadc_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmadc_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmadc_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmadc.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmadc_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmadc_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmadc_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmadc.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmadc_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmadc_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmadc_carry_in_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmadc.carry.in.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmadc_carry_in_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmadc_carry_in_8xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmadc_carry_in_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmadc.carry.in.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmadc_carry_in_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmadc_carry_in_4xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmadc_carry_in_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmadc.carry.in.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmadc_carry_in_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmadc_carry_in_2xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmadc_carry_in_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmadc.carry.in.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmadc_carry_in_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmadc_carry_in_1xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmadc_carry_in_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmadc.carry.in.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmadc_carry_in_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmadc_carry_in_16xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmadc_carry_in_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmadc.carry.in.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmadc_carry_in_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmadc_carry_in_8xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmadc_carry_in_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmadc.carry.in.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmadc_carry_in_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmadc_carry_in_4xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmadc_carry_in_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmadc.carry.in.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmadc_carry_in_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmadc_carry_in_2xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmadc_carry_in_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmadc.carry.in.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmadc_carry_in_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmadc_carry_in_32xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmadc_carry_in_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmadc.carry.in.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmadc_carry_in_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmadc_carry_in_16xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmadc_carry_in_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmadc.carry.in.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmadc_carry_in_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmadc_carry_in_8xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmadc_carry_in_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmadc.carry.in.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmadc_carry_in_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmadc_carry_in_4xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmand_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmand.nxv8i1.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmand_8xi1(__epi_8xi1 arg_0, __epi_8xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmand_8xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmand_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmand.nxv4i1.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmand_4xi1(__epi_4xi1 arg_0, __epi_4xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmand_4xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmand_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmand.nxv2i1.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmand_2xi1(__epi_2xi1 arg_0, __epi_2xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmand_2xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmand_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmand.nxv1i1.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmand_1xi1(__epi_1xi1 arg_0, __epi_1xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmand_1xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmand_16xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmand.nxv16i1.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmand_16xi1(__epi_16xi1 arg_0, __epi_16xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmand_16xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmand_32xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmand.nxv32i1.i64(<vscale x 32 x i1> [[ARG_0:%.*]], <vscale x 32 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmand_32xi1(__epi_32xi1 arg_0, __epi_32xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmand_32xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmandnot_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmandn.nxv8i1.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmandnot_8xi1(__epi_8xi1 arg_0, __epi_8xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmandnot_8xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmandnot_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmandn.nxv4i1.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmandnot_4xi1(__epi_4xi1 arg_0, __epi_4xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmandnot_4xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmandnot_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmandn.nxv2i1.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmandnot_2xi1(__epi_2xi1 arg_0, __epi_2xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmandnot_2xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmandnot_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmandn.nxv1i1.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmandnot_1xi1(__epi_1xi1 arg_0, __epi_1xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmandnot_1xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmandnot_16xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmandn.nxv16i1.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmandnot_16xi1(__epi_16xi1 arg_0, __epi_16xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmandnot_16xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmandnot_32xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmandn.nxv32i1.i64(<vscale x 32 x i1> [[ARG_0:%.*]], <vscale x 32 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmandnot_32xi1(__epi_32xi1 arg_0, __epi_32xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmandnot_32xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmax_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vmax.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vmax_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmax_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmax_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vmax.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vmax_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmax_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmax_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vmax.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vmax_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmax_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmax_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vmax.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vmax_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmax_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmax_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vmax.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vmax_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmax_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmax_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vmax.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vmax_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmax_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmax_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vmax.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vmax_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmax_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmax_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vmax.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vmax_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmax_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmax_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vmax.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vmax_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmax_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmax_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vmax.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vmax_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmax_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmax_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vmax.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vmax_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmax_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmax_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vmax.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vmax_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmax_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmax_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vmax.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vmax_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmax_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmax_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vmax.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vmax_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmax_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmax_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vmax.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vmax_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmax_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmax_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vmax.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vmax_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmax_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmax_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vmax.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vmax_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmax_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmax_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vmax.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vmax_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmax_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmax_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vmax.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vmax_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmax_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmax_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vmax.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vmax_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmax_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmax_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vmax.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vmax_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmax_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmax_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vmax.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vmax_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmax_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmax_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vmax.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vmax_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmax_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmax_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vmax.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vmax_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmax_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmaxu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vmaxu.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vmaxu_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmaxu_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmaxu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vmaxu.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vmaxu_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmaxu_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmaxu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vmaxu.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vmaxu_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmaxu_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmaxu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vmaxu.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vmaxu_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmaxu_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmaxu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vmaxu.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vmaxu_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmaxu_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmaxu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vmaxu.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vmaxu_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmaxu_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmaxu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vmaxu.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vmaxu_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmaxu_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmaxu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vmaxu.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vmaxu_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmaxu_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmaxu_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vmaxu.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vmaxu_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmaxu_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmaxu_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vmaxu.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vmaxu_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmaxu_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmaxu_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vmaxu.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vmaxu_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmaxu_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmaxu_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vmaxu.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vmaxu_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmaxu_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmaxu_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vmaxu.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vmaxu_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmaxu_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmaxu_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vmaxu.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vmaxu_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmaxu_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmaxu_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vmaxu.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vmaxu_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmaxu_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmaxu_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vmaxu.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vmaxu_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmaxu_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmaxu_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vmaxu.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vmaxu_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmaxu_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmaxu_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vmaxu.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vmaxu_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmaxu_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmaxu_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vmaxu.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vmaxu_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmaxu_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmaxu_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vmaxu.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vmaxu_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmaxu_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmaxu_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vmaxu.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vmaxu_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmaxu_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmaxu_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vmaxu.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vmaxu_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmaxu_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmaxu_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vmaxu.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vmaxu_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmaxu_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmaxu_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vmaxu.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vmaxu_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmaxu_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmerge_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vmerge.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vmerge_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmerge_8xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmerge_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vmerge.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vmerge_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmerge_4xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmerge_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vmerge.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> poison, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vmerge_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmerge_2xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmerge_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vmerge.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> poison, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vmerge_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmerge_1xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmerge_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vmerge.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vmerge_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmerge_16xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmerge_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vmerge.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> poison, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vmerge_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmerge_8xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmerge_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vmerge.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> poison, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vmerge_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmerge_4xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmerge_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vmerge.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> poison, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vmerge_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmerge_2xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmerge_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vmerge.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vmerge_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmerge_32xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmerge_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vmerge.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> poison, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vmerge_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmerge_16xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmerge_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vmerge.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> poison, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vmerge_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmerge_8xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmerge_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vmerge.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> poison, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vmerge_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmerge_4xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmfeq_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmfeq.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmfeq_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfeq_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfeq_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmfeq.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmfeq_2xf32_mask(__epi_2xi1 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfeq_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfeq_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmfeq.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmfeq_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfeq_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfeq_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmfeq.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmfeq_1xf64_mask(__epi_1xi1 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfeq_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfeq_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmfeq.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmfeq_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfeq_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfeq_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmfeq.mask.nxv4f32.nxv4f32.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmfeq_4xf32_mask(__epi_4xi1 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfeq_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfeq_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmfeq.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmfeq_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfeq_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfeq_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmfeq.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmfeq_2xf64_mask(__epi_2xi1 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfeq_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfeq_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmfeq.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmfeq_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfeq_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfeq_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmfeq.mask.nxv8f32.nxv8f32.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmfeq_8xf32_mask(__epi_8xi1 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfeq_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfeq_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmfeq.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmfeq_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfeq_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfeq_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmfeq.mask.nxv4f64.nxv4f64.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmfeq_4xf64_mask(__epi_4xi1 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfeq_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfge_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmfge.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmfge_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfge_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfge_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmfge.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmfge_2xf32_mask(__epi_2xi1 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfge_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfge_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmfge.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmfge_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfge_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfge_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmfge.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmfge_1xf64_mask(__epi_1xi1 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfge_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfge_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmfge.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmfge_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfge_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfge_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmfge.mask.nxv4f32.nxv4f32.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmfge_4xf32_mask(__epi_4xi1 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfge_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfge_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmfge.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmfge_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfge_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfge_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmfge.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmfge_2xf64_mask(__epi_2xi1 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfge_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfge_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmfge.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmfge_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfge_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfge_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmfge.mask.nxv8f32.nxv8f32.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmfge_8xf32_mask(__epi_8xi1 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfge_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfge_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmfge.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmfge_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfge_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfge_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmfge.mask.nxv4f64.nxv4f64.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmfge_4xf64_mask(__epi_4xi1 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfge_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfgt_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmfgt.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmfgt_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfgt_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfgt_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmfgt.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmfgt_2xf32_mask(__epi_2xi1 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfgt_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfgt_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmfgt.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmfgt_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfgt_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfgt_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmfgt.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmfgt_1xf64_mask(__epi_1xi1 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfgt_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfgt_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmfgt.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmfgt_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfgt_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfgt_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmfgt.mask.nxv4f32.nxv4f32.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmfgt_4xf32_mask(__epi_4xi1 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfgt_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfgt_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmfgt.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmfgt_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfgt_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfgt_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmfgt.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmfgt_2xf64_mask(__epi_2xi1 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfgt_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfgt_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmfgt.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmfgt_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfgt_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfgt_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmfgt.mask.nxv8f32.nxv8f32.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmfgt_8xf32_mask(__epi_8xi1 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfgt_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfgt_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmfgt.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmfgt_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfgt_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfgt_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmfgt.mask.nxv4f64.nxv4f64.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmfgt_4xf64_mask(__epi_4xi1 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfgt_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfle_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmfle.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmfle_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfle_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfle_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmfle.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmfle_2xf32_mask(__epi_2xi1 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfle_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfle_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmfle.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmfle_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfle_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfle_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmfle.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmfle_1xf64_mask(__epi_1xi1 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfle_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfle_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmfle.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmfle_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfle_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfle_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmfle.mask.nxv4f32.nxv4f32.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmfle_4xf32_mask(__epi_4xi1 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfle_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfle_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmfle.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmfle_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfle_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfle_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmfle.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmfle_2xf64_mask(__epi_2xi1 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfle_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfle_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmfle.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmfle_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfle_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfle_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmfle.mask.nxv8f32.nxv8f32.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmfle_8xf32_mask(__epi_8xi1 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfle_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfle_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmfle.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmfle_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfle_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfle_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmfle.mask.nxv4f64.nxv4f64.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmfle_4xf64_mask(__epi_4xi1 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfle_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmflt_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmflt.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmflt_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmflt_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmflt_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmflt.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmflt_2xf32_mask(__epi_2xi1 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmflt_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmflt_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmflt.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmflt_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmflt_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmflt_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmflt.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmflt_1xf64_mask(__epi_1xi1 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmflt_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmflt_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmflt.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmflt_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmflt_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmflt_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmflt.mask.nxv4f32.nxv4f32.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmflt_4xf32_mask(__epi_4xi1 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmflt_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmflt_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmflt.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmflt_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmflt_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmflt_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmflt.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmflt_2xf64_mask(__epi_2xi1 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmflt_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmflt_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmflt.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmflt_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmflt_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmflt_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmflt.mask.nxv8f32.nxv8f32.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmflt_8xf32_mask(__epi_8xi1 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmflt_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmflt_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmflt.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmflt_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmflt_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmflt_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmflt.mask.nxv4f64.nxv4f64.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmflt_4xf64_mask(__epi_4xi1 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmflt_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfne_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmfne.nxv2f32.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmfne_2xf32(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfne_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfne_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmfne.mask.nxv2f32.nxv2f32.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmfne_2xf32_mask(__epi_2xi1 arg_0, __epi_2xf32 arg_1, __epi_2xf32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfne_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfne_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmfne.nxv1f64.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmfne_1xf64(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfne_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfne_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmfne.mask.nxv1f64.nxv1f64.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmfne_1xf64_mask(__epi_1xi1 arg_0, __epi_1xf64 arg_1, __epi_1xf64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfne_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfne_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmfne.nxv4f32.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmfne_4xf32(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfne_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfne_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmfne.mask.nxv4f32.nxv4f32.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmfne_4xf32_mask(__epi_4xi1 arg_0, __epi_4xf32 arg_1, __epi_4xf32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfne_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfne_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmfne.nxv2f64.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmfne_2xf64(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfne_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfne_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmfne.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmfne_2xf64_mask(__epi_2xi1 arg_0, __epi_2xf64 arg_1, __epi_2xf64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfne_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfne_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmfne.nxv8f32.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmfne_8xf32(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfne_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfne_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmfne.mask.nxv8f32.nxv8f32.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmfne_8xf32_mask(__epi_8xi1 arg_0, __epi_8xf32 arg_1, __epi_8xf32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfne_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmfne_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmfne.nxv4f64.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmfne_4xf64(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmfne_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmfne_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmfne.mask.nxv4f64.nxv4f64.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmfne_4xf64_mask(__epi_4xi1 arg_0, __epi_4xf64 arg_1, __epi_4xf64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmfne_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmin_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vmin.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vmin_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmin_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmin_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vmin.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vmin_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmin_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmin_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vmin.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vmin_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmin_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmin_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vmin.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vmin_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmin_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmin_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vmin.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vmin_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmin_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmin_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vmin.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vmin_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmin_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmin_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vmin.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vmin_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmin_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmin_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vmin.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vmin_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmin_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmin_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vmin.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vmin_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmin_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmin_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vmin.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vmin_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmin_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmin_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vmin.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vmin_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmin_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmin_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vmin.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vmin_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmin_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmin_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vmin.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vmin_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmin_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmin_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vmin.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vmin_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmin_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmin_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vmin.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vmin_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmin_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmin_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vmin.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vmin_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmin_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmin_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vmin.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vmin_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmin_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmin_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vmin.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vmin_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmin_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmin_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vmin.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vmin_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmin_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmin_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vmin.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vmin_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmin_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmin_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vmin.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vmin_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmin_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmin_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vmin.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vmin_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmin_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmin_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vmin.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vmin_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmin_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmin_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vmin.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vmin_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmin_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vminu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vminu.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vminu_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vminu_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vminu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vminu.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vminu_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vminu_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vminu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vminu.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vminu_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vminu_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vminu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vminu.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vminu_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vminu_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vminu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vminu.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vminu_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vminu_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vminu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vminu.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vminu_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vminu_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vminu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vminu.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vminu_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vminu_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vminu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vminu.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vminu_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vminu_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vminu_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vminu.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vminu_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vminu_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vminu_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vminu.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vminu_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vminu_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vminu_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vminu.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vminu_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vminu_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vminu_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vminu.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vminu_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vminu_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vminu_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vminu.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vminu_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vminu_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vminu_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vminu.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vminu_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vminu_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vminu_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vminu.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vminu_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vminu_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vminu_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vminu.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vminu_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vminu_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vminu_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vminu.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vminu_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vminu_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vminu_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vminu.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vminu_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vminu_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vminu_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vminu.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vminu_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vminu_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vminu_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vminu.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vminu_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vminu_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vminu_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vminu.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vminu_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vminu_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vminu_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vminu.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vminu_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vminu_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vminu_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vminu.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vminu_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vminu_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vminu_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vminu.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vminu_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vminu_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmnand_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmnand.nxv8i1.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmnand_8xi1(__epi_8xi1 arg_0, __epi_8xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmnand_8xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmnand_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmnand.nxv4i1.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmnand_4xi1(__epi_4xi1 arg_0, __epi_4xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmnand_4xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmnand_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmnand.nxv2i1.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmnand_2xi1(__epi_2xi1 arg_0, __epi_2xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmnand_2xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmnand_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmnand.nxv1i1.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmnand_1xi1(__epi_1xi1 arg_0, __epi_1xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmnand_1xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmnand_16xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmnand.nxv16i1.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmnand_16xi1(__epi_16xi1 arg_0, __epi_16xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmnand_16xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmnand_32xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmnand.nxv32i1.i64(<vscale x 32 x i1> [[ARG_0:%.*]], <vscale x 32 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmnand_32xi1(__epi_32xi1 arg_0, __epi_32xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmnand_32xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmnor_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmnor.nxv8i1.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmnor_8xi1(__epi_8xi1 arg_0, __epi_8xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmnor_8xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmnor_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmnor.nxv4i1.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmnor_4xi1(__epi_4xi1 arg_0, __epi_4xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmnor_4xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmnor_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmnor.nxv2i1.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmnor_2xi1(__epi_2xi1 arg_0, __epi_2xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmnor_2xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmnor_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmnor.nxv1i1.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmnor_1xi1(__epi_1xi1 arg_0, __epi_1xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmnor_1xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmnor_16xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmnor.nxv16i1.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmnor_16xi1(__epi_16xi1 arg_0, __epi_16xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmnor_16xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmnor_32xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmnor.nxv32i1.i64(<vscale x 32 x i1> [[ARG_0:%.*]], <vscale x 32 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmnor_32xi1(__epi_32xi1 arg_0, __epi_32xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmnor_32xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmor_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmor.nxv8i1.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmor_8xi1(__epi_8xi1 arg_0, __epi_8xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmor_8xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmor_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmor.nxv4i1.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmor_4xi1(__epi_4xi1 arg_0, __epi_4xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmor_4xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmor_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmor.nxv2i1.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmor_2xi1(__epi_2xi1 arg_0, __epi_2xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmor_2xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmor_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmor.nxv1i1.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmor_1xi1(__epi_1xi1 arg_0, __epi_1xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmor_1xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmor_16xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmor.nxv16i1.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmor_16xi1(__epi_16xi1 arg_0, __epi_16xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmor_16xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmor_32xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmor.nxv32i1.i64(<vscale x 32 x i1> [[ARG_0:%.*]], <vscale x 32 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmor_32xi1(__epi_32xi1 arg_0, __epi_32xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmor_32xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmornot_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmorn.nxv8i1.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmornot_8xi1(__epi_8xi1 arg_0, __epi_8xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmornot_8xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmornot_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmorn.nxv4i1.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmornot_4xi1(__epi_4xi1 arg_0, __epi_4xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmornot_4xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmornot_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmorn.nxv2i1.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmornot_2xi1(__epi_2xi1 arg_0, __epi_2xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmornot_2xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmornot_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmorn.nxv1i1.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmornot_1xi1(__epi_1xi1 arg_0, __epi_1xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmornot_1xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmornot_16xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmorn.nxv16i1.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmornot_16xi1(__epi_16xi1 arg_0, __epi_16xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmornot_16xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmornot_32xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmorn.nxv32i1.i64(<vscale x 32 x i1> [[ARG_0:%.*]], <vscale x 32 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmornot_32xi1(__epi_32xi1 arg_0, __epi_32xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmornot_32xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsbc_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsbc.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsbc_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsbc_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsbc_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsbc.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsbc_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsbc_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsbc_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsbc.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsbc_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsbc_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsbc_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmsbc.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmsbc_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsbc_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsbc_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsbc.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsbc_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsbc_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsbc_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsbc.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsbc_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsbc_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsbc_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsbc.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsbc_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsbc_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsbc_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsbc.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsbc_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsbc_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsbc_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmsbc.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmsbc_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsbc_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsbc_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsbc.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsbc_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsbc_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsbc_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsbc.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsbc_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsbc_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsbc_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsbc.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsbc_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsbc_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsbc_borrow_in_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsbc.borrow.in.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsbc_borrow_in_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmsbc_borrow_in_8xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmsbc_borrow_in_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsbc.borrow.in.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsbc_borrow_in_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmsbc_borrow_in_4xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmsbc_borrow_in_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsbc.borrow.in.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsbc_borrow_in_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmsbc_borrow_in_2xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmsbc_borrow_in_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmsbc.borrow.in.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmsbc_borrow_in_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmsbc_borrow_in_1xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmsbc_borrow_in_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsbc.borrow.in.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsbc_borrow_in_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmsbc_borrow_in_16xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmsbc_borrow_in_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsbc.borrow.in.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsbc_borrow_in_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmsbc_borrow_in_8xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmsbc_borrow_in_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsbc.borrow.in.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsbc_borrow_in_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmsbc_borrow_in_4xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmsbc_borrow_in_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsbc.borrow.in.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsbc_borrow_in_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmsbc_borrow_in_2xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmsbc_borrow_in_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmsbc.borrow.in.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmsbc_borrow_in_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmsbc_borrow_in_32xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmsbc_borrow_in_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsbc.borrow.in.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsbc_borrow_in_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmsbc_borrow_in_16xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmsbc_borrow_in_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsbc.borrow.in.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsbc_borrow_in_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmsbc_borrow_in_8xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmsbc_borrow_in_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsbc.borrow.in.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsbc_borrow_in_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vmsbc_borrow_in_4xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vmsbf_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsbf.nxv8i1.i64(<vscale x 8 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsbf_8xi1(__epi_8xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmsbf_8xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmsbf_8xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmset.nxv8i1.i64(i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsbf.mask.nxv8i1.i64(<vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i1> [[ARG_1:%.*]], i64 [[ARG_2]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP1]]
//
__epi_8xi1 test_vmsbf_8xi1_mask(__epi_8xi1 arg_0, __epi_8xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsbf_8xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsbf_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsbf.nxv4i1.i64(<vscale x 4 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsbf_4xi1(__epi_4xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmsbf_4xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmsbf_4xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmset.nxv4i1.i64(i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsbf.mask.nxv4i1.i64(<vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], i64 [[ARG_2]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP1]]
//
__epi_4xi1 test_vmsbf_4xi1_mask(__epi_4xi1 arg_0, __epi_4xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsbf_4xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsbf_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsbf.nxv2i1.i64(<vscale x 2 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsbf_2xi1(__epi_2xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmsbf_2xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmsbf_2xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmset.nxv2i1.i64(i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsbf.mask.nxv2i1.i64(<vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i1> [[ARG_1:%.*]], i64 [[ARG_2]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP1]]
//
__epi_2xi1 test_vmsbf_2xi1_mask(__epi_2xi1 arg_0, __epi_2xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsbf_2xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsbf_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmsbf.nxv1i1.i64(<vscale x 1 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmsbf_1xi1(__epi_1xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmsbf_1xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmsbf_1xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmset.nxv1i1.i64(i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmsbf.mask.nxv1i1.i64(<vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x i1> [[ARG_1:%.*]], i64 [[ARG_2]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP1]]
//
__epi_1xi1 test_vmsbf_1xi1_mask(__epi_1xi1 arg_0, __epi_1xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsbf_1xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsbf_16xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsbf.nxv16i1.i64(<vscale x 16 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsbf_16xi1(__epi_16xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmsbf_16xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmsbf_16xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmset.nxv16i1.i64(i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsbf.mask.nxv16i1.i64(<vscale x 16 x i1> [[TMP0]], <vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i1> [[ARG_1:%.*]], i64 [[ARG_2]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP1]]
//
__epi_16xi1 test_vmsbf_16xi1_mask(__epi_16xi1 arg_0, __epi_16xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsbf_16xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsbf_32xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmsbf.nxv32i1.i64(<vscale x 32 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmsbf_32xi1(__epi_32xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmsbf_32xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmsbf_32xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmset.nxv32i1.i64(i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmsbf.mask.nxv32i1.i64(<vscale x 32 x i1> [[TMP0]], <vscale x 32 x i1> [[ARG_0:%.*]], <vscale x 32 x i1> [[ARG_1:%.*]], i64 [[ARG_2]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP1]]
//
__epi_32xi1 test_vmsbf_32xi1_mask(__epi_32xi1 arg_0, __epi_32xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsbf_32xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmseq_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmseq_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmseq_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmseq_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmseq_8xi8_mask(__epi_8xi1 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmseq_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmseq_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmseq_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmseq_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmseq_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmseq_4xi16_mask(__epi_4xi1 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmseq_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmseq_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmseq_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmseq_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmseq_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmseq_2xi32_mask(__epi_2xi1 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmseq_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmseq_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmseq_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmseq_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmseq_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmseq.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmseq_1xi64_mask(__epi_1xi1 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmseq_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmseq_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmseq_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmseq_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmseq_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmseq_16xi8_mask(__epi_16xi1 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmseq_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmseq_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmseq_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmseq_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmseq_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmseq_8xi16_mask(__epi_8xi1 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmseq_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmseq_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmseq_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmseq_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmseq_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmseq_4xi32_mask(__epi_4xi1 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmseq_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmseq_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmseq_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmseq_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmseq_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmseq.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmseq_2xi64_mask(__epi_2xi1 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmseq_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmseq_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmseq_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmseq_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmseq_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmseq.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i1> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmseq_32xi8_mask(__epi_32xi1 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmseq_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmseq_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmseq_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmseq_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmseq_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmseq.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmseq_16xi16_mask(__epi_16xi1 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmseq_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmseq_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmseq_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmseq_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmseq_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmseq.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmseq_8xi32_mask(__epi_8xi1 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmseq_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmseq_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmseq_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmseq_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmseq_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmseq.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmseq_4xi64_mask(__epi_4xi1 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmseq_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgt_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsgt.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsgt_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgt_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgt_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsgt.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsgt_8xi8_mask(__epi_8xi1 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgt_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgt_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsgt.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsgt_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgt_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgt_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsgt.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsgt_4xi16_mask(__epi_4xi1 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgt_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgt_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsgt.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsgt_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgt_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgt_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsgt.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsgt_2xi32_mask(__epi_2xi1 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgt_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgt_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmsgt.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmsgt_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgt_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgt_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmsgt.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmsgt_1xi64_mask(__epi_1xi1 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgt_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgt_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsgt.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsgt_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgt_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgt_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsgt.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsgt_16xi8_mask(__epi_16xi1 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgt_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgt_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsgt.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsgt_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgt_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgt_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsgt.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsgt_8xi16_mask(__epi_8xi1 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgt_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgt_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsgt.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsgt_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgt_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgt_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsgt.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsgt_4xi32_mask(__epi_4xi1 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgt_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgt_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsgt.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsgt_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgt_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgt_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsgt.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsgt_2xi64_mask(__epi_2xi1 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgt_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgt_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmsgt.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmsgt_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgt_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgt_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmsgt.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i1> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmsgt_32xi8_mask(__epi_32xi1 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgt_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgt_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsgt.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsgt_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgt_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgt_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsgt.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsgt_16xi16_mask(__epi_16xi1 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgt_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgt_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsgt.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsgt_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgt_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgt_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsgt.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsgt_8xi32_mask(__epi_8xi1 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgt_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgt_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsgt.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsgt_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgt_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgt_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsgt.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsgt_4xi64_mask(__epi_4xi1 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgt_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgtu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsgtu.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsgtu_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgtu_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgtu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsgtu.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsgtu_8xi8_mask(__epi_8xi1 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgtu_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgtu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsgtu.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsgtu_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgtu_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgtu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsgtu.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsgtu_4xi16_mask(__epi_4xi1 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgtu_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgtu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsgtu.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsgtu_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgtu_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgtu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsgtu.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsgtu_2xi32_mask(__epi_2xi1 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgtu_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgtu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmsgtu.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmsgtu_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgtu_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgtu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmsgtu.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmsgtu_1xi64_mask(__epi_1xi1 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgtu_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgtu_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsgtu.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsgtu_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgtu_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgtu_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsgtu.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsgtu_16xi8_mask(__epi_16xi1 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgtu_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgtu_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsgtu.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsgtu_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgtu_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgtu_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsgtu.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsgtu_8xi16_mask(__epi_8xi1 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgtu_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgtu_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsgtu.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsgtu_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgtu_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgtu_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsgtu.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsgtu_4xi32_mask(__epi_4xi1 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgtu_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgtu_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsgtu.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsgtu_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgtu_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgtu_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsgtu.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsgtu_2xi64_mask(__epi_2xi1 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgtu_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgtu_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmsgtu.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmsgtu_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgtu_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgtu_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmsgtu.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i1> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmsgtu_32xi8_mask(__epi_32xi1 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgtu_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgtu_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsgtu.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsgtu_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgtu_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgtu_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsgtu.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsgtu_16xi16_mask(__epi_16xi1 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgtu_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgtu_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsgtu.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsgtu_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgtu_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgtu_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsgtu.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsgtu_8xi32_mask(__epi_8xi1 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgtu_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsgtu_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsgtu.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsgtu_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsgtu_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsgtu_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsgtu.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsgtu_4xi64_mask(__epi_4xi1 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsgtu_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsif_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsif.nxv8i1.i64(<vscale x 8 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsif_8xi1(__epi_8xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmsif_8xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmsif_8xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmset.nxv8i1.i64(i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsif.mask.nxv8i1.i64(<vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i1> [[ARG_1:%.*]], i64 [[ARG_2]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP1]]
//
__epi_8xi1 test_vmsif_8xi1_mask(__epi_8xi1 arg_0, __epi_8xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsif_8xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsif_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsif.nxv4i1.i64(<vscale x 4 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsif_4xi1(__epi_4xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmsif_4xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmsif_4xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmset.nxv4i1.i64(i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsif.mask.nxv4i1.i64(<vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], i64 [[ARG_2]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP1]]
//
__epi_4xi1 test_vmsif_4xi1_mask(__epi_4xi1 arg_0, __epi_4xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsif_4xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsif_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsif.nxv2i1.i64(<vscale x 2 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsif_2xi1(__epi_2xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmsif_2xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmsif_2xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmset.nxv2i1.i64(i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsif.mask.nxv2i1.i64(<vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i1> [[ARG_1:%.*]], i64 [[ARG_2]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP1]]
//
__epi_2xi1 test_vmsif_2xi1_mask(__epi_2xi1 arg_0, __epi_2xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsif_2xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsif_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmsif.nxv1i1.i64(<vscale x 1 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmsif_1xi1(__epi_1xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmsif_1xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmsif_1xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmset.nxv1i1.i64(i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmsif.mask.nxv1i1.i64(<vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x i1> [[ARG_1:%.*]], i64 [[ARG_2]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP1]]
//
__epi_1xi1 test_vmsif_1xi1_mask(__epi_1xi1 arg_0, __epi_1xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsif_1xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsif_16xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsif.nxv16i1.i64(<vscale x 16 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsif_16xi1(__epi_16xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmsif_16xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmsif_16xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmset.nxv16i1.i64(i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsif.mask.nxv16i1.i64(<vscale x 16 x i1> [[TMP0]], <vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i1> [[ARG_1:%.*]], i64 [[ARG_2]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP1]]
//
__epi_16xi1 test_vmsif_16xi1_mask(__epi_16xi1 arg_0, __epi_16xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsif_16xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsif_32xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmsif.nxv32i1.i64(<vscale x 32 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmsif_32xi1(__epi_32xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmsif_32xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmsif_32xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmset.nxv32i1.i64(i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmsif.mask.nxv32i1.i64(<vscale x 32 x i1> [[TMP0]], <vscale x 32 x i1> [[ARG_0:%.*]], <vscale x 32 x i1> [[ARG_1:%.*]], i64 [[ARG_2]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP1]]
//
__epi_32xi1 test_vmsif_32xi1_mask(__epi_32xi1 arg_0, __epi_32xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsif_32xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsle_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsle.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsle_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsle_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsle_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsle.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsle_8xi8_mask(__epi_8xi1 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsle_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsle_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsle.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsle_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsle_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsle_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsle.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsle_4xi16_mask(__epi_4xi1 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsle_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsle_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsle.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsle_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsle_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsle_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsle.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsle_2xi32_mask(__epi_2xi1 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsle_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsle_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmsle.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmsle_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsle_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsle_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmsle.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmsle_1xi64_mask(__epi_1xi1 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsle_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsle_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsle.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsle_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsle_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsle_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsle.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsle_16xi8_mask(__epi_16xi1 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsle_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsle_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsle.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsle_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsle_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsle_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsle.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsle_8xi16_mask(__epi_8xi1 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsle_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsle_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsle.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsle_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsle_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsle_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsle.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsle_4xi32_mask(__epi_4xi1 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsle_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsle_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsle.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsle_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsle_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsle_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsle.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsle_2xi64_mask(__epi_2xi1 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsle_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsle_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmsle.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmsle_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsle_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsle_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmsle.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i1> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmsle_32xi8_mask(__epi_32xi1 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsle_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsle_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsle.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsle_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsle_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsle_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsle.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsle_16xi16_mask(__epi_16xi1 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsle_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsle_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsle.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsle_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsle_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsle_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsle.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsle_8xi32_mask(__epi_8xi1 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsle_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsle_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsle.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsle_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsle_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsle_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsle.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsle_4xi64_mask(__epi_4xi1 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsle_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsleu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsleu.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsleu_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsleu_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsleu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsleu.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsleu_8xi8_mask(__epi_8xi1 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsleu_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsleu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsleu.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsleu_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsleu_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsleu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsleu.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsleu_4xi16_mask(__epi_4xi1 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsleu_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsleu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsleu.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsleu_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsleu_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsleu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsleu.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsleu_2xi32_mask(__epi_2xi1 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsleu_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsleu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmsleu.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmsleu_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsleu_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsleu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmsleu.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmsleu_1xi64_mask(__epi_1xi1 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsleu_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsleu_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsleu.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsleu_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsleu_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsleu_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsleu.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsleu_16xi8_mask(__epi_16xi1 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsleu_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsleu_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsleu.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsleu_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsleu_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsleu_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsleu.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsleu_8xi16_mask(__epi_8xi1 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsleu_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsleu_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsleu.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsleu_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsleu_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsleu_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsleu.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsleu_4xi32_mask(__epi_4xi1 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsleu_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsleu_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsleu.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsleu_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsleu_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsleu_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsleu.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsleu_2xi64_mask(__epi_2xi1 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsleu_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsleu_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmsleu.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmsleu_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsleu_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsleu_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmsleu.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i1> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmsleu_32xi8_mask(__epi_32xi1 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsleu_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsleu_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsleu.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsleu_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsleu_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsleu_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsleu.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsleu_16xi16_mask(__epi_16xi1 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsleu_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsleu_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsleu.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsleu_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsleu_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsleu_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsleu.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsleu_8xi32_mask(__epi_8xi1 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsleu_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsleu_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsleu.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsleu_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsleu_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsleu_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsleu.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsleu_4xi64_mask(__epi_4xi1 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsleu_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmslt_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmslt.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmslt_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmslt_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmslt_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmslt.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmslt_8xi8_mask(__epi_8xi1 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmslt_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmslt_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmslt.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmslt_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmslt_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmslt_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmslt.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmslt_4xi16_mask(__epi_4xi1 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmslt_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmslt_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmslt.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmslt_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmslt_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmslt_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmslt.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmslt_2xi32_mask(__epi_2xi1 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmslt_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmslt_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmslt.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmslt_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmslt_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmslt_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmslt.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmslt_1xi64_mask(__epi_1xi1 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmslt_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmslt_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmslt.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmslt_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmslt_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmslt_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmslt.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmslt_16xi8_mask(__epi_16xi1 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmslt_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmslt_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmslt.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmslt_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmslt_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmslt_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmslt.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmslt_8xi16_mask(__epi_8xi1 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmslt_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmslt_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmslt.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmslt_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmslt_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmslt_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmslt.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmslt_4xi32_mask(__epi_4xi1 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmslt_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmslt_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmslt.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmslt_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmslt_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmslt_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmslt.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmslt_2xi64_mask(__epi_2xi1 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmslt_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmslt_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmslt.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmslt_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmslt_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmslt_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmslt.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i1> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmslt_32xi8_mask(__epi_32xi1 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmslt_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmslt_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmslt.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmslt_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmslt_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmslt_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmslt.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmslt_16xi16_mask(__epi_16xi1 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmslt_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmslt_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmslt.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmslt_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmslt_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmslt_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmslt.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmslt_8xi32_mask(__epi_8xi1 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmslt_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmslt_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmslt.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmslt_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmslt_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmslt_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmslt.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmslt_4xi64_mask(__epi_4xi1 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmslt_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsltu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsltu.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsltu_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsltu_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsltu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsltu.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsltu_8xi8_mask(__epi_8xi1 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsltu_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsltu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsltu.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsltu_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsltu_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsltu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsltu.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsltu_4xi16_mask(__epi_4xi1 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsltu_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsltu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsltu.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsltu_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsltu_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsltu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsltu.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsltu_2xi32_mask(__epi_2xi1 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsltu_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsltu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmsltu.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmsltu_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsltu_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsltu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmsltu.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmsltu_1xi64_mask(__epi_1xi1 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsltu_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsltu_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsltu.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsltu_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsltu_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsltu_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsltu.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsltu_16xi8_mask(__epi_16xi1 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsltu_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsltu_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsltu.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsltu_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsltu_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsltu_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsltu.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsltu_8xi16_mask(__epi_8xi1 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsltu_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsltu_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsltu.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsltu_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsltu_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsltu_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsltu.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsltu_4xi32_mask(__epi_4xi1 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsltu_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsltu_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsltu.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsltu_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsltu_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsltu_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsltu.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsltu_2xi64_mask(__epi_2xi1 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsltu_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsltu_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmsltu.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmsltu_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsltu_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsltu_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmsltu.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i1> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmsltu_32xi8_mask(__epi_32xi1 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsltu_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsltu_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsltu.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsltu_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsltu_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsltu_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsltu.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsltu_16xi16_mask(__epi_16xi1 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsltu_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsltu_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsltu.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsltu_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsltu_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsltu_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsltu.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsltu_8xi32_mask(__epi_8xi1 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsltu_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsltu_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsltu.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsltu_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsltu_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsltu_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsltu.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsltu_4xi64_mask(__epi_4xi1 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsltu_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsne_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsne.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsne_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsne_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsne_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsne.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsne_8xi8_mask(__epi_8xi1 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsne_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsne_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsne.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsne_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsne_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsne_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsne.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsne_4xi16_mask(__epi_4xi1 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsne_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsne_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsne.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsne_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsne_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsne_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsne.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsne_2xi32_mask(__epi_2xi1 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsne_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsne_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmsne.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmsne_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsne_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsne_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmsne.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmsne_1xi64_mask(__epi_1xi1 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsne_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsne_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsne.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsne_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsne_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsne_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsne.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsne_16xi8_mask(__epi_16xi1 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsne_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsne_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsne.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsne_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsne_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsne_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsne.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsne_8xi16_mask(__epi_8xi1 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsne_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsne_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsne.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsne_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsne_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsne_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsne.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsne_4xi32_mask(__epi_4xi1 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsne_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsne_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsne.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsne_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsne_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsne_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsne.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsne_2xi64_mask(__epi_2xi1 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsne_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsne_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmsne.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmsne_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsne_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsne_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmsne.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i1> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmsne_32xi8_mask(__epi_32xi1 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsne_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsne_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsne.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsne_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsne_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsne_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsne.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsne_16xi16_mask(__epi_16xi1 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsne_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsne_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsne.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsne_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsne_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsne_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsne.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsne_8xi32_mask(__epi_8xi1 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsne_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsne_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsne.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsne_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsne_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsne_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsne.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsne_4xi64_mask(__epi_4xi1 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmsne_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmsof_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsof.nxv8i1.i64(<vscale x 8 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmsof_8xi1(__epi_8xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmsof_8xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmsof_8xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmset.nxv8i1.i64(i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmsof.mask.nxv8i1.i64(<vscale x 8 x i1> [[TMP0]], <vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i1> [[ARG_1:%.*]], i64 [[ARG_2]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP1]]
//
__epi_8xi1 test_vmsof_8xi1_mask(__epi_8xi1 arg_0, __epi_8xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsof_8xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsof_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsof.nxv4i1.i64(<vscale x 4 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmsof_4xi1(__epi_4xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmsof_4xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmsof_4xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmset.nxv4i1.i64(i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmsof.mask.nxv4i1.i64(<vscale x 4 x i1> [[TMP0]], <vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], i64 [[ARG_2]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP1]]
//
__epi_4xi1 test_vmsof_4xi1_mask(__epi_4xi1 arg_0, __epi_4xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsof_4xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsof_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsof.nxv2i1.i64(<vscale x 2 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmsof_2xi1(__epi_2xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmsof_2xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmsof_2xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmset.nxv2i1.i64(i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmsof.mask.nxv2i1.i64(<vscale x 2 x i1> [[TMP0]], <vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i1> [[ARG_1:%.*]], i64 [[ARG_2]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP1]]
//
__epi_2xi1 test_vmsof_2xi1_mask(__epi_2xi1 arg_0, __epi_2xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsof_2xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsof_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmsof.nxv1i1.i64(<vscale x 1 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmsof_1xi1(__epi_1xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmsof_1xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmsof_1xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmset.nxv1i1.i64(i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmsof.mask.nxv1i1.i64(<vscale x 1 x i1> [[TMP0]], <vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x i1> [[ARG_1:%.*]], i64 [[ARG_2]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP1]]
//
__epi_1xi1 test_vmsof_1xi1_mask(__epi_1xi1 arg_0, __epi_1xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsof_1xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsof_16xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsof.nxv16i1.i64(<vscale x 16 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmsof_16xi1(__epi_16xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmsof_16xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmsof_16xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmset.nxv16i1.i64(i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmsof.mask.nxv16i1.i64(<vscale x 16 x i1> [[TMP0]], <vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i1> [[ARG_1:%.*]], i64 [[ARG_2]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP1]]
//
__epi_16xi1 test_vmsof_16xi1_mask(__epi_16xi1 arg_0, __epi_16xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsof_16xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmsof_32xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmsof.nxv32i1.i64(<vscale x 32 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmsof_32xi1(__epi_32xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmsof_32xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmsof_32xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmset.nxv32i1.i64(i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmsof.mask.nxv32i1.i64(<vscale x 32 x i1> [[TMP0]], <vscale x 32 x i1> [[ARG_0:%.*]], <vscale x 32 x i1> [[ARG_1:%.*]], i64 [[ARG_2]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP1]]
//
__epi_32xi1 test_vmsof_32xi1_mask(__epi_32xi1 arg_0, __epi_32xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmsof_32xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmul_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vmul.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vmul_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmul_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmul_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vmul.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vmul_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmul_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmul_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vmul.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vmul_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmul_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmul_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vmul.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vmul_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmul_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmul_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vmul.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vmul_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmul_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmul_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vmul.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vmul_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmul_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmul_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vmul.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vmul_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmul_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmul_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vmul.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vmul_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmul_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmul_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vmul.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vmul_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmul_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmul_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vmul.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vmul_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmul_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmul_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vmul.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vmul_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmul_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmul_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vmul.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vmul_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmul_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmul_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vmul.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vmul_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmul_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmul_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vmul.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vmul_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmul_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmul_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vmul.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vmul_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmul_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmul_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vmul.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vmul_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmul_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmul_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vmul.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vmul_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmul_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmul_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vmul.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vmul_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmul_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmul_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vmul.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vmul_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmul_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmul_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vmul.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vmul_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmul_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmul_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vmul.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vmul_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmul_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmul_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vmul.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vmul_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmul_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmul_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vmul.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vmul_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmul_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmul_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vmul.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vmul_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmul_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulh_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vmulh.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vmulh_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulh_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulh_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vmulh.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vmulh_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulh_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulh_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vmulh.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vmulh_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulh_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulh_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vmulh.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vmulh_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulh_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulh_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vmulh.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vmulh_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulh_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulh_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vmulh.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vmulh_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulh_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulh_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vmulh.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vmulh_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulh_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulh_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vmulh.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vmulh_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulh_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulh_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vmulh.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vmulh_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulh_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulh_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vmulh.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vmulh_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulh_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulh_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vmulh.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vmulh_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulh_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulh_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vmulh.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vmulh_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulh_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulh_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vmulh.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vmulh_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulh_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulh_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vmulh.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vmulh_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulh_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulh_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vmulh.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vmulh_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulh_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulh_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vmulh.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vmulh_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulh_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulh_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vmulh.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vmulh_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulh_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulh_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vmulh.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vmulh_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulh_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulh_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vmulh.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vmulh_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulh_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulh_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vmulh.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vmulh_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulh_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulh_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vmulh.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vmulh_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulh_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulh_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vmulh.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vmulh_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulh_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulh_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vmulh.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vmulh_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulh_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulh_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vmulh.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vmulh_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulh_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhsu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vmulhsu.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vmulhsu_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhsu_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhsu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vmulhsu.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vmulhsu_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhsu_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhsu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vmulhsu.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vmulhsu_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhsu_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhsu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vmulhsu.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vmulhsu_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhsu_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhsu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vmulhsu.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vmulhsu_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhsu_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhsu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vmulhsu.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vmulhsu_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhsu_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhsu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vmulhsu.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vmulhsu_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhsu_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhsu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vmulhsu.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vmulhsu_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhsu_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhsu_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vmulhsu.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vmulhsu_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhsu_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhsu_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vmulhsu.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vmulhsu_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhsu_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhsu_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vmulhsu.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vmulhsu_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhsu_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhsu_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vmulhsu.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vmulhsu_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhsu_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhsu_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vmulhsu.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vmulhsu_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhsu_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhsu_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vmulhsu.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vmulhsu_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhsu_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhsu_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vmulhsu.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vmulhsu_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhsu_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhsu_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vmulhsu.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vmulhsu_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhsu_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhsu_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vmulhsu.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vmulhsu_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhsu_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhsu_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vmulhsu.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vmulhsu_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhsu_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhsu_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vmulhsu.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vmulhsu_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhsu_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhsu_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vmulhsu.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vmulhsu_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhsu_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhsu_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vmulhsu.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vmulhsu_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhsu_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhsu_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vmulhsu.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vmulhsu_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhsu_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhsu_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vmulhsu.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vmulhsu_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhsu_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhsu_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vmulhsu.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vmulhsu_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhsu_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vmulhu.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vmulhu_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhu_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vmulhu.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vmulhu_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhu_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vmulhu.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vmulhu_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhu_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vmulhu.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vmulhu_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhu_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vmulhu.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vmulhu_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhu_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vmulhu.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vmulhu_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhu_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vmulhu.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vmulhu_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhu_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vmulhu.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vmulhu_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhu_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhu_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vmulhu.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vmulhu_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhu_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhu_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vmulhu.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vmulhu_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhu_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhu_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vmulhu.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vmulhu_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhu_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhu_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vmulhu.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vmulhu_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhu_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhu_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vmulhu.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vmulhu_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhu_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhu_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vmulhu.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vmulhu_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhu_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhu_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vmulhu.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vmulhu_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhu_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhu_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vmulhu.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vmulhu_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhu_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhu_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vmulhu.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vmulhu_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhu_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhu_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vmulhu.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vmulhu_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhu_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhu_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vmulhu.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vmulhu_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhu_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhu_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vmulhu.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vmulhu_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhu_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhu_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vmulhu.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vmulhu_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhu_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhu_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vmulhu.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vmulhu_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhu_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmulhu_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vmulhu.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vmulhu_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmulhu_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmulhu_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vmulhu.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vmulhu_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vmulhu_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vmv_s_x_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vmv.s.x.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], i8 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vmv_s_x_8xi8(__epi_8xi8 arg_0, signed char arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmv_s_x_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmv_s_x_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vmv.s.x.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], i16 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vmv_s_x_4xi16(__epi_4xi16 arg_0, signed short int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmv_s_x_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmv_s_x_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vmv.s.x.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], i32 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vmv_s_x_2xi32(__epi_2xi32 arg_0, signed int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmv_s_x_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmv_s_x_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vmv.s.x.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vmv_s_x_1xi64(__epi_1xi64 arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmv_s_x_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmv_s_x_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vmv.s.x.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], i8 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vmv_s_x_16xi8(__epi_16xi8 arg_0, signed char arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmv_s_x_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmv_s_x_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vmv.s.x.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], i16 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vmv_s_x_8xi16(__epi_8xi16 arg_0, signed short int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmv_s_x_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmv_s_x_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vmv.s.x.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], i32 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vmv_s_x_4xi32(__epi_4xi32 arg_0, signed int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmv_s_x_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmv_s_x_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vmv.s.x.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vmv_s_x_2xi64(__epi_2xi64 arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmv_s_x_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmv_s_x_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vmv.s.x.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], i8 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vmv_s_x_32xi8(__epi_32xi8 arg_0, signed char arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmv_s_x_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmv_s_x_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vmv.s.x.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], i16 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vmv_s_x_16xi16(__epi_16xi16 arg_0, signed short int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmv_s_x_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmv_s_x_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vmv.s.x.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], i32 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vmv_s_x_8xi32(__epi_8xi32 arg_0, signed int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmv_s_x_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmv_s_x_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vmv.s.x.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vmv_s_x_4xi64(__epi_4xi64 arg_0, signed long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmv_s_x_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmv_v_x_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vmv.v.x.nxv8i8.i64(<vscale x 8 x i8> undef, i8 [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vmv_v_x_8xi8(signed char arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmv_v_x_8xi8(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmv_v_x_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vmv.v.x.nxv4i16.i64(<vscale x 4 x i16> undef, i16 [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vmv_v_x_4xi16(signed short int arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmv_v_x_4xi16(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmv_v_x_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vmv.v.x.nxv2i32.i64(<vscale x 2 x i32> undef, i32 [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vmv_v_x_2xi32(signed int arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmv_v_x_2xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmv_v_x_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vmv.v.x.nxv1i64.i64(<vscale x 1 x i64> undef, i64 [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vmv_v_x_1xi64(signed long int arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmv_v_x_1xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmv_v_x_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vmv.v.x.nxv16i8.i64(<vscale x 16 x i8> undef, i8 [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vmv_v_x_16xi8(signed char arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmv_v_x_16xi8(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmv_v_x_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vmv.v.x.nxv8i16.i64(<vscale x 8 x i16> undef, i16 [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vmv_v_x_8xi16(signed short int arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmv_v_x_8xi16(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmv_v_x_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vmv.v.x.nxv4i32.i64(<vscale x 4 x i32> undef, i32 [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vmv_v_x_4xi32(signed int arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmv_v_x_4xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmv_v_x_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vmv.v.x.nxv2i64.i64(<vscale x 2 x i64> undef, i64 [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vmv_v_x_2xi64(signed long int arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmv_v_x_2xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmv_v_x_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vmv.v.x.nxv32i8.i64(<vscale x 32 x i8> undef, i8 [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vmv_v_x_32xi8(signed char arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmv_v_x_32xi8(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmv_v_x_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vmv.v.x.nxv16i16.i64(<vscale x 16 x i16> undef, i16 [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vmv_v_x_16xi16(signed short int arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmv_v_x_16xi16(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmv_v_x_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vmv.v.x.nxv8i32.i64(<vscale x 8 x i32> undef, i32 [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vmv_v_x_8xi32(signed int arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmv_v_x_8xi32(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmv_v_x_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vmv.v.x.nxv4i64.i64(<vscale x 4 x i64> undef, i64 [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vmv_v_x_4xi64(signed long int arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vmv_v_x_4xi64(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vmv_x_s_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i8 @llvm.riscv.vmv.x.s.nxv8i8(<vscale x 8 x i8> [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret i8 [[TMP0]]
//
signed char test_vmv_x_s_8xi8(__epi_8xi8 arg_0)
{
    return __builtin_epi_vmv_x_s_8xi8(arg_0);
}

// CHECK-O2-LABEL: @test_vmv_x_s_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i16 @llvm.riscv.vmv.x.s.nxv4i16(<vscale x 4 x i16> [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret i16 [[TMP0]]
//
signed short int test_vmv_x_s_4xi16(__epi_4xi16 arg_0)
{
    return __builtin_epi_vmv_x_s_4xi16(arg_0);
}

// CHECK-O2-LABEL: @test_vmv_x_s_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.riscv.vmv.x.s.nxv2i32(<vscale x 2 x i32> [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret i32 [[TMP0]]
//
signed int test_vmv_x_s_2xi32(__epi_2xi32 arg_0)
{
    return __builtin_epi_vmv_x_s_2xi32(arg_0);
}

// CHECK-O2-LABEL: @test_vmv_x_s_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vmv.x.s.nxv1i64(<vscale x 1 x i64> [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vmv_x_s_1xi64(__epi_1xi64 arg_0)
{
    return __builtin_epi_vmv_x_s_1xi64(arg_0);
}

// CHECK-O2-LABEL: @test_vmv_x_s_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i8 @llvm.riscv.vmv.x.s.nxv16i8(<vscale x 16 x i8> [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret i8 [[TMP0]]
//
signed char test_vmv_x_s_16xi8(__epi_16xi8 arg_0)
{
    return __builtin_epi_vmv_x_s_16xi8(arg_0);
}

// CHECK-O2-LABEL: @test_vmv_x_s_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i16 @llvm.riscv.vmv.x.s.nxv8i16(<vscale x 8 x i16> [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret i16 [[TMP0]]
//
signed short int test_vmv_x_s_8xi16(__epi_8xi16 arg_0)
{
    return __builtin_epi_vmv_x_s_8xi16(arg_0);
}

// CHECK-O2-LABEL: @test_vmv_x_s_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.riscv.vmv.x.s.nxv4i32(<vscale x 4 x i32> [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret i32 [[TMP0]]
//
signed int test_vmv_x_s_4xi32(__epi_4xi32 arg_0)
{
    return __builtin_epi_vmv_x_s_4xi32(arg_0);
}

// CHECK-O2-LABEL: @test_vmv_x_s_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vmv.x.s.nxv2i64(<vscale x 2 x i64> [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vmv_x_s_2xi64(__epi_2xi64 arg_0)
{
    return __builtin_epi_vmv_x_s_2xi64(arg_0);
}

// CHECK-O2-LABEL: @test_vmv_x_s_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i8 @llvm.riscv.vmv.x.s.nxv32i8(<vscale x 32 x i8> [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret i8 [[TMP0]]
//
signed char test_vmv_x_s_32xi8(__epi_32xi8 arg_0)
{
    return __builtin_epi_vmv_x_s_32xi8(arg_0);
}

// CHECK-O2-LABEL: @test_vmv_x_s_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i16 @llvm.riscv.vmv.x.s.nxv16i16(<vscale x 16 x i16> [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret i16 [[TMP0]]
//
signed short int test_vmv_x_s_16xi16(__epi_16xi16 arg_0)
{
    return __builtin_epi_vmv_x_s_16xi16(arg_0);
}

// CHECK-O2-LABEL: @test_vmv_x_s_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.riscv.vmv.x.s.nxv8i32(<vscale x 8 x i32> [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret i32 [[TMP0]]
//
signed int test_vmv_x_s_8xi32(__epi_8xi32 arg_0)
{
    return __builtin_epi_vmv_x_s_8xi32(arg_0);
}

// CHECK-O2-LABEL: @test_vmv_x_s_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vmv.x.s.nxv4i64(<vscale x 4 x i64> [[ARG_0:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vmv_x_s_4xi64(__epi_4xi64 arg_0)
{
    return __builtin_epi_vmv_x_s_4xi64(arg_0);
}

// CHECK-O2-LABEL: @test_vmxnor_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmxnor.nxv8i1.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmxnor_8xi1(__epi_8xi1 arg_0, __epi_8xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmxnor_8xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmxnor_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmxnor.nxv4i1.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmxnor_4xi1(__epi_4xi1 arg_0, __epi_4xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmxnor_4xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmxnor_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmxnor.nxv2i1.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmxnor_2xi1(__epi_2xi1 arg_0, __epi_2xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmxnor_2xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmxnor_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmxnor.nxv1i1.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmxnor_1xi1(__epi_1xi1 arg_0, __epi_1xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmxnor_1xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmxnor_16xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmxnor.nxv16i1.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmxnor_16xi1(__epi_16xi1 arg_0, __epi_16xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmxnor_16xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmxnor_32xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmxnor.nxv32i1.i64(<vscale x 32 x i1> [[ARG_0:%.*]], <vscale x 32 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmxnor_32xi1(__epi_32xi1 arg_0, __epi_32xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmxnor_32xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmxor_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i1> @llvm.riscv.vmxor.nxv8i1.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i1> [[TMP0]]
//
__epi_8xi1 test_vmxor_8xi1(__epi_8xi1 arg_0, __epi_8xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmxor_8xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmxor_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i1> @llvm.riscv.vmxor.nxv4i1.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i1> [[TMP0]]
//
__epi_4xi1 test_vmxor_4xi1(__epi_4xi1 arg_0, __epi_4xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmxor_4xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmxor_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i1> @llvm.riscv.vmxor.nxv2i1.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i1> [[TMP0]]
//
__epi_2xi1 test_vmxor_2xi1(__epi_2xi1 arg_0, __epi_2xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmxor_2xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmxor_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i1> @llvm.riscv.vmxor.nxv1i1.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i1> [[TMP0]]
//
__epi_1xi1 test_vmxor_1xi1(__epi_1xi1 arg_0, __epi_1xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmxor_1xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmxor_16xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i1> @llvm.riscv.vmxor.nxv16i1.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
//
__epi_16xi1 test_vmxor_16xi1(__epi_16xi1 arg_0, __epi_16xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmxor_16xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vmxor_32xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i1> @llvm.riscv.vmxor.nxv32i1.i64(<vscale x 32 x i1> [[ARG_0:%.*]], <vscale x 32 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i1> [[TMP0]]
//
__epi_32xi1 test_vmxor_32xi1(__epi_32xi1 arg_0, __epi_32xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vmxor_32xi1(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vnsra_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vnsra.nxv8i8.nxv8i16.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vnsra_8xi8(__epi_8xi16 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vnsra_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vnsra_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vnsra.mask.nxv8i8.nxv8i16.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vnsra_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi16 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vnsra_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vnsra_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vnsra.nxv4i16.nxv4i32.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vnsra_4xi16(__epi_4xi32 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vnsra_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vnsra_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vnsra.mask.nxv4i16.nxv4i32.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vnsra_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi32 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vnsra_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vnsra_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vnsra.nxv2i32.nxv2i64.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vnsra_2xi32(__epi_2xi64 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vnsra_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vnsra_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vnsra.mask.nxv2i32.nxv2i64.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vnsra_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi64 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vnsra_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vnsra_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vnsra.nxv16i8.nxv16i16.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vnsra_16xi8(__epi_16xi16 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vnsra_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vnsra_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vnsra.mask.nxv16i8.nxv16i16.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vnsra_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi16 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vnsra_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vnsra_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vnsra.nxv8i16.nxv8i32.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vnsra_8xi16(__epi_8xi32 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vnsra_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vnsra_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vnsra.mask.nxv8i16.nxv8i32.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vnsra_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi32 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vnsra_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vnsra_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vnsra.nxv4i32.nxv4i64.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vnsra_4xi32(__epi_4xi64 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vnsra_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vnsra_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vnsra.mask.nxv4i32.nxv4i64.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vnsra_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi64 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vnsra_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vnsra_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vnsra.nxv32i8.nxv32i16.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i16> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vnsra_32xi8(__epi_32xi16 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vnsra_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vnsra_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vnsra.mask.nxv32i8.nxv32i16.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i16> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vnsra_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi16 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vnsra_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vnsra_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vnsra.nxv16i16.nxv16i32.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i32> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vnsra_16xi16(__epi_16xi32 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vnsra_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vnsra_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vnsra.mask.nxv16i16.nxv16i32.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i32> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vnsra_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi32 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vnsra_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vnsra_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vnsra.nxv8i32.nxv8i64.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i64> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vnsra_8xi32(__epi_8xi64 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vnsra_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vnsra_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vnsra.mask.nxv8i32.nxv8i64.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i64> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vnsra_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi64 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vnsra_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vnsrl_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vnsrl.nxv8i8.nxv8i16.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vnsrl_8xi8(__epi_8xi16 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vnsrl_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vnsrl_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vnsrl.mask.nxv8i8.nxv8i16.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vnsrl_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi16 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vnsrl_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vnsrl_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vnsrl.nxv4i16.nxv4i32.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vnsrl_4xi16(__epi_4xi32 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vnsrl_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vnsrl_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vnsrl.mask.nxv4i16.nxv4i32.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vnsrl_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi32 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vnsrl_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vnsrl_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vnsrl.nxv2i32.nxv2i64.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vnsrl_2xi32(__epi_2xi64 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vnsrl_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vnsrl_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vnsrl.mask.nxv2i32.nxv2i64.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vnsrl_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi64 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vnsrl_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vnsrl_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vnsrl.nxv16i8.nxv16i16.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vnsrl_16xi8(__epi_16xi16 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vnsrl_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vnsrl_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vnsrl.mask.nxv16i8.nxv16i16.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vnsrl_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi16 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vnsrl_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vnsrl_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vnsrl.nxv8i16.nxv8i32.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vnsrl_8xi16(__epi_8xi32 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vnsrl_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vnsrl_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vnsrl.mask.nxv8i16.nxv8i32.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vnsrl_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi32 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vnsrl_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vnsrl_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vnsrl.nxv4i32.nxv4i64.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vnsrl_4xi32(__epi_4xi64 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vnsrl_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vnsrl_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vnsrl.mask.nxv4i32.nxv4i64.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vnsrl_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi64 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vnsrl_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vnsrl_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vnsrl.nxv32i8.nxv32i16.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i16> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vnsrl_32xi8(__epi_32xi16 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vnsrl_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vnsrl_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vnsrl.mask.nxv32i8.nxv32i16.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i16> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vnsrl_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi16 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vnsrl_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vnsrl_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vnsrl.nxv16i16.nxv16i32.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i32> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vnsrl_16xi16(__epi_16xi32 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vnsrl_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vnsrl_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vnsrl.mask.nxv16i16.nxv16i32.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i32> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vnsrl_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi32 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vnsrl_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vnsrl_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vnsrl.nxv8i32.nxv8i64.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i64> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vnsrl_8xi32(__epi_8xi64 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vnsrl_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vnsrl_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vnsrl.mask.nxv8i32.nxv8i64.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i64> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vnsrl_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi64 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vnsrl_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vor_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vor.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vor_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vor_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vor_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vor.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vor_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vor_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vor_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vor.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vor_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vor_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vor_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vor.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vor_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vor_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vor_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vor.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vor_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vor_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vor_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vor.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vor_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vor_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vor_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vor.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vor_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vor_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vor_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vor.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vor_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vor_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vor_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vor.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vor_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vor_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vor_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vor.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vor_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vor_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vor_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vor.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vor_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vor_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vor_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vor.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vor_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vor_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vor_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vor.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vor_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vor_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vor_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vor.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vor_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vor_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vor_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vor.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vor_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vor_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vor_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vor.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vor_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vor_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vor_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vor.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vor_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vor_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vor_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vor.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vor_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vor_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vor_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vor.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vor_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vor_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vor_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vor.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vor_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vor_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vor_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vor.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vor_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vor_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vor_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vor.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vor_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vor_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vor_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vor.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vor_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vor_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vor_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vor.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vor_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vor_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vpopc_8xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vcpop.nxv8i1.i64(<vscale x 8 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vpopc_8xi1(__epi_8xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vpopc_8xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vpopc_8xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vcpop.mask.nxv8i1.i64(<vscale x 8 x i1> [[ARG_0:%.*]], <vscale x 8 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vpopc_8xi1_mask(__epi_8xi1 arg_0, __epi_8xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vpopc_8xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vpopc_4xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vcpop.nxv4i1.i64(<vscale x 4 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vpopc_4xi1(__epi_4xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vpopc_4xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vpopc_4xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vcpop.mask.nxv4i1.i64(<vscale x 4 x i1> [[ARG_0:%.*]], <vscale x 4 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vpopc_4xi1_mask(__epi_4xi1 arg_0, __epi_4xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vpopc_4xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vpopc_2xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vcpop.nxv2i1.i64(<vscale x 2 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vpopc_2xi1(__epi_2xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vpopc_2xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vpopc_2xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vcpop.mask.nxv2i1.i64(<vscale x 2 x i1> [[ARG_0:%.*]], <vscale x 2 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vpopc_2xi1_mask(__epi_2xi1 arg_0, __epi_2xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vpopc_2xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vpopc_1xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vcpop.nxv1i1.i64(<vscale x 1 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vpopc_1xi1(__epi_1xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vpopc_1xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vpopc_1xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vcpop.mask.nxv1i1.i64(<vscale x 1 x i1> [[ARG_0:%.*]], <vscale x 1 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vpopc_1xi1_mask(__epi_1xi1 arg_0, __epi_1xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vpopc_1xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vpopc_16xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vcpop.nxv16i1.i64(<vscale x 16 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vpopc_16xi1(__epi_16xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vpopc_16xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vpopc_16xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vcpop.mask.nxv16i1.i64(<vscale x 16 x i1> [[ARG_0:%.*]], <vscale x 16 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vpopc_16xi1_mask(__epi_16xi1 arg_0, __epi_16xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vpopc_16xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vpopc_32xi1(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vcpop.nxv32i1.i64(<vscale x 32 x i1> [[ARG_0:%.*]], i64 [[ARG_1:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vpopc_32xi1(__epi_32xi1 arg_0, unsigned long int arg_1)
{
    return __builtin_epi_vpopc_32xi1(arg_0, arg_1);
}

// CHECK-O2-LABEL: @test_vpopc_32xi1_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.riscv.vcpop.mask.nxv32i1.i64(<vscale x 32 x i1> [[ARG_0:%.*]], <vscale x 32 x i1> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret i64 [[TMP0]]
//
signed long int test_vpopc_32xi1_mask(__epi_32xi1 arg_0, __epi_32xi1 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vpopc_32xi1_mask(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredand_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredand.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vredand_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredand_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredand_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredand.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vredand_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredand_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredand_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredand.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vredand_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredand_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredand_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredand.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vredand_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredand_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredand_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredand.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> poison, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vredand_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredand_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredand_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredand.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vredand_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredand_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredand_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredand.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> poison, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vredand_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredand_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredand_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredand.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vredand_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredand_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredand_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredand.nxv8i8.nxv16i8.i64(<vscale x 8 x i8> poison, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP2]]
//
__epi_16xi8 test_vredand_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredand_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredand_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredand.mask.nxv8i8.nxv16i8.i64(<vscale x 8 x i8> [[TMP0]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[TMP1]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP3]]
//
__epi_16xi8 test_vredand_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredand_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredand_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredand.nxv4i16.nxv8i16.i64(<vscale x 4 x i16> poison, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.nxv4i16(<vscale x 8 x i16> poison, <vscale x 4 x i16> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP2]]
//
__epi_8xi16 test_vredand_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredand_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredand_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredand.mask.nxv4i16.nxv8i16.i64(<vscale x 4 x i16> [[TMP0]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.nxv4i16(<vscale x 8 x i16> poison, <vscale x 4 x i16> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP3]]
//
__epi_8xi16 test_vredand_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredand_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredand_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredand.nxv2i32.nxv4i32.i64(<vscale x 2 x i32> poison, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.nxv2i32(<vscale x 4 x i32> poison, <vscale x 2 x i32> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP2]]
//
__epi_4xi32 test_vredand_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredand_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredand_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredand.mask.nxv2i32.nxv4i32.i64(<vscale x 2 x i32> [[TMP0]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.nxv2i32(<vscale x 4 x i32> poison, <vscale x 2 x i32> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP3]]
//
__epi_4xi32 test_vredand_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredand_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredand_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredand.nxv1i64.nxv2i64.i64(<vscale x 1 x i64> poison, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.nxv1i64(<vscale x 2 x i64> poison, <vscale x 1 x i64> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP2]]
//
__epi_2xi64 test_vredand_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredand_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredand_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredand.mask.nxv1i64.nxv2i64.i64(<vscale x 1 x i64> [[TMP0]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[TMP1]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.nxv1i64(<vscale x 2 x i64> poison, <vscale x 1 x i64> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP3]]
//
__epi_2xi64 test_vredand_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredand_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredand_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredand.nxv8i8.nxv32i8.i64(<vscale x 8 x i8> poison, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv8i8(<vscale x 32 x i8> poison, <vscale x 8 x i8> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP2]]
//
__epi_32xi8 test_vredand_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredand_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredand_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredand.mask.nxv8i8.nxv32i8.i64(<vscale x 8 x i8> [[TMP0]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[TMP1]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv8i8(<vscale x 32 x i8> poison, <vscale x 8 x i8> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP3]]
//
__epi_32xi8 test_vredand_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredand_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredand_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredand.nxv4i16.nxv16i16.i64(<vscale x 4 x i16> poison, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 16 x i16> @llvm.vector.insert.nxv16i16.nxv4i16(<vscale x 16 x i16> poison, <vscale x 4 x i16> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP2]]
//
__epi_16xi16 test_vredand_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredand_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredand_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredand.mask.nxv4i16.nxv16i16.i64(<vscale x 4 x i16> [[TMP0]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[TMP1]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 16 x i16> @llvm.vector.insert.nxv16i16.nxv4i16(<vscale x 16 x i16> poison, <vscale x 4 x i16> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP3]]
//
__epi_16xi16 test_vredand_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredand_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredand_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredand.nxv2i32.nxv8i32.i64(<vscale x 2 x i32> poison, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv2i32(<vscale x 8 x i32> poison, <vscale x 2 x i32> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP2]]
//
__epi_8xi32 test_vredand_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredand_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredand_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredand.mask.nxv2i32.nxv8i32.i64(<vscale x 2 x i32> [[TMP0]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv2i32(<vscale x 8 x i32> poison, <vscale x 2 x i32> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP3]]
//
__epi_8xi32 test_vredand_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredand_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredand_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredand.nxv1i64.nxv4i64.i64(<vscale x 1 x i64> poison, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i64> @llvm.vector.insert.nxv4i64.nxv1i64(<vscale x 4 x i64> poison, <vscale x 1 x i64> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP2]]
//
__epi_4xi64 test_vredand_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredand_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredand_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredand.mask.nxv1i64.nxv4i64.i64(<vscale x 1 x i64> [[TMP0]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x i64> @llvm.vector.insert.nxv4i64.nxv1i64(<vscale x 4 x i64> poison, <vscale x 1 x i64> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP3]]
//
__epi_4xi64 test_vredand_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredand_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmax_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredmax.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vredmax_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmax_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmax_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredmax.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vredmax_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmax_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmax_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredmax.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vredmax_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmax_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmax_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredmax.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vredmax_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmax_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmax_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredmax.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> poison, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vredmax_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmax_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmax_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredmax.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vredmax_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmax_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmax_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredmax.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> poison, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vredmax_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmax_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmax_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredmax.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vredmax_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmax_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmax_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredmax.nxv8i8.nxv16i8.i64(<vscale x 8 x i8> poison, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP2]]
//
__epi_16xi8 test_vredmax_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmax_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmax_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredmax.mask.nxv8i8.nxv16i8.i64(<vscale x 8 x i8> [[TMP0]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[TMP1]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP3]]
//
__epi_16xi8 test_vredmax_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmax_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmax_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredmax.nxv4i16.nxv8i16.i64(<vscale x 4 x i16> poison, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.nxv4i16(<vscale x 8 x i16> poison, <vscale x 4 x i16> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP2]]
//
__epi_8xi16 test_vredmax_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmax_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmax_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredmax.mask.nxv4i16.nxv8i16.i64(<vscale x 4 x i16> [[TMP0]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.nxv4i16(<vscale x 8 x i16> poison, <vscale x 4 x i16> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP3]]
//
__epi_8xi16 test_vredmax_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmax_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmax_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredmax.nxv2i32.nxv4i32.i64(<vscale x 2 x i32> poison, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.nxv2i32(<vscale x 4 x i32> poison, <vscale x 2 x i32> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP2]]
//
__epi_4xi32 test_vredmax_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmax_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmax_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredmax.mask.nxv2i32.nxv4i32.i64(<vscale x 2 x i32> [[TMP0]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.nxv2i32(<vscale x 4 x i32> poison, <vscale x 2 x i32> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP3]]
//
__epi_4xi32 test_vredmax_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmax_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmax_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredmax.nxv1i64.nxv2i64.i64(<vscale x 1 x i64> poison, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.nxv1i64(<vscale x 2 x i64> poison, <vscale x 1 x i64> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP2]]
//
__epi_2xi64 test_vredmax_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmax_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmax_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredmax.mask.nxv1i64.nxv2i64.i64(<vscale x 1 x i64> [[TMP0]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[TMP1]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.nxv1i64(<vscale x 2 x i64> poison, <vscale x 1 x i64> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP3]]
//
__epi_2xi64 test_vredmax_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmax_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmax_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredmax.nxv8i8.nxv32i8.i64(<vscale x 8 x i8> poison, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv8i8(<vscale x 32 x i8> poison, <vscale x 8 x i8> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP2]]
//
__epi_32xi8 test_vredmax_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmax_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmax_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredmax.mask.nxv8i8.nxv32i8.i64(<vscale x 8 x i8> [[TMP0]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[TMP1]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv8i8(<vscale x 32 x i8> poison, <vscale x 8 x i8> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP3]]
//
__epi_32xi8 test_vredmax_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmax_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmax_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredmax.nxv4i16.nxv16i16.i64(<vscale x 4 x i16> poison, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 16 x i16> @llvm.vector.insert.nxv16i16.nxv4i16(<vscale x 16 x i16> poison, <vscale x 4 x i16> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP2]]
//
__epi_16xi16 test_vredmax_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmax_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmax_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredmax.mask.nxv4i16.nxv16i16.i64(<vscale x 4 x i16> [[TMP0]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[TMP1]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 16 x i16> @llvm.vector.insert.nxv16i16.nxv4i16(<vscale x 16 x i16> poison, <vscale x 4 x i16> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP3]]
//
__epi_16xi16 test_vredmax_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmax_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmax_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredmax.nxv2i32.nxv8i32.i64(<vscale x 2 x i32> poison, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv2i32(<vscale x 8 x i32> poison, <vscale x 2 x i32> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP2]]
//
__epi_8xi32 test_vredmax_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmax_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmax_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredmax.mask.nxv2i32.nxv8i32.i64(<vscale x 2 x i32> [[TMP0]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv2i32(<vscale x 8 x i32> poison, <vscale x 2 x i32> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP3]]
//
__epi_8xi32 test_vredmax_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmax_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmax_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredmax.nxv1i64.nxv4i64.i64(<vscale x 1 x i64> poison, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i64> @llvm.vector.insert.nxv4i64.nxv1i64(<vscale x 4 x i64> poison, <vscale x 1 x i64> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP2]]
//
__epi_4xi64 test_vredmax_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmax_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmax_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredmax.mask.nxv1i64.nxv4i64.i64(<vscale x 1 x i64> [[TMP0]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x i64> @llvm.vector.insert.nxv4i64.nxv1i64(<vscale x 4 x i64> poison, <vscale x 1 x i64> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP3]]
//
__epi_4xi64 test_vredmax_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmax_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmaxu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredmaxu.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vredmaxu_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmaxu_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmaxu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredmaxu.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vredmaxu_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmaxu_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmaxu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredmaxu.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vredmaxu_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmaxu_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmaxu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredmaxu.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vredmaxu_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmaxu_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmaxu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredmaxu.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> poison, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vredmaxu_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmaxu_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmaxu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredmaxu.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vredmaxu_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmaxu_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmaxu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredmaxu.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> poison, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vredmaxu_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmaxu_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmaxu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredmaxu.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vredmaxu_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmaxu_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmaxu_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredmaxu.nxv8i8.nxv16i8.i64(<vscale x 8 x i8> poison, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP2]]
//
__epi_16xi8 test_vredmaxu_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmaxu_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmaxu_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredmaxu.mask.nxv8i8.nxv16i8.i64(<vscale x 8 x i8> [[TMP0]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[TMP1]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP3]]
//
__epi_16xi8 test_vredmaxu_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmaxu_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmaxu_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredmaxu.nxv4i16.nxv8i16.i64(<vscale x 4 x i16> poison, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.nxv4i16(<vscale x 8 x i16> poison, <vscale x 4 x i16> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP2]]
//
__epi_8xi16 test_vredmaxu_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmaxu_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmaxu_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredmaxu.mask.nxv4i16.nxv8i16.i64(<vscale x 4 x i16> [[TMP0]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.nxv4i16(<vscale x 8 x i16> poison, <vscale x 4 x i16> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP3]]
//
__epi_8xi16 test_vredmaxu_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmaxu_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmaxu_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredmaxu.nxv2i32.nxv4i32.i64(<vscale x 2 x i32> poison, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.nxv2i32(<vscale x 4 x i32> poison, <vscale x 2 x i32> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP2]]
//
__epi_4xi32 test_vredmaxu_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmaxu_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmaxu_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredmaxu.mask.nxv2i32.nxv4i32.i64(<vscale x 2 x i32> [[TMP0]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.nxv2i32(<vscale x 4 x i32> poison, <vscale x 2 x i32> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP3]]
//
__epi_4xi32 test_vredmaxu_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmaxu_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmaxu_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredmaxu.nxv1i64.nxv2i64.i64(<vscale x 1 x i64> poison, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.nxv1i64(<vscale x 2 x i64> poison, <vscale x 1 x i64> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP2]]
//
__epi_2xi64 test_vredmaxu_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmaxu_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmaxu_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredmaxu.mask.nxv1i64.nxv2i64.i64(<vscale x 1 x i64> [[TMP0]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[TMP1]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.nxv1i64(<vscale x 2 x i64> poison, <vscale x 1 x i64> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP3]]
//
__epi_2xi64 test_vredmaxu_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmaxu_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmaxu_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredmaxu.nxv8i8.nxv32i8.i64(<vscale x 8 x i8> poison, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv8i8(<vscale x 32 x i8> poison, <vscale x 8 x i8> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP2]]
//
__epi_32xi8 test_vredmaxu_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmaxu_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmaxu_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredmaxu.mask.nxv8i8.nxv32i8.i64(<vscale x 8 x i8> [[TMP0]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[TMP1]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv8i8(<vscale x 32 x i8> poison, <vscale x 8 x i8> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP3]]
//
__epi_32xi8 test_vredmaxu_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmaxu_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmaxu_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredmaxu.nxv4i16.nxv16i16.i64(<vscale x 4 x i16> poison, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 16 x i16> @llvm.vector.insert.nxv16i16.nxv4i16(<vscale x 16 x i16> poison, <vscale x 4 x i16> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP2]]
//
__epi_16xi16 test_vredmaxu_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmaxu_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmaxu_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredmaxu.mask.nxv4i16.nxv16i16.i64(<vscale x 4 x i16> [[TMP0]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[TMP1]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 16 x i16> @llvm.vector.insert.nxv16i16.nxv4i16(<vscale x 16 x i16> poison, <vscale x 4 x i16> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP3]]
//
__epi_16xi16 test_vredmaxu_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmaxu_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmaxu_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredmaxu.nxv2i32.nxv8i32.i64(<vscale x 2 x i32> poison, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv2i32(<vscale x 8 x i32> poison, <vscale x 2 x i32> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP2]]
//
__epi_8xi32 test_vredmaxu_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmaxu_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmaxu_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredmaxu.mask.nxv2i32.nxv8i32.i64(<vscale x 2 x i32> [[TMP0]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv2i32(<vscale x 8 x i32> poison, <vscale x 2 x i32> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP3]]
//
__epi_8xi32 test_vredmaxu_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmaxu_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmaxu_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredmaxu.nxv1i64.nxv4i64.i64(<vscale x 1 x i64> poison, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i64> @llvm.vector.insert.nxv4i64.nxv1i64(<vscale x 4 x i64> poison, <vscale x 1 x i64> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP2]]
//
__epi_4xi64 test_vredmaxu_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmaxu_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmaxu_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredmaxu.mask.nxv1i64.nxv4i64.i64(<vscale x 1 x i64> [[TMP0]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x i64> @llvm.vector.insert.nxv4i64.nxv1i64(<vscale x 4 x i64> poison, <vscale x 1 x i64> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP3]]
//
__epi_4xi64 test_vredmaxu_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmaxu_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmin_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredmin.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vredmin_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmin_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmin_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredmin.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vredmin_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmin_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmin_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredmin.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vredmin_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmin_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmin_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredmin.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vredmin_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmin_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmin_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredmin.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> poison, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vredmin_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmin_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmin_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredmin.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vredmin_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmin_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmin_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredmin.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> poison, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vredmin_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmin_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmin_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredmin.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vredmin_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmin_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmin_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredmin.nxv8i8.nxv16i8.i64(<vscale x 8 x i8> poison, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP2]]
//
__epi_16xi8 test_vredmin_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmin_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmin_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredmin.mask.nxv8i8.nxv16i8.i64(<vscale x 8 x i8> [[TMP0]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[TMP1]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP3]]
//
__epi_16xi8 test_vredmin_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmin_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmin_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredmin.nxv4i16.nxv8i16.i64(<vscale x 4 x i16> poison, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.nxv4i16(<vscale x 8 x i16> poison, <vscale x 4 x i16> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP2]]
//
__epi_8xi16 test_vredmin_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmin_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmin_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredmin.mask.nxv4i16.nxv8i16.i64(<vscale x 4 x i16> [[TMP0]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.nxv4i16(<vscale x 8 x i16> poison, <vscale x 4 x i16> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP3]]
//
__epi_8xi16 test_vredmin_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmin_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmin_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredmin.nxv2i32.nxv4i32.i64(<vscale x 2 x i32> poison, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.nxv2i32(<vscale x 4 x i32> poison, <vscale x 2 x i32> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP2]]
//
__epi_4xi32 test_vredmin_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmin_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmin_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredmin.mask.nxv2i32.nxv4i32.i64(<vscale x 2 x i32> [[TMP0]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.nxv2i32(<vscale x 4 x i32> poison, <vscale x 2 x i32> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP3]]
//
__epi_4xi32 test_vredmin_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmin_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmin_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredmin.nxv1i64.nxv2i64.i64(<vscale x 1 x i64> poison, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.nxv1i64(<vscale x 2 x i64> poison, <vscale x 1 x i64> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP2]]
//
__epi_2xi64 test_vredmin_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmin_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmin_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredmin.mask.nxv1i64.nxv2i64.i64(<vscale x 1 x i64> [[TMP0]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[TMP1]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.nxv1i64(<vscale x 2 x i64> poison, <vscale x 1 x i64> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP3]]
//
__epi_2xi64 test_vredmin_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmin_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmin_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredmin.nxv8i8.nxv32i8.i64(<vscale x 8 x i8> poison, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv8i8(<vscale x 32 x i8> poison, <vscale x 8 x i8> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP2]]
//
__epi_32xi8 test_vredmin_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmin_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmin_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredmin.mask.nxv8i8.nxv32i8.i64(<vscale x 8 x i8> [[TMP0]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[TMP1]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv8i8(<vscale x 32 x i8> poison, <vscale x 8 x i8> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP3]]
//
__epi_32xi8 test_vredmin_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmin_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmin_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredmin.nxv4i16.nxv16i16.i64(<vscale x 4 x i16> poison, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 16 x i16> @llvm.vector.insert.nxv16i16.nxv4i16(<vscale x 16 x i16> poison, <vscale x 4 x i16> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP2]]
//
__epi_16xi16 test_vredmin_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmin_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmin_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredmin.mask.nxv4i16.nxv16i16.i64(<vscale x 4 x i16> [[TMP0]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[TMP1]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 16 x i16> @llvm.vector.insert.nxv16i16.nxv4i16(<vscale x 16 x i16> poison, <vscale x 4 x i16> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP3]]
//
__epi_16xi16 test_vredmin_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmin_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmin_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredmin.nxv2i32.nxv8i32.i64(<vscale x 2 x i32> poison, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv2i32(<vscale x 8 x i32> poison, <vscale x 2 x i32> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP2]]
//
__epi_8xi32 test_vredmin_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmin_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmin_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredmin.mask.nxv2i32.nxv8i32.i64(<vscale x 2 x i32> [[TMP0]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv2i32(<vscale x 8 x i32> poison, <vscale x 2 x i32> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP3]]
//
__epi_8xi32 test_vredmin_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmin_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredmin_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredmin.nxv1i64.nxv4i64.i64(<vscale x 1 x i64> poison, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i64> @llvm.vector.insert.nxv4i64.nxv1i64(<vscale x 4 x i64> poison, <vscale x 1 x i64> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP2]]
//
__epi_4xi64 test_vredmin_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredmin_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredmin_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredmin.mask.nxv1i64.nxv4i64.i64(<vscale x 1 x i64> [[TMP0]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x i64> @llvm.vector.insert.nxv4i64.nxv1i64(<vscale x 4 x i64> poison, <vscale x 1 x i64> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP3]]
//
__epi_4xi64 test_vredmin_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredmin_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredminu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredminu.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vredminu_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredminu_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredminu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredminu.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vredminu_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredminu_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredminu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredminu.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vredminu_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredminu_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredminu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredminu.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vredminu_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredminu_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredminu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredminu.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> poison, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vredminu_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredminu_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredminu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredminu.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vredminu_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredminu_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredminu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredminu.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> poison, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vredminu_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredminu_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredminu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredminu.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vredminu_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredminu_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredminu_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredminu.nxv8i8.nxv16i8.i64(<vscale x 8 x i8> poison, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP2]]
//
__epi_16xi8 test_vredminu_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredminu_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredminu_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredminu.mask.nxv8i8.nxv16i8.i64(<vscale x 8 x i8> [[TMP0]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[TMP1]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP3]]
//
__epi_16xi8 test_vredminu_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredminu_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredminu_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredminu.nxv4i16.nxv8i16.i64(<vscale x 4 x i16> poison, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.nxv4i16(<vscale x 8 x i16> poison, <vscale x 4 x i16> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP2]]
//
__epi_8xi16 test_vredminu_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredminu_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredminu_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredminu.mask.nxv4i16.nxv8i16.i64(<vscale x 4 x i16> [[TMP0]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.nxv4i16(<vscale x 8 x i16> poison, <vscale x 4 x i16> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP3]]
//
__epi_8xi16 test_vredminu_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredminu_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredminu_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredminu.nxv2i32.nxv4i32.i64(<vscale x 2 x i32> poison, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.nxv2i32(<vscale x 4 x i32> poison, <vscale x 2 x i32> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP2]]
//
__epi_4xi32 test_vredminu_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredminu_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredminu_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredminu.mask.nxv2i32.nxv4i32.i64(<vscale x 2 x i32> [[TMP0]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.nxv2i32(<vscale x 4 x i32> poison, <vscale x 2 x i32> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP3]]
//
__epi_4xi32 test_vredminu_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredminu_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredminu_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredminu.nxv1i64.nxv2i64.i64(<vscale x 1 x i64> poison, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.nxv1i64(<vscale x 2 x i64> poison, <vscale x 1 x i64> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP2]]
//
__epi_2xi64 test_vredminu_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredminu_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredminu_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredminu.mask.nxv1i64.nxv2i64.i64(<vscale x 1 x i64> [[TMP0]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[TMP1]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.nxv1i64(<vscale x 2 x i64> poison, <vscale x 1 x i64> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP3]]
//
__epi_2xi64 test_vredminu_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredminu_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredminu_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredminu.nxv8i8.nxv32i8.i64(<vscale x 8 x i8> poison, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv8i8(<vscale x 32 x i8> poison, <vscale x 8 x i8> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP2]]
//
__epi_32xi8 test_vredminu_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredminu_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredminu_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredminu.mask.nxv8i8.nxv32i8.i64(<vscale x 8 x i8> [[TMP0]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[TMP1]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv8i8(<vscale x 32 x i8> poison, <vscale x 8 x i8> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP3]]
//
__epi_32xi8 test_vredminu_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredminu_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredminu_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredminu.nxv4i16.nxv16i16.i64(<vscale x 4 x i16> poison, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 16 x i16> @llvm.vector.insert.nxv16i16.nxv4i16(<vscale x 16 x i16> poison, <vscale x 4 x i16> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP2]]
//
__epi_16xi16 test_vredminu_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredminu_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredminu_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredminu.mask.nxv4i16.nxv16i16.i64(<vscale x 4 x i16> [[TMP0]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[TMP1]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 16 x i16> @llvm.vector.insert.nxv16i16.nxv4i16(<vscale x 16 x i16> poison, <vscale x 4 x i16> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP3]]
//
__epi_16xi16 test_vredminu_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredminu_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredminu_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredminu.nxv2i32.nxv8i32.i64(<vscale x 2 x i32> poison, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv2i32(<vscale x 8 x i32> poison, <vscale x 2 x i32> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP2]]
//
__epi_8xi32 test_vredminu_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredminu_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredminu_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredminu.mask.nxv2i32.nxv8i32.i64(<vscale x 2 x i32> [[TMP0]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv2i32(<vscale x 8 x i32> poison, <vscale x 2 x i32> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP3]]
//
__epi_8xi32 test_vredminu_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredminu_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredminu_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredminu.nxv1i64.nxv4i64.i64(<vscale x 1 x i64> poison, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i64> @llvm.vector.insert.nxv4i64.nxv1i64(<vscale x 4 x i64> poison, <vscale x 1 x i64> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP2]]
//
__epi_4xi64 test_vredminu_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredminu_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredminu_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredminu.mask.nxv1i64.nxv4i64.i64(<vscale x 1 x i64> [[TMP0]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x i64> @llvm.vector.insert.nxv4i64.nxv1i64(<vscale x 4 x i64> poison, <vscale x 1 x i64> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP3]]
//
__epi_4xi64 test_vredminu_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredminu_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredor_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredor.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vredor_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredor_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredor_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredor.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vredor_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredor_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredor_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredor.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vredor_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredor_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredor_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredor.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vredor_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredor_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredor_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredor.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> poison, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vredor_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredor_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredor_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredor.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vredor_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredor_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredor_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredor.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> poison, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vredor_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredor_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredor_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredor.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vredor_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredor_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredor_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredor.nxv8i8.nxv16i8.i64(<vscale x 8 x i8> poison, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP2]]
//
__epi_16xi8 test_vredor_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredor_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredor_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredor.mask.nxv8i8.nxv16i8.i64(<vscale x 8 x i8> [[TMP0]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[TMP1]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP3]]
//
__epi_16xi8 test_vredor_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredor_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredor_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredor.nxv4i16.nxv8i16.i64(<vscale x 4 x i16> poison, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.nxv4i16(<vscale x 8 x i16> poison, <vscale x 4 x i16> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP2]]
//
__epi_8xi16 test_vredor_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredor_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredor_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredor.mask.nxv4i16.nxv8i16.i64(<vscale x 4 x i16> [[TMP0]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.nxv4i16(<vscale x 8 x i16> poison, <vscale x 4 x i16> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP3]]
//
__epi_8xi16 test_vredor_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredor_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredor_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredor.nxv2i32.nxv4i32.i64(<vscale x 2 x i32> poison, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.nxv2i32(<vscale x 4 x i32> poison, <vscale x 2 x i32> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP2]]
//
__epi_4xi32 test_vredor_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredor_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredor_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredor.mask.nxv2i32.nxv4i32.i64(<vscale x 2 x i32> [[TMP0]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.nxv2i32(<vscale x 4 x i32> poison, <vscale x 2 x i32> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP3]]
//
__epi_4xi32 test_vredor_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredor_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredor_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredor.nxv1i64.nxv2i64.i64(<vscale x 1 x i64> poison, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.nxv1i64(<vscale x 2 x i64> poison, <vscale x 1 x i64> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP2]]
//
__epi_2xi64 test_vredor_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredor_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredor_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredor.mask.nxv1i64.nxv2i64.i64(<vscale x 1 x i64> [[TMP0]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[TMP1]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.nxv1i64(<vscale x 2 x i64> poison, <vscale x 1 x i64> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP3]]
//
__epi_2xi64 test_vredor_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredor_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredor_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredor.nxv8i8.nxv32i8.i64(<vscale x 8 x i8> poison, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv8i8(<vscale x 32 x i8> poison, <vscale x 8 x i8> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP2]]
//
__epi_32xi8 test_vredor_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredor_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredor_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredor.mask.nxv8i8.nxv32i8.i64(<vscale x 8 x i8> [[TMP0]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[TMP1]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv8i8(<vscale x 32 x i8> poison, <vscale x 8 x i8> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP3]]
//
__epi_32xi8 test_vredor_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredor_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredor_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredor.nxv4i16.nxv16i16.i64(<vscale x 4 x i16> poison, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 16 x i16> @llvm.vector.insert.nxv16i16.nxv4i16(<vscale x 16 x i16> poison, <vscale x 4 x i16> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP2]]
//
__epi_16xi16 test_vredor_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredor_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredor_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredor.mask.nxv4i16.nxv16i16.i64(<vscale x 4 x i16> [[TMP0]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[TMP1]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 16 x i16> @llvm.vector.insert.nxv16i16.nxv4i16(<vscale x 16 x i16> poison, <vscale x 4 x i16> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP3]]
//
__epi_16xi16 test_vredor_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredor_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredor_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredor.nxv2i32.nxv8i32.i64(<vscale x 2 x i32> poison, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv2i32(<vscale x 8 x i32> poison, <vscale x 2 x i32> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP2]]
//
__epi_8xi32 test_vredor_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredor_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredor_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredor.mask.nxv2i32.nxv8i32.i64(<vscale x 2 x i32> [[TMP0]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv2i32(<vscale x 8 x i32> poison, <vscale x 2 x i32> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP3]]
//
__epi_8xi32 test_vredor_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredor_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredor_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredor.nxv1i64.nxv4i64.i64(<vscale x 1 x i64> poison, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i64> @llvm.vector.insert.nxv4i64.nxv1i64(<vscale x 4 x i64> poison, <vscale x 1 x i64> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP2]]
//
__epi_4xi64 test_vredor_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredor_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredor_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredor.mask.nxv1i64.nxv4i64.i64(<vscale x 1 x i64> [[TMP0]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x i64> @llvm.vector.insert.nxv4i64.nxv1i64(<vscale x 4 x i64> poison, <vscale x 1 x i64> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP3]]
//
__epi_4xi64 test_vredor_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredor_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredsum_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredsum.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vredsum_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredsum_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredsum_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredsum.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vredsum_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredsum_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredsum_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredsum.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vredsum_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredsum_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredsum_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredsum.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vredsum_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredsum_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredsum_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredsum.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> poison, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vredsum_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredsum_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredsum_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredsum.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vredsum_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredsum_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredsum_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredsum.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> poison, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vredsum_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredsum_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredsum_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredsum.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vredsum_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredsum_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredsum_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredsum.nxv8i8.nxv16i8.i64(<vscale x 8 x i8> poison, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP2]]
//
__epi_16xi8 test_vredsum_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredsum_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredsum_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredsum.mask.nxv8i8.nxv16i8.i64(<vscale x 8 x i8> [[TMP0]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[TMP1]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP3]]
//
__epi_16xi8 test_vredsum_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredsum_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredsum_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredsum.nxv4i16.nxv8i16.i64(<vscale x 4 x i16> poison, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.nxv4i16(<vscale x 8 x i16> poison, <vscale x 4 x i16> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP2]]
//
__epi_8xi16 test_vredsum_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredsum_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredsum_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredsum.mask.nxv4i16.nxv8i16.i64(<vscale x 4 x i16> [[TMP0]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.nxv4i16(<vscale x 8 x i16> poison, <vscale x 4 x i16> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP3]]
//
__epi_8xi16 test_vredsum_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredsum_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredsum_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredsum.nxv2i32.nxv4i32.i64(<vscale x 2 x i32> poison, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.nxv2i32(<vscale x 4 x i32> poison, <vscale x 2 x i32> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP2]]
//
__epi_4xi32 test_vredsum_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredsum_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredsum_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredsum.mask.nxv2i32.nxv4i32.i64(<vscale x 2 x i32> [[TMP0]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.nxv2i32(<vscale x 4 x i32> poison, <vscale x 2 x i32> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP3]]
//
__epi_4xi32 test_vredsum_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredsum_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredsum_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredsum.nxv1i64.nxv2i64.i64(<vscale x 1 x i64> poison, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.nxv1i64(<vscale x 2 x i64> poison, <vscale x 1 x i64> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP2]]
//
__epi_2xi64 test_vredsum_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredsum_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredsum_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredsum.mask.nxv1i64.nxv2i64.i64(<vscale x 1 x i64> [[TMP0]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[TMP1]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.nxv1i64(<vscale x 2 x i64> poison, <vscale x 1 x i64> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP3]]
//
__epi_2xi64 test_vredsum_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredsum_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredsum_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredsum.nxv8i8.nxv32i8.i64(<vscale x 8 x i8> poison, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv8i8(<vscale x 32 x i8> poison, <vscale x 8 x i8> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP2]]
//
__epi_32xi8 test_vredsum_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredsum_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredsum_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredsum.mask.nxv8i8.nxv32i8.i64(<vscale x 8 x i8> [[TMP0]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[TMP1]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv8i8(<vscale x 32 x i8> poison, <vscale x 8 x i8> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP3]]
//
__epi_32xi8 test_vredsum_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredsum_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredsum_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredsum.nxv4i16.nxv16i16.i64(<vscale x 4 x i16> poison, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 16 x i16> @llvm.vector.insert.nxv16i16.nxv4i16(<vscale x 16 x i16> poison, <vscale x 4 x i16> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP2]]
//
__epi_16xi16 test_vredsum_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredsum_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredsum_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredsum.mask.nxv4i16.nxv16i16.i64(<vscale x 4 x i16> [[TMP0]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[TMP1]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 16 x i16> @llvm.vector.insert.nxv16i16.nxv4i16(<vscale x 16 x i16> poison, <vscale x 4 x i16> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP3]]
//
__epi_16xi16 test_vredsum_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredsum_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredsum_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredsum.nxv2i32.nxv8i32.i64(<vscale x 2 x i32> poison, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv2i32(<vscale x 8 x i32> poison, <vscale x 2 x i32> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP2]]
//
__epi_8xi32 test_vredsum_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredsum_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredsum_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredsum.mask.nxv2i32.nxv8i32.i64(<vscale x 2 x i32> [[TMP0]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv2i32(<vscale x 8 x i32> poison, <vscale x 2 x i32> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP3]]
//
__epi_8xi32 test_vredsum_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredsum_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredsum_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredsum.nxv1i64.nxv4i64.i64(<vscale x 1 x i64> poison, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i64> @llvm.vector.insert.nxv4i64.nxv1i64(<vscale x 4 x i64> poison, <vscale x 1 x i64> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP2]]
//
__epi_4xi64 test_vredsum_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredsum_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredsum_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredsum.mask.nxv1i64.nxv4i64.i64(<vscale x 1 x i64> [[TMP0]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x i64> @llvm.vector.insert.nxv4i64.nxv1i64(<vscale x 4 x i64> poison, <vscale x 1 x i64> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP3]]
//
__epi_4xi64 test_vredsum_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredsum_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredxor_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredxor.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vredxor_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredxor_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredxor_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredxor.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vredxor_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredxor_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredxor_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredxor.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vredxor_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredxor_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredxor_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredxor.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vredxor_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredxor_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredxor_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredxor.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> poison, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vredxor_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredxor_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredxor_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredxor.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vredxor_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredxor_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredxor_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredxor.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> poison, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vredxor_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredxor_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredxor_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredxor.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vredxor_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredxor_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredxor_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredxor.nxv8i8.nxv16i8.i64(<vscale x 8 x i8> poison, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP2]]
//
__epi_16xi8 test_vredxor_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredxor_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredxor_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv16i8(<vscale x 16 x i8> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredxor.mask.nxv8i8.nxv16i8.i64(<vscale x 8 x i8> [[TMP0]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[TMP1]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 16 x i8> @llvm.vector.insert.nxv16i8.nxv8i8(<vscale x 16 x i8> poison, <vscale x 8 x i8> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP3]]
//
__epi_16xi8 test_vredxor_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredxor_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredxor_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredxor.nxv4i16.nxv8i16.i64(<vscale x 4 x i16> poison, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.nxv4i16(<vscale x 8 x i16> poison, <vscale x 4 x i16> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP2]]
//
__epi_8xi16 test_vredxor_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredxor_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredxor_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredxor.mask.nxv4i16.nxv8i16.i64(<vscale x 4 x i16> [[TMP0]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.nxv4i16(<vscale x 8 x i16> poison, <vscale x 4 x i16> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP3]]
//
__epi_8xi16 test_vredxor_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredxor_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredxor_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredxor.nxv2i32.nxv4i32.i64(<vscale x 2 x i32> poison, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.nxv2i32(<vscale x 4 x i32> poison, <vscale x 2 x i32> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP2]]
//
__epi_4xi32 test_vredxor_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredxor_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredxor_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredxor.mask.nxv2i32.nxv4i32.i64(<vscale x 2 x i32> [[TMP0]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.nxv2i32(<vscale x 4 x i32> poison, <vscale x 2 x i32> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP3]]
//
__epi_4xi32 test_vredxor_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredxor_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredxor_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredxor.nxv1i64.nxv2i64.i64(<vscale x 1 x i64> poison, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.nxv1i64(<vscale x 2 x i64> poison, <vscale x 1 x i64> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP2]]
//
__epi_2xi64 test_vredxor_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredxor_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredxor_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredxor.mask.nxv1i64.nxv2i64.i64(<vscale x 1 x i64> [[TMP0]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[TMP1]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.nxv1i64(<vscale x 2 x i64> poison, <vscale x 1 x i64> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP3]]
//
__epi_2xi64 test_vredxor_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredxor_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredxor_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredxor.nxv8i8.nxv32i8.i64(<vscale x 8 x i8> poison, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv8i8(<vscale x 32 x i8> poison, <vscale x 8 x i8> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP2]]
//
__epi_32xi8 test_vredxor_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredxor_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredxor_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 8 x i8> @llvm.vector.extract.nxv8i8.nxv32i8(<vscale x 32 x i8> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vredxor.mask.nxv8i8.nxv32i8.i64(<vscale x 8 x i8> [[TMP0]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[TMP1]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 32 x i8> @llvm.vector.insert.nxv32i8.nxv8i8(<vscale x 32 x i8> poison, <vscale x 8 x i8> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP3]]
//
__epi_32xi8 test_vredxor_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredxor_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredxor_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredxor.nxv4i16.nxv16i16.i64(<vscale x 4 x i16> poison, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 16 x i16> @llvm.vector.insert.nxv16i16.nxv4i16(<vscale x 16 x i16> poison, <vscale x 4 x i16> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP2]]
//
__epi_16xi16 test_vredxor_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredxor_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredxor_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vredxor.mask.nxv4i16.nxv16i16.i64(<vscale x 4 x i16> [[TMP0]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[TMP1]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 16 x i16> @llvm.vector.insert.nxv16i16.nxv4i16(<vscale x 16 x i16> poison, <vscale x 4 x i16> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP3]]
//
__epi_16xi16 test_vredxor_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredxor_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredxor_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredxor.nxv2i32.nxv8i32.i64(<vscale x 2 x i32> poison, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv2i32(<vscale x 8 x i32> poison, <vscale x 2 x i32> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP2]]
//
__epi_8xi32 test_vredxor_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredxor_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredxor_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vredxor.mask.nxv2i32.nxv8i32.i64(<vscale x 2 x i32> [[TMP0]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv2i32(<vscale x 8 x i32> poison, <vscale x 2 x i32> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP3]]
//
__epi_8xi32 test_vredxor_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredxor_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vredxor_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredxor.nxv1i64.nxv4i64.i64(<vscale x 1 x i64> poison, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i64> @llvm.vector.insert.nxv4i64.nxv1i64(<vscale x 4 x i64> poison, <vscale x 1 x i64> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP2]]
//
__epi_4xi64 test_vredxor_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vredxor_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vredxor_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vredxor.mask.nxv1i64.nxv4i64.i64(<vscale x 1 x i64> [[TMP0]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x i64> @llvm.vector.insert.nxv4i64.nxv1i64(<vscale x 4 x i64> poison, <vscale x 1 x i64> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP3]]
//
__epi_4xi64 test_vredxor_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vredxor_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrem_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vrem.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vrem_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrem_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrem_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vrem.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vrem_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrem_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrem_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vrem.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vrem_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrem_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrem_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vrem.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vrem_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrem_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrem_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vrem.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vrem_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrem_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrem_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vrem.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vrem_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrem_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrem_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vrem.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vrem_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrem_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrem_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vrem.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vrem_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrem_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrem_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vrem.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vrem_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrem_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrem_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vrem.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vrem_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrem_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrem_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vrem.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vrem_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrem_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrem_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vrem.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vrem_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrem_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrem_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vrem.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vrem_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrem_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrem_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vrem.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vrem_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrem_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrem_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vrem.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vrem_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrem_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrem_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vrem.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vrem_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrem_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrem_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vrem.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vrem_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrem_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrem_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vrem.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vrem_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrem_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrem_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vrem.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vrem_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrem_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrem_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vrem.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vrem_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrem_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrem_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vrem.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vrem_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrem_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrem_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vrem.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vrem_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrem_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrem_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vrem.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vrem_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrem_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrem_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vrem.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vrem_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrem_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vremu_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vremu.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vremu_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vremu_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vremu_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vremu.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vremu_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vremu_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vremu_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vremu.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vremu_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vremu_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vremu_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vremu.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vremu_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vremu_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vremu_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vremu.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vremu_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vremu_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vremu_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vremu.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vremu_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vremu_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vremu_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vremu.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vremu_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vremu_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vremu_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vremu.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vremu_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vremu_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vremu_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vremu.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vremu_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vremu_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vremu_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vremu.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vremu_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vremu_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vremu_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vremu.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vremu_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vremu_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vremu_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vremu.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vremu_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vremu_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vremu_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vremu.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vremu_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vremu_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vremu_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vremu.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vremu_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vremu_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vremu_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vremu.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vremu_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vremu_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vremu_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vremu.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vremu_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vremu_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vremu_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vremu.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vremu_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vremu_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vremu_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vremu.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vremu_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vremu_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vremu_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vremu.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vremu_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vremu_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vremu_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vremu.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vremu_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vremu_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vremu_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vremu.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vremu_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vremu_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vremu_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vremu.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vremu_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vremu_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vremu_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vremu.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vremu_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vremu_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vremu_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vremu.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vremu_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vremu_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrgather_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vrgather.vv.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vrgather_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrgather_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrgather_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vrgather.vv.mask.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vrgather_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrgather_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrgather_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vrgather.vv.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vrgather_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrgather_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrgather_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vrgather.vv.mask.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vrgather_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrgather_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrgather_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vrgather.vv.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vrgather_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrgather_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrgather_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vrgather.vv.mask.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vrgather_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrgather_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrgather_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vrgather.vv.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vrgather_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrgather_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrgather_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vrgather.vv.mask.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vrgather_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrgather_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrgather_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vrgather.vv.nxv2f32.i64(<vscale x 2 x float> undef, <vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vrgather_2xf32(__epi_2xf32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrgather_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrgather_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vrgather.vv.mask.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vrgather_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrgather_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrgather_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vrgather.vv.nxv1f64.i64(<vscale x 1 x double> undef, <vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vrgather_1xf64(__epi_1xf64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrgather_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrgather_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vrgather.vv.mask.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vrgather_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrgather_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrgather_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vrgather.vv.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vrgather_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrgather_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrgather_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vrgather.vv.mask.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vrgather_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrgather_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrgather_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vrgather.vv.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vrgather_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrgather_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrgather_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vrgather.vv.mask.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vrgather_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrgather_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrgather_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vrgather.vv.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vrgather_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrgather_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrgather_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vrgather.vv.mask.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vrgather_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrgather_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrgather_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vrgather.vv.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vrgather_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrgather_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrgather_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vrgather.vv.mask.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vrgather_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrgather_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrgather_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vrgather.vv.nxv4f32.i64(<vscale x 4 x float> undef, <vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vrgather_4xf32(__epi_4xf32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrgather_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrgather_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vrgather.vv.mask.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vrgather_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrgather_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrgather_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vrgather.vv.nxv2f64.i64(<vscale x 2 x double> undef, <vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vrgather_2xf64(__epi_2xf64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrgather_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrgather_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vrgather.vv.mask.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vrgather_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrgather_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrgather_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vrgather.vv.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vrgather_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrgather_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrgather_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vrgather.vv.mask.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vrgather_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrgather_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrgather_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vrgather.vv.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vrgather_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrgather_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrgather_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vrgather.vv.mask.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vrgather_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrgather_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrgather_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vrgather.vv.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vrgather_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrgather_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrgather_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vrgather.vv.mask.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vrgather_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrgather_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrgather_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vrgather.vv.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vrgather_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrgather_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrgather_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vrgather.vv.mask.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vrgather_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrgather_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrgather_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vrgather.vv.nxv8f32.i64(<vscale x 8 x float> undef, <vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vrgather_8xf32(__epi_8xf32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrgather_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrgather_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vrgather.vv.mask.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vrgather_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrgather_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vrgather_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vrgather.vv.nxv4f64.i64(<vscale x 4 x double> undef, <vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vrgather_4xf64(__epi_4xf64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vrgather_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vrgather_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vrgather.vv.mask.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vrgather_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vrgather_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsbc_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vsbc.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vsbc_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vsbc_8xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vsbc_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vsbc.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vsbc_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vsbc_4xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vsbc_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vsbc.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vsbc_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vsbc_2xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vsbc_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vsbc.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vsbc_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vsbc_1xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vsbc_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vsbc.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vsbc_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vsbc_16xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vsbc_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vsbc.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vsbc_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vsbc_8xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vsbc_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vsbc.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vsbc_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vsbc_4xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vsbc_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vsbc.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vsbc_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vsbc_2xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vsbc_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vsbc.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vsbc_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vsbc_32xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vsbc_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vsbc.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vsbc_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vsbc_16xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vsbc_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vsbc.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vsbc_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vsbc_8xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vsbc_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vsbc.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vsbc_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vsbc_4xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vslide1down_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_1:%.*]] to i8
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vslide1down.nxv8i8.i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], i8 [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vslide1down_8xi8(__epi_8xi8 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1down_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1down_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_2:%.*]] to i8
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vslide1down.mask.nxv8i8.i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i8 [[CONV]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vslide1down_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1down_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1down_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_1:%.*]] to i16
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vslide1down.nxv4i16.i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], i16 [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vslide1down_4xi16(__epi_4xi16 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1down_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1down_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_2:%.*]] to i16
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vslide1down.mask.nxv4i16.i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i16 [[CONV]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vslide1down_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1down_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1down_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_1:%.*]] to i32
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vslide1down.nxv2i32.i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], i32 [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vslide1down_2xi32(__epi_2xi32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1down_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1down_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_2:%.*]] to i32
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vslide1down.mask.nxv2i32.i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i32 [[CONV]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vslide1down_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1down_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1down_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vslide1down.nxv1i64.i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vslide1down_1xi64(__epi_1xi64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1down_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1down_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vslide1down.mask.nxv1i64.i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vslide1down_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1down_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1down_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_1:%.*]] to float
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfslide1down.nxv2f32.f32.i64(<vscale x 2 x float> undef, <vscale x 2 x float> [[ARG_0:%.*]], float [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vslide1down_2xf32(__epi_2xf32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1down_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1down_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_2:%.*]] to float
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfslide1down.mask.nxv2f32.f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], float [[CONV]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vslide1down_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1down_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1down_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_1:%.*]] to double
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfslide1down.nxv1f64.f64.i64(<vscale x 1 x double> undef, <vscale x 1 x double> [[ARG_0:%.*]], double [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vslide1down_1xf64(__epi_1xf64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1down_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1down_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_2:%.*]] to double
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfslide1down.mask.nxv1f64.f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], double [[CONV]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vslide1down_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1down_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1down_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_1:%.*]] to i8
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vslide1down.nxv16i8.i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], i8 [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vslide1down_16xi8(__epi_16xi8 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1down_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1down_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_2:%.*]] to i8
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vslide1down.mask.nxv16i8.i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i8 [[CONV]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vslide1down_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1down_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1down_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_1:%.*]] to i16
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vslide1down.nxv8i16.i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], i16 [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vslide1down_8xi16(__epi_8xi16 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1down_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1down_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_2:%.*]] to i16
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vslide1down.mask.nxv8i16.i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i16 [[CONV]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vslide1down_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1down_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1down_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_1:%.*]] to i32
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vslide1down.nxv4i32.i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], i32 [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vslide1down_4xi32(__epi_4xi32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1down_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1down_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_2:%.*]] to i32
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vslide1down.mask.nxv4i32.i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i32 [[CONV]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vslide1down_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1down_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1down_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vslide1down.nxv2i64.i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vslide1down_2xi64(__epi_2xi64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1down_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1down_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vslide1down.mask.nxv2i64.i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vslide1down_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1down_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1down_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_1:%.*]] to float
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfslide1down.nxv4f32.f32.i64(<vscale x 4 x float> undef, <vscale x 4 x float> [[ARG_0:%.*]], float [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vslide1down_4xf32(__epi_4xf32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1down_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1down_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_2:%.*]] to float
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfslide1down.mask.nxv4f32.f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], float [[CONV]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vslide1down_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1down_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1down_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_1:%.*]] to double
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfslide1down.nxv2f64.f64.i64(<vscale x 2 x double> undef, <vscale x 2 x double> [[ARG_0:%.*]], double [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vslide1down_2xf64(__epi_2xf64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1down_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1down_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_2:%.*]] to double
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfslide1down.mask.nxv2f64.f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], double [[CONV]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vslide1down_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1down_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1down_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_1:%.*]] to i8
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vslide1down.nxv32i8.i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], i8 [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vslide1down_32xi8(__epi_32xi8 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1down_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1down_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_2:%.*]] to i8
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vslide1down.mask.nxv32i8.i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i8 [[CONV]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vslide1down_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1down_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1down_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_1:%.*]] to i16
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vslide1down.nxv16i16.i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], i16 [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vslide1down_16xi16(__epi_16xi16 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1down_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1down_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_2:%.*]] to i16
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vslide1down.mask.nxv16i16.i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i16 [[CONV]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vslide1down_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1down_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1down_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_1:%.*]] to i32
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vslide1down.nxv8i32.i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], i32 [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vslide1down_8xi32(__epi_8xi32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1down_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1down_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_2:%.*]] to i32
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vslide1down.mask.nxv8i32.i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i32 [[CONV]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vslide1down_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1down_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1down_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vslide1down.nxv4i64.i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vslide1down_4xi64(__epi_4xi64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1down_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1down_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vslide1down.mask.nxv4i64.i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vslide1down_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1down_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1down_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_1:%.*]] to float
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfslide1down.nxv8f32.f32.i64(<vscale x 8 x float> undef, <vscale x 8 x float> [[ARG_0:%.*]], float [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vslide1down_8xf32(__epi_8xf32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1down_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1down_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_2:%.*]] to float
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfslide1down.mask.nxv8f32.f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], float [[CONV]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vslide1down_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1down_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1down_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_1:%.*]] to double
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfslide1down.nxv4f64.f64.i64(<vscale x 4 x double> undef, <vscale x 4 x double> [[ARG_0:%.*]], double [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vslide1down_4xf64(__epi_4xf64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1down_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1down_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_2:%.*]] to double
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfslide1down.mask.nxv4f64.f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], double [[CONV]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vslide1down_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1down_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1up_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_1:%.*]] to i8
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vslide1up.nxv8i8.i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], i8 [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vslide1up_8xi8(__epi_8xi8 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1up_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1up_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_2:%.*]] to i8
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vslide1up.mask.nxv8i8.i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i8 [[CONV]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vslide1up_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1up_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1up_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_1:%.*]] to i16
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vslide1up.nxv4i16.i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], i16 [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vslide1up_4xi16(__epi_4xi16 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1up_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1up_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_2:%.*]] to i16
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vslide1up.mask.nxv4i16.i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i16 [[CONV]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vslide1up_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1up_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1up_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_1:%.*]] to i32
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vslide1up.nxv2i32.i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], i32 [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vslide1up_2xi32(__epi_2xi32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1up_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1up_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_2:%.*]] to i32
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vslide1up.mask.nxv2i32.i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i32 [[CONV]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vslide1up_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1up_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1up_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vslide1up.nxv1i64.i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vslide1up_1xi64(__epi_1xi64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1up_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1up_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vslide1up.mask.nxv1i64.i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vslide1up_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1up_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1up_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_1:%.*]] to float
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfslide1up.nxv2f32.f32.i64(<vscale x 2 x float> undef, <vscale x 2 x float> [[ARG_0:%.*]], float [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vslide1up_2xf32(__epi_2xf32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1up_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1up_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_2:%.*]] to float
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vfslide1up.mask.nxv2f32.f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], float [[CONV]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vslide1up_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1up_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1up_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_1:%.*]] to double
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfslide1up.nxv1f64.f64.i64(<vscale x 1 x double> undef, <vscale x 1 x double> [[ARG_0:%.*]], double [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vslide1up_1xf64(__epi_1xf64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1up_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1up_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_2:%.*]] to double
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vfslide1up.mask.nxv1f64.f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], double [[CONV]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vslide1up_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1up_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1up_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_1:%.*]] to i8
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vslide1up.nxv16i8.i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], i8 [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vslide1up_16xi8(__epi_16xi8 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1up_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1up_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_2:%.*]] to i8
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vslide1up.mask.nxv16i8.i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i8 [[CONV]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vslide1up_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1up_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1up_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_1:%.*]] to i16
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vslide1up.nxv8i16.i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], i16 [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vslide1up_8xi16(__epi_8xi16 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1up_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1up_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_2:%.*]] to i16
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vslide1up.mask.nxv8i16.i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i16 [[CONV]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vslide1up_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1up_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1up_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_1:%.*]] to i32
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vslide1up.nxv4i32.i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], i32 [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vslide1up_4xi32(__epi_4xi32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1up_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1up_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_2:%.*]] to i32
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vslide1up.mask.nxv4i32.i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i32 [[CONV]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vslide1up_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1up_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1up_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vslide1up.nxv2i64.i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vslide1up_2xi64(__epi_2xi64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1up_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1up_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vslide1up.mask.nxv2i64.i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vslide1up_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1up_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1up_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_1:%.*]] to float
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfslide1up.nxv4f32.f32.i64(<vscale x 4 x float> undef, <vscale x 4 x float> [[ARG_0:%.*]], float [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vslide1up_4xf32(__epi_4xf32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1up_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1up_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_2:%.*]] to float
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vfslide1up.mask.nxv4f32.f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], float [[CONV]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vslide1up_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1up_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1up_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_1:%.*]] to double
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfslide1up.nxv2f64.f64.i64(<vscale x 2 x double> undef, <vscale x 2 x double> [[ARG_0:%.*]], double [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vslide1up_2xf64(__epi_2xf64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1up_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1up_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_2:%.*]] to double
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vfslide1up.mask.nxv2f64.f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], double [[CONV]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vslide1up_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1up_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1up_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_1:%.*]] to i8
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vslide1up.nxv32i8.i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], i8 [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vslide1up_32xi8(__epi_32xi8 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1up_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1up_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_2:%.*]] to i8
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vslide1up.mask.nxv32i8.i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i8 [[CONV]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vslide1up_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1up_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1up_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_1:%.*]] to i16
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vslide1up.nxv16i16.i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], i16 [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vslide1up_16xi16(__epi_16xi16 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1up_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1up_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_2:%.*]] to i16
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vslide1up.mask.nxv16i16.i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i16 [[CONV]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vslide1up_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1up_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1up_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_1:%.*]] to i32
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vslide1up.nxv8i32.i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], i32 [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vslide1up_8xi32(__epi_8xi32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1up_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1up_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = trunc i64 [[ARG_2:%.*]] to i32
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vslide1up.mask.nxv8i32.i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i32 [[CONV]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vslide1up_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1up_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1up_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vslide1up.nxv4i64.i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vslide1up_4xi64(__epi_4xi64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1up_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1up_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vslide1up.mask.nxv4i64.i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vslide1up_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1up_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1up_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_1:%.*]] to float
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfslide1up.nxv8f32.f32.i64(<vscale x 8 x float> undef, <vscale x 8 x float> [[ARG_0:%.*]], float [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vslide1up_8xf32(__epi_8xf32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1up_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1up_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_2:%.*]] to float
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vfslide1up.mask.nxv8f32.f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], float [[CONV]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vslide1up_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1up_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslide1up_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_1:%.*]] to double
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfslide1up.nxv4f64.f64.i64(<vscale x 4 x double> undef, <vscale x 4 x double> [[ARG_0:%.*]], double [[CONV]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vslide1up_4xf64(__epi_4xf64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslide1up_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslide1up_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[CONV:%.*]] = uitofp i64 [[ARG_2:%.*]] to double
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vfslide1up.mask.nxv4f64.f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], double [[CONV]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vslide1up_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslide1up_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslidedown_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vslidedown_8xi8(__epi_8xi8 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslidedown_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslidedown_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.mask.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vslidedown_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslidedown_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslidedown_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vslidedown.nxv4i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vslidedown_4xi16(__epi_4xi16 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslidedown_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslidedown_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vslidedown.mask.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vslidedown_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslidedown_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslidedown_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vslidedown.nxv2i32.i64(<vscale x 2 x i32> poison, <vscale x 2 x i32> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vslidedown_2xi32(__epi_2xi32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslidedown_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslidedown_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vslidedown.mask.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vslidedown_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslidedown_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslidedown_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vslidedown.nxv1i64.i64(<vscale x 1 x i64> poison, <vscale x 1 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vslidedown_1xi64(__epi_1xi64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslidedown_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslidedown_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vslidedown.mask.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vslidedown_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslidedown_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslidedown_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vslidedown.nxv2f32.i64(<vscale x 2 x float> poison, <vscale x 2 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vslidedown_2xf32(__epi_2xf32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslidedown_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslidedown_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vslidedown.mask.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vslidedown_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslidedown_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslidedown_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vslidedown.nxv1f64.i64(<vscale x 1 x double> poison, <vscale x 1 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vslidedown_1xf64(__epi_1xf64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslidedown_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslidedown_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vslidedown.mask.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vslidedown_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslidedown_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslidedown_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vslidedown.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vslidedown_16xi8(__epi_16xi8 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslidedown_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslidedown_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vslidedown.mask.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vslidedown_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslidedown_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslidedown_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vslidedown.nxv8i16.i64(<vscale x 8 x i16> poison, <vscale x 8 x i16> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vslidedown_8xi16(__epi_8xi16 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslidedown_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslidedown_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vslidedown.mask.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vslidedown_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslidedown_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslidedown_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vslidedown.nxv4i32.i64(<vscale x 4 x i32> poison, <vscale x 4 x i32> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vslidedown_4xi32(__epi_4xi32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslidedown_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslidedown_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vslidedown.mask.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vslidedown_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslidedown_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslidedown_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vslidedown.nxv2i64.i64(<vscale x 2 x i64> poison, <vscale x 2 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vslidedown_2xi64(__epi_2xi64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslidedown_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslidedown_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vslidedown.mask.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vslidedown_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslidedown_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslidedown_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vslidedown.nxv4f32.i64(<vscale x 4 x float> poison, <vscale x 4 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vslidedown_4xf32(__epi_4xf32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslidedown_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslidedown_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vslidedown.mask.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vslidedown_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslidedown_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslidedown_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vslidedown.nxv2f64.i64(<vscale x 2 x double> poison, <vscale x 2 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vslidedown_2xf64(__epi_2xf64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslidedown_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslidedown_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vslidedown.mask.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vslidedown_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslidedown_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslidedown_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vslidedown.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vslidedown_32xi8(__epi_32xi8 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslidedown_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslidedown_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vslidedown.mask.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vslidedown_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslidedown_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslidedown_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vslidedown.nxv16i16.i64(<vscale x 16 x i16> poison, <vscale x 16 x i16> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vslidedown_16xi16(__epi_16xi16 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslidedown_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslidedown_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vslidedown.mask.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vslidedown_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslidedown_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslidedown_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vslidedown.nxv8i32.i64(<vscale x 8 x i32> poison, <vscale x 8 x i32> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vslidedown_8xi32(__epi_8xi32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslidedown_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslidedown_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vslidedown.mask.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vslidedown_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslidedown_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslidedown_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vslidedown.nxv4i64.i64(<vscale x 4 x i64> poison, <vscale x 4 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vslidedown_4xi64(__epi_4xi64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslidedown_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslidedown_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vslidedown.mask.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vslidedown_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslidedown_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslidedown_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vslidedown.nxv8f32.i64(<vscale x 8 x float> poison, <vscale x 8 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vslidedown_8xf32(__epi_8xf32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslidedown_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslidedown_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vslidedown.mask.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vslidedown_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslidedown_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslidedown_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vslidedown.nxv4f64.i64(<vscale x 4 x double> poison, <vscale x 4 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vslidedown_4xf64(__epi_4xf64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslidedown_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslidedown_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vslidedown.mask.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vslidedown_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslidedown_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslideup_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vslideup_8xi8(__epi_8xi8 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslideup_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslideup_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vslideup.mask.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vslideup_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslideup_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslideup_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vslideup.nxv4i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vslideup_4xi16(__epi_4xi16 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslideup_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslideup_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vslideup.mask.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vslideup_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslideup_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslideup_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vslideup.nxv2i32.i64(<vscale x 2 x i32> poison, <vscale x 2 x i32> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vslideup_2xi32(__epi_2xi32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslideup_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslideup_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vslideup.mask.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vslideup_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslideup_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslideup_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vslideup.nxv1i64.i64(<vscale x 1 x i64> poison, <vscale x 1 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vslideup_1xi64(__epi_1xi64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslideup_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslideup_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vslideup.mask.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vslideup_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslideup_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslideup_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vslideup.nxv2f32.i64(<vscale x 2 x float> poison, <vscale x 2 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vslideup_2xf32(__epi_2xf32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslideup_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslideup_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vslideup.mask.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vslideup_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslideup_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslideup_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vslideup.nxv1f64.i64(<vscale x 1 x double> poison, <vscale x 1 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vslideup_1xf64(__epi_1xf64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslideup_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslideup_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vslideup.mask.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vslideup_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslideup_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslideup_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.nxv16i8.i64(<vscale x 16 x i8> poison, <vscale x 16 x i8> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vslideup_16xi8(__epi_16xi8 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslideup_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslideup_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vslideup.mask.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vslideup_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslideup_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslideup_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vslideup.nxv8i16.i64(<vscale x 8 x i16> poison, <vscale x 8 x i16> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vslideup_8xi16(__epi_8xi16 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslideup_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslideup_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vslideup.mask.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vslideup_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslideup_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslideup_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vslideup.nxv4i32.i64(<vscale x 4 x i32> poison, <vscale x 4 x i32> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vslideup_4xi32(__epi_4xi32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslideup_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslideup_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vslideup.mask.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vslideup_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslideup_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslideup_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vslideup.nxv2i64.i64(<vscale x 2 x i64> poison, <vscale x 2 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vslideup_2xi64(__epi_2xi64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslideup_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslideup_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vslideup.mask.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vslideup_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslideup_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslideup_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vslideup.nxv4f32.i64(<vscale x 4 x float> poison, <vscale x 4 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vslideup_4xf32(__epi_4xf32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslideup_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslideup_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vslideup.mask.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vslideup_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslideup_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslideup_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vslideup.nxv2f64.i64(<vscale x 2 x double> poison, <vscale x 2 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vslideup_2xf64(__epi_2xf64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslideup_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslideup_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vslideup.mask.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vslideup_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslideup_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslideup_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.nxv32i8.i64(<vscale x 32 x i8> poison, <vscale x 32 x i8> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vslideup_32xi8(__epi_32xi8 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslideup_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslideup_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vslideup.mask.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vslideup_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslideup_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslideup_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vslideup.nxv16i16.i64(<vscale x 16 x i16> poison, <vscale x 16 x i16> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vslideup_16xi16(__epi_16xi16 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslideup_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslideup_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vslideup.mask.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vslideup_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslideup_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslideup_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vslideup.nxv8i32.i64(<vscale x 8 x i32> poison, <vscale x 8 x i32> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vslideup_8xi32(__epi_8xi32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslideup_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslideup_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vslideup.mask.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vslideup_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslideup_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslideup_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vslideup.nxv4i64.i64(<vscale x 4 x i64> poison, <vscale x 4 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vslideup_4xi64(__epi_4xi64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslideup_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslideup_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vslideup.mask.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vslideup_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslideup_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslideup_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vslideup.nxv8f32.i64(<vscale x 8 x float> poison, <vscale x 8 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vslideup_8xf32(__epi_8xf32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslideup_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslideup_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vslideup.mask.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vslideup_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslideup_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vslideup_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vslideup.nxv4f64.i64(<vscale x 4 x double> poison, <vscale x 4 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vslideup_4xf64(__epi_4xf64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vslideup_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vslideup_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vslideup.mask.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vslideup_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vslideup_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsll_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vsll.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vsll_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsll_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsll_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vsll.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vsll_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsll_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsll_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vsll.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vsll_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsll_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsll_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vsll.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vsll_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsll_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsll_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vsll.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vsll_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsll_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsll_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vsll.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vsll_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsll_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsll_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vsll.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vsll_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsll_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsll_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vsll.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vsll_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsll_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsll_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vsll.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vsll_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsll_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsll_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vsll.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vsll_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsll_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsll_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vsll.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vsll_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsll_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsll_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vsll.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vsll_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsll_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsll_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vsll.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vsll_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsll_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsll_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vsll.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vsll_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsll_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsll_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vsll.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vsll_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsll_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsll_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vsll.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vsll_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsll_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsll_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vsll.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vsll_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsll_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsll_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vsll.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vsll_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsll_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsll_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vsll.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vsll_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsll_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsll_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vsll.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vsll_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsll_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsll_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vsll.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vsll_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsll_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsll_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vsll.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vsll_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsll_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsll_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vsll.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vsll_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsll_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsll_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vsll.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vsll_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsll_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsplat_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vrgather.vx.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vsplat_8xi8(__epi_8xi8 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsplat_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsplat_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vrgather.vx.mask.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vsplat_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsplat_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsplat_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vrgather.vx.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vsplat_4xi16(__epi_4xi16 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsplat_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsplat_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vrgather.vx.mask.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vsplat_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsplat_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsplat_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vrgather.vx.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vsplat_2xi32(__epi_2xi32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsplat_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsplat_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vrgather.vx.mask.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vsplat_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsplat_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsplat_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vrgather.vx.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vsplat_1xi64(__epi_1xi64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsplat_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsplat_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vrgather.vx.mask.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vsplat_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsplat_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsplat_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vrgather.vx.nxv2f32.i64(<vscale x 2 x float> undef, <vscale x 2 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vsplat_2xf32(__epi_2xf32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsplat_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsplat_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x float> @llvm.riscv.vrgather.vx.mask.nxv2f32.i64(<vscale x 2 x float> [[ARG_0:%.*]], <vscale x 2 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x float> [[TMP0]]
//
__epi_2xf32 test_vsplat_2xf32_mask(__epi_2xf32 arg_0, __epi_2xf32 arg_1, unsigned long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsplat_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsplat_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vrgather.vx.nxv1f64.i64(<vscale x 1 x double> undef, <vscale x 1 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vsplat_1xf64(__epi_1xf64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsplat_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsplat_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x double> @llvm.riscv.vrgather.vx.mask.nxv1f64.i64(<vscale x 1 x double> [[ARG_0:%.*]], <vscale x 1 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x double> [[TMP0]]
//
__epi_1xf64 test_vsplat_1xf64_mask(__epi_1xf64 arg_0, __epi_1xf64 arg_1, unsigned long int arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsplat_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsplat_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vrgather.vx.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vsplat_16xi8(__epi_16xi8 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsplat_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsplat_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vrgather.vx.mask.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vsplat_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsplat_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsplat_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vrgather.vx.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vsplat_8xi16(__epi_8xi16 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsplat_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsplat_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vrgather.vx.mask.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vsplat_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsplat_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsplat_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vrgather.vx.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vsplat_4xi32(__epi_4xi32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsplat_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsplat_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vrgather.vx.mask.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vsplat_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsplat_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsplat_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vrgather.vx.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vsplat_2xi64(__epi_2xi64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsplat_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsplat_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vrgather.vx.mask.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vsplat_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsplat_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsplat_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vrgather.vx.nxv4f32.i64(<vscale x 4 x float> undef, <vscale x 4 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vsplat_4xf32(__epi_4xf32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsplat_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsplat_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x float> @llvm.riscv.vrgather.vx.mask.nxv4f32.i64(<vscale x 4 x float> [[ARG_0:%.*]], <vscale x 4 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x float> [[TMP0]]
//
__epi_4xf32 test_vsplat_4xf32_mask(__epi_4xf32 arg_0, __epi_4xf32 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsplat_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsplat_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vrgather.vx.nxv2f64.i64(<vscale x 2 x double> undef, <vscale x 2 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vsplat_2xf64(__epi_2xf64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsplat_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsplat_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x double> @llvm.riscv.vrgather.vx.mask.nxv2f64.i64(<vscale x 2 x double> [[ARG_0:%.*]], <vscale x 2 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x double> [[TMP0]]
//
__epi_2xf64 test_vsplat_2xf64_mask(__epi_2xf64 arg_0, __epi_2xf64 arg_1, unsigned long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsplat_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsplat_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vrgather.vx.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vsplat_32xi8(__epi_32xi8 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsplat_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsplat_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vrgather.vx.mask.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vsplat_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsplat_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsplat_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vrgather.vx.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vsplat_16xi16(__epi_16xi16 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsplat_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsplat_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vrgather.vx.mask.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vsplat_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsplat_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsplat_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vrgather.vx.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vsplat_8xi32(__epi_8xi32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsplat_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsplat_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vrgather.vx.mask.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vsplat_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsplat_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsplat_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vrgather.vx.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vsplat_4xi64(__epi_4xi64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsplat_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsplat_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vrgather.vx.mask.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vsplat_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsplat_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsplat_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vrgather.vx.nxv8f32.i64(<vscale x 8 x float> undef, <vscale x 8 x float> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vsplat_8xf32(__epi_8xf32 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsplat_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsplat_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x float> @llvm.riscv.vrgather.vx.mask.nxv8f32.i64(<vscale x 8 x float> [[ARG_0:%.*]], <vscale x 8 x float> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x float> [[TMP0]]
//
__epi_8xf32 test_vsplat_8xf32_mask(__epi_8xf32 arg_0, __epi_8xf32 arg_1, unsigned long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsplat_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsplat_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vrgather.vx.nxv4f64.i64(<vscale x 4 x double> undef, <vscale x 4 x double> [[ARG_0:%.*]], i64 [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vsplat_4xf64(__epi_4xf64 arg_0, unsigned long int arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsplat_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsplat_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x double> @llvm.riscv.vrgather.vx.mask.nxv4f64.i64(<vscale x 4 x double> [[ARG_0:%.*]], <vscale x 4 x double> [[ARG_1:%.*]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x double> [[TMP0]]
//
__epi_4xf64 test_vsplat_4xf64_mask(__epi_4xf64 arg_0, __epi_4xf64 arg_1, unsigned long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsplat_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsra_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vsra.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vsra_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsra_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsra_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vsra.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vsra_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsra_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsra_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vsra.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vsra_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsra_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsra_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vsra.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vsra_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsra_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsra_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vsra.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vsra_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsra_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsra_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vsra.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vsra_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsra_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsra_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vsra.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vsra_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsra_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsra_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vsra.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vsra_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsra_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsra_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vsra.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vsra_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsra_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsra_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vsra.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vsra_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsra_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsra_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vsra.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vsra_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsra_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsra_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vsra.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vsra_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsra_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsra_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vsra.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vsra_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsra_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsra_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vsra.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vsra_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsra_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsra_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vsra.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vsra_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsra_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsra_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vsra.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vsra_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsra_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsra_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vsra.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vsra_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsra_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsra_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vsra.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vsra_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsra_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsra_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vsra.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vsra_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsra_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsra_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vsra.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vsra_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsra_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsra_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vsra.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vsra_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsra_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsra_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vsra.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vsra_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsra_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsra_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vsra.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vsra_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsra_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsra_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vsra.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vsra_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsra_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsrl_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vsrl.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vsrl_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsrl_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsrl_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vsrl.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vsrl_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsrl_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsrl_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vsrl.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vsrl_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsrl_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsrl_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vsrl.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vsrl_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsrl_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsrl_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vsrl.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vsrl_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsrl_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsrl_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vsrl.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vsrl_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsrl_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsrl_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vsrl.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vsrl_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsrl_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsrl_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vsrl.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vsrl_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsrl_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsrl_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vsrl.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vsrl_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsrl_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsrl_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vsrl.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vsrl_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsrl_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsrl_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vsrl.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vsrl_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsrl_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsrl_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vsrl.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vsrl_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsrl_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsrl_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vsrl.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vsrl_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsrl_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsrl_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vsrl.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vsrl_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsrl_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsrl_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vsrl.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vsrl_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsrl_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsrl_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vsrl.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vsrl_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsrl_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsrl_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vsrl.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vsrl_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsrl_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsrl_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vsrl.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vsrl_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsrl_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsrl_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vsrl.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vsrl_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsrl_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsrl_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vsrl.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vsrl_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsrl_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsrl_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vsrl.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vsrl_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsrl_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsrl_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vsrl.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vsrl_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsrl_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsrl_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vsrl.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vsrl_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsrl_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsrl_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vsrl.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vsrl_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsrl_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv8i8.i64(<vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_8xi8(signed char*  arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv8i8.i64(<vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8>* [[TMP0]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_8xi8_mask(signed char*  arg_0, __epi_8xi8 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_8xi8_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv4i16.i64(<vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_4xi16(signed short int*  arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv4i16.i64(<vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16>* [[TMP0]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_4xi16_mask(signed short int*  arg_0, __epi_4xi16 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_4xi16_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv2i32.i64(<vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_2xi32(signed int*  arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv2i32.i64(<vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32>* [[TMP0]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_2xi32_mask(signed int*  arg_0, __epi_2xi32 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_2xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv1i64.i64(<vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_1xi64(signed long int*  arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv1i64.i64(<vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64>* [[TMP0]], <vscale x 1 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_1xi64_mask(signed long int*  arg_0, __epi_1xi64 arg_1, __epi_1xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_1xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 2 x float>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv2f32.i64(<vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_2xf32(float*  arg_0, __epi_2xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_2xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 2 x float>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv2f32.i64(<vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float>* [[TMP0]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_2xf32_mask(float*  arg_0, __epi_2xf32 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_2xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 1 x double>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv1f64.i64(<vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_1xf64(double*  arg_0, __epi_1xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_1xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 1 x double>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv1f64.i64(<vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double>* [[TMP0]], <vscale x 1 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_1xf64_mask(double*  arg_0, __epi_1xf64 arg_1, __epi_1xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_1xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv16i8.i64(<vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_16xi8(signed char*  arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv16i8.i64(<vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8>* [[TMP0]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_16xi8_mask(signed char*  arg_0, __epi_16xi8 arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_16xi8_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv8i16.i64(<vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_8xi16(signed short int*  arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv8i16.i64(<vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16>* [[TMP0]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_8xi16_mask(signed short int*  arg_0, __epi_8xi16 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_8xi16_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv4i32.i64(<vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_4xi32(signed int*  arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv4i32.i64(<vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32>* [[TMP0]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_4xi32_mask(signed int*  arg_0, __epi_4xi32 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_4xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv2i64.i64(<vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_2xi64(signed long int*  arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv2i64.i64(<vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64>* [[TMP0]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_2xi64_mask(signed long int*  arg_0, __epi_2xi64 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_2xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 4 x float>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv4f32.i64(<vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_4xf32(float*  arg_0, __epi_4xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_4xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 4 x float>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv4f32.i64(<vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float>* [[TMP0]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_4xf32_mask(float*  arg_0, __epi_4xf32 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_4xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 2 x double>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv2f64.i64(<vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_2xf64(double*  arg_0, __epi_2xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_2xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 2 x double>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv2f64.i64(<vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double>* [[TMP0]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_2xf64_mask(double*  arg_0, __epi_2xf64 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_2xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv32i8.i64(<vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_32xi8(signed char*  arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv32i8.i64(<vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8>* [[TMP0]], <vscale x 32 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_32xi8_mask(signed char*  arg_0, __epi_32xi8 arg_1, __epi_32xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_32xi8_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv16i16.i64(<vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_16xi16(signed short int*  arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv16i16.i64(<vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16>* [[TMP0]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_16xi16_mask(signed short int*  arg_0, __epi_16xi16 arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_16xi16_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv8i32.i64(<vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_8xi32(signed int*  arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv8i32.i64(<vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32>* [[TMP0]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_8xi32_mask(signed int*  arg_0, __epi_8xi32 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_8xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv4i64.i64(<vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_4xi64(signed long int*  arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv4i64.i64(<vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64>* [[TMP0]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_4xi64_mask(signed long int*  arg_0, __epi_4xi64 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_4xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 8 x float>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv8f32.i64(<vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_8xf32(float*  arg_0, __epi_8xf32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_8xf32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 8 x float>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv8f32.i64(<vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float>* [[TMP0]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_8xf32_mask(float*  arg_0, __epi_8xf32 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_8xf32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 4 x double>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv4f64.i64(<vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_4xf64(double*  arg_0, __epi_4xf64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_4xf64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 4 x double>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv4f64.i64(<vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double>* [[TMP0]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_4xf64_mask(double*  arg_0, __epi_4xf64 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_4xf64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8>* [[TMP0]], <vscale x 8 x i8> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_8xi8(signed char*  arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_8xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8>* [[TMP0]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_8xi8_mask(signed char*  arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16>* [[TMP0]], <vscale x 4 x i16> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_4xi16(signed short int*  arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_4xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16>* [[TMP0]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_4xi16_mask(signed short int*  arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32>* [[TMP0]], <vscale x 2 x i32> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_2xi32(signed int*  arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_2xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32>* [[TMP0]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_2xi32_mask(signed int*  arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64>* [[TMP0]], <vscale x 1 x i64> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_1xi64(signed long int*  arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_1xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64>* [[TMP0]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_1xi64_mask(signed long int*  arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 2 x float>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv2f32.nxv2i32.i64(<vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float>* [[TMP0]], <vscale x 2 x i32> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_2xf32(float*  arg_0, __epi_2xf32 arg_1, __epi_2xi32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_2xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 2 x float>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv2f32.nxv2i32.i64(<vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float>* [[TMP0]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_2xf32_mask(float*  arg_0, __epi_2xf32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 1 x double>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv1f64.nxv1i64.i64(<vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double>* [[TMP0]], <vscale x 1 x i64> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_1xf64(double*  arg_0, __epi_1xf64 arg_1, __epi_1xi64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_1xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 1 x double>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv1f64.nxv1i64.i64(<vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double>* [[TMP0]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_1xf64_mask(double*  arg_0, __epi_1xf64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8>* [[TMP0]], <vscale x 16 x i8> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_16xi8(signed char*  arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_16xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8>* [[TMP0]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_16xi8_mask(signed char*  arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16>* [[TMP0]], <vscale x 8 x i16> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_8xi16(signed short int*  arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_8xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16>* [[TMP0]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_8xi16_mask(signed short int*  arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32>* [[TMP0]], <vscale x 4 x i32> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_4xi32(signed int*  arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_4xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32>* [[TMP0]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_4xi32_mask(signed int*  arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64>* [[TMP0]], <vscale x 2 x i64> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_2xi64(signed long int*  arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_2xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64>* [[TMP0]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_2xi64_mask(signed long int*  arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 4 x float>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv4f32.nxv4i32.i64(<vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float>* [[TMP0]], <vscale x 4 x i32> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_4xf32(float*  arg_0, __epi_4xf32 arg_1, __epi_4xi32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_4xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 4 x float>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv4f32.nxv4i32.i64(<vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float>* [[TMP0]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_4xf32_mask(float*  arg_0, __epi_4xf32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 2 x double>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv2f64.nxv2i64.i64(<vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double>* [[TMP0]], <vscale x 2 x i64> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_2xf64(double*  arg_0, __epi_2xf64 arg_1, __epi_2xi64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_2xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 2 x double>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv2f64.nxv2i64.i64(<vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double>* [[TMP0]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_2xf64_mask(double*  arg_0, __epi_2xf64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8>* [[TMP0]], <vscale x 32 x i8> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_32xi8(signed char*  arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_32xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8>* [[TMP0]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_32xi8_mask(signed char*  arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16>* [[TMP0]], <vscale x 16 x i16> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_16xi16(signed short int*  arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_16xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16>* [[TMP0]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_16xi16_mask(signed short int*  arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32>* [[TMP0]], <vscale x 8 x i32> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_8xi32(signed int*  arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_8xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32>* [[TMP0]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_8xi32_mask(signed int*  arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64>* [[TMP0]], <vscale x 4 x i64> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_4xi64(signed long int*  arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_4xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64>* [[TMP0]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_4xi64_mask(signed long int*  arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 8 x float>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv8f32.nxv8i32.i64(<vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float>* [[TMP0]], <vscale x 8 x i32> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_8xf32(float*  arg_0, __epi_8xf32 arg_1, __epi_8xi32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_8xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 8 x float>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv8f32.nxv8i32.i64(<vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float>* [[TMP0]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_8xf32_mask(float*  arg_0, __epi_8xf32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 4 x double>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv4f64.nxv4i64.i64(<vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double>* [[TMP0]], <vscale x 4 x i64> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_4xf64(double*  arg_0, __epi_4xf64 arg_1, __epi_4xi64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_4xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 4 x double>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv4f64.nxv4i64.i64(<vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double>* [[TMP0]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_4xf64_mask(double*  arg_0, __epi_4xf64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8>* [[TMP0]], <vscale x 8 x i8> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_8xi8(unsigned char*  arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_unsigned_8xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8>* [[TMP0]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_8xi8_mask(unsigned char*  arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_unsigned_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16>* [[TMP0]], <vscale x 4 x i16> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_4xi16(unsigned short int*  arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_unsigned_4xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16>* [[TMP0]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_4xi16_mask(unsigned short int*  arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_unsigned_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32>* [[TMP0]], <vscale x 2 x i32> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_2xi32(unsigned int*  arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_unsigned_2xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32>* [[TMP0]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_2xi32_mask(unsigned int*  arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_unsigned_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64>* [[TMP0]], <vscale x 1 x i64> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_1xi64(unsigned long int*  arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_unsigned_1xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64>* [[TMP0]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_1xi64_mask(unsigned long int*  arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_unsigned_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8>* [[TMP0]], <vscale x 16 x i8> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_16xi8(unsigned char*  arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_unsigned_16xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8>* [[TMP0]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_16xi8_mask(unsigned char*  arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_unsigned_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16>* [[TMP0]], <vscale x 8 x i16> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_8xi16(unsigned short int*  arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_unsigned_8xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16>* [[TMP0]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_8xi16_mask(unsigned short int*  arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_unsigned_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32>* [[TMP0]], <vscale x 4 x i32> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_4xi32(unsigned int*  arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_unsigned_4xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32>* [[TMP0]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_4xi32_mask(unsigned int*  arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_unsigned_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64>* [[TMP0]], <vscale x 2 x i64> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_2xi64(unsigned long int*  arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_unsigned_2xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64>* [[TMP0]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_2xi64_mask(unsigned long int*  arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_unsigned_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8>* [[TMP0]], <vscale x 32 x i8> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_32xi8(unsigned char*  arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_unsigned_32xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8>* [[TMP0]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_32xi8_mask(unsigned char*  arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_unsigned_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16>* [[TMP0]], <vscale x 16 x i16> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_16xi16(unsigned short int*  arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_unsigned_16xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16>* [[TMP0]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_16xi16_mask(unsigned short int*  arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_unsigned_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32>* [[TMP0]], <vscale x 8 x i32> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_8xi32(unsigned int*  arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_unsigned_8xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32>* [[TMP0]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_8xi32_mask(unsigned int*  arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_unsigned_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64>* [[TMP0]], <vscale x 4 x i64> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_4xi64(unsigned long int*  arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_indexed_unsigned_4xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_indexed_unsigned_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsuxei.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64>* [[TMP0]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_indexed_unsigned_4xi64_mask(unsigned long int*  arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_indexed_unsigned_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv8i8.i64(<vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_8xi8(signed char*  arg_0, __epi_8xi8 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_8xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv8i8.i64(<vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_8xi8_mask(signed char*  arg_0, __epi_8xi8 arg_1, signed long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv4i16.i64(<vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_4xi16(signed short int*  arg_0, __epi_4xi16 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_4xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv4i16.i64(<vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_4xi16_mask(signed short int*  arg_0, __epi_4xi16 arg_1, signed long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv2i32.i64(<vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_2xi32(signed int*  arg_0, __epi_2xi32 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_2xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv2i32.i64(<vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_2xi32_mask(signed int*  arg_0, __epi_2xi32 arg_1, signed long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv1i64.i64(<vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_1xi64(signed long int*  arg_0, __epi_1xi64 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_1xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv1i64.i64(<vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_1xi64_mask(signed long int*  arg_0, __epi_1xi64 arg_1, signed long int arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_2xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 2 x float>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv2f32.i64(<vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_2xf32(float*  arg_0, __epi_2xf32 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_2xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_2xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 2 x float>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv2f32.i64(<vscale x 2 x float> [[ARG_1:%.*]], <vscale x 2 x float>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_2xf32_mask(float*  arg_0, __epi_2xf32 arg_1, signed long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_2xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_1xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 1 x double>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv1f64.i64(<vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_1xf64(double*  arg_0, __epi_1xf64 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_1xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_1xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 1 x double>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv1f64.i64(<vscale x 1 x double> [[ARG_1:%.*]], <vscale x 1 x double>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_1xf64_mask(double*  arg_0, __epi_1xf64 arg_1, signed long int arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_1xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv16i8.i64(<vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_16xi8(signed char*  arg_0, __epi_16xi8 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_16xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv16i8.i64(<vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_16xi8_mask(signed char*  arg_0, __epi_16xi8 arg_1, signed long int arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv8i16.i64(<vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_8xi16(signed short int*  arg_0, __epi_8xi16 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_8xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv8i16.i64(<vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_8xi16_mask(signed short int*  arg_0, __epi_8xi16 arg_1, signed long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv4i32.i64(<vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_4xi32(signed int*  arg_0, __epi_4xi32 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_4xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv4i32.i64(<vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_4xi32_mask(signed int*  arg_0, __epi_4xi32 arg_1, signed long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv2i64.i64(<vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_2xi64(signed long int*  arg_0, __epi_2xi64 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_2xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv2i64.i64(<vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_2xi64_mask(signed long int*  arg_0, __epi_2xi64 arg_1, signed long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_4xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 4 x float>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv4f32.i64(<vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_4xf32(float*  arg_0, __epi_4xf32 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_4xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_4xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 4 x float>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv4f32.i64(<vscale x 4 x float> [[ARG_1:%.*]], <vscale x 4 x float>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_4xf32_mask(float*  arg_0, __epi_4xf32 arg_1, signed long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_4xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_2xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 2 x double>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv2f64.i64(<vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_2xf64(double*  arg_0, __epi_2xf64 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_2xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_2xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 2 x double>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv2f64.i64(<vscale x 2 x double> [[ARG_1:%.*]], <vscale x 2 x double>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_2xf64_mask(double*  arg_0, __epi_2xf64 arg_1, signed long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_2xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv32i8.i64(<vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_32xi8(signed char*  arg_0, __epi_32xi8 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_32xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv32i8.i64(<vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_32xi8_mask(signed char*  arg_0, __epi_32xi8 arg_1, signed long int arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv16i16.i64(<vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_16xi16(signed short int*  arg_0, __epi_16xi16 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_16xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv16i16.i64(<vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_16xi16_mask(signed short int*  arg_0, __epi_16xi16 arg_1, signed long int arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv8i32.i64(<vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_8xi32(signed int*  arg_0, __epi_8xi32 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_8xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv8i32.i64(<vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_8xi32_mask(signed int*  arg_0, __epi_8xi32 arg_1, signed long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv4i64.i64(<vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_4xi64(signed long int*  arg_0, __epi_4xi64 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_4xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv4i64.i64(<vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_4xi64_mask(signed long int*  arg_0, __epi_4xi64 arg_1, signed long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_8xf32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 8 x float>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv8f32.i64(<vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_8xf32(float*  arg_0, __epi_8xf32 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_8xf32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_8xf32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast float* [[ARG_0:%.*]] to <vscale x 8 x float>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv8f32.i64(<vscale x 8 x float> [[ARG_1:%.*]], <vscale x 8 x float>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_8xf32_mask(float*  arg_0, __epi_8xf32 arg_1, signed long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_8xf32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_4xf64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 4 x double>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv4f64.i64(<vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_4xf64(double*  arg_0, __epi_4xf64 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_4xf64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_4xf64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast double* [[ARG_0:%.*]] to <vscale x 4 x double>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv4f64.i64(<vscale x 4 x double> [[ARG_1:%.*]], <vscale x 4 x double>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_4xf64_mask(double*  arg_0, __epi_4xf64 arg_1, signed long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_4xf64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv8i8.i64(<vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_8xi8(unsigned char*  arg_0, __epi_8xi8 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_unsigned_8xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv8i8.i64(<vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_8xi8_mask(unsigned char*  arg_0, __epi_8xi8 arg_1, signed long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_unsigned_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv4i16.i64(<vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_4xi16(unsigned short int*  arg_0, __epi_4xi16 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_unsigned_4xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv4i16.i64(<vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_4xi16_mask(unsigned short int*  arg_0, __epi_4xi16 arg_1, signed long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_unsigned_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv2i32.i64(<vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_2xi32(unsigned int*  arg_0, __epi_2xi32 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_unsigned_2xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv2i32.i64(<vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_2xi32_mask(unsigned int*  arg_0, __epi_2xi32 arg_1, signed long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_unsigned_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv1i64.i64(<vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_1xi64(unsigned long int*  arg_0, __epi_1xi64 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_unsigned_1xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv1i64.i64(<vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_1xi64_mask(unsigned long int*  arg_0, __epi_1xi64 arg_1, signed long int arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_unsigned_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv16i8.i64(<vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_16xi8(unsigned char*  arg_0, __epi_16xi8 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_unsigned_16xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv16i8.i64(<vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_16xi8_mask(unsigned char*  arg_0, __epi_16xi8 arg_1, signed long int arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_unsigned_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv8i16.i64(<vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_8xi16(unsigned short int*  arg_0, __epi_8xi16 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_unsigned_8xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv8i16.i64(<vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_8xi16_mask(unsigned short int*  arg_0, __epi_8xi16 arg_1, signed long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_unsigned_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv4i32.i64(<vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_4xi32(unsigned int*  arg_0, __epi_4xi32 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_unsigned_4xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv4i32.i64(<vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_4xi32_mask(unsigned int*  arg_0, __epi_4xi32 arg_1, signed long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_unsigned_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv2i64.i64(<vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_2xi64(unsigned long int*  arg_0, __epi_2xi64 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_unsigned_2xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv2i64.i64(<vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_2xi64_mask(unsigned long int*  arg_0, __epi_2xi64 arg_1, signed long int arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_unsigned_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv32i8.i64(<vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_32xi8(unsigned char*  arg_0, __epi_32xi8 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_unsigned_32xi8(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv32i8.i64(<vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_32xi8_mask(unsigned char*  arg_0, __epi_32xi8 arg_1, signed long int arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_unsigned_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv16i16.i64(<vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_16xi16(unsigned short int*  arg_0, __epi_16xi16 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_unsigned_16xi16(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv16i16.i64(<vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_16xi16_mask(unsigned short int*  arg_0, __epi_16xi16 arg_1, signed long int arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_unsigned_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv8i32.i64(<vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_8xi32(unsigned int*  arg_0, __epi_8xi32 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_unsigned_8xi32(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv8i32.i64(<vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_8xi32_mask(unsigned int*  arg_0, __epi_8xi32 arg_1, signed long int arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_unsigned_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.nxv4i64.i64(<vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64>* [[TMP0]], i64 [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_4xi64(unsigned long int*  arg_0, __epi_4xi64 arg_1, signed long int arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_strided_unsigned_4xi64(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_strided_unsigned_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vsse.mask.nxv4i64.i64(<vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64>* [[TMP0]], i64 [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_strided_unsigned_4xi64_mask(unsigned long int*  arg_0, __epi_4xi64 arg_1, signed long int arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vstore_strided_unsigned_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv8i8.i64(<vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_8xi8(unsigned char*  arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_unsigned_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 8 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv8i8.i64(<vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8>* [[TMP0]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_8xi8_mask(unsigned char*  arg_0, __epi_8xi8 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_unsigned_8xi8_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv4i16.i64(<vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_4xi16(unsigned short int*  arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_unsigned_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 4 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv4i16.i64(<vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16>* [[TMP0]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_4xi16_mask(unsigned short int*  arg_0, __epi_4xi16 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_unsigned_4xi16_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv2i32.i64(<vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_2xi32(unsigned int*  arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_unsigned_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 2 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv2i32.i64(<vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32>* [[TMP0]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_2xi32_mask(unsigned int*  arg_0, __epi_2xi32 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_unsigned_2xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv1i64.i64(<vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_1xi64(unsigned long int*  arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_unsigned_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 1 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv1i64.i64(<vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64>* [[TMP0]], <vscale x 1 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_1xi64_mask(unsigned long int*  arg_0, __epi_1xi64 arg_1, __epi_1xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_unsigned_1xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv16i8.i64(<vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_16xi8(unsigned char*  arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_unsigned_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 16 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv16i8.i64(<vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8>* [[TMP0]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_16xi8_mask(unsigned char*  arg_0, __epi_16xi8 arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_unsigned_16xi8_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv8i16.i64(<vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_8xi16(unsigned short int*  arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_unsigned_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 8 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv8i16.i64(<vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16>* [[TMP0]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_8xi16_mask(unsigned short int*  arg_0, __epi_8xi16 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_unsigned_8xi16_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv4i32.i64(<vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_4xi32(unsigned int*  arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_unsigned_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 4 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv4i32.i64(<vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32>* [[TMP0]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_4xi32_mask(unsigned int*  arg_0, __epi_4xi32 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_unsigned_4xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv2i64.i64(<vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_2xi64(unsigned long int*  arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_unsigned_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 2 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv2i64.i64(<vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64>* [[TMP0]], <vscale x 2 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_2xi64_mask(unsigned long int*  arg_0, __epi_2xi64 arg_1, __epi_2xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_unsigned_2xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv32i8.i64(<vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_32xi8(unsigned char*  arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_unsigned_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i8* [[ARG_0:%.*]] to <vscale x 32 x i8>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv32i8.i64(<vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8>* [[TMP0]], <vscale x 32 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_32xi8_mask(unsigned char*  arg_0, __epi_32xi8 arg_1, __epi_32xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_unsigned_32xi8_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv16i16.i64(<vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_16xi16(unsigned short int*  arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_unsigned_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i16* [[ARG_0:%.*]] to <vscale x 16 x i16>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv16i16.i64(<vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16>* [[TMP0]], <vscale x 16 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_16xi16_mask(unsigned short int*  arg_0, __epi_16xi16 arg_1, __epi_16xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_unsigned_16xi16_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv8i32.i64(<vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_8xi32(unsigned int*  arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_unsigned_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i32* [[ARG_0:%.*]] to <vscale x 8 x i32>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv8i32.i64(<vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32>* [[TMP0]], <vscale x 8 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_8xi32_mask(unsigned int*  arg_0, __epi_8xi32 arg_1, __epi_8xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_unsigned_8xi32_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.nxv4i64.i64(<vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64>* [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_4xi64(unsigned long int*  arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vstore_unsigned_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vstore_unsigned_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = bitcast i64* [[ARG_0:%.*]] to <vscale x 4 x i64>*
// CHECK-O2-NEXT:    tail call void @llvm.riscv.vse.mask.nxv4i64.i64(<vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64>* [[TMP0]], <vscale x 4 x i1> [[ARG_2:%.*]], i64 [[ARG_3:%.*]])
// CHECK-O2-NEXT:    ret void
//
void test_vstore_unsigned_4xi64_mask(unsigned long int*  arg_0, __epi_4xi64 arg_1, __epi_4xi1 arg_2, unsigned long int arg_3)
{
    return __builtin_epi_vstore_unsigned_4xi64_mask(arg_0, arg_1, arg_2, arg_3);
}

// CHECK-O2-LABEL: @test_vsub_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vsub.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vsub_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsub_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsub_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vsub.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vsub_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsub_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsub_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vsub.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vsub_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsub_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsub_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vsub.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vsub_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsub_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsub_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vsub.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vsub_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsub_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsub_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vsub.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vsub_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsub_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsub_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vsub.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vsub_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsub_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsub_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vsub.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vsub_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsub_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsub_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vsub.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vsub_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsub_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsub_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vsub.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vsub_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsub_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsub_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vsub.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vsub_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsub_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsub_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vsub.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vsub_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsub_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsub_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vsub.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vsub_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsub_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsub_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vsub.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vsub_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsub_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsub_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vsub.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vsub_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsub_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsub_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vsub.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vsub_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsub_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsub_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vsub.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vsub_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsub_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsub_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vsub.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vsub_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsub_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsub_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vsub.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vsub_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsub_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsub_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vsub.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vsub_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsub_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsub_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vsub.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vsub_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsub_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsub_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vsub.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vsub_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsub_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vsub_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vsub.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vsub_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vsub_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vsub_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vsub.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vsub_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vsub_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwadd_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vwadd.nxv8i16.nxv8i8.nxv8i8.i64(<vscale x 8 x i16> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vwadd_8xi16(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwadd_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwadd_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vwadd.mask.nxv8i16.nxv8i8.nxv8i8.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vwadd_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwadd_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwadd_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vwadd.nxv4i32.nxv4i16.nxv4i16.i64(<vscale x 4 x i32> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vwadd_4xi32(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwadd_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwadd_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vwadd.mask.nxv4i32.nxv4i16.nxv4i16.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vwadd_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwadd_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwadd_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vwadd.nxv2i64.nxv2i32.nxv2i32.i64(<vscale x 2 x i64> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vwadd_2xi64(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwadd_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwadd_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vwadd.mask.nxv2i64.nxv2i32.nxv2i32.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vwadd_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwadd_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwadd_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vwadd.nxv16i16.nxv16i8.nxv16i8.i64(<vscale x 16 x i16> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vwadd_16xi16(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwadd_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwadd_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vwadd.mask.nxv16i16.nxv16i8.nxv16i8.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vwadd_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwadd_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwadd_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vwadd.nxv8i32.nxv8i16.nxv8i16.i64(<vscale x 8 x i32> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vwadd_8xi32(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwadd_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwadd_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vwadd.mask.nxv8i32.nxv8i16.nxv8i16.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vwadd_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwadd_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwadd_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vwadd.nxv4i64.nxv4i32.nxv4i32.i64(<vscale x 4 x i64> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vwadd_4xi64(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwadd_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwadd_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vwadd.mask.nxv4i64.nxv4i32.nxv4i32.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vwadd_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwadd_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwadd_32xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i16> @llvm.riscv.vwadd.nxv32i16.nxv32i8.nxv32i8.i64(<vscale x 32 x i16> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP0]]
//
__epi_32xi16 test_vwadd_32xi16(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwadd_32xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwadd_32xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i16> @llvm.riscv.vwadd.mask.nxv32i16.nxv32i8.nxv32i8.i64(<vscale x 32 x i16> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP0]]
//
__epi_32xi16 test_vwadd_32xi16_mask(__epi_32xi16 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwadd_32xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwadd_16xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i32> @llvm.riscv.vwadd.nxv16i32.nxv16i16.nxv16i16.i64(<vscale x 16 x i32> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP0]]
//
__epi_16xi32 test_vwadd_16xi32(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwadd_16xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwadd_16xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i32> @llvm.riscv.vwadd.mask.nxv16i32.nxv16i16.nxv16i16.i64(<vscale x 16 x i32> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP0]]
//
__epi_16xi32 test_vwadd_16xi32_mask(__epi_16xi32 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwadd_16xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwadd_8xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vwadd.nxv8i64.nxv8i32.nxv8i32.i64(<vscale x 8 x i64> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vwadd_8xi64(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwadd_8xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwadd_8xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vwadd.mask.nxv8i64.nxv8i32.nxv8i32.i64(<vscale x 8 x i64> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vwadd_8xi64_mask(__epi_8xi64 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwadd_8xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwadd_w_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vwadd.w.nxv8i16.nxv8i8.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vwadd_w_8xi16(__epi_8xi16 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwadd_w_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwadd_w_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vwadd.w.mask.nxv8i16.nxv8i8.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vwadd_w_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwadd_w_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwadd_w_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vwadd.w.nxv4i32.nxv4i16.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vwadd_w_4xi32(__epi_4xi32 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwadd_w_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwadd_w_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vwadd.w.mask.nxv4i32.nxv4i16.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vwadd_w_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwadd_w_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwadd_w_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vwadd.w.nxv2i64.nxv2i32.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vwadd_w_2xi64(__epi_2xi64 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwadd_w_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwadd_w_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vwadd.w.mask.nxv2i64.nxv2i32.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vwadd_w_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwadd_w_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwadd_w_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vwadd.w.nxv16i16.nxv16i8.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vwadd_w_16xi16(__epi_16xi16 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwadd_w_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwadd_w_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vwadd.w.mask.nxv16i16.nxv16i8.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vwadd_w_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwadd_w_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwadd_w_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vwadd.w.nxv8i32.nxv8i16.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vwadd_w_8xi32(__epi_8xi32 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwadd_w_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwadd_w_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vwadd.w.mask.nxv8i32.nxv8i16.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vwadd_w_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwadd_w_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwadd_w_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vwadd.w.nxv4i64.nxv4i32.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vwadd_w_4xi64(__epi_4xi64 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwadd_w_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwadd_w_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vwadd.w.mask.nxv4i64.nxv4i32.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vwadd_w_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwadd_w_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwadd_w_32xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i16> @llvm.riscv.vwadd.w.nxv32i16.nxv32i8.i64(<vscale x 32 x i16> undef, <vscale x 32 x i16> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP0]]
//
__epi_32xi16 test_vwadd_w_32xi16(__epi_32xi16 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwadd_w_32xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwadd_w_32xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i16> @llvm.riscv.vwadd.w.mask.nxv32i16.nxv32i8.i64(<vscale x 32 x i16> [[ARG_0:%.*]], <vscale x 32 x i16> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP0]]
//
__epi_32xi16 test_vwadd_w_32xi16_mask(__epi_32xi16 arg_0, __epi_32xi16 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwadd_w_32xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwadd_w_16xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i32> @llvm.riscv.vwadd.w.nxv16i32.nxv16i16.i64(<vscale x 16 x i32> undef, <vscale x 16 x i32> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP0]]
//
__epi_16xi32 test_vwadd_w_16xi32(__epi_16xi32 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwadd_w_16xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwadd_w_16xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i32> @llvm.riscv.vwadd.w.mask.nxv16i32.nxv16i16.i64(<vscale x 16 x i32> [[ARG_0:%.*]], <vscale x 16 x i32> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP0]]
//
__epi_16xi32 test_vwadd_w_16xi32_mask(__epi_16xi32 arg_0, __epi_16xi32 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwadd_w_16xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwadd_w_8xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vwadd.w.nxv8i64.nxv8i32.i64(<vscale x 8 x i64> undef, <vscale x 8 x i64> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vwadd_w_8xi64(__epi_8xi64 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwadd_w_8xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwadd_w_8xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vwadd.w.mask.nxv8i64.nxv8i32.i64(<vscale x 8 x i64> [[ARG_0:%.*]], <vscale x 8 x i64> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vwadd_w_8xi64_mask(__epi_8xi64 arg_0, __epi_8xi64 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwadd_w_8xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwaddu_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vwaddu.nxv8i16.nxv8i8.nxv8i8.i64(<vscale x 8 x i16> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vwaddu_8xi16(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwaddu_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwaddu_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vwaddu.mask.nxv8i16.nxv8i8.nxv8i8.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vwaddu_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwaddu_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwaddu_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vwaddu.nxv4i32.nxv4i16.nxv4i16.i64(<vscale x 4 x i32> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vwaddu_4xi32(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwaddu_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwaddu_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vwaddu.mask.nxv4i32.nxv4i16.nxv4i16.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vwaddu_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwaddu_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwaddu_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vwaddu.nxv2i64.nxv2i32.nxv2i32.i64(<vscale x 2 x i64> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vwaddu_2xi64(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwaddu_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwaddu_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vwaddu.mask.nxv2i64.nxv2i32.nxv2i32.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vwaddu_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwaddu_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwaddu_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vwaddu.nxv16i16.nxv16i8.nxv16i8.i64(<vscale x 16 x i16> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vwaddu_16xi16(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwaddu_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwaddu_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vwaddu.mask.nxv16i16.nxv16i8.nxv16i8.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vwaddu_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwaddu_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwaddu_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vwaddu.nxv8i32.nxv8i16.nxv8i16.i64(<vscale x 8 x i32> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vwaddu_8xi32(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwaddu_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwaddu_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vwaddu.mask.nxv8i32.nxv8i16.nxv8i16.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vwaddu_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwaddu_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwaddu_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vwaddu.nxv4i64.nxv4i32.nxv4i32.i64(<vscale x 4 x i64> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vwaddu_4xi64(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwaddu_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwaddu_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vwaddu.mask.nxv4i64.nxv4i32.nxv4i32.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vwaddu_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwaddu_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwaddu_32xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i16> @llvm.riscv.vwaddu.nxv32i16.nxv32i8.nxv32i8.i64(<vscale x 32 x i16> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP0]]
//
__epi_32xi16 test_vwaddu_32xi16(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwaddu_32xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwaddu_32xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i16> @llvm.riscv.vwaddu.mask.nxv32i16.nxv32i8.nxv32i8.i64(<vscale x 32 x i16> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP0]]
//
__epi_32xi16 test_vwaddu_32xi16_mask(__epi_32xi16 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwaddu_32xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwaddu_16xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i32> @llvm.riscv.vwaddu.nxv16i32.nxv16i16.nxv16i16.i64(<vscale x 16 x i32> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP0]]
//
__epi_16xi32 test_vwaddu_16xi32(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwaddu_16xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwaddu_16xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i32> @llvm.riscv.vwaddu.mask.nxv16i32.nxv16i16.nxv16i16.i64(<vscale x 16 x i32> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP0]]
//
__epi_16xi32 test_vwaddu_16xi32_mask(__epi_16xi32 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwaddu_16xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwaddu_8xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vwaddu.nxv8i64.nxv8i32.nxv8i32.i64(<vscale x 8 x i64> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vwaddu_8xi64(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwaddu_8xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwaddu_8xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vwaddu.mask.nxv8i64.nxv8i32.nxv8i32.i64(<vscale x 8 x i64> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vwaddu_8xi64_mask(__epi_8xi64 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwaddu_8xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwaddu_w_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vwaddu.w.nxv8i16.nxv8i8.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vwaddu_w_8xi16(__epi_8xi16 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwaddu_w_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwaddu_w_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vwaddu.w.mask.nxv8i16.nxv8i8.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vwaddu_w_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwaddu_w_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwaddu_w_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vwaddu.w.nxv4i32.nxv4i16.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vwaddu_w_4xi32(__epi_4xi32 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwaddu_w_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwaddu_w_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vwaddu.w.mask.nxv4i32.nxv4i16.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vwaddu_w_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwaddu_w_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwaddu_w_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vwaddu.w.nxv2i64.nxv2i32.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vwaddu_w_2xi64(__epi_2xi64 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwaddu_w_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwaddu_w_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vwaddu.w.mask.nxv2i64.nxv2i32.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vwaddu_w_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwaddu_w_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwaddu_w_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vwaddu.w.nxv16i16.nxv16i8.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vwaddu_w_16xi16(__epi_16xi16 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwaddu_w_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwaddu_w_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vwaddu.w.mask.nxv16i16.nxv16i8.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vwaddu_w_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwaddu_w_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwaddu_w_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vwaddu.w.nxv8i32.nxv8i16.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vwaddu_w_8xi32(__epi_8xi32 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwaddu_w_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwaddu_w_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vwaddu.w.mask.nxv8i32.nxv8i16.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vwaddu_w_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwaddu_w_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwaddu_w_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vwaddu.w.nxv4i64.nxv4i32.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vwaddu_w_4xi64(__epi_4xi64 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwaddu_w_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwaddu_w_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vwaddu.w.mask.nxv4i64.nxv4i32.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vwaddu_w_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwaddu_w_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwaddu_w_32xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i16> @llvm.riscv.vwaddu.w.nxv32i16.nxv32i8.i64(<vscale x 32 x i16> undef, <vscale x 32 x i16> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP0]]
//
__epi_32xi16 test_vwaddu_w_32xi16(__epi_32xi16 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwaddu_w_32xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwaddu_w_32xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i16> @llvm.riscv.vwaddu.w.mask.nxv32i16.nxv32i8.i64(<vscale x 32 x i16> [[ARG_0:%.*]], <vscale x 32 x i16> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP0]]
//
__epi_32xi16 test_vwaddu_w_32xi16_mask(__epi_32xi16 arg_0, __epi_32xi16 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwaddu_w_32xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwaddu_w_16xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i32> @llvm.riscv.vwaddu.w.nxv16i32.nxv16i16.i64(<vscale x 16 x i32> undef, <vscale x 16 x i32> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP0]]
//
__epi_16xi32 test_vwaddu_w_16xi32(__epi_16xi32 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwaddu_w_16xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwaddu_w_16xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i32> @llvm.riscv.vwaddu.w.mask.nxv16i32.nxv16i16.i64(<vscale x 16 x i32> [[ARG_0:%.*]], <vscale x 16 x i32> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP0]]
//
__epi_16xi32 test_vwaddu_w_16xi32_mask(__epi_16xi32 arg_0, __epi_16xi32 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwaddu_w_16xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwaddu_w_8xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vwaddu.w.nxv8i64.nxv8i32.i64(<vscale x 8 x i64> undef, <vscale x 8 x i64> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vwaddu_w_8xi64(__epi_8xi64 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwaddu_w_8xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwaddu_w_8xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vwaddu.w.mask.nxv8i64.nxv8i32.i64(<vscale x 8 x i64> [[ARG_0:%.*]], <vscale x 8 x i64> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vwaddu_w_8xi64_mask(__epi_8xi64 arg_0, __epi_8xi64 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwaddu_w_8xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmul_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vwmul.nxv8i16.nxv8i8.nxv8i8.i64(<vscale x 8 x i16> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vwmul_8xi16(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmul_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmul_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vwmul.mask.nxv8i16.nxv8i8.nxv8i8.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vwmul_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmul_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmul_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vwmul.nxv4i32.nxv4i16.nxv4i16.i64(<vscale x 4 x i32> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vwmul_4xi32(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmul_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmul_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vwmul.mask.nxv4i32.nxv4i16.nxv4i16.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vwmul_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmul_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmul_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vwmul.nxv2i64.nxv2i32.nxv2i32.i64(<vscale x 2 x i64> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vwmul_2xi64(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmul_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmul_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vwmul.mask.nxv2i64.nxv2i32.nxv2i32.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vwmul_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmul_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmul_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vwmul.nxv16i16.nxv16i8.nxv16i8.i64(<vscale x 16 x i16> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vwmul_16xi16(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmul_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmul_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vwmul.mask.nxv16i16.nxv16i8.nxv16i8.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vwmul_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmul_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmul_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vwmul.nxv8i32.nxv8i16.nxv8i16.i64(<vscale x 8 x i32> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vwmul_8xi32(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmul_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmul_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vwmul.mask.nxv8i32.nxv8i16.nxv8i16.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vwmul_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmul_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmul_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vwmul.nxv4i64.nxv4i32.nxv4i32.i64(<vscale x 4 x i64> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vwmul_4xi64(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmul_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmul_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vwmul.mask.nxv4i64.nxv4i32.nxv4i32.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vwmul_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmul_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmul_32xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i16> @llvm.riscv.vwmul.nxv32i16.nxv32i8.nxv32i8.i64(<vscale x 32 x i16> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP0]]
//
__epi_32xi16 test_vwmul_32xi16(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmul_32xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmul_32xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i16> @llvm.riscv.vwmul.mask.nxv32i16.nxv32i8.nxv32i8.i64(<vscale x 32 x i16> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP0]]
//
__epi_32xi16 test_vwmul_32xi16_mask(__epi_32xi16 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmul_32xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmul_16xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i32> @llvm.riscv.vwmul.nxv16i32.nxv16i16.nxv16i16.i64(<vscale x 16 x i32> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP0]]
//
__epi_16xi32 test_vwmul_16xi32(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmul_16xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmul_16xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i32> @llvm.riscv.vwmul.mask.nxv16i32.nxv16i16.nxv16i16.i64(<vscale x 16 x i32> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP0]]
//
__epi_16xi32 test_vwmul_16xi32_mask(__epi_16xi32 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmul_16xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmul_8xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vwmul.nxv8i64.nxv8i32.nxv8i32.i64(<vscale x 8 x i64> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vwmul_8xi64(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmul_8xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmul_8xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vwmul.mask.nxv8i64.nxv8i32.nxv8i32.i64(<vscale x 8 x i64> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vwmul_8xi64_mask(__epi_8xi64 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmul_8xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmulsu_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vwmulsu.nxv8i16.nxv8i8.nxv8i8.i64(<vscale x 8 x i16> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vwmulsu_8xi16(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmulsu_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmulsu_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vwmulsu.mask.nxv8i16.nxv8i8.nxv8i8.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vwmulsu_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmulsu_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmulsu_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vwmulsu.nxv4i32.nxv4i16.nxv4i16.i64(<vscale x 4 x i32> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vwmulsu_4xi32(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmulsu_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmulsu_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vwmulsu.mask.nxv4i32.nxv4i16.nxv4i16.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vwmulsu_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmulsu_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmulsu_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vwmulsu.nxv2i64.nxv2i32.nxv2i32.i64(<vscale x 2 x i64> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vwmulsu_2xi64(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmulsu_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmulsu_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vwmulsu.mask.nxv2i64.nxv2i32.nxv2i32.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vwmulsu_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmulsu_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmulsu_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vwmulsu.nxv16i16.nxv16i8.nxv16i8.i64(<vscale x 16 x i16> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vwmulsu_16xi16(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmulsu_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmulsu_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vwmulsu.mask.nxv16i16.nxv16i8.nxv16i8.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vwmulsu_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmulsu_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmulsu_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vwmulsu.nxv8i32.nxv8i16.nxv8i16.i64(<vscale x 8 x i32> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vwmulsu_8xi32(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmulsu_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmulsu_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vwmulsu.mask.nxv8i32.nxv8i16.nxv8i16.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vwmulsu_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmulsu_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmulsu_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vwmulsu.nxv4i64.nxv4i32.nxv4i32.i64(<vscale x 4 x i64> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vwmulsu_4xi64(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmulsu_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmulsu_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vwmulsu.mask.nxv4i64.nxv4i32.nxv4i32.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vwmulsu_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmulsu_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmulsu_32xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i16> @llvm.riscv.vwmulsu.nxv32i16.nxv32i8.nxv32i8.i64(<vscale x 32 x i16> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP0]]
//
__epi_32xi16 test_vwmulsu_32xi16(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmulsu_32xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmulsu_32xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i16> @llvm.riscv.vwmulsu.mask.nxv32i16.nxv32i8.nxv32i8.i64(<vscale x 32 x i16> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP0]]
//
__epi_32xi16 test_vwmulsu_32xi16_mask(__epi_32xi16 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmulsu_32xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmulsu_16xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i32> @llvm.riscv.vwmulsu.nxv16i32.nxv16i16.nxv16i16.i64(<vscale x 16 x i32> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP0]]
//
__epi_16xi32 test_vwmulsu_16xi32(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmulsu_16xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmulsu_16xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i32> @llvm.riscv.vwmulsu.mask.nxv16i32.nxv16i16.nxv16i16.i64(<vscale x 16 x i32> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP0]]
//
__epi_16xi32 test_vwmulsu_16xi32_mask(__epi_16xi32 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmulsu_16xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmulsu_8xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vwmulsu.nxv8i64.nxv8i32.nxv8i32.i64(<vscale x 8 x i64> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vwmulsu_8xi64(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmulsu_8xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmulsu_8xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vwmulsu.mask.nxv8i64.nxv8i32.nxv8i32.i64(<vscale x 8 x i64> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vwmulsu_8xi64_mask(__epi_8xi64 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmulsu_8xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmulu_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vwmulu.nxv8i16.nxv8i8.nxv8i8.i64(<vscale x 8 x i16> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vwmulu_8xi16(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmulu_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmulu_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vwmulu.mask.nxv8i16.nxv8i8.nxv8i8.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vwmulu_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmulu_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmulu_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vwmulu.nxv4i32.nxv4i16.nxv4i16.i64(<vscale x 4 x i32> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vwmulu_4xi32(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmulu_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmulu_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vwmulu.mask.nxv4i32.nxv4i16.nxv4i16.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vwmulu_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmulu_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmulu_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vwmulu.nxv2i64.nxv2i32.nxv2i32.i64(<vscale x 2 x i64> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vwmulu_2xi64(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmulu_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmulu_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vwmulu.mask.nxv2i64.nxv2i32.nxv2i32.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vwmulu_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmulu_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmulu_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vwmulu.nxv16i16.nxv16i8.nxv16i8.i64(<vscale x 16 x i16> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vwmulu_16xi16(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmulu_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmulu_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vwmulu.mask.nxv16i16.nxv16i8.nxv16i8.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vwmulu_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmulu_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmulu_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vwmulu.nxv8i32.nxv8i16.nxv8i16.i64(<vscale x 8 x i32> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vwmulu_8xi32(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmulu_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmulu_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vwmulu.mask.nxv8i32.nxv8i16.nxv8i16.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vwmulu_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmulu_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmulu_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vwmulu.nxv4i64.nxv4i32.nxv4i32.i64(<vscale x 4 x i64> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vwmulu_4xi64(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmulu_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmulu_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vwmulu.mask.nxv4i64.nxv4i32.nxv4i32.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vwmulu_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmulu_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmulu_32xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i16> @llvm.riscv.vwmulu.nxv32i16.nxv32i8.nxv32i8.i64(<vscale x 32 x i16> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP0]]
//
__epi_32xi16 test_vwmulu_32xi16(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmulu_32xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmulu_32xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i16> @llvm.riscv.vwmulu.mask.nxv32i16.nxv32i8.nxv32i8.i64(<vscale x 32 x i16> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP0]]
//
__epi_32xi16 test_vwmulu_32xi16_mask(__epi_32xi16 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmulu_32xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmulu_16xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i32> @llvm.riscv.vwmulu.nxv16i32.nxv16i16.nxv16i16.i64(<vscale x 16 x i32> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP0]]
//
__epi_16xi32 test_vwmulu_16xi32(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmulu_16xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmulu_16xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i32> @llvm.riscv.vwmulu.mask.nxv16i32.nxv16i16.nxv16i16.i64(<vscale x 16 x i32> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP0]]
//
__epi_16xi32 test_vwmulu_16xi32_mask(__epi_16xi32 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmulu_16xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwmulu_8xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vwmulu.nxv8i64.nxv8i32.nxv8i32.i64(<vscale x 8 x i64> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vwmulu_8xi64(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwmulu_8xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwmulu_8xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vwmulu.mask.nxv8i64.nxv8i32.nxv8i32.i64(<vscale x 8 x i64> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vwmulu_8xi64_mask(__epi_8xi64 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwmulu_8xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwredsum_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vwredsum.nxv4i16.nxv8i8.i64(<vscale x 4 x i16> poison, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 4 x i16> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.nxv4i16(<vscale x 8 x i16> poison, <vscale x 4 x i16> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP2]]
//
__epi_8xi16 test_vwredsum_8xi16(__epi_8xi8 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwredsum_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwredsum_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv8i16(<vscale x 8 x i16> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vwredsum.mask.nxv4i16.nxv8i8.i64(<vscale x 4 x i16> [[TMP0]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 4 x i16> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x i16> @llvm.vector.insert.nxv8i16.nxv4i16(<vscale x 8 x i16> poison, <vscale x 4 x i16> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP3]]
//
__epi_8xi16 test_vwredsum_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi8 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwredsum_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwredsum_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vwredsum.nxv2i32.nxv4i16.i64(<vscale x 2 x i32> poison, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 2 x i32> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.nxv2i32(<vscale x 4 x i32> poison, <vscale x 2 x i32> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP2]]
//
__epi_4xi32 test_vwredsum_4xi32(__epi_4xi16 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwredsum_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwredsum_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv4i32(<vscale x 4 x i32> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vwredsum.mask.nxv2i32.nxv4i16.i64(<vscale x 2 x i32> [[TMP0]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 2 x i32> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x i32> @llvm.vector.insert.nxv4i32.nxv2i32(<vscale x 4 x i32> poison, <vscale x 2 x i32> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP3]]
//
__epi_4xi32 test_vwredsum_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi16 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwredsum_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwredsum_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vwredsum.nxv1i64.nxv2i32.i64(<vscale x 1 x i64> poison, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 1 x i64> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.nxv1i64(<vscale x 2 x i64> poison, <vscale x 1 x i64> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP2]]
//
__epi_2xi64 test_vwredsum_2xi64(__epi_2xi32 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwredsum_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwredsum_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv2i64(<vscale x 2 x i64> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vwredsum.mask.nxv1i64.nxv2i32.i64(<vscale x 1 x i64> [[TMP0]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 1 x i64> [[TMP1]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 2 x i64> @llvm.vector.insert.nxv2i64.nxv1i64(<vscale x 2 x i64> poison, <vscale x 1 x i64> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP3]]
//
__epi_2xi64 test_vwredsum_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi32 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwredsum_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwredsum_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vwredsum.nxv4i16.nxv16i8.i64(<vscale x 4 x i16> poison, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 4 x i16> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 16 x i16> @llvm.vector.insert.nxv16i16.nxv4i16(<vscale x 16 x i16> poison, <vscale x 4 x i16> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP2]]
//
__epi_16xi16 test_vwredsum_16xi16(__epi_16xi8 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwredsum_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwredsum_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv16i16(<vscale x 16 x i16> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vwredsum.mask.nxv4i16.nxv16i8.i64(<vscale x 4 x i16> [[TMP0]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 4 x i16> [[TMP1]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 16 x i16> @llvm.vector.insert.nxv16i16.nxv4i16(<vscale x 16 x i16> poison, <vscale x 4 x i16> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP3]]
//
__epi_16xi16 test_vwredsum_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi8 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwredsum_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwredsum_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vwredsum.nxv2i32.nxv8i16.i64(<vscale x 2 x i32> poison, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 2 x i32> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv2i32(<vscale x 8 x i32> poison, <vscale x 2 x i32> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP2]]
//
__epi_8xi32 test_vwredsum_8xi32(__epi_8xi16 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwredsum_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwredsum_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv8i32(<vscale x 8 x i32> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vwredsum.mask.nxv2i32.nxv8i16.i64(<vscale x 2 x i32> [[TMP0]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 2 x i32> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x i32> @llvm.vector.insert.nxv8i32.nxv2i32(<vscale x 8 x i32> poison, <vscale x 2 x i32> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP3]]
//
__epi_8xi32 test_vwredsum_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi16 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwredsum_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwredsum_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vwredsum.nxv1i64.nxv4i32.i64(<vscale x 1 x i64> poison, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 1 x i64> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i64> @llvm.vector.insert.nxv4i64.nxv1i64(<vscale x 4 x i64> poison, <vscale x 1 x i64> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP2]]
//
__epi_4xi64 test_vwredsum_4xi64(__epi_4xi32 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwredsum_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwredsum_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv4i64(<vscale x 4 x i64> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vwredsum.mask.nxv1i64.nxv4i32.i64(<vscale x 1 x i64> [[TMP0]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 1 x i64> [[TMP1]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 4 x i64> @llvm.vector.insert.nxv4i64.nxv1i64(<vscale x 4 x i64> poison, <vscale x 1 x i64> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP3]]
//
__epi_4xi64 test_vwredsum_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi32 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwredsum_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwredsum_32xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv32i16(<vscale x 32 x i16> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vwredsum.nxv4i16.nxv32i8.i64(<vscale x 4 x i16> poison, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 4 x i16> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 32 x i16> @llvm.vector.insert.nxv32i16.nxv4i16(<vscale x 32 x i16> poison, <vscale x 4 x i16> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP2]]
//
__epi_32xi16 test_vwredsum_32xi16(__epi_32xi8 arg_0, __epi_32xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwredsum_32xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwredsum_32xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv32i16(<vscale x 32 x i16> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 4 x i16> @llvm.vector.extract.nxv4i16.nxv32i16(<vscale x 32 x i16> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vwredsum.mask.nxv4i16.nxv32i8.i64(<vscale x 4 x i16> [[TMP0]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 4 x i16> [[TMP1]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 32 x i16> @llvm.vector.insert.nxv32i16.nxv4i16(<vscale x 32 x i16> poison, <vscale x 4 x i16> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP3]]
//
__epi_32xi16 test_vwredsum_32xi16_mask(__epi_32xi16 arg_0, __epi_32xi8 arg_1, __epi_32xi16 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwredsum_32xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwredsum_16xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv16i32(<vscale x 16 x i32> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vwredsum.nxv2i32.nxv16i16.i64(<vscale x 2 x i32> poison, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 2 x i32> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 16 x i32> @llvm.vector.insert.nxv16i32.nxv2i32(<vscale x 16 x i32> poison, <vscale x 2 x i32> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP2]]
//
__epi_16xi32 test_vwredsum_16xi32(__epi_16xi16 arg_0, __epi_16xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwredsum_16xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwredsum_16xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv16i32(<vscale x 16 x i32> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 2 x i32> @llvm.vector.extract.nxv2i32.nxv16i32(<vscale x 16 x i32> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vwredsum.mask.nxv2i32.nxv16i16.i64(<vscale x 2 x i32> [[TMP0]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 2 x i32> [[TMP1]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 16 x i32> @llvm.vector.insert.nxv16i32.nxv2i32(<vscale x 16 x i32> poison, <vscale x 2 x i32> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP3]]
//
__epi_16xi32 test_vwredsum_16xi32_mask(__epi_16xi32 arg_0, __epi_16xi16 arg_1, __epi_16xi32 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwredsum_16xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwredsum_8xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv8i64(<vscale x 8 x i64> [[ARG_1:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vwredsum.nxv1i64.nxv8i32.i64(<vscale x 1 x i64> poison, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 1 x i64> [[TMP0]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 8 x i64> @llvm.vector.insert.nxv8i64.nxv1i64(<vscale x 8 x i64> poison, <vscale x 1 x i64> [[TMP1]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP2]]
//
__epi_8xi64 test_vwredsum_8xi64(__epi_8xi32 arg_0, __epi_8xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwredsum_8xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwredsum_8xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv8i64(<vscale x 8 x i64> [[ARG_0:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP1:%.*]] = tail call <vscale x 1 x i64> @llvm.vector.extract.nxv1i64.nxv8i64(<vscale x 8 x i64> [[ARG_2:%.*]], i64 0)
// CHECK-O2-NEXT:    [[TMP2:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vwredsum.mask.nxv1i64.nxv8i32.i64(<vscale x 1 x i64> [[TMP0]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 1 x i64> [[TMP1]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]])
// CHECK-O2-NEXT:    [[TMP3:%.*]] = tail call <vscale x 8 x i64> @llvm.vector.insert.nxv8i64.nxv1i64(<vscale x 8 x i64> poison, <vscale x 1 x i64> [[TMP2]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP3]]
//
__epi_8xi64 test_vwredsum_8xi64_mask(__epi_8xi64 arg_0, __epi_8xi32 arg_1, __epi_8xi64 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwredsum_8xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsub_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vwsub.nxv8i16.nxv8i8.nxv8i8.i64(<vscale x 8 x i16> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vwsub_8xi16(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsub_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsub_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vwsub.mask.nxv8i16.nxv8i8.nxv8i8.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vwsub_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsub_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsub_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vwsub.nxv4i32.nxv4i16.nxv4i16.i64(<vscale x 4 x i32> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vwsub_4xi32(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsub_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsub_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vwsub.mask.nxv4i32.nxv4i16.nxv4i16.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vwsub_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsub_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsub_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vwsub.nxv2i64.nxv2i32.nxv2i32.i64(<vscale x 2 x i64> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vwsub_2xi64(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsub_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsub_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vwsub.mask.nxv2i64.nxv2i32.nxv2i32.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vwsub_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsub_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsub_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vwsub.nxv16i16.nxv16i8.nxv16i8.i64(<vscale x 16 x i16> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vwsub_16xi16(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsub_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsub_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vwsub.mask.nxv16i16.nxv16i8.nxv16i8.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vwsub_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsub_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsub_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vwsub.nxv8i32.nxv8i16.nxv8i16.i64(<vscale x 8 x i32> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vwsub_8xi32(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsub_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsub_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vwsub.mask.nxv8i32.nxv8i16.nxv8i16.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vwsub_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsub_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsub_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vwsub.nxv4i64.nxv4i32.nxv4i32.i64(<vscale x 4 x i64> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vwsub_4xi64(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsub_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsub_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vwsub.mask.nxv4i64.nxv4i32.nxv4i32.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vwsub_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsub_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsub_32xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i16> @llvm.riscv.vwsub.nxv32i16.nxv32i8.nxv32i8.i64(<vscale x 32 x i16> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP0]]
//
__epi_32xi16 test_vwsub_32xi16(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsub_32xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsub_32xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i16> @llvm.riscv.vwsub.mask.nxv32i16.nxv32i8.nxv32i8.i64(<vscale x 32 x i16> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP0]]
//
__epi_32xi16 test_vwsub_32xi16_mask(__epi_32xi16 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsub_32xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsub_16xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i32> @llvm.riscv.vwsub.nxv16i32.nxv16i16.nxv16i16.i64(<vscale x 16 x i32> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP0]]
//
__epi_16xi32 test_vwsub_16xi32(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsub_16xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsub_16xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i32> @llvm.riscv.vwsub.mask.nxv16i32.nxv16i16.nxv16i16.i64(<vscale x 16 x i32> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP0]]
//
__epi_16xi32 test_vwsub_16xi32_mask(__epi_16xi32 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsub_16xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsub_8xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vwsub.nxv8i64.nxv8i32.nxv8i32.i64(<vscale x 8 x i64> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vwsub_8xi64(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsub_8xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsub_8xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vwsub.mask.nxv8i64.nxv8i32.nxv8i32.i64(<vscale x 8 x i64> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vwsub_8xi64_mask(__epi_8xi64 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsub_8xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsub_w_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vwsub.w.nxv8i16.nxv8i8.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vwsub_w_8xi16(__epi_8xi16 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsub_w_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsub_w_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vwsub.w.mask.nxv8i16.nxv8i8.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vwsub_w_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsub_w_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsub_w_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vwsub.w.nxv4i32.nxv4i16.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vwsub_w_4xi32(__epi_4xi32 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsub_w_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsub_w_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vwsub.w.mask.nxv4i32.nxv4i16.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vwsub_w_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsub_w_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsub_w_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vwsub.w.nxv2i64.nxv2i32.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vwsub_w_2xi64(__epi_2xi64 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsub_w_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsub_w_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vwsub.w.mask.nxv2i64.nxv2i32.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vwsub_w_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsub_w_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsub_w_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vwsub.w.nxv16i16.nxv16i8.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vwsub_w_16xi16(__epi_16xi16 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsub_w_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsub_w_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vwsub.w.mask.nxv16i16.nxv16i8.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vwsub_w_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsub_w_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsub_w_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vwsub.w.nxv8i32.nxv8i16.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vwsub_w_8xi32(__epi_8xi32 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsub_w_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsub_w_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vwsub.w.mask.nxv8i32.nxv8i16.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vwsub_w_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsub_w_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsub_w_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vwsub.w.nxv4i64.nxv4i32.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vwsub_w_4xi64(__epi_4xi64 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsub_w_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsub_w_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vwsub.w.mask.nxv4i64.nxv4i32.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vwsub_w_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsub_w_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsub_w_32xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i16> @llvm.riscv.vwsub.w.nxv32i16.nxv32i8.i64(<vscale x 32 x i16> undef, <vscale x 32 x i16> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP0]]
//
__epi_32xi16 test_vwsub_w_32xi16(__epi_32xi16 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsub_w_32xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsub_w_32xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i16> @llvm.riscv.vwsub.w.mask.nxv32i16.nxv32i8.i64(<vscale x 32 x i16> [[ARG_0:%.*]], <vscale x 32 x i16> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP0]]
//
__epi_32xi16 test_vwsub_w_32xi16_mask(__epi_32xi16 arg_0, __epi_32xi16 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsub_w_32xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsub_w_16xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i32> @llvm.riscv.vwsub.w.nxv16i32.nxv16i16.i64(<vscale x 16 x i32> undef, <vscale x 16 x i32> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP0]]
//
__epi_16xi32 test_vwsub_w_16xi32(__epi_16xi32 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsub_w_16xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsub_w_16xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i32> @llvm.riscv.vwsub.w.mask.nxv16i32.nxv16i16.i64(<vscale x 16 x i32> [[ARG_0:%.*]], <vscale x 16 x i32> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP0]]
//
__epi_16xi32 test_vwsub_w_16xi32_mask(__epi_16xi32 arg_0, __epi_16xi32 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsub_w_16xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsub_w_8xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vwsub.w.nxv8i64.nxv8i32.i64(<vscale x 8 x i64> undef, <vscale x 8 x i64> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vwsub_w_8xi64(__epi_8xi64 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsub_w_8xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsub_w_8xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vwsub.w.mask.nxv8i64.nxv8i32.i64(<vscale x 8 x i64> [[ARG_0:%.*]], <vscale x 8 x i64> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vwsub_w_8xi64_mask(__epi_8xi64 arg_0, __epi_8xi64 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsub_w_8xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsubu_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vwsubu.nxv8i16.nxv8i8.nxv8i8.i64(<vscale x 8 x i16> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vwsubu_8xi16(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsubu_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsubu_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vwsubu.mask.nxv8i16.nxv8i8.nxv8i8.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vwsubu_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsubu_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsubu_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vwsubu.nxv4i32.nxv4i16.nxv4i16.i64(<vscale x 4 x i32> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vwsubu_4xi32(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsubu_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsubu_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vwsubu.mask.nxv4i32.nxv4i16.nxv4i16.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vwsubu_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsubu_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsubu_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vwsubu.nxv2i64.nxv2i32.nxv2i32.i64(<vscale x 2 x i64> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vwsubu_2xi64(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsubu_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsubu_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vwsubu.mask.nxv2i64.nxv2i32.nxv2i32.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vwsubu_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsubu_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsubu_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vwsubu.nxv16i16.nxv16i8.nxv16i8.i64(<vscale x 16 x i16> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vwsubu_16xi16(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsubu_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsubu_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vwsubu.mask.nxv16i16.nxv16i8.nxv16i8.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vwsubu_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsubu_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsubu_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vwsubu.nxv8i32.nxv8i16.nxv8i16.i64(<vscale x 8 x i32> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vwsubu_8xi32(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsubu_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsubu_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vwsubu.mask.nxv8i32.nxv8i16.nxv8i16.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vwsubu_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsubu_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsubu_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vwsubu.nxv4i64.nxv4i32.nxv4i32.i64(<vscale x 4 x i64> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vwsubu_4xi64(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsubu_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsubu_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vwsubu.mask.nxv4i64.nxv4i32.nxv4i32.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vwsubu_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsubu_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsubu_32xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i16> @llvm.riscv.vwsubu.nxv32i16.nxv32i8.nxv32i8.i64(<vscale x 32 x i16> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP0]]
//
__epi_32xi16 test_vwsubu_32xi16(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsubu_32xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsubu_32xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i16> @llvm.riscv.vwsubu.mask.nxv32i16.nxv32i8.nxv32i8.i64(<vscale x 32 x i16> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP0]]
//
__epi_32xi16 test_vwsubu_32xi16_mask(__epi_32xi16 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsubu_32xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsubu_16xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i32> @llvm.riscv.vwsubu.nxv16i32.nxv16i16.nxv16i16.i64(<vscale x 16 x i32> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP0]]
//
__epi_16xi32 test_vwsubu_16xi32(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsubu_16xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsubu_16xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i32> @llvm.riscv.vwsubu.mask.nxv16i32.nxv16i16.nxv16i16.i64(<vscale x 16 x i32> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP0]]
//
__epi_16xi32 test_vwsubu_16xi32_mask(__epi_16xi32 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsubu_16xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsubu_8xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vwsubu.nxv8i64.nxv8i32.nxv8i32.i64(<vscale x 8 x i64> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vwsubu_8xi64(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsubu_8xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsubu_8xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vwsubu.mask.nxv8i64.nxv8i32.nxv8i32.i64(<vscale x 8 x i64> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vwsubu_8xi64_mask(__epi_8xi64 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsubu_8xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsubu_w_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vwsubu.w.nxv8i16.nxv8i8.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vwsubu_w_8xi16(__epi_8xi16 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsubu_w_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsubu_w_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vwsubu.w.mask.nxv8i16.nxv8i8.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vwsubu_w_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsubu_w_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsubu_w_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vwsubu.w.nxv4i32.nxv4i16.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vwsubu_w_4xi32(__epi_4xi32 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsubu_w_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsubu_w_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vwsubu.w.mask.nxv4i32.nxv4i16.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vwsubu_w_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsubu_w_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsubu_w_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vwsubu.w.nxv2i64.nxv2i32.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vwsubu_w_2xi64(__epi_2xi64 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsubu_w_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsubu_w_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vwsubu.w.mask.nxv2i64.nxv2i32.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vwsubu_w_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsubu_w_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsubu_w_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vwsubu.w.nxv16i16.nxv16i8.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vwsubu_w_16xi16(__epi_16xi16 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsubu_w_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsubu_w_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vwsubu.w.mask.nxv16i16.nxv16i8.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vwsubu_w_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsubu_w_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsubu_w_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vwsubu.w.nxv8i32.nxv8i16.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vwsubu_w_8xi32(__epi_8xi32 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsubu_w_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsubu_w_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vwsubu.w.mask.nxv8i32.nxv8i16.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vwsubu_w_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsubu_w_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsubu_w_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vwsubu.w.nxv4i64.nxv4i32.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vwsubu_w_4xi64(__epi_4xi64 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsubu_w_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsubu_w_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vwsubu.w.mask.nxv4i64.nxv4i32.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vwsubu_w_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsubu_w_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsubu_w_32xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i16> @llvm.riscv.vwsubu.w.nxv32i16.nxv32i8.i64(<vscale x 32 x i16> undef, <vscale x 32 x i16> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP0]]
//
__epi_32xi16 test_vwsubu_w_32xi16(__epi_32xi16 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsubu_w_32xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsubu_w_32xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i16> @llvm.riscv.vwsubu.w.mask.nxv32i16.nxv32i8.i64(<vscale x 32 x i16> [[ARG_0:%.*]], <vscale x 32 x i16> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i16> [[TMP0]]
//
__epi_32xi16 test_vwsubu_w_32xi16_mask(__epi_32xi16 arg_0, __epi_32xi16 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsubu_w_32xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsubu_w_16xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i32> @llvm.riscv.vwsubu.w.nxv16i32.nxv16i16.i64(<vscale x 16 x i32> undef, <vscale x 16 x i32> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP0]]
//
__epi_16xi32 test_vwsubu_w_16xi32(__epi_16xi32 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsubu_w_16xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsubu_w_16xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i32> @llvm.riscv.vwsubu.w.mask.nxv16i32.nxv16i16.i64(<vscale x 16 x i32> [[ARG_0:%.*]], <vscale x 16 x i32> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i32> [[TMP0]]
//
__epi_16xi32 test_vwsubu_w_16xi32_mask(__epi_16xi32 arg_0, __epi_16xi32 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsubu_w_16xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vwsubu_w_8xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vwsubu.w.nxv8i64.nxv8i32.i64(<vscale x 8 x i64> undef, <vscale x 8 x i64> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vwsubu_w_8xi64(__epi_8xi64 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vwsubu_w_8xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vwsubu_w_8xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i64> @llvm.riscv.vwsubu.w.mask.nxv8i64.nxv8i32.i64(<vscale x 8 x i64> [[ARG_0:%.*]], <vscale x 8 x i64> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i64> [[TMP0]]
//
__epi_8xi64 test_vwsubu_w_8xi64_mask(__epi_8xi64 arg_0, __epi_8xi64 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vwsubu_w_8xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vxor_8xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vxor.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> undef, <vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vxor_8xi8(__epi_8xi8 arg_0, __epi_8xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vxor_8xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vxor_8xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i8> @llvm.riscv.vxor.mask.nxv8i8.nxv8i8.i64(<vscale x 8 x i8> [[ARG_0:%.*]], <vscale x 8 x i8> [[ARG_1:%.*]], <vscale x 8 x i8> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i8> [[TMP0]]
//
__epi_8xi8 test_vxor_8xi8_mask(__epi_8xi8 arg_0, __epi_8xi8 arg_1, __epi_8xi8 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vxor_8xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vxor_4xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vxor.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> undef, <vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vxor_4xi16(__epi_4xi16 arg_0, __epi_4xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vxor_4xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vxor_4xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i16> @llvm.riscv.vxor.mask.nxv4i16.nxv4i16.i64(<vscale x 4 x i16> [[ARG_0:%.*]], <vscale x 4 x i16> [[ARG_1:%.*]], <vscale x 4 x i16> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i16> [[TMP0]]
//
__epi_4xi16 test_vxor_4xi16_mask(__epi_4xi16 arg_0, __epi_4xi16 arg_1, __epi_4xi16 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vxor_4xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vxor_2xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vxor.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> undef, <vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vxor_2xi32(__epi_2xi32 arg_0, __epi_2xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vxor_2xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vxor_2xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i32> @llvm.riscv.vxor.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> [[ARG_0:%.*]], <vscale x 2 x i32> [[ARG_1:%.*]], <vscale x 2 x i32> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i32> [[TMP0]]
//
__epi_2xi32 test_vxor_2xi32_mask(__epi_2xi32 arg_0, __epi_2xi32 arg_1, __epi_2xi32 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vxor_2xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vxor_1xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vxor.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vxor_1xi64(__epi_1xi64 arg_0, __epi_1xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vxor_1xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vxor_1xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 1 x i64> @llvm.riscv.vxor.mask.nxv1i64.nxv1i64.i64(<vscale x 1 x i64> [[ARG_0:%.*]], <vscale x 1 x i64> [[ARG_1:%.*]], <vscale x 1 x i64> [[ARG_2:%.*]], <vscale x 1 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 1 x i64> [[TMP0]]
//
__epi_1xi64 test_vxor_1xi64_mask(__epi_1xi64 arg_0, __epi_1xi64 arg_1, __epi_1xi64 arg_2, __epi_1xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vxor_1xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vxor_16xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vxor.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vxor_16xi8(__epi_16xi8 arg_0, __epi_16xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vxor_16xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vxor_16xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i8> @llvm.riscv.vxor.mask.nxv16i8.nxv16i8.i64(<vscale x 16 x i8> [[ARG_0:%.*]], <vscale x 16 x i8> [[ARG_1:%.*]], <vscale x 16 x i8> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i8> [[TMP0]]
//
__epi_16xi8 test_vxor_16xi8_mask(__epi_16xi8 arg_0, __epi_16xi8 arg_1, __epi_16xi8 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vxor_16xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vxor_8xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vxor.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> undef, <vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vxor_8xi16(__epi_8xi16 arg_0, __epi_8xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vxor_8xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vxor_8xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i16> @llvm.riscv.vxor.mask.nxv8i16.nxv8i16.i64(<vscale x 8 x i16> [[ARG_0:%.*]], <vscale x 8 x i16> [[ARG_1:%.*]], <vscale x 8 x i16> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i16> [[TMP0]]
//
__epi_8xi16 test_vxor_8xi16_mask(__epi_8xi16 arg_0, __epi_8xi16 arg_1, __epi_8xi16 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vxor_8xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vxor_4xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vxor.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> undef, <vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vxor_4xi32(__epi_4xi32 arg_0, __epi_4xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vxor_4xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vxor_4xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i32> @llvm.riscv.vxor.mask.nxv4i32.nxv4i32.i64(<vscale x 4 x i32> [[ARG_0:%.*]], <vscale x 4 x i32> [[ARG_1:%.*]], <vscale x 4 x i32> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i32> [[TMP0]]
//
__epi_4xi32 test_vxor_4xi32_mask(__epi_4xi32 arg_0, __epi_4xi32 arg_1, __epi_4xi32 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vxor_4xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vxor_2xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vxor.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vxor_2xi64(__epi_2xi64 arg_0, __epi_2xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vxor_2xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vxor_2xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 2 x i64> @llvm.riscv.vxor.mask.nxv2i64.nxv2i64.i64(<vscale x 2 x i64> [[ARG_0:%.*]], <vscale x 2 x i64> [[ARG_1:%.*]], <vscale x 2 x i64> [[ARG_2:%.*]], <vscale x 2 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 2 x i64> [[TMP0]]
//
__epi_2xi64 test_vxor_2xi64_mask(__epi_2xi64 arg_0, __epi_2xi64 arg_1, __epi_2xi64 arg_2, __epi_2xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vxor_2xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vxor_32xi8(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vxor.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> undef, <vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vxor_32xi8(__epi_32xi8 arg_0, __epi_32xi8 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vxor_32xi8(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vxor_32xi8_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 32 x i8> @llvm.riscv.vxor.mask.nxv32i8.nxv32i8.i64(<vscale x 32 x i8> [[ARG_0:%.*]], <vscale x 32 x i8> [[ARG_1:%.*]], <vscale x 32 x i8> [[ARG_2:%.*]], <vscale x 32 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 32 x i8> [[TMP0]]
//
__epi_32xi8 test_vxor_32xi8_mask(__epi_32xi8 arg_0, __epi_32xi8 arg_1, __epi_32xi8 arg_2, __epi_32xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vxor_32xi8_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vxor_16xi16(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vxor.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> undef, <vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vxor_16xi16(__epi_16xi16 arg_0, __epi_16xi16 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vxor_16xi16(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vxor_16xi16_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 16 x i16> @llvm.riscv.vxor.mask.nxv16i16.nxv16i16.i64(<vscale x 16 x i16> [[ARG_0:%.*]], <vscale x 16 x i16> [[ARG_1:%.*]], <vscale x 16 x i16> [[ARG_2:%.*]], <vscale x 16 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 16 x i16> [[TMP0]]
//
__epi_16xi16 test_vxor_16xi16_mask(__epi_16xi16 arg_0, __epi_16xi16 arg_1, __epi_16xi16 arg_2, __epi_16xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vxor_16xi16_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vxor_8xi32(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vxor.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> undef, <vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vxor_8xi32(__epi_8xi32 arg_0, __epi_8xi32 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vxor_8xi32(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vxor_8xi32_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 8 x i32> @llvm.riscv.vxor.mask.nxv8i32.nxv8i32.i64(<vscale x 8 x i32> [[ARG_0:%.*]], <vscale x 8 x i32> [[ARG_1:%.*]], <vscale x 8 x i32> [[ARG_2:%.*]], <vscale x 8 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 8 x i32> [[TMP0]]
//
__epi_8xi32 test_vxor_8xi32_mask(__epi_8xi32 arg_0, __epi_8xi32 arg_1, __epi_8xi32 arg_2, __epi_8xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vxor_8xi32_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

// CHECK-O2-LABEL: @test_vxor_4xi64(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vxor.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], i64 [[ARG_2:%.*]])
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vxor_4xi64(__epi_4xi64 arg_0, __epi_4xi64 arg_1, unsigned long int arg_2)
{
    return __builtin_epi_vxor_4xi64(arg_0, arg_1, arg_2);
}

// CHECK-O2-LABEL: @test_vxor_4xi64_mask(
// CHECK-O2-NEXT:  entry:
// CHECK-O2-NEXT:    [[TMP0:%.*]] = tail call <vscale x 4 x i64> @llvm.riscv.vxor.mask.nxv4i64.nxv4i64.i64(<vscale x 4 x i64> [[ARG_0:%.*]], <vscale x 4 x i64> [[ARG_1:%.*]], <vscale x 4 x i64> [[ARG_2:%.*]], <vscale x 4 x i1> [[ARG_3:%.*]], i64 [[ARG_4:%.*]], i64 0)
// CHECK-O2-NEXT:    ret <vscale x 4 x i64> [[TMP0]]
//
__epi_4xi64 test_vxor_4xi64_mask(__epi_4xi64 arg_0, __epi_4xi64 arg_1, __epi_4xi64 arg_2, __epi_4xi1 arg_3, unsigned long int arg_4)
{
    return __builtin_epi_vxor_4xi64_mask(arg_0, arg_1, arg_2, arg_3, arg_4);
}

